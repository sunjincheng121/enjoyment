{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/catalog/index/ma.png","path":"catalog/index/ma.png","modified":0,"renderable":0},{"_id":"source/about/index/ma.png","path":"about/index/ma.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/affix.js","path":"js/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/exturl.js","path":"js/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/js.cookie.js","path":"js/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/scroll-cookie.js","path":"js/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/post-details.js","path":"js/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/scrollspy.js","path":"js/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/LICENSE","path":"lib/algolia-instant-search/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/README.md","path":"lib/algolia-instant-search/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css.map","path":"lib/algolia-instant-search/instantsearch.min.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"source/about/index/avatar.gif","path":"about/index/avatar.gif","modified":0,"renderable":0},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"source/catalog/index/avatar.gif","path":"catalog/index/avatar.gif","modified":0,"renderable":0},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"source/about/index/family.jpg","path":"about/index/family.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js.map","path":"lib/algolia-instant-search/instantsearch.min.js.map","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"200db7743389b47f46ea1eb8d97b018b84a3ee7d","modified":1577961514948},{"_id":"themes/next/.all-contributorsrc","hash":"d139a3b623b2e40bbff5c96ad44adf7dbdbc5be1","modified":1562367983970},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1562367983971},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1562367983971},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1562367983971},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1562367983972},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1562367983971},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1562367983972},{"_id":"themes/next/README.md","hash":"7958c3f70b2133b479ddaf525cc4b6d87a37e04a","modified":1562367983973},{"_id":"themes/next/bower.json","hash":"e6a80b9ed2d618d1cca5781952c67167a7cfac07","modified":1562367983974},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1562367983974},{"_id":"themes/next/_config.yml","hash":"d0bbdbd8a7a34319adffbd82c350b5ee223e96b0","modified":1563031549100},{"_id":"themes/next/gulpfile.coffee","hash":"23bd9587807edc4dbecb5c5a29ab96ade24458b5","modified":1562367983984},{"_id":"themes/next/package.json","hash":"5870ec2b6d2159a57d84c67bad0a535b76398d5a","modified":1562367984020},{"_id":"source/catalog/flink_training.md","hash":"66b609a0824fdb8ee1ebcd48931f23c1bf675efc","modified":1568167742909},{"_id":"source/catalog/index.md","hash":"7084660e81c5cb0c16166485183d2295965edc24","modified":1565333957102},{"_id":"source/_posts/.DS_Store","hash":"1b32e4aa1f12ed4050697494644ff7341205a858","modified":1568010894064},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark.md","hash":"bf3fa6beffa28dbc2e05533210bf50c8cf3601d4","modified":1563023352155},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance.md","hash":"ea639c6baab762fc7713ab1fa1f312cdea8cb998","modified":1563023365679},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL.md","hash":"c0b566de069d8afbe68a9f5746634cc8a4c5f44e","modified":1563023374845},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子.md","hash":"19d4e5350b227822b9dbdc31e73041d0a5096493","modified":1563023379586},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览.md","hash":"df6f1e10453a7884697276cc77ae7997319a7f68","modified":1563023384190},{"_id":"source/_posts/Apache Flink 漫谈系列 - State.md","hash":"43811275ddea2a239fbde58e1907b781318e2d45","modified":1563023388250},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概述.md","hash":"67849bff339d2cb6110e8c38913eb45ddd559ca2","modified":1563024057786},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN.md","hash":"7ebf077237a5aeb0bf09a3856af32d59be7b9673","modified":1563023396043},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN.md","hash":"d73b14344a8cf363b99f6c067c5037338c496c07","modified":1563023405847},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries).md","hash":"48a61a2be3128497fb3c13f8dca65aa34a9d70ac","modified":1563023401207},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述.md","hash":"62629b337ff4761fccc13fd00b71287bd75564b4","modified":1563023410275},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性.md","hash":"8b993161b51f0a093be4e42902a45cfed955fa5b","modified":1563023414570},{"_id":"source/_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林).md","hash":"db68fd5343233110afb5d0ae924571335dbae7b2","modified":1565333756739},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka.md","hash":"78800d24cb542da0ee30a194b4193a68678dd21c","modified":1566966925875},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).md","hash":"644b5977265a954dbed186dba482404fc2eab48f","modified":1568010900819},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物.md","hash":"8c7db18e643a8599fe8e6154c82b2987969e34fa","modified":1564739282465},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建.md","hash":"002b3f363087bfae660a922d050d6d85fd875ab1","modified":1564065390629},{"_id":"source/_posts/Apache-Flink-说道系列-序.md","hash":"0d6bae980c05fbc3dd716a030cedff388395bb05","modified":1563029790861},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业.md","hash":"a5d3bfb675122e39f0b144ed372924673e4019a8","modified":1565527771733},{"_id":"source/_posts/Apache-Flink-漫谈系列-序.md","hash":"12d34f305069f2174235dd4a0b4c50b8ffeb544b","modified":1563025486216},{"_id":"source/about/index.md","hash":"5449f7a337fcc686dd4ebce99ab4d4cdc647418a","modified":1563030328582},{"_id":"source/about/.DS_Store","hash":"062ece847cb1cca837024389b7b5b10bec2fadeb","modified":1562940678258},{"_id":"source/tags/index.md","hash":"2828b922f29c914faf3e3c67adea711adb6df5e9","modified":1562419935658},{"_id":"source/categories/index.md","hash":"dd38fd510c1d6407efeab71c15df7e1acfbc425e","modified":1562419863217},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1562367983976},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"0d2f22ea09dd1ef63c66164e048d8239d2ccb2b8","modified":1562367983975},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1562367983976},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1562367983976},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1562367983975},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"212a36d57495990b5f56e46ca8dce1d76c199660","modified":1562367983976},{"_id":"themes/next/docs/MATH.md","hash":"026d2cff73c22a30ea39c50783557ff4913aceac","modified":1562367983977},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1562367983977},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1562367983977},{"_id":"themes/next/layout/_layout.swig","hash":"74701fcf2303d59400587436ab4c244e04df7ad9","modified":1562367983994},{"_id":"themes/next/layout/archive.swig","hash":"7e8f3a41a68e912f2b2aaba905d314306ccaf794","modified":1562367984018},{"_id":"themes/next/layout/category.swig","hash":"dda0e6b2139decaf5e865d22ec9d45fdb615a703","modified":1562367984018},{"_id":"themes/next/layout/index.swig","hash":"9b4733d037c360e8504645b1d6c6dd17817c9d7b","modified":1562367984019},{"_id":"themes/next/layout/page.swig","hash":"29c64c7031aaf276d3d11cdf2e95025996fd6eed","modified":1562367984019},{"_id":"themes/next/layout/schedule.swig","hash":"3268dd3d90d8b0e142cfa1a2ebb23355baeda148","modified":1562367984019},{"_id":"themes/next/layout/post.swig","hash":"f74929fd792541916eb25c2addfb35431be071ba","modified":1562367984019},{"_id":"themes/next/layout/tag.swig","hash":"a6be69a90924c9d2f4d90fb4867234859bd2c2e9","modified":1562367984020},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1562367983985},{"_id":"themes/next/languages/de.yml","hash":"9e524b2bdfb848504b93a51c5650e76bba5fa9e0","modified":1562367983985},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1562367983985},{"_id":"themes/next/languages/es.yml","hash":"1752429687861b5cedd063c6ebe5dacefbe7e5a7","modified":1562367983986},{"_id":"themes/next/languages/fa.yml","hash":"cd41db832af5e399590b70a5227cfe0b0e98e101","modified":1562367983986},{"_id":"themes/next/languages/fr.yml","hash":"7005c2b42c2c6e82bd7a1be5cc2f443b5fc79105","modified":1562367983987},{"_id":"themes/next/languages/id.yml","hash":"1c4868837f5109f1df863b04fe627352c31d404b","modified":1562367983987},{"_id":"themes/next/languages/ja.yml","hash":"1dc35e436da6214cdb3c2ff44bc4a06d0be5b9a0","modified":1562367983988},{"_id":"themes/next/languages/it.yml","hash":"b30ff77ad8044e3b021a3b09187cd377dc789fd2","modified":1562367983987},{"_id":"themes/next/languages/pt-BR.yml","hash":"08b913a5cf4cc160083069cb4dfb2d66eecd1218","modified":1562367983989},{"_id":"themes/next/languages/ko.yml","hash":"20bfaa7600d35235996c18e5c13dcef89c119626","modified":1562367983988},{"_id":"themes/next/languages/pt.yml","hash":"8ddac820e2c17b484b56c0da8881e142b10e221b","modified":1562367983990},{"_id":"themes/next/languages/nl.yml","hash":"1c44b3cb2f817808607f3bf6ef47f58ce7599995","modified":1562367983989},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1562367983990},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1562367983991},{"_id":"themes/next/languages/uk.yml","hash":"1eb59e581568da9a81d6e20541b4ada5fc1c55c0","modified":1562367983991},{"_id":"themes/next/languages/vi.yml","hash":"ba7aff8f88e03f69a0acf7f1b90ee03e077ee88e","modified":1562367983991},{"_id":"themes/next/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1562367983992},{"_id":"themes/next/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1562367983992},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1562367984022},{"_id":"themes/next/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1562367983992},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1562367984023},{"_id":"themes/next/source/.DS_Store","hash":"495dc0d217dbc3e7cb0c015a4aa9af3e7011d37f","modified":1562975524840},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1562367984110},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1562367984110},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1562367984110},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984059},{"_id":"source/catalog/index/ma.png","hash":"f364a2208039839e0c2a849926260079733b5088","modified":1563030317926},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/66FB2539-A89C-4AE8-8639-F62F54B18E78.png","hash":"422440dad810d9a63aa498f87a733a5db1fcd358","modified":1562499793086},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/1ED29F10-86D2-40CD-B9FD-ADF32D4247CD.png","hash":"a459a170efac47470a831c44de48d42a3a3e9643","modified":1562499614962},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png","hash":"722025a1d8bb70171acf05ae2c69075b47a55f19","modified":1562499614964},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png","hash":"f0db852276a4d1832163401d642459409fe74b88","modified":1562499614971},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/37259B89-DD8F-4349-A378-000AAA886479.png","hash":"41e4acc6ba1eaf6713ef2038efeae146ea5984ad","modified":1562499614975},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/42B8EBC9-CD5B-498C-B191-F4241C91483F.png","hash":"aec378795aa031d9ce5a0002005e4797e3525973","modified":1562499614967},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/52B538BB-F615-4B04-AAC5-52293EAFC919.png","hash":"f2cbcd491ad5f1ac3e5a747d14cc46d50e4e9c16","modified":1562499614976},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/5DA82C35-6E03-43AD-B695-35C85D85D641.png","hash":"2cf04208a2eadb4908a9917905af4e2ad5540234","modified":1562499614967},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png","hash":"885f85652539c19459ebd00ef1795d960cfa3ba2","modified":1562499614974},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/90BC2538-6A12-4F5F-AE42-B87BA5C12871.png","hash":"9ecf15f2693f9d6ebb65fecb2b5c03d2bc89b6d9","modified":1562499614964},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png","hash":"fbd5400bace1950b04ecd8ff05cc9b8e1beb4635","modified":1562499614968},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png","hash":"ce4479f7ba03ee48f7bc74829937a498e2c7e370","modified":1562499614973},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/E3709EBB-89AE-44EA-98C7-06A2794597D9.png","hash":"987c8a7eee94ff32bca6092c2c34ff25ba02837c","modified":1562499614969},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png","hash":"99deb742c20878ada86390e0752b998c451948ff","modified":1562499614965},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png","hash":"5c6bfd2c8237ef58ed7de75cc00c260d5e71e42e","modified":1562499614971},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/ED960882-39B0-40FD-9400-60D0D66F366E.png","hash":"41e6af2798eebc0bc4a7f39b2b9f64bfbcebbd56","modified":1562499614972},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/49829548-F09C-47E6-A245-02B7A92749CA.png","hash":"68f84ce645efab98206ec9dfa56904d899445086","modified":1562499597793},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png","hash":"c8588fa40ab988e7d07ef473d008685e8e4b59fa","modified":1562499597797},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png","hash":"e5cf946ba5037f01048361ae480f11467fb2d7c9","modified":1562499597800},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/05A64357-27DF-4714-BDDB-B390B7A0814A.png","hash":"a7606f813b7260c959f38dbcec80b831110da631","modified":1562501077355},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/1757B41A-113E-4115-9476-EB9972D296AF.png","hash":"e7f206129f18643ab105b8ae3764ed26122691fe","modified":1562501077354},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/1D8D2D78-5454-4919-A7C3-48B2338BDC78.png","hash":"3c4864c0c525d94d6a33d6ca7e5e51bf2bb8c44f","modified":1562501077350},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/391D7615-63C5-4A18-85BA-540730B6B5E2.png","hash":"fadf843d2dcfd5dc076c2dd8a0c30f03bb1bd54b","modified":1562501077362},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/480B5CC6-5363-40FF-92EE-2B242494EBD7.jpg","hash":"abcc17bc818853a56841749ddcea1ca559c53779","modified":1562501077363},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png","hash":"8816ce567b04750a7cf055ceca00e7b46de16163","modified":1562501077358},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/C1407477-934D-40DA-B0F0-78C48B96BEA7.png","hash":"4a07632a6426bcff13ac2556a76b4002790c5f1c","modified":1562501077352},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/E3CA71CA-8B44-4802-8165-53684A152503.png","hash":"57606cf9816000e138e17ef7a3694712cac67d8a","modified":1562501077356},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/7230573C-621C-4792-BB1C-7EB96A61DBEE.png","hash":"b88210b23cbb5dd3eae2962d0e0c00cd6c966c2d","modified":1562499809723},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/3402EE52-F2D5-452E-A0A5-66F0EC14C878.png","hash":"3c4864c0c525d94d6a33d6ca7e5e51bf2bb8c44f","modified":1562499556730},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/15657C5E-DD6E-4339-824E-6B17C30886C9.png","hash":"84e8fadfa6a5064196fe60a4b0707694526cad1c","modified":1562499556732},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png","hash":"e7f206129f18643ab105b8ae3764ed26122691fe","modified":1562499556735},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/AE6DCCF9-32A4-4E7A-B7C5-334983354240.png","hash":"8816ce567b04750a7cf055ceca00e7b46de16163","modified":1562499556733},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png","hash":"a7606f813b7260c959f38dbcec80b831110da631","modified":1562499556738},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png","hash":"119fb94b395160dd193ea57f31a22b58ccd20477","modified":1562499556740},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png","hash":"4e48aacad99d769b5dde7b52d7f54dd7b5737ad3","modified":1562499556734},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/EEED7340-FE9D-4791-961C-9996D35E148F.png","hash":"fadf843d2dcfd5dc076c2dd8a0c30f03bb1bd54b","modified":1562499556730},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/D56FE35E-0212-444C-8B0F-C650F1300C3C.png","hash":"57606cf9816000e138e17ef7a3694712cac67d8a","modified":1562499556727},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/172A63B1-C30D-43EA-B04E-B9BE8706838B.png","hash":"4c54f206c4b3bc185df325c3d2955eb344728bd3","modified":1562499522764},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png","hash":"62829dea7d6aa8cc095a06decff5dd8249cb2edb","modified":1562499522770},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/6EAC4329-C402-4497-BAE2-7E7B778E613B.png","hash":"815d369e492a2f0bd8dc2d858e12fd2a94384f3c","modified":1562499522767},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png","hash":"20d8e316bdc0ed8ea8fc8e6c36ffc7eb8abcdbb4","modified":1562499522765},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png","hash":"dcf2739b8992a3f712d2085454cb77253c7872df","modified":1562499660738},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/4134FE05-67E4-4625-824D-BBD7B444BFE6.png","hash":"4f1f1dcda1d5c211ecf42ed6c4f89d7a8f90afdd","modified":1562499660729},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/43A59ED3-C317-4B5D-80F3-CC332933B93B.png","hash":"d91df315db4c450008ba74a2fd243756325c573a","modified":1562499660727},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png","hash":"7605d50eb886fda5d8ddded497d302028a39c4f3","modified":1562499660733},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B2EEEC59-21F9-41E1-A306-8338A7A72A64.png","hash":"5494069bbc3055387d8ad9c4d2681e6e6e2580b7","modified":1562499660726},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BE66E18F-DE01-42EC-9336-5F6172B22636.png","hash":"826f1ac5abcb1d880517f645508bc0c8b472ec3b","modified":1562499660735},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/013D77B3-0D57-433F-B8D7-476D8CD3384E.png","hash":"456580283196ba90d48e64c4aee323855c86a7b0","modified":1562415018928},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png","hash":"5f02591bca481a4c05b75c153e373e1c9bf30e32","modified":1562415018927},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/27BED9E7-C89F-4109-BB21-6917378E1634.png","hash":"cda6b1cf2f2481dd91e57c00349b70fe41ef5d3e","modified":1562415018932},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/428A6C6A-8070-4D19-A380-122AAF34EDFF.png","hash":"80ce1bf42788449a16236512729c54eb53ad81bf","modified":1562415018928},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/7DD374B0-1901-424C-8683-2AE890B8A5FA.png","hash":"7b9eee30a2159f3f55c1b5b6dc2f7e32672aedac","modified":1562415018935},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/83147F8B-064E-43AB-ACD5-B6243689462B.png","hash":"e81d36ff77fc0f7cb99e8c86d5246ca54312bf96","modified":1562415018921},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/BF1A3338-5B92-4898-BC81-A4A6626D4208.png","hash":"8a2b4436328b546c2a4095ba2a51396360da4a52","modified":1562415018924},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png","hash":"2c334a6998c3b7e60810c32034401a0ee1a582f4","modified":1562415018922},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180919_180.png","hash":"7fd5741a782cf854037dbd77a78d59f3fd21a03b","modified":1562415018920},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180919_181.png","hash":"e23991f14a01bc1d2318720ce8424943f4393a9e","modified":1562415018933},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_4.png","hash":"30542886f79ccb55f3cc6c2e4a1d55c92a83b221","modified":1562415018934},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_7.png","hash":"82eebd656bb1382a9976cff2c327ce9291742b98","modified":1562415018929},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_6.png","hash":"ac401d72f5b2a237902101dffe53ec7f3be50e73","modified":1562415018923},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_18.png","hash":"e881eddb514388cde06963175e2153156a614991","modified":1562415018934},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_22.png","hash":"203eb73f5b82c64259ce594e17e00cc65340e4ff","modified":1562415018930},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/2052C4F1-2EE4-41D1-8A8D-165A40827462.png","hash":"53b4b82a7d5fb16d10eaa8bdaa39fc8390ef1c47","modified":1562499779816},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/380A4B6E-0F4F-4031-8ED3-286DAA7B0343.jpg","hash":"26bbb0c826c8b690a92733da261bfe9b7517fdfb","modified":1562499779815},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png","hash":"8b19f8e777979e45293eecfc0e9695fe126895c0","modified":1562499779817},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/BC393F56-E637-471B-B5B4-8A73B9A1D30A.jpg","hash":"72678f43a3c01a37de4b173c4e1a2741657658bc","modified":1567994037520},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/BDDAD217-10C4-4896-91D9-BAC1B8B1EA20.jpg","hash":"2e4411e71fc515fdeb64be9a89a85fbd9b995bae","modified":1567994037519},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/02C1D4A6-7290-408E-832A-1FBDCDED75E5.png","hash":"ce3fdee9b12e1fdd2f0b38cedf7aa5c72fd22abf","modified":1566961501161},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/.DS_Store","hash":"0e6b163b07ea524cf17bb08129c384bbdb5882e2","modified":1564005136220},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/070B3285-C790-4D6F-BB9B-6981756255CA.jpg","hash":"5581c2c9ba8656904bda6e1a9d2bb332b5c0ed49","modified":1563721471126},{"_id":"source/about/index/ma.png","hash":"f364a2208039839e0c2a849926260079733b5088","modified":1563030157111},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1562367983978},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1562367983978},{"_id":"themes/next/docs/ru/README.md","hash":"4d7ef717d0b57288e606996ee56c20ffd59d5a99","modified":1562367983979},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1562367983979},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"caa624092175d44e3d3a8c6ca23922718da2354c","modified":1562367983980},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"b218e30df4126b6adc87684775ac4c86ea7f7958","modified":1562367983980},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"650fcb9135b6f09d48e866c19e0dbccd831367f1","modified":1562367983981},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1562367983982},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1562367983981},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"115ffbde2b3ce01ef1f8c2b3833e6f6794650132","modified":1562367983982},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"83feca62190abcca0332915ffe0eefe582573085","modified":1562367983983},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"5da70d7fa0c988a66a469b9795d33d471a4a4433","modified":1562367983984},{"_id":"themes/next/docs/zh-CN/README.md","hash":"cdd7a8bdcf4a83ff4c74ee6c95c6bcc0b8c1831c","modified":1562367983984},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1562367983993},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1562367983993},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1562367983993},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"89b0a0e64637bf5b0cfea0a23642df3d95eedfa4","modified":1562367983995},{"_id":"themes/next/layout/_macro/post.swig","hash":"7920866b88d2c6c2ad0e2e7201e58d37fb0d7cff","modified":1562367983996},{"_id":"themes/next/layout/_partials/comments.swig","hash":"784356dd77fe96ea1bc4cb0008e2b40de71bf2f0","modified":1562367983996},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"480d93619479dcfcbec6906803bb38b2dfbeae53","modified":1562367983996},{"_id":"themes/next/layout/_partials/footer.swig","hash":"589f545333e21a8c7823bce89ab45cf1eb7db6e2","modified":1562367983997},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"6357537ac0bb114aed4d61bafb39e6690a413697","modified":1562367983997},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1562367984000},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"06dac109504812b63766a80ede9ddacbd42d227d","modified":1562367984000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"50be1762f60222379a8bef5e42ab1a0f3872b7ff","modified":1562367984004},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"012e3ece672cc3b13d5e032139f328d3426d7d65","modified":1562367984004},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"61ae10d41f67ece004a025077fdb28724af05090","modified":1562367984004},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"edaff4766e0c05fd5c889d9dd32884d376bef9d9","modified":1562367984005},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"ccd13d73429ef91ef5e8b7d9fa43c8188facdf41","modified":1562367984006},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9cd491b8ff2dc9d6976cd9e89c4e56678e3bcefa","modified":1562367984006},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"4ccf2abbfd070874265b0436a3eff21f7c998dfb","modified":1562367984011},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"aa0893cddc803bd3fd34ab78d7d003bd86be86b6","modified":1562367984011},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"10b61a8bac671e375916a4d234c120117098a78f","modified":1562367984011},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a7126355227236f9433615edfd89e86fd51ed676","modified":1562367984013},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"d6e6ddda836bd9e2e8d9767a910c7d3280080e81","modified":1562367984015},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"2c4a66be4677d3e4dec3f169ac8a769098dad1fe","modified":1562367984015},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"c28f9dc96ab735daeb7f599f86470aa5a83c03cf","modified":1562367984015},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"810a9b2a6059f46c4a2ddb178f1eaa4c5e23750b","modified":1562367984015},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"7757bd285732e857996b99af9d917953589fac5e","modified":1562367984016},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"cbe40cb67dad15ade967b0f396c1a95b6871f76a","modified":1562367984016},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"2398e5cd0cb466953b6e7a42c2b2caddebf3c348","modified":1562367984016},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"912368c41de675f458b267a49a99ae3e7e420ebb","modified":1562367984018},{"_id":"themes/next/scripts/filters/exturl.js","hash":"79ad823ca803cb00e0bfc648aa6c9d59711e0519","modified":1562367984021},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"799a042bbf497a4c7a2981aa2014ff28fa1bb382","modified":1562367984022},{"_id":"themes/next/scripts/helpers/engine.js","hash":"60eb1554456d9d0e5afc4a2d16f1580a0aa02da8","modified":1562367984021},{"_id":"themes/next/scripts/tags/button.js","hash":"f3b4f7ae7e58072bbf410d950a99a0b53cbc866d","modified":1562367984023},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1562367984023},{"_id":"themes/next/scripts/tags/exturl.js","hash":"d605918cf819887e9555212dbe12da97fd887a0b","modified":1562367984024},{"_id":"themes/next/scripts/tags/full-image.js","hash":"fcb41c1c81560ed49dc4024654388a28ee7d32b0","modified":1562367984024},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"598220fa92ff3540dcab74f633ba41523daa8364","modified":1562367984024},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1562367984025},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"5db59d56f4f4082382bf1c16722e6c383892b0c5","modified":1562367984024},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1562367984025},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1562367984025},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1562367984025},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1562367984026},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1562367984026},{"_id":"themes/next/source/css/main.styl","hash":"e010ec8ac73268a0f137204c89e0080ab8d59b3d","modified":1562367984059},{"_id":"themes/next/source/images/.DS_Store","hash":"8938627989d12c9ebf9f655a2c9f64bf6de16205","modified":1562982201816},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1562367984060},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1562367984060},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1562367984062},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1562367984062},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1562367984063},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1562367984063},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1562367984064},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1562367984063},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1562367984064},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562367984068},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562367984069},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1562367984070},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1562367984070},{"_id":"themes/next/source/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1562367984070},{"_id":"themes/next/source/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1562367984071},{"_id":"themes/next/source/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1562367984071},{"_id":"themes/next/source/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1562367984072},{"_id":"themes/next/source/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1562367984072},{"_id":"themes/next/source/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1562367984072},{"_id":"themes/next/source/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1562367984073},{"_id":"themes/next/source/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1562367984075},{"_id":"themes/next/source/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1562367984073},{"_id":"themes/next/source/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1562367984075},{"_id":"themes/next/source/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1562367984075},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png","hash":"48e8f8fdae48781f5a138b4d08e3f5fb134a6640","modified":1562499834367},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/BC6D8C93-A068-4799-B2CC-6A75AD43549C.png","hash":"b3d5425f459fbbcb605d21872a0b62df240e3aa7","modified":1562499834364},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/E5D7CBF0-A452-4907-8FCF-F97896C6993A.png","hash":"9fb7c4506cca256bdb105af7d476421db4fd6fd1","modified":1562499834366},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png","hash":"d1613ff57bfda353997ca41c37e4cf6d67a200e0","modified":1562499614963},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png","hash":"aad5fff4b5fa23e4485b98c948c3c69aff3077cc","modified":1562499597798},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/3AE44275-DFE0-45D4-8259-EB03928B784B.png","hash":"06a5c9fb6c67066c681c3de70c80acefb048a3b6","modified":1562499597797},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png","hash":"bb9da0388b6f6a4c491c28c56c02f86570515030","modified":1562499597796},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/D9E93F8F-D39A-40BD-A78D-8E3404225B86.png","hash":"4cd711f9690a1a014f4e6a57546eddf6faca9409","modified":1562499597799},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/16080D31-EB87-48F9-A32E-A992CC6DCEAC.png","hash":"2798e3f7300e3cac5fa2702342fa5eacd97cf16d","modified":1562501077349},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/0A4F6AF5-68D5-4E26-BFF3-8E0AEA2B829C.png","hash":"a173fd5f6c067f0d2b68ad4267546aaacda20e29","modified":1562501077358},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/9F6085AD-4792-4A30-9503-82DF82540304.png","hash":"84e8fadfa6a5064196fe60a4b0707694526cad1c","modified":1562501077348},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/E18BAB16-4094-48B3-92E5-D45EA18A62C1.png","hash":"06a7e30201653a30fcf30397efd3110e33a32868","modified":1562501077347},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png","hash":"c0cc018c310d7d468a8d6e5e5a31c9720baa591e","modified":1562499556739},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png","hash":"4e09d3d73fb8ad4426cf4cdaa6378272974d8cb5","modified":1562499522759},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png","hash":"919c0e48017decd372634531c75f19a6c2407aa7","modified":1562499522770},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/75AC8012-59C7-4105-AE66-4453384E762B.png","hash":"8557b21b8ab359e9b3bec25b0a8ddb385898ab33","modified":1562499522758},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/CFC8CEDA-8049-459F-B331-15CE8454330F.png","hash":"ea6cbddcc28d9e6d60d475f239bf121baaefde9a","modified":1562499522763},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/D05B8A19-6D97-4809-8723-B4B95FE075A7.png","hash":"7db8a9a6d95ff3fe5c03b9b47b3b678a2ae2d5cc","modified":1562499522769},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/FAB41F2D-25A8-4166-9583-D857FE9A19F1.png","hash":"5e7f9441c99e5b8aed343783650d7fe71c8e58e2","modified":1562499522765},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/b26733d7d193bbea8d594f098bcd98cbea926a88.png","hash":"b26733d7d193bbea8d594f098bcd98cbea926a88","modified":1562502270518},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png","hash":"e385a6908814bd16785389b0d1baef739978a217","modified":1562499660729},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/1401BEB3-37C3-4203-B48E-AABF6B08DB65.png","hash":"f1e5f4c7dfc8b24c645b284882a4ecfe15494c1b","modified":1562499660728},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/17FB1F6A-C813-43D4-AD6D-31FC03B84533.png","hash":"8fdfb8af4f20f328fbde2a3ea11d0b6cd8a132e4","modified":1562499660739},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/78D31D91-3693-44C8-A7E4-71D6798A78AE.png","hash":"078c8f5194123190c691bae18ca078cc2a9aa243","modified":1562499660725},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png","hash":"224da46b95877492c084de417323ac3230c077eb","modified":1562499660730},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/E095C802-8CCF-4922-837C-6AAE54A335D8.png","hash":"b2c51bc967c7b1138aab03c8602cea38f20b57fb","modified":1562499660724},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/40302667-3CD6-4EF5-9904-1834343B3E4D.png","hash":"6838e8d829291748910f773a4c631dc5aab52da6","modified":1562415018917},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/800B7916-C9BF-4422-ADC9-2C5D90E30339.png","hash":"5492be2ea6703470b7456f3476c89a28a09abf02","modified":1562415018926},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/8711EDAA-E214-46DA-AB5A-BB63FCC55950.png","hash":"8e56d1c25ebef8fab416d432f232b609538c09ce","modified":1562415018931},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/4C991819-3CA8-4E8F-B5EF-11B6AA2EB5B8.png","hash":"2ad034149ed1eb83663968762a76b0cc0d500502","modified":1562415018919},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png","hash":"4a3b40c6d2f13ce26140ad4935c46bf25c18079e","modified":1562415018925},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/CF9B03E9-DD03-48DC-B0F1-2C0CEC1CEAFA.png","hash":"d894411ba2916384c53c7c4b02e1239560b1eef4","modified":1562415018927},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/B995477B-C4CC-4A04-BE20-09C9100725E0.png","hash":"8c5eba6484715ef7c2ff07c6e743535443272aaf","modified":1562415018916},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png","hash":"7d6d2ba66e08e05e4e666586413258bde910e35d","modified":1562415018932},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_17.png","hash":"7b6db960ee731ba210013f24fdd230d34546a3d8","modified":1562415018929},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/12D4B3D0-D367-40B3-B729-95095807CA46.png","hash":"1783c54b70c76418125e1a90f2e060012db7a810","modified":1562499779814},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/41690DE4-8705-4305-BF74-AC47F8C870B3.png","hash":"0a9207570163d8f8ecea5d873df3ecdc0b7b1f68","modified":1562499779818},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png","hash":"ac5a7ff7c9e98d2d18035afa94984475344997d7","modified":1562499779812},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/EF81F039-86CC-4799-8DCF-F3B0144B76EC.png","hash":"17f78da8f562a5abe08fac8325224e1594d07402","modified":1562499779819},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png","hash":"40ba5afa5de4206369ca2ef26d477d247db5db51","modified":1562499779818},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/C3DCF0D9-7C1B-4DB7-88E0-657A6B8D81EC.jpg","hash":"869cc4a61ff52743f4c189a8230dc6666af71f7c","modified":1563721471114},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png","hash":"97bd06881ec08cd8b6d07cabab366d38eff1762b","modified":1563721471111},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984049},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984049},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984049},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984058},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562367984059},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/77C24B94-7F26-4F72-BCA3-D0F20A791772.png","hash":"097aee85ad38424503df4068335e961f909cfa7e","modified":1562499834363},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/40FE5816-DA56-4222-ADDE-1B023EE71BCF.png","hash":"8b6c4ce0046a9bf5f259a6880cd7ca0ffcbce3dc","modified":1562499793083},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png","hash":"8eea8cf94b0d107e0acdd955fd5f2c703bd6bf2a","modified":1562499793086},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/051876D7-5A89-4134-8ED0-E6939FC3F412.png","hash":"4e33056778983594b80e516bf9fbc81b474bcdb9","modified":1562501077356},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/26FE2626-FDA4-4474-9B25-5505FEF5FB37.png","hash":"a3780f1b056661e9fbc6f51adc0c093e0d6cf063","modified":1562501077362},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png","hash":"c0a44cbd0d04602dca385e59950d6bb43f3ee643","modified":1562499809728},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png","hash":"7463a756657927ccb276c37f71a766ff8d9e688a","modified":1562499809726},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/FC821998-4A3E-4F10-9351-C7B3060E7349.png","hash":"4e33056778983594b80e516bf9fbc81b474bcdb9","modified":1562499556725},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png","hash":"0e48261b89e227d08d90f03c8bba61f794c224bb","modified":1562499522758},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/9741456E-BDBB-4609-A014-85FEF5D351AB.png","hash":"90705c543ad16c15993fa63e36eed16cd3f9b872","modified":1562499522771},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/146851FF-D705-4B2D-8C3B-77528BB8DAB5.png","hash":"ec46c93d4822dc179142adad8b1ad6952a41e2c8","modified":1562499660740},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/266A687D-8E31-4266-B52F-200B75244DDB.png","hash":"c24864782598385f7765c5b5a54ff48db85c6e09","modified":1562499660736},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png","hash":"b948197e816e644eb963cf625052895642377cb4","modified":1562499660726},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png","hash":"81649c672691c748391829fb667b4a90e3bde665","modified":1562499660736},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/1AF4C656-6ABA-4F69-A771-E6EC623B382D.png","hash":"a68f42c79e6bdd075cb6ba2d99a113c0f63ebc91","modified":1563721471123},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/9B952F03-53BF-43DF-8051-A2FAC84A600C.png","hash":"50eb7149c164cfbb64395ba851008e6615652dec","modified":1563721471117},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/0B58FCBF-4D98-41F9-98A3-13B31F939916.png","hash":"951ca0e7d7ea863d640fd6682b6b625a9622e025","modified":1564060724541},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png","hash":"9864481316d2ff8cda06ee4bbfa5b26a590e6688","modified":1564060724540},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/8308BB64-2AB2-480C-9B2E-4AADD515F183.png","hash":"43d2e64680e2b81b7eaa8376173e1ff30dfe1d4e","modified":1564060724556},{"_id":"source/_posts/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1563977934012},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1562367983994},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"9257da95bd032bb3bd1da670e302fd2c7d5610b6","modified":1562367983995},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"02bb5748e8540b024e7f4008a9e640890b45280f","modified":1562367983997},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"fc6bafc8c633afadc538c5afa5620ea2a1cdcb84","modified":1562367983997},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"648bf7eda66629592cb915c4004534b3913cbc22","modified":1562367983998},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"f537846ace6a1afdacf122848dd01a32ceb66006","modified":1562367983998},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1562367983998},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"5adc60100e129c1d0307bdcaa0c7b8e8375a6ea4","modified":1562367983999},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"71af31fea5913fd30c233e555ef13cf2c9768f72","modified":1562367983999},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"0fa4fadb39467b01cede49f21b22e86b1a2da805","modified":1562367983999},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"2940df694fff28e8bf71b6546b4162f1e38227db","modified":1562367984000},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"3615db591dd910fb9fa96542734c7ec0ef05019c","modified":1562367984000},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"eea95b785c9c36d28e1839619793f66e89773bee","modified":1562367984001},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"ef11b5be5bfb2f0affe82cf521c002b37fef9819","modified":1562367984002},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"d44f025eb93c99ddf90202d8293ccf80689a00c7","modified":1562367984001},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"d9fe715fee716f78c7976c4e8838da71439ee0e0","modified":1562367984002},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"7d1693416a5dc098f4723a53da2e2d1fc2d6e075","modified":1562367984002},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1562367984003},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a5587bd1f60d35e58618576cec45e662aa44ea1f","modified":1562367984003},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"15b542f5b06b7532234af367340b9ed9fcebb0ac","modified":1562367984003},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"b45e934d24d76ec6b6a790e92bdb3d56186b0e2a","modified":1562367984004},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"6f181cc188ecbe5e607fd989756e470d4cb9765d","modified":1562367984003},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ffc8e8836714ea79abeb77b75859634615652877","modified":1562367984005},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"5b05f165547391bf231e52f56f3d925efc09bc44","modified":1562367984005},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"108b157fbd1ac3baaf19ae87234fa8728ab79556","modified":1562367984005},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0097e45e7b671f8006b8b2d3c4f95cacc76a983c","modified":1562367984006},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"e42604fbb17648484e5f12afe230d826de089388","modified":1562367984006},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"a5723950c343d220270bfd27bd30050eda6c3fb3","modified":1562367984007},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"798d67e4a736613ab899eabe6529091bbcda7850","modified":1562367984007},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"591b2ccd9713ccb922b9fcf5e278b6de9c5ec30b","modified":1562367984007},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"8eadb929c9e50e58502ccad2dc2657746f8c592a","modified":1562367984007},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"08cd47ef8572121b7811342d3c9a84a338a18191","modified":1562367984008},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"3d01fa6edc0ad73f81813613f2e8a610777f1852","modified":1562367984008},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"9fa1ca7059243197d8fbbd35108c36629a254570","modified":1562367984009},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"fae69a0e1a1d42f7bb44e594a29857d94594698b","modified":1562367984008},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"4a966b7ffe2d80ff1b3dd0fd14b355766dc5c70f","modified":1562367984009},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"438c6f5e6665d72f4ea7ee206011d669246f6102","modified":1562367984009},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"a09d2af2a8470555eeb265b0eb14dc678079e870","modified":1562367984009},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"f240a50cd9b627620d9a374a29cf95f0c5e99d7c","modified":1562367984010},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"92e04a2b9e0c3df594bc22235d1894e5ad458dfc","modified":1562367984010},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"0dd5b315d1da55dbfc10f51a1f8952f72eba2720","modified":1562367984010},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"3533167c4295637b91d90f3bae7c651cd128bb6e","modified":1562367984012},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1a00b1b78c429721d6477c2d8f6f68f005285cc8","modified":1562367984012},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"074a995cd630f56fc4a3135173515c86f2cb34b6","modified":1562367984012},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"53a59cba82cad49f15a90e1a18007aaac525bddd","modified":1562367984012},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"e8f91c571ceb4b80aafebc4d36b89fb41b1ae040","modified":1562367984012},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"40bab84a4a7a368fa31f0f8ce49af6ec3e5983c9","modified":1562367984013},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"15a4d60d3ecc59db2f23629477f8e7b8324981ed","modified":1562367984013},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a7e304b05a44279d3e4f611908d7faef9dc14d7c","modified":1562367984014},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"c2cb2f384bc30d31cdccf9794a729c03e687b45c","modified":1562367984014},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"601774d8672577aefbcefac82c94b01f0338da31","modified":1562367984014},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"0a13dfd2de52a96901039098c6fc7b515edfc50b","modified":1562367984017},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"ea94aa85034c6d1b6bb865aecea55c73f8a14501","modified":1562367984017},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b3eaab6a269aa3fcbafe24fd06f0c9206dc12716","modified":1562367984017},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1562367984048},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1562367984049},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2e8fb29aa92325df39054b5450757858c6cebc41","modified":1562367984049},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"2036bbb73afd43251982ce824f06c6e88d35a2ef","modified":1562367984049},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"a8aa41625b94cf17a7f473ed10dcbe683b1db705","modified":1562367984058},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1562367984058},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"fc15e277d1504532a09b7b1bd31f900ad95ec4b8","modified":1562367984058},{"_id":"themes/next/source/css/_variables/base.styl","hash":"640f25a63770af5566ccc9cec79c40a4f1c0b29e","modified":1562367984059},{"_id":"themes/next/source/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1562367984074},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1562367984074},{"_id":"themes/next/source/lib/algolia-instant-search/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1562368007019},{"_id":"themes/next/source/lib/algolia-instant-search/README.md","hash":"9fa5175cdb7d3d939fe7174b6d68608ca996c174","modified":1562368007019},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"6e01a39d7f6d58a0895957361b0a942543c18332","modified":1562368007019},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css.map","hash":"d055d06395af598682873d1b458166dc6f513072","modified":1562368007020},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1562367984097},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1562367984096},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1562367984097},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1562367984098},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1562367984108},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/237CED69-52AB-4206-BF02-F99183C7186E.png","hash":"fa5c1113edf5ef33bc1d0f49d88a1be122d0a048","modified":1562499834361},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1562367984108},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1562367984109},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png","hash":"5a4539a5ec58fee21caa443c3ae73559f123e694","modified":1562501077351},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/95F7D755-E087-4808-83F7-2A20CA5C685F.png","hash":"28d42e954c9bf084ed505de27c75f86f7b9585ce","modified":1562501077353},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png","hash":"674cb9c4570acf3de5a7fb8515a2e3592511f5c9","modified":1562499809727},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png","hash":"aaa8762b1667b978fa0805092cdee182c95fa066","modified":1562499556733},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/79DC098C-1171-4C0E-A735-C4F63B6010FD.png","hash":"28d42e954c9bf084ed505de27c75f86f7b9585ce","modified":1562499556729},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/C2A514D5-61F1-4464-95ED-17EB2A030EC2.png","hash":"a3780f1b056661e9fbc6f51adc0c093e0d6cf063","modified":1562499556731},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/D1197B0B-2DCF-434E-A7BB-A250187079F0.png","hash":"5a4539a5ec58fee21caa443c3ae73559f123e694","modified":1562499556736},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A94E2FAD-F04A-4691-8C34-3C697FF81476.png","hash":"fbc175ca5e3a99fb091191f8ac0ce1cd04d88aa7","modified":1562499522762},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/DE913257-7FD0-4983-A511-F8FE11584F9C.png","hash":"6bf496d50b267c4efd28539742879ea5c9bafeb2","modified":1562499522768},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/452ada49a233d68c6e614e05e61ba158aa180f83.png","hash":"452ada49a233d68c6e614e05e61ba158aa180f83","modified":1562502270516},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/D9E4DDB2-3FF6-471E-9AEC-6754FB9A88F2.png","hash":"8e04b36ace78a49d430c75897c54e2cef244f692","modified":1562499779813},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png","hash":"ff71695517601a0c53347479e957251048441787","modified":1563721471115},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/9C73FBBB-80AD-45DA-91E5-21436A7811C2.png","hash":"441f6008d4b432aeb800f972b2198eb32392c26d","modified":1565526745147},{"_id":"source/about/index/avatar.gif","hash":"c270a40edcfc5b99606e76539ba3cec353e203d0","modified":1562978566162},{"_id":"themes/next/source/images/avatar.gif","hash":"c270a40edcfc5b99606e76539ba3cec353e203d0","modified":1562975536774},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"c270a40edcfc5b99606e76539ba3cec353e203d0","modified":1562975595283},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"c270a40edcfc5b99606e76539ba3cec353e203d0","modified":1562975584669},{"_id":"themes/next/source/images/logo.svg","hash":"87211871659931265855662ce3f34e4b359015b0","modified":1562368007018},{"_id":"source/catalog/index/avatar.gif","hash":"c270a40edcfc5b99606e76539ba3cec353e203d0","modified":1562981102260},{"_id":"themes/next/source/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1562367984105},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/C15AA404-D132-4D6C-BA35-B103CA53AE61.png","hash":"43707a95baf023daf83e4947f22839476e6fa0c0","modified":1562499834365},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/678508E9-D79F-438A-B236-17A942D31F73.png","hash":"a0e73402f61f0c29e798bbfdbf21c76c97d019cb","modified":1562499793085},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/A3A08123-0490-41BA-9E07-95B4557A19F5.png","hash":"fbf8f6f47bc4e0b6c927835733aba77685f9495a","modified":1562499614970},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/096879FE-B4A4-4802-8749-9784162D19D0.png","hash":"4c0604fc67a252b35534e764be24b322a6a724a3","modified":1562499597801},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png","hash":"4c321e1d964c4aad29fe19b0ca0ad638ac681d57","modified":1562499597794},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/70173B48-E4B8-466C-9846-AD68FEB6F9B2.png","hash":"79781c495513b5bd0115cfeb69f9561af693f328","modified":1562499597795},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/0A65AF2C-7482-4582-870C-88309807339F.png","hash":"c18c4aa2a8ce004011aa7eb517c51caf2123968a","modified":1562499556728},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png","hash":"53329127eb2f4fa23a77f24281db15eb5a294ddf","modified":1562499556726},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png","hash":"c65201cc34dd4f2751e62414e5917e382869ef77","modified":1562502270515},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png","hash":"7843c12a4430df6028dcc6d6c33ef3713a8a353b","modified":1562499660734},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/3794F522-3458-4386-A101-9458FD2D8DBF.png","hash":"1ee34f041f8c895da72d7babb3a4d38834668fb4","modified":1562499779820},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1567994129592},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1566961678082},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/6DFCA46F-DEB5-4C5C-8540-655D5A4F2423.png","hash":"f502896f86df10f9e463d10a075357861123d43f","modified":1564060724554},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/77394C3C-D401-4DE6-A005-D4ADFFBFB892.png","hash":"07be5fa4567201ba2cb5b6e879327e416f4ba59b","modified":1564060724553},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/99DE5B9F-777D-4BDC-BA36-6E39E2B82F79.png","hash":"218cad21d782340536c39ecf5510399abc271038","modified":1564060724549},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/E371FA28-BAAE-448F-A930-E795CBF09B0E.png","hash":"edbedb7fd06ec41f5637a58b106c8133d38c2e9d","modified":1564060724555},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1563977934012},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/6F22FA6A-2892-4137-BA10-6D643C60DBEE.png","hash":"adaacb4356661f490fcdea42a9f75a8f8c266e78","modified":1565526745145},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1565526922849},{"_id":"source/about/index/family.jpg","hash":"3afb29990af461cbe2fc33aee227b46ae44e4bc5","modified":1562934448842},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"b98c65006e2546fbf3870c16fbbcbc009dbaab15","modified":1562367984027},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"c8b3225396cb444d8baeb94bac78e5216b992a81","modified":1562367984027},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1562367984027},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"fe5ff961b86004a306778c7d33a85b32e5e00e48","modified":1562367984026},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"9d71f34fa13a41b8c8cd2fbdf3fdea608385277c","modified":1562367984028},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"ce826aedf42b9eca424a044452f5d193866726a6","modified":1562367984033},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"ce2aae8f3ed8ceac3a2417e0481044cf69c788aa","modified":1562367984038},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"d7b8bcf2a6031296c84bb4f4ecfb037af01d2d82","modified":1562367984038},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7e51ea64611ab5d678c112b4688d4db4fd2737e2","modified":1562367984047},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"6d900b4159eeb869196a619602578bf4d83a117b","modified":1562367984047},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"8e0740a9ad349ce5555122325da872923135a698","modified":1562367984047},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9a190ef2f49bdbf69604b48ad1dc7197895ee9b6","modified":1562367984047},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"7ffde343bdf10add1f052f3c4308a15180eb4404","modified":1562367984048},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1562367984048},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"33456264a74d1bba38264d14713544d67d003733","modified":1562367984048},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"9a2d298dbdcbfd758518fd74b63897bc80ce15a5","modified":1562367984050},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1562367984051},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1562367984050},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"24230e46fc9fb7b8551f97bb36e9bc1f7423098e","modified":1562367984051},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"75d2d92af070eb10273558b2436972d3f12b361c","modified":1562367984051},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fa33213aceed7bf4bf25437ca9c1a00f7734ae65","modified":1562367984052},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1562367984052},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a96e46a6ae86c423f932bc2bc78b9f7453e4e4e5","modified":1562367984052},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"6565b4a309325596768d0d32e022c80ef23066cb","modified":1562367984053},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"fc160583f742c94316a0fee05c18468033173534","modified":1562367984054},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1562367984054},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"1edf4e69d0ec0dc9cefed6c35d3e803e0da4093d","modified":1562367984054},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1562367984055},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"75737591682a2bafa71db4c03fb79e970ac0e7aa","modified":1562367984056},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"57044a6d19eb418c1c3d28787e82c69efa9e0ca6","modified":1562367984055},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"b6dac5bbf20f090cf4b67d156f030d7170dfb39c","modified":1562367984056},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"6400c98a9fd2b9a8502269f33355bd7ab3ff793b","modified":1562367984057},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5b936dddb7b4de4720cd1e8428b30a2f06d63fb","modified":1562367984057},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"232aedbd44243b3b80c4503c947060d3269c1afc","modified":1562367984057},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1562367984098},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1562367984100},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1562367984099},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png","hash":"b14cc2530685f00c3dc0067519b61987b56e2343","modified":1562499834363},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/AE17D459-66B1-4946-A962-BFC3B9438E79.png","hash":"aaa8762b1667b978fa0805092cdee182c95fa066","modified":1562501077359},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png","hash":"624fdef3f24c21760213c7ac0f612f9090597f5c","modified":1562499809726},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png","hash":"26e4f7acad8b5e25a28733104b53fb490039d08b","modified":1562499556724},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png","hash":"22e561f184339e8e578d1b9d61590f4a273ccf97","modified":1562499522761},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png","hash":"e7ef320a074b23ff8f3996373660bb1cf2f9e87c","modified":1562502270514},{"_id":"source/_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1565329136620},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/comment.png","hash":"aab59f569a31b2e885b3435d62f93620b4241a5c","modified":1563977934012},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png","hash":"52acefe3843e1099aa52f12f6688697c46c82dfb","modified":1564060724552},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/32183E30-E096-46DE-A855-932377D70BFE.png","hash":"fde225deefdd97563481d97c6ed291be9c004db7","modified":1562499614966},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/64F21679-7168-47B0-A0B2-31C876FF3830.png","hash":"e0abf2cea8441e74a6765f6b3eae225cac5298f1","modified":1562499597793},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1562367984103},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/E512586F-5E27-4EDF-8087-1526D21D5792.png","hash":"ebb641dc5f73d537960e7a675a280b47717afcf6","modified":1562499522766},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/eef2a485ae6f7de29d5965457a1ae0da89055a58.png","hash":"eef2a485ae6f7de29d5965457a1ae0da89055a58","modified":1562502270517},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B5F9DD64-3574-480A-A409-BEE96561091B.png","hash":"10aabf90899351325618085c0a8fc5c629f92c68","modified":1562499660737},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"4cfeec9434a72d5efc6ca225d3445d084d4590f7","modified":1562367984028},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"a410ed529afd46ddf4a96ecf0de6599488716887","modified":1562367984029},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"6c4990d375b640ee4551e62c48c1cbe4c3d62212","modified":1562367984028},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1562367984029},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ca97f0b6990eef947039faede80c56d9c4381ee1","modified":1562367984028},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"cc6ee18f47f2e1e06df6fa0eadb37079e580fd11","modified":1562367984030},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"c0d9e18a9210fdcaf33e488518b3b288eb58c0a1","modified":1562367984029},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"71d8d1cc22a2a7627a6db7240f0c4902a14f9bea","modified":1562367984030},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"352796ec0a0cbbdb45d2351711d136ae6112b757","modified":1562367984030},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"2d142c6f39853916256ad8fc79eb6b85f4001ae8","modified":1562367984031},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"6904fd7ea6455e008d9884558b68254608af9a3c","modified":1562367984031},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1562367984030},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1562367984031},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f1d52954b9a5d1ca8e224382349f525e598dd923","modified":1562367984032},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1562367984032},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"80addb9b725e329915c05c27b9fadaf56457a9b3","modified":1562367984032},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"6a75bb1f2435f4e895cbbb5abbddf6e8f7257804","modified":1562367984033},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"61ca40856e5cacd48e0fa9728fde4605c7dd4c94","modified":1562367984032},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1562367984033},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"2356226157e8068b0e9bbe2f7d0f74e1ab49199b","modified":1562367984034},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"df3c19fd447da6d4a807683345007a41338f9a04","modified":1562367984034},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"c961d37190d9bec58a36306c7e716c4e72c4582f","modified":1562367984034},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0bf899fab331add63f0c8ead31ca3a3db2ad74d9","modified":1562367984035},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"67165cd8836e03c289162b96ef06f8b024afe9af","modified":1562367984035},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"3f33bb862c2aa993f54987fbb345da067b79b112","modified":1562367984036},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"5440013a081201ca791582db98159dce93ea9e75","modified":1562367984036},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"496f931e3a7e313ba8088fb91bb20789cace72c9","modified":1562367984035},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1562367984037},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1562367984036},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"d5c8ffed7f2c701052b7a53abaf5ef437374ea72","modified":1562367984037},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"a6c24393dffbdd94dd5c01cdbec5e180b0bfbbbd","modified":1562367984037},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"8e058c99dd7d41f0bd34c7c28b6ac9fbb17dcb5e","modified":1562367984037},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"9224b566cd2632f64c1a964e2c786cee93b93286","modified":1562367984038},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a3170630d8e085889a4bdc20eb7f09c5a0479c47","modified":1562367984038},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"cc83816614f21c7e1d8d3f867d547ff7c658cec4","modified":1562367984039},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"517d541a80d59ad99a3f648be74891e0c7bc72a8","modified":1562367984039},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bdd7f2d197a31f53c36dbe28efdd75593581d816","modified":1562980260509},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"c2d9c3b6fbfa65544e6b5a55d3cb2149df04a8a9","modified":1562367984040},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"9a3bfc878ca797946815bed23cd6f92b24a16358","modified":1562367984040},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"8a24b56524a388fbabd408ffc8ba9b56eb9e01ce","modified":1562367984041},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"c01609176929590f8f347075a9a12b661acd661e","modified":1562367984041},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"8e5c884fb950937afa350c608545455c87aa6129","modified":1562367984041},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"967fb3a3c6c851b34ec5df2d945dc266ed63d146","modified":1562367984042},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"58ec00eebe68d0eebd2eea435c710063877447df","modified":1562367984042},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"6ec8ea7b11a146777b6b8da0f71f0cc1dbd129df","modified":1562367984043},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1562367984043},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"d7501ae01fc45fa15b00d1bc5233b9fffa20a3c9","modified":1562367984043},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"21b32840d8b3a14b10770153114778304ba6d1b0","modified":1562367984043},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"da8d34729fb6eb0fcb8ee81e67d2be3c02bc1bc4","modified":1562367984044},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"6e4400d6704dee076434726b7a03ac464eb7bcb4","modified":1562367984044},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fc58498d4f5081fcf6218e9e18c5bf2328275bef","modified":1562367984045},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"cbc0be5a3285b469858ec9ead48e2ea90bd47ae1","modified":1562367984044},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"688ca3eccc26727d050ad098b32b40934719588a","modified":1562367984045},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"ac7753d536341aa824d7bce0332735e838916995","modified":1562367984045},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"9fac89c8146eb2675721a26f528d7d0f8be7debe","modified":1562367984045},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1562367984046},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"61466e3e5459960b5802a267751a0c8018918b0b","modified":1562367984046},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"dd44d8ca93ad5d366a96d797a0a8b4f3b46f9a77","modified":1562367984046},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"3ae3f3c276d444862033fd3434c632ad0d2f84e6","modified":1562367984046},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1562367984053},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1562367984053},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1562367984055},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/45343A7B-866B-4A70-A3A5-944C989E35BD.png","hash":"7912a5421dabde5c38af6f3161b971a7f185d6fe","modified":1562499597799},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1562367984103},{"_id":"source/_posts/Apache Flink 漫谈系列 - Table API 概览/6DEBA2F2-3A2A-40BD-B461-B4E0FDF7D8CA.png","hash":"e8e0cec9ce8b445490c569cb258bb11dd90b1bb2","modified":1562499556737},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png","hash":"4adc6c221b1278c586e3887b08d45d8542bc5629","modified":1562499809724},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/44FDFE22-3B21-4514-8E91-E339F9188E4E.png","hash":"b48f7b3bf228e663cf5fb0cd525c774f6e36c73e","modified":1562499522760},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png","hash":"1b70fa1a91b87eae70e4bc92895c8946a5402f2b","modified":1562499660731},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png","hash":"a19e614b08fda328294205b9894dc0bc97e68bd2","modified":1564060724550},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png","hash":"8f4763dc5b03c50473a4ad21e93852827c51d250","modified":1564060724539},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1562367984107},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/B0D184CA-95DE-40C7-AA80-C4FE44134D16.png","hash":"2251130cdc67f73303bf1aa5a5f12a7c02738b5a","modified":1562499793084},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png","hash":"bb69ae53a0176226a29f007d3ef99bd3c7129ec1","modified":1562499779821},{"_id":"source/_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/3C082C16-792A-4824-8112-9FD3C87768B5.png","hash":"df9eac23d7621870be4a0ae16a86db75a07b4ec4","modified":1565328921056},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png","hash":"8af7618e04f987b60a258c4e2501b0b430d4f868","modified":1563721471108},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A05E658F-C511-45E2-870F-5E3227A94570.png","hash":"8c2f8de6353e01520a50add57c5dc043f49f06eb","modified":1562499522772},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1562367984102},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/7A1AA068-AA3C-4439-898D-5DE507CC24C6.png","hash":"9ec6ec8d980e8e082298a043618e1d1d3568ed82","modified":1566961501160},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/B31690BA-A8E2-4C22-A948-8735728542B0.png","hash":"1f1e932a633cfbb138f55ffb7d3a34114858a4ce","modified":1563721471119},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/B0C5D286-6107-40D0-8F76-9C18DA4650D0.png","hash":"ac6e95bf01bf535b66866ba301f02cbd0ce9a90b","modified":1564060724543},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png","hash":"b58290862f6e7114953a9f8ab6d433cad2babaa6","modified":1562499522757},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/34546866-F865-4660-8177-A64C5A14C002.png","hash":"2f634b1cb8004b115a5e92e4ba1f3edb44aaf3bc","modified":1564060724545},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"d8f2e1de2142c44500d41debeeb319ba8adb1ccd","modified":1562368007022},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png","hash":"b49f2cc792a41a5681d293b2d2365dea9a5f036f","modified":1567994037517},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/166F2FE8-93FB-4966-87B7-F6C98DFF4915.png","hash":"b1f008568d3e099e660df7a4d2595296ab6019ad","modified":1562499660732},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/9AB5B4ED-4880-4C8E-A80C-42F562CF1194.png","hash":"951dcb30a90eece5e98bed0887db5f4b1dd580c7","modified":1567994037522},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/B26FF58C-1E74-42DC-B75A-963F898C24B7.png","hash":"2c2393fcc2b5a6cc3359d0a9e5b8fffc187fb389","modified":1567994037514},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/hehua.png","hash":"322609760570c49aa50451a97721c205982d7ec0","modified":1568010827997},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/B83130D4-9E1F-4BBC-88FA-2286951D18E5.png","hash":"901d8588b611d3842723088c532ae4c283ac0d7a","modified":1563722085990},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/59761691-219B-47D5-8DC8-FC1D9C3E949E.png","hash":"d6ccef7ceab16d69777ce3557fbb1dfd1d40ed57","modified":1562501077361},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/52A99F20-9648-4AB7-BA20-AA890797A9F0.png","hash":"4803b472de0ad297dfd0059b4d4fad360726ea7c","modified":1565526745142},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js.map","hash":"753adf35c67993265fb13b827aeee3f95e3442af","modified":1562368007037},{"_id":"public/search.xml","hash":"e3d0b97ab62cf1c444b6bbf19f877606f8791ce1","modified":1568167850032},{"_id":"public/catalog/index.html","hash":"19cbb2488ff393f0d9c33470c277a3c0aa38ca7d","modified":1568167850094},{"_id":"public/about/index.html","hash":"3cfad8c58d09432b9fdaa97cba5bca699e6bb96a","modified":1568167850095},{"_id":"public/categories/index.html","hash":"741a18425dcc5244f9b6eafc7167cb5c28bf30ff","modified":1568167850095},{"_id":"public/tags/index.html","hash":"e97a06b8144d69395dc23a75ee1b40d34755f262","modified":1568167850095},{"_id":"public/2019/09/09/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/index.html","hash":"34ce997a651f7cfdcf64bf2618beeca1e1396f6b","modified":1568167850095},{"_id":"public/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/index.html","hash":"cbf2c7f814e5256e9d6f8c5f6d6d32b058803262","modified":1568167850095},{"_id":"public/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/index.html","hash":"552add5f4a566801f8607c717daf3607a22a7c8a","modified":1568167850095},{"_id":"public/2019/08/08/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/index.html","hash":"c58de3012f58e2a5c2455c73ab02ef1b03d389fb","modified":1568167850095},{"_id":"public/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/index.html","hash":"7434131a6cd7732a0229294b7b431b896ca69bb2","modified":1568167850095},{"_id":"public/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/index.html","hash":"c4ad0199c1323e586733b6cd2c9efc14797660e3","modified":1568167850095},{"_id":"public/2019/07/13/Apache-Flink-说道系列-序/index.html","hash":"944e8d8498ba8c376f6c3f7396d9552e73d58681","modified":1568167850095},{"_id":"public/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/index.html","hash":"be3ff14b5975f46a34b0fa3c39cf7314c7200354","modified":1568167850095},{"_id":"public/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/index.html","hash":"35446fc645932fbe0823f954d520cc95a29ede41","modified":1568167850095},{"_id":"public/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/index.html","hash":"2de6ea53a3f3f6bcd23692329e6df19583fd5ab9","modified":1568167850096},{"_id":"public/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/index.html","hash":"f763a8ec85a91412299729aa196a218e49a915f3","modified":1568167850096},{"_id":"public/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/index.html","hash":"fe1b2f16374d38c5930224f73dc9f95e76dd22e3","modified":1568167850096},{"_id":"public/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/index.html","hash":"6360796dc59cd2b541db9871f10dd9c80f6026a5","modified":1568167850096},{"_id":"public/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/index.html","hash":"61a3df84eb6dbde4a7258b44bbada91e5328a79a","modified":1568167850096},{"_id":"public/2019/01/05/Apache Flink 漫谈系列 - 概述/index.html","hash":"af98c55c28ee6b6ec675d368fa2a51d623ebcb48","modified":1568167850096},{"_id":"public/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/index.html","hash":"68a8e75891e732bc09f7da9f2af44ffa7379b158","modified":1568167850096},{"_id":"public/2019/01/01/Apache Flink 漫谈系列 -  Watermark/index.html","hash":"b47be7358f3a5228c70a805d0fb12d014c5cbea0","modified":1568167850096},{"_id":"public/2019/01/01/Apache Flink 漫谈系列 - State/index.html","hash":"e10c8124e902ea84e01309da0af54e92d31cf6d9","modified":1568167850096},{"_id":"public/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/index.html","hash":"672cabea7250079b3f5cbe4b0a4bceaa05f1d60c","modified":1568167850096},{"_id":"public/2019/01/01/Apache-Flink-漫谈系列-序/index.html","hash":"ebdbe9392dad338f5c01a7f425f6eaf929cbe2bf","modified":1568167850096},{"_id":"public/index.html","hash":"bdc15e290e60b5fd1391d92b4ff2a3d9e6b29677","modified":1568167850096},{"_id":"public/page/2/index.html","hash":"086c98d97b82a3571bf5daa94b1cf775a7002433","modified":1568167850096},{"_id":"public/page/3/index.html","hash":"1a44915575fed106b6a36edcae2e117af1cb7b83","modified":1568167850096},{"_id":"public/page/4/index.html","hash":"4bf19de0ac23c31286c19de6e1d4173d84010b93","modified":1568167850097},{"_id":"public/archives/Apache-Flink-漫谈/index.html","hash":"3cf0a82e0b1ad9e5c9713180b8a638de7cb96e3e","modified":1568167850097},{"_id":"public/archives/Apache-Flink-漫谈/page/2/index.html","hash":"44e660c7953b45935003fa04e31eba364ea813ea","modified":1568167850097},{"_id":"public/archives/Apache-Flink-漫谈/page/3/index.html","hash":"4eab174ca0f4e30ae15172e814754e52ceba00bc","modified":1568167850097},{"_id":"public/archives/Apache-Flink-视频/index.html","hash":"7af0f478a3d4f7a8b50527efd030ffa649700fc7","modified":1568167850097},{"_id":"public/archives/Apache-Flink-说道/index.html","hash":"b3e8d9d7c3530bf47457bd0e277ab8f61cc4722c","modified":1568167850097},{"_id":"public/archives/Apache-Flink-说道/page/2/index.html","hash":"d57c4ca0da20698f63319024294a5defadb18d1b","modified":1568167850097},{"_id":"public/archives/index.html","hash":"83f60b0ee3e26ecdb0cab31e97403ed7c6e9a4ee","modified":1568167850097},{"_id":"public/archives/page/2/index.html","hash":"6090969c3c36757447e888ec2c2e48724f2f8286","modified":1568167850097},{"_id":"public/archives/page/3/index.html","hash":"12df75a480013b02b5ed5056e91e8652a44e238a","modified":1568167850097},{"_id":"public/archives/page/4/index.html","hash":"972e9e112eeda6d97842f39658398092fbefd6e3","modified":1568167850097},{"_id":"public/archives/2019/index.html","hash":"0113b2f54359b63e91f674857ba0704b1ac25923","modified":1568167850097},{"_id":"public/archives/2019/page/2/index.html","hash":"5931fe9be9de54fe3e19260e88925684e2515f82","modified":1568167850097},{"_id":"public/archives/2019/page/3/index.html","hash":"ac6f85ac904142863ca2c735a6c8885da1ece5c5","modified":1568167850097},{"_id":"public/archives/2019/page/4/index.html","hash":"0ccb0ca4e3e2876a74216acfd87ff7fc8a99b86e","modified":1568167850097},{"_id":"public/archives/2019/01/index.html","hash":"133429131b1d92e28cd1a2a5a562f8aba9799073","modified":1568167850097},{"_id":"public/archives/2019/01/page/2/index.html","hash":"3ed610d4f20d3e923bad5e21d364b899f7245a90","modified":1568167850097},{"_id":"public/archives/2019/03/index.html","hash":"d21610d21844273e1346657a6ae62cb1fbbd8937","modified":1568167850097},{"_id":"public/archives/2019/03/page/2/index.html","hash":"d473583dad4117fb24272baec6aa539d82b6266b","modified":1568167850098},{"_id":"public/archives/2019/04/index.html","hash":"8939e6568aa002ebe81f4431a8498a8618c3bbf8","modified":1568167850098},{"_id":"public/archives/2019/07/index.html","hash":"96ed986dff3a876b452923a0e91f85ae5ed3c208","modified":1568167850098},{"_id":"public/archives/2019/08/index.html","hash":"5de20348235295d2ef236e39deeee7102011b28b","modified":1568167850098},{"_id":"public/archives/2019/09/index.html","hash":"3d9b94c15c44d0f4c97a887ab0488bf47539afb9","modified":1568167850098},{"_id":"public/archives/Flink/index.html","hash":"a2f206c0742583cd10faa54961f9f7e754ae86f5","modified":1568167850098},{"_id":"public/archives/Flink/page/2/index.html","hash":"f244747def854a104c8ef7bf4ef45b3d32c3c1b5","modified":1568167850098},{"_id":"public/archives/Flink/page/3/index.html","hash":"4b00968c703ed95cb07c58b313093ed78adaedb8","modified":1568167850098},{"_id":"public/archives/Flink/page/4/index.html","hash":"1ec690aab3d197aa295f9b29697f7164ecf1c698","modified":1568167850098},{"_id":"public/archives/视频，Python/index.html","hash":"d5e91eee63ee194c22e054cbbca3d1bc15733d0c","modified":1568167850098},{"_id":"public/archives/道德经/index.html","hash":"27c85aed95df12170a28d6ae7da8747e9a655e56","modified":1568167850098},{"_id":"public/archives/道德经/page/2/index.html","hash":"86843b652d6ba96070f885ef161b65e99b9d5e98","modified":1568167850098},{"_id":"public/archives/圣人为腹不为目/index.html","hash":"cf73585c465f5dcffa4c5ae5644edeccf1c6f2d6","modified":1568167850099},{"_id":"public/archives/Python/index.html","hash":"bf4af14d8f4b6d74d7f7a67402aee1e8a8ab6f80","modified":1568167850099},{"_id":"public/archives/Connector/index.html","hash":"fa81339cbb976cd371e1b7277e084b664c742081","modified":1568167850099},{"_id":"public/archives/孰能浊以静之徐清/index.html","hash":"ab410ebb7ef833941757c52f9ea38cd9111744a4","modified":1568167850099},{"_id":"public/archives/孰能安以动之徐生/index.html","hash":"9ebc596edcf10fd70defd1e1dd0d3602444ee800","modified":1568167850099},{"_id":"public/archives/上善若水/index.html","hash":"63b12b6026f9912c4619bf15ff3a5cc6cac3ab2f","modified":1568167850099},{"_id":"public/archives/持而盈之-不如其已/index.html","hash":"36ac42023b80265bceed32ce34ec8f226efb06a4","modified":1568167850099},{"_id":"public/archives/曲则全/index.html","hash":"e159ff6d3819be13fef86add38d1c0f38f3cecac","modified":1568167850099},{"_id":"public/catalog/flink_training.html","hash":"872052fc7676140ec4fd709ccd3ef8dad5c23a99","modified":1568167850100},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式.md","hash":"ab7604f89924ae26336a49893965a2ccac0fae97","modified":1577961102326},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777817105172.jpg","hash":"c0b7dbd80e07d49a4616abb5bc64b1bae9fd6207","modified":1577781710521},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821361064.jpg","hash":"0984d8e060424afc367a2f59295fdd24c9579288","modified":1577782136110},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821194059.jpg","hash":"74666e57f1f14b158ab8ec5539c0b1d42dadf6a4","modified":1577782119409},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778077301840.jpg","hash":"f9d6c331a4f4f64a0f87e6db8b9cf475c0d45e36","modified":1577807730188},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15776988009185.jpg","hash":"9c4dd170c9a5369888a3c5612affc40891abb728","modified":1577698800931},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777916683830.jpg","hash":"df2eb474a207354b6683f39546082186dcd8a6ed","modified":1577791668391},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778372533417.jpg","hash":"c50cb2ecc0436c417401375b26c27eb30b4d0a6f","modified":1577837253348},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417681405.jpg","hash":"dca9273bec3289ff42c9c347d0608d73975828cc","modified":1577941768148},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779465411786.jpg","hash":"8a89432eab412a1147fba9366e9ead4ce3c76682","modified":1577946541184},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779466238977.jpg","hash":"9f71b8b4999909b869998ff455e412b4a3ad4c7c","modified":1577946623904},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779503938890.jpg","hash":"37951864a9d5fe86f8595aea617e5c99cf480b6e","modified":1577950393897},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779542617667.jpg","hash":"68512acaf03ad8fbc5712912c81061baa923bdd1","modified":1577954261772},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777536894368.jpg","hash":"e57a9456d1eea8568778073194627aa504429777","modified":1577753689446},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777784863901.jpg","hash":"8d2ae39e515a570f7b34d7fb1ddafc2fa8b64bc9","modified":1577778486399},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373195478.jpg","hash":"229d9deeea8a5c6778004aa14ae2866b0352d3d2","modified":1577837319563},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779456315383.jpg","hash":"b41a0c367e8388850c47dbbf2facca43de815e0c","modified":1577945631563},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779569378844.jpg","hash":"26e04bd6b7b4a4e1dc264c64b38a074984ac1c85","modified":1577956937894},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777810183663.jpg","hash":"e1324665b6f41dfece70a21192edcede53c7d764","modified":1577781018379},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777816500228.jpg","hash":"1cc1855bef844e83040afa05da0eb2ccb438c16b","modified":1577781650047},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777853568978.jpg","hash":"4ff7d30822a215a814797f393885c7db506dd1e7","modified":1577785356922},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777907875791.jpg","hash":"82442289ba008129b949161ebaa8a9dcff370840","modified":1577790787604},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373437959.jpg","hash":"26b7c18348e318064ae93172eb75a656f663b6c2","modified":1577837343812},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779454827330.jpg","hash":"dcd049374ff41d37a8f80ea6533be311277259bc","modified":1577945482759},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779449909178.jpg","hash":"a47725a4f716498aedaf8482bb9c2a301cf8edd5","modified":1577944990942},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777878055973.jpg","hash":"c937557024a7e0620271f4f01ccc7a08c6797cfd","modified":1577787805617},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777972228895.jpg","hash":"b502bba123b5f7718507d909200ae2fe746c11ae","modified":1577797222919},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417223687.jpg","hash":"6e9a0e23c34006a988b3bc952b10cc50958975b6","modified":1577941722398},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777913540792.jpg","hash":"6e9d360bc97b9833ab32498eac7a2769c16667ec","modified":1577791354107},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778389671396.jpg","hash":"01381538e4d2739991910629dea5360019b0387c","modified":1577838967159}],"Category":[{"name":"Apache Flink 漫谈","_id":"ck0emm5f60004s94hyfa6xl78"},{"name":"Apache Flink 视频","_id":"ck0emm5fu000vs94h7jd1s0k5"},{"name":"Apache Flink 说道","_id":"ck0emm5g50016s94h3c7o59r1"}],"Data":[],"Page":[{"_content":"第一部分 初识 Flink\n\n- 第一章 概述\n - 1. 大数据发展历史\n - 2. 为什么要学习 Apache Flink？\n - 3. Flink 学习建议\n - 4. Flink 基本概念与术语\n- 第二章 安装上手\n - 1. Flink 安装部署\n - 2. 在 Docker 中运行 Flink \n - 3. Flink 开发环境配置\n- 第三章 基本概念\n - 1. Flink 架构与生态\n - 2. 编程模型\n - 3. 运行时概念\n\n第二部分 Flink 开发入门\n\n- 第四章 DataStream\n   - 1.  DataStreamContext环境\n\n   - 2.  数据源(DataSource)\n\n   - 3.  转化(Transformation)\n\n   - 4.  数据Sink\n- 第五章 时间语义\n - 1. 时间语义\n - 2. Watermark\n - 3. 乱序问题的解决\n - 4. 如何设置 watermark 策略\n- 第六章 Window\n - 1. 为什么要有 Window？\n - 2. Window 的不同类型\n - 3. Window API 的使用\n - 4. 自定义Window的三大组件\n - 5. Window的内部实现原理\n- 第五章 状态管理 \n\n - 1.  状态管理的基本概念\n\n - 2.  KeyState 基本类型及用法\n\n - 3.  OperatorState基本用法\n - 4. 什么时候应该用什么 State？\n - 5. Checkpoint 机制\n - 6. Savepoint 机制\n - 7. 可选的状态存储方式\n - 8. 如何选择最适合的状态存储方式？\n\n第三部分 Flink 开发进阶\n\n- 第六章 Flink Table API & SQL\n  - 1. Flink Table 新架构与双 Planner \n  - 2. 五个 TableEnvironment 我该用哪个？\n  - 3. 如何使用 DDL 连接外部系统\n  - 4. 维表 JOIN 介绍\n  - 5. 流式 TopN 入门和优化技巧\n  - 6. 高效的流式去重方案\n  - 7. 迈向高吞吐：MiniBatch 原理介绍\n  - 8. 数据倾斜的一种解决方案：Local Global 优化介绍\n  - 9. COUNT DISTINCT 性能优化指南\n  - 10. 多路输出与子图复用\n - 第七章 Flink Python API\n  - 1. Flink Python 架构\n  - 2. Flink Python 环境搭建\n  - 3. Flink Python API 入门与开发\n\n\n第四部分 Flink 运维和管理\n\n- 第八章 集群部署\n - 1. Flink On YARN\n - 2. Flink On K8S\n - 3. 如何配置高可用\n - 4. 上线前的检查清单\n- 第九章 监控与报警\n\n   - 1.  指标的种类\n\n   - 2.  指标的获取方式\n\n        a.  Web Ui\n\n        b.  Rest API\n\n        c.  MetricReporter\n   - 4. 常用的指标\n\n   - 5. 自定义指标方式\n   - 6. 推荐的指标聚合方式\n   - 7. 如何基于指标诊断延时/反压？\n   - 8. 第三方指标分析系统\n   - 9. 基于 Prometheus 构建 Flink 的监控报警系统","source":"catalog/flink_training.md","raw":"第一部分 初识 Flink\n\n- 第一章 概述\n - 1. 大数据发展历史\n - 2. 为什么要学习 Apache Flink？\n - 3. Flink 学习建议\n - 4. Flink 基本概念与术语\n- 第二章 安装上手\n - 1. Flink 安装部署\n - 2. 在 Docker 中运行 Flink \n - 3. Flink 开发环境配置\n- 第三章 基本概念\n - 1. Flink 架构与生态\n - 2. 编程模型\n - 3. 运行时概念\n\n第二部分 Flink 开发入门\n\n- 第四章 DataStream\n   - 1.  DataStreamContext环境\n\n   - 2.  数据源(DataSource)\n\n   - 3.  转化(Transformation)\n\n   - 4.  数据Sink\n- 第五章 时间语义\n - 1. 时间语义\n - 2. Watermark\n - 3. 乱序问题的解决\n - 4. 如何设置 watermark 策略\n- 第六章 Window\n - 1. 为什么要有 Window？\n - 2. Window 的不同类型\n - 3. Window API 的使用\n - 4. 自定义Window的三大组件\n - 5. Window的内部实现原理\n- 第五章 状态管理 \n\n - 1.  状态管理的基本概念\n\n - 2.  KeyState 基本类型及用法\n\n - 3.  OperatorState基本用法\n - 4. 什么时候应该用什么 State？\n - 5. Checkpoint 机制\n - 6. Savepoint 机制\n - 7. 可选的状态存储方式\n - 8. 如何选择最适合的状态存储方式？\n\n第三部分 Flink 开发进阶\n\n- 第六章 Flink Table API & SQL\n  - 1. Flink Table 新架构与双 Planner \n  - 2. 五个 TableEnvironment 我该用哪个？\n  - 3. 如何使用 DDL 连接外部系统\n  - 4. 维表 JOIN 介绍\n  - 5. 流式 TopN 入门和优化技巧\n  - 6. 高效的流式去重方案\n  - 7. 迈向高吞吐：MiniBatch 原理介绍\n  - 8. 数据倾斜的一种解决方案：Local Global 优化介绍\n  - 9. COUNT DISTINCT 性能优化指南\n  - 10. 多路输出与子图复用\n - 第七章 Flink Python API\n  - 1. Flink Python 架构\n  - 2. Flink Python 环境搭建\n  - 3. Flink Python API 入门与开发\n\n\n第四部分 Flink 运维和管理\n\n- 第八章 集群部署\n - 1. Flink On YARN\n - 2. Flink On K8S\n - 3. 如何配置高可用\n - 4. 上线前的检查清单\n- 第九章 监控与报警\n\n   - 1.  指标的种类\n\n   - 2.  指标的获取方式\n\n        a.  Web Ui\n\n        b.  Rest API\n\n        c.  MetricReporter\n   - 4. 常用的指标\n\n   - 5. 自定义指标方式\n   - 6. 推荐的指标聚合方式\n   - 7. 如何基于指标诊断延时/反压？\n   - 8. 第三方指标分析系统\n   - 9. 基于 Prometheus 构建 Flink 的监控报警系统","date":"2019-09-11T02:09:02.910Z","updated":"2019-09-11T02:09:02.909Z","path":"catalog/flink_training.html","title":"","comments":1,"layout":"page","_id":"ck0emm5ez0000s94h7hqxiweo","content":"<p>第一部分 初识 Flink</p>\n<ul>\n<li>第一章 概述</li>\n<li>\n<ol>\n<li>大数据发展历史</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>为什么要学习 Apache Flink？</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink 学习建议</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>Flink 基本概念与术语</li>\n</ol>\n</li>\n<li>第二章 安装上手</li>\n<li>\n<ol>\n<li>Flink 安装部署</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>在 Docker 中运行 Flink</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink 开发环境配置</li>\n</ol>\n</li>\n<li>第三章 基本概念</li>\n<li>\n<ol>\n<li>Flink 架构与生态</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>编程模型</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>运行时概念</li>\n</ol>\n</li>\n</ul>\n<p>第二部分 Flink 开发入门</p>\n<ul>\n<li>\n<p>第四章 DataStream</p>\n<ul>\n<li>\n<ol>\n<li>DataStreamContext环境</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>数据源(DataSource)</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>转化(Transformation)</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>数据Sink</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>第五章 时间语义</p>\n</li>\n<li>\n<ol>\n<li>时间语义</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Watermark</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>乱序问题的解决</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>如何设置 watermark 策略</li>\n</ol>\n</li>\n<li>\n<p>第六章 Window</p>\n</li>\n<li>\n<ol>\n<li>为什么要有 Window？</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Window 的不同类型</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Window API 的使用</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>自定义Window的三大组件</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>Window的内部实现原理</li>\n</ol>\n</li>\n<li>\n<p>第五章 状态管理</p>\n</li>\n<li>\n<ol>\n<li>状态管理的基本概念</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>KeyState 基本类型及用法</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>OperatorState基本用法</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>什么时候应该用什么 State？</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>Checkpoint 机制</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>Savepoint 机制</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>可选的状态存储方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>如何选择最适合的状态存储方式？</li>\n</ol>\n</li>\n</ul>\n<p>第三部分 Flink 开发进阶</p>\n<ul>\n<li>第六章 Flink Table API &amp; SQL\n<ul>\n<li>\n<ol>\n<li>Flink Table 新架构与双 Planner</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>五个 TableEnvironment 我该用哪个？</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>如何使用 DDL 连接外部系统</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>维表 JOIN 介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>流式 TopN 入门和优化技巧</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>高效的流式去重方案</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>迈向高吞吐：MiniBatch 原理介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>数据倾斜的一种解决方案：Local Global 优化介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"9\">\n<li>COUNT DISTINCT 性能优化指南</li>\n</ol>\n</li>\n<li>\n<ol start=\"10\">\n<li>多路输出与子图复用</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>第七章 Flink Python API</li>\n<li>\n<ol>\n<li>Flink Python 架构</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Flink Python 环境搭建</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink Python API 入门与开发</li>\n</ol>\n</li>\n</ul>\n<p>第四部分 Flink 运维和管理</p>\n<ul>\n<li>\n<p>第八章 集群部署</p>\n</li>\n<li>\n<ol>\n<li>Flink On YARN</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Flink On K8S</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>如何配置高可用</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>上线前的检查清单</li>\n</ol>\n</li>\n<li>\n<p>第九章 监控与报警</p>\n<ul>\n<li>\n<ol>\n<li>指标的种类</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>指标的获取方式</li>\n</ol>\n<p>a.  Web Ui</p>\n<p>b.  Rest API</p>\n<p>c.  MetricReporter</p>\n</li>\n<li>\n<ol start=\"4\">\n<li>常用的指标</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>自定义指标方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>推荐的指标聚合方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>如何基于指标诊断延时/反压？</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>第三方指标分析系统</li>\n</ol>\n</li>\n<li>\n<ol start=\"9\">\n<li>基于 Prometheus 构建 Flink 的监控报警系统</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>第一部分 初识 Flink</p>\n<ul>\n<li>第一章 概述</li>\n<li>\n<ol>\n<li>大数据发展历史</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>为什么要学习 Apache Flink？</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink 学习建议</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>Flink 基本概念与术语</li>\n</ol>\n</li>\n<li>第二章 安装上手</li>\n<li>\n<ol>\n<li>Flink 安装部署</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>在 Docker 中运行 Flink</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink 开发环境配置</li>\n</ol>\n</li>\n<li>第三章 基本概念</li>\n<li>\n<ol>\n<li>Flink 架构与生态</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>编程模型</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>运行时概念</li>\n</ol>\n</li>\n</ul>\n<p>第二部分 Flink 开发入门</p>\n<ul>\n<li>\n<p>第四章 DataStream</p>\n<ul>\n<li>\n<ol>\n<li>DataStreamContext环境</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>数据源(DataSource)</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>转化(Transformation)</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>数据Sink</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>第五章 时间语义</p>\n</li>\n<li>\n<ol>\n<li>时间语义</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Watermark</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>乱序问题的解决</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>如何设置 watermark 策略</li>\n</ol>\n</li>\n<li>\n<p>第六章 Window</p>\n</li>\n<li>\n<ol>\n<li>为什么要有 Window？</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Window 的不同类型</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Window API 的使用</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>自定义Window的三大组件</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>Window的内部实现原理</li>\n</ol>\n</li>\n<li>\n<p>第五章 状态管理</p>\n</li>\n<li>\n<ol>\n<li>状态管理的基本概念</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>KeyState 基本类型及用法</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>OperatorState基本用法</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>什么时候应该用什么 State？</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>Checkpoint 机制</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>Savepoint 机制</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>可选的状态存储方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>如何选择最适合的状态存储方式？</li>\n</ol>\n</li>\n</ul>\n<p>第三部分 Flink 开发进阶</p>\n<ul>\n<li>第六章 Flink Table API &amp; SQL\n<ul>\n<li>\n<ol>\n<li>Flink Table 新架构与双 Planner</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>五个 TableEnvironment 我该用哪个？</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>如何使用 DDL 连接外部系统</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>维表 JOIN 介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>流式 TopN 入门和优化技巧</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>高效的流式去重方案</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>迈向高吞吐：MiniBatch 原理介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>数据倾斜的一种解决方案：Local Global 优化介绍</li>\n</ol>\n</li>\n<li>\n<ol start=\"9\">\n<li>COUNT DISTINCT 性能优化指南</li>\n</ol>\n</li>\n<li>\n<ol start=\"10\">\n<li>多路输出与子图复用</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>第七章 Flink Python API</li>\n<li>\n<ol>\n<li>Flink Python 架构</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Flink Python 环境搭建</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Flink Python API 入门与开发</li>\n</ol>\n</li>\n</ul>\n<p>第四部分 Flink 运维和管理</p>\n<ul>\n<li>\n<p>第八章 集群部署</p>\n</li>\n<li>\n<ol>\n<li>Flink On YARN</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Flink On K8S</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>如何配置高可用</li>\n</ol>\n</li>\n<li>\n<ol start=\"4\">\n<li>上线前的检查清单</li>\n</ol>\n</li>\n<li>\n<p>第九章 监控与报警</p>\n<ul>\n<li>\n<ol>\n<li>指标的种类</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>指标的获取方式</li>\n</ol>\n<p>a.  Web Ui</p>\n<p>b.  Rest API</p>\n<p>c.  MetricReporter</p>\n</li>\n<li>\n<ol start=\"4\">\n<li>常用的指标</li>\n</ol>\n</li>\n<li>\n<ol start=\"5\">\n<li>自定义指标方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"6\">\n<li>推荐的指标聚合方式</li>\n</ol>\n</li>\n<li>\n<ol start=\"7\">\n<li>如何基于指标诊断延时/反压？</li>\n</ol>\n</li>\n<li>\n<ol start=\"8\">\n<li>第三方指标分析系统</li>\n</ol>\n</li>\n<li>\n<ol start=\"9\">\n<li>基于 Prometheus 构建 Flink 的监控报警系统</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"导读","date":"2019-07-12T13:19:19.000Z","categories":"catalog","tags":"catalog","_content":"\n![](ma.png)\n\n\n\n# Apache Flink 说“道”系列\nApache Flink 说\"道\"系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对\"道\"的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。\n\n[查阅详情](../archives/Apache-Flink-说道/)\n\n# Apache Flink 漫谈系列\n本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。\n\n[查阅详情](../archives/Apache-Flink-漫谈/)\n\n# Apache Flink 视频系列\nApache Flink 视频 系列主要是以视频的方式分享一些有关Apache Flink的技术知识，主要包括Meetup，Flink forward，Qcon，网络直播等视频演讲内容。\n\n[查阅详情](../archives/Apache-Flink-视频/)\n","source":"catalog/index.md","raw":"---\ntitle: 导读\ndate: 2019-07-12 21:19:19\ncategories: catalog\ntags: catalog\n---\n\n![](ma.png)\n\n\n\n# Apache Flink 说“道”系列\nApache Flink 说\"道\"系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对\"道\"的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。\n\n[查阅详情](../archives/Apache-Flink-说道/)\n\n# Apache Flink 漫谈系列\n本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。\n\n[查阅详情](../archives/Apache-Flink-漫谈/)\n\n# Apache Flink 视频系列\nApache Flink 视频 系列主要是以视频的方式分享一些有关Apache Flink的技术知识，主要包括Meetup，Flink forward，Qcon，网络直播等视频演讲内容。\n\n[查阅详情](../archives/Apache-Flink-视频/)\n","updated":"2019-08-09T06:59:17.102Z","path":"catalog/index.html","comments":1,"layout":"page","_id":"ck0emm5f40002s94h8a6i9ri6","content":"<p><img src=\"/catalog/index/ma.png\" alt></p>\n<h1>Apache Flink 说“道”系列</h1>\n<p>Apache Flink 说&quot;道&quot;系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对&quot;道&quot;的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。</p>\n<p><a href=\"../archives/Apache-Flink-%E8%AF%B4%E9%81%93/\">查阅详情</a></p>\n<h1>Apache Flink 漫谈系列</h1>\n<p>本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。</p>\n<p><a href=\"../archives/Apache-Flink-%E6%BC%AB%E8%B0%88/\">查阅详情</a></p>\n<h1>Apache Flink 视频系列</h1>\n<p>Apache Flink 视频 系列主要是以视频的方式分享一些有关Apache Flink的技术知识，主要包括Meetup，Flink forward，Qcon，网络直播等视频演讲内容。</p>\n<p><a href=\"../archives/Apache-Flink-%E8%A7%86%E9%A2%91/\">查阅详情</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/catalog/index/ma.png\" alt></p>\n<h1>Apache Flink 说“道”系列</h1>\n<p>Apache Flink 说&quot;道&quot;系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对&quot;道&quot;的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。</p>\n<p><a href=\"../archives/Apache-Flink-%E8%AF%B4%E9%81%93/\">查阅详情</a></p>\n<h1>Apache Flink 漫谈系列</h1>\n<p>本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。</p>\n<p><a href=\"../archives/Apache-Flink-%E6%BC%AB%E8%B0%88/\">查阅详情</a></p>\n<h1>Apache Flink 视频系列</h1>\n<p>Apache Flink 视频 系列主要是以视频的方式分享一些有关Apache Flink的技术知识，主要包括Meetup，Flink forward，Qcon，网络直播等视频演讲内容。</p>\n<p><a href=\"../archives/Apache-Flink-%E8%A7%86%E9%A2%91/\">查阅详情</a></p>\n"},{"title":"我的5W...","date":"2019-07-12T11:19:19.000Z","updated":"2019-07-12T11:19:19.000Z","_content":"\n![](ma.png)\n\n# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。 \n\n# What\n我将以blog的方式分享自己对技术和生活的认知，包括大数据计算领域的知识，也包括我个人对现实生活和与人为事的行为认知！\n\n# Why\n分享是最大的享受，予人成功是最大的成功，魅力不体现在你会什么，而着眼于你能让别人擅长什么，无欲无求但予人所求！\n\n# When\n From now on ...\n\n# Whole\n 我有一个简单，平凡 永远拥有 晴天 的小家:)\n\n - 我 -> 昵称 “平凡”， 乐享 平凡 生活；\n - 她 -> 昵称 “简单”， 追求 简单 思维；\n - 他 -> 昵称。“晴天☀️”， 阳光☀️男孩，爱画画也爱跆拳道；\n\n![](family.jpg)\n\n","source":"about/index.md","raw":"---\ntitle: 我的5W...\ndate: 2019-07-12 19:19:19\nupdated: 2019-07-12 19:19:19\n---\n\n![](ma.png)\n\n# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。 \n\n# What\n我将以blog的方式分享自己对技术和生活的认知，包括大数据计算领域的知识，也包括我个人对现实生活和与人为事的行为认知！\n\n# Why\n分享是最大的享受，予人成功是最大的成功，魅力不体现在你会什么，而着眼于你能让别人擅长什么，无欲无求但予人所求！\n\n# When\n From now on ...\n\n# Whole\n 我有一个简单，平凡 永远拥有 晴天 的小家:)\n\n - 我 -> 昵称 “平凡”， 乐享 平凡 生活；\n - 她 -> 昵称 “简单”， 追求 简单 思维；\n - 他 -> 昵称。“晴天☀️”， 阳光☀️男孩，爱画画也爱跆拳道；\n\n![](family.jpg)\n\n","path":"about/index.html","comments":1,"layout":"page","_id":"ck0emm5f80006s94hs81x8zhx","content":"<p><img src=\"/about/index/ma.png\" alt></p>\n<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>我将以blog的方式分享自己对技术和生活的认知，包括大数据计算领域的知识，也包括我个人对现实生活和与人为事的行为认知！</p>\n<h1>Why</h1>\n<p>分享是最大的享受，予人成功是最大的成功，魅力不体现在你会什么，而着眼于你能让别人擅长什么，无欲无求但予人所求！</p>\n<h1>When</h1>\n<p>From now on ...</p>\n<h1>Whole</h1>\n<p>我有一个简单，平凡 永远拥有 晴天 的小家:)</p>\n<ul>\n<li>我 -&gt; 昵称 “平凡”， 乐享 平凡 生活；</li>\n<li>她 -&gt; 昵称 “简单”， 追求 简单 思维；</li>\n<li>他 -&gt; 昵称。“晴天☀️”， 阳光☀️男孩，爱画画也爱跆拳道；</li>\n</ul>\n<p><img src=\"/about/index/family.jpg\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/about/index/ma.png\" alt></p>\n<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>我将以blog的方式分享自己对技术和生活的认知，包括大数据计算领域的知识，也包括我个人对现实生活和与人为事的行为认知！</p>\n<h1>Why</h1>\n<p>分享是最大的享受，予人成功是最大的成功，魅力不体现在你会什么，而着眼于你能让别人擅长什么，无欲无求但予人所求！</p>\n<h1>When</h1>\n<p>From now on ...</p>\n<h1>Whole</h1>\n<p>我有一个简单，平凡 永远拥有 晴天 的小家:)</p>\n<ul>\n<li>我 -&gt; 昵称 “平凡”， 乐享 平凡 生活；</li>\n<li>她 -&gt; 昵称 “简单”， 追求 简单 思维；</li>\n<li>他 -&gt; 昵称。“晴天☀️”， 阳光☀️男孩，爱画画也爱跆拳道；</li>\n</ul>\n<p><img src=\"/about/index/family.jpg\" alt></p>\n"},{"title":"categories","date":"2019-07-06T13:29:02.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-07-06 21:29:02\ntype: categories\ncomments: false\n---\n","updated":"2019-07-06T13:31:03.217Z","path":"categories/index.html","layout":"page","_id":"ck0emm5l3002ss94hb0knc2w6","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2019-07-06T13:31:59.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-07-06 21:31:59\ntype: tags\ncomments: false\n---\n","updated":"2019-07-06T13:32:15.658Z","path":"tags/index.html","layout":"page","_id":"ck0emm5l3002ts94hm1ylmca7","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Apache Flink 漫谈系列 - Fault Tolerance","date":"2019-01-01T10:18:18.000Z","updated":"2019-01-21T10:18:18.000Z","_content":"# 实际问题\n在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？\n在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。\n\n# 什么是Fault Tolerance\n容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。\n\n# 传统数据库Fault Tolerance\n我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下：\n\n* Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别...\n* Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别...\n* Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别...\n* Serializable  - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。\n\n主备复制，事物控制都是传统数据库容错的机制。\n\n# 流计算Fault Tolerance的挑战\n流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。\n\n# Apache Flink的Fault Tolerance 机制\nApache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/pdf/1506.08603.pdf) 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 [Determining-Global-States-of-a-Distributed-System Paper](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf)，该Paper描述了在分布式系统如何解决全局状态一致性问题。\n\n在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。\n\n# Checkpointing 的算法逻辑\n上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。\n\n![](40FE5816-DA56-4222-ADDE-1B023EE71BCF.png)\n\n上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点：\n\n* barrier 由source节点发出；\n* barrier会将流上event切分到不同的checkpoint中；\n* 汇聚到当前节点的多流的barrier要对齐；\n* barrier对齐之后会进行Checkpointing，生成snapshot；\n* 完成snapshot之后向下游发出barrier，继续直到Sink节点；\n\n这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图：\n\n![](66FB2539-A89C-4AE8-8639-F62F54B18E78.png)\n\n生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。\n\n# Checkpointing 的控制\n上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义）\n\n* checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式；\n* checkpointInterval - 检查点时间间隔，单位是毫秒。\n* checkpointTimeout - 检查点超时时间，单位毫秒。\n\n在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下：\n* ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。\n* ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。\n\n# Apache Flink 如何做到Exactly-once\n通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？\n\n## 语义\n* At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据）\n* Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复）\n\n从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。\n\n## 实现\n那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。\n\n我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图：\n\n![](B0D184CA-95DE-40C7-AA80-C4FE44134D16.png)\n\n上图示意了某个节点进行Checkpointing的过程：\n\n* 当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段；\n* 在进行对齐期间早到的input的数据会被缓存到buffer中；\n* 当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化；\n* 当完Checkpointing时候将barrier广播给下游Operator；\n\n多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。\n\n在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是：\n\n```\npublic interface CheckpointBarrierHandler {\n     ...\n     //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent\n     BufferOrEvent getNextNonBlocked() throws Exception;\n     ...\n}\n```\n其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下: \n\n* Exactly-Once模式 - BarrierBuffer\n    BarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。\n\nBarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。\n\n* At-Least-Once模式 - BarrierTracker\nBarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。\n\nBarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。\n\n如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。\n\n# 完整Job的Checkpointing过程\n在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？\n\n## 整体Checkpointing流程\n![](678508E9-D79F-438A-B236-17A942D31F73.png)\n\n上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。\n\n## Incremental Checkpointing\n对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中：\n![](F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png)\n\n由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。\n\n# 端到端exactly-once\n根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下：\n\n* 外部Source的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。\n\n* 外部Sink的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。\n\n# 小结\n本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。\n\n","source":"_posts/Apache Flink 漫谈系列 - Fault Tolerance.md","raw":"---\ntitle: Apache Flink 漫谈系列 - Fault Tolerance\ndate: 2019-01-01 18:18:18\nupdated: 2019-01-21 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 实际问题\n在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？\n在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。\n\n# 什么是Fault Tolerance\n容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。\n\n# 传统数据库Fault Tolerance\n我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下：\n\n* Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别...\n* Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别...\n* Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别...\n* Serializable  - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。\n\n主备复制，事物控制都是传统数据库容错的机制。\n\n# 流计算Fault Tolerance的挑战\n流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。\n\n# Apache Flink的Fault Tolerance 机制\nApache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/pdf/1506.08603.pdf) 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 [Determining-Global-States-of-a-Distributed-System Paper](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf)，该Paper描述了在分布式系统如何解决全局状态一致性问题。\n\n在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。\n\n# Checkpointing 的算法逻辑\n上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。\n\n![](40FE5816-DA56-4222-ADDE-1B023EE71BCF.png)\n\n上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点：\n\n* barrier 由source节点发出；\n* barrier会将流上event切分到不同的checkpoint中；\n* 汇聚到当前节点的多流的barrier要对齐；\n* barrier对齐之后会进行Checkpointing，生成snapshot；\n* 完成snapshot之后向下游发出barrier，继续直到Sink节点；\n\n这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图：\n\n![](66FB2539-A89C-4AE8-8639-F62F54B18E78.png)\n\n生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。\n\n# Checkpointing 的控制\n上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义）\n\n* checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式；\n* checkpointInterval - 检查点时间间隔，单位是毫秒。\n* checkpointTimeout - 检查点超时时间，单位毫秒。\n\n在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下：\n* ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。\n* ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。\n\n# Apache Flink 如何做到Exactly-once\n通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？\n\n## 语义\n* At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据）\n* Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复）\n\n从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。\n\n## 实现\n那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。\n\n我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图：\n\n![](B0D184CA-95DE-40C7-AA80-C4FE44134D16.png)\n\n上图示意了某个节点进行Checkpointing的过程：\n\n* 当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段；\n* 在进行对齐期间早到的input的数据会被缓存到buffer中；\n* 当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化；\n* 当完Checkpointing时候将barrier广播给下游Operator；\n\n多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。\n\n在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是：\n\n```\npublic interface CheckpointBarrierHandler {\n     ...\n     //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent\n     BufferOrEvent getNextNonBlocked() throws Exception;\n     ...\n}\n```\n其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下: \n\n* Exactly-Once模式 - BarrierBuffer\n    BarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。\n\nBarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。\n\n* At-Least-Once模式 - BarrierTracker\nBarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。\n\nBarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。\n\n如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。\n\n# 完整Job的Checkpointing过程\n在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？\n\n## 整体Checkpointing流程\n![](678508E9-D79F-438A-B236-17A942D31F73.png)\n\n上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。\n\n## Incremental Checkpointing\n对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中：\n![](F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png)\n\n由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。\n\n# 端到端exactly-once\n根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下：\n\n* 外部Source的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。\n\n* 外部Sink的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。\n\n# 小结\n本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。\n\n","slug":"Apache Flink 漫谈系列 - Fault Tolerance","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5f00001s94hhkr2qea5","content":"<h1>实际问题</h1>\n<p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？\n在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。</p>\n<h1>什么是Fault Tolerance</h1>\n<p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。</p>\n<h1>传统数据库Fault Tolerance</h1>\n<p>我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下：</p>\n<ul>\n<li>Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别...</li>\n<li>Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别...</li>\n<li>Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别...</li>\n<li>Serializable  - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</li>\n</ul>\n<p>主备复制，事物控制都是传统数据库容错的机制。</p>\n<h1>流计算Fault Tolerance的挑战</h1>\n<p>流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。</p>\n<h1>Apache Flink的Fault Tolerance 机制</h1>\n<p>Apache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 <a href=\"https://arxiv.org/pdf/1506.08603.pdf\" target=\"_blank\" rel=\"noopener\">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf\" target=\"_blank\" rel=\"noopener\">Determining-Global-States-of-a-Distributed-System Paper</a>，该Paper描述了在分布式系统如何解决全局状态一致性问题。</p>\n<p>在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。</p>\n<h1>Checkpointing 的算法逻辑</h1>\n<p>上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/40FE5816-DA56-4222-ADDE-1B023EE71BCF.png\" alt></p>\n<p>上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点：</p>\n<ul>\n<li>barrier 由source节点发出；</li>\n<li>barrier会将流上event切分到不同的checkpoint中；</li>\n<li>汇聚到当前节点的多流的barrier要对齐；</li>\n<li>barrier对齐之后会进行Checkpointing，生成snapshot；</li>\n<li>完成snapshot之后向下游发出barrier，继续直到Sink节点；</li>\n</ul>\n<p>这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/66FB2539-A89C-4AE8-8639-F62F54B18E78.png\" alt></p>\n<p>生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。</p>\n<h1>Checkpointing 的控制</h1>\n<p>上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义）</p>\n<ul>\n<li>checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式；</li>\n<li>checkpointInterval - 检查点时间间隔，单位是毫秒。</li>\n<li>checkpointTimeout - 检查点超时时间，单位毫秒。</li>\n</ul>\n<p>在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下：</p>\n<ul>\n<li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。</li>\n<li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。</li>\n</ul>\n<h1>Apache Flink 如何做到Exactly-once</h1>\n<p>通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？</p>\n<h2>语义</h2>\n<ul>\n<li>At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据）</li>\n<li>Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复）</li>\n</ul>\n<p>从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。</p>\n<h2>实现</h2>\n<p>那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。</p>\n<p>我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/B0D184CA-95DE-40C7-AA80-C4FE44134D16.png\" alt></p>\n<p>上图示意了某个节点进行Checkpointing的过程：</p>\n<ul>\n<li>当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段；</li>\n<li>在进行对齐期间早到的input的数据会被缓存到buffer中；</li>\n<li>当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化；</li>\n<li>当完Checkpointing时候将barrier广播给下游Operator；</li>\n</ul>\n<p>多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。</p>\n<p>在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface CheckpointBarrierHandler &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent</span><br><span class=\"line\">     BufferOrEvent getNextNonBlocked() throws Exception;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下:</p>\n<ul>\n<li>Exactly-Once模式 - BarrierBuffer\nBarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。</li>\n</ul>\n<p>BarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。</p>\n<ul>\n<li>At-Least-Once模式 - BarrierTracker\nBarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。</li>\n</ul>\n<p>BarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。</p>\n<p>如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。</p>\n<h1>完整Job的Checkpointing过程</h1>\n<p>在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？</p>\n<h2>整体Checkpointing流程</h2>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/678508E9-D79F-438A-B236-17A942D31F73.png\" alt></p>\n<p>上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。</p>\n<h2>Incremental Checkpointing</h2>\n<p>对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png\" alt></p>\n<p>由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。</p>\n<h1>端到端exactly-once</h1>\n<p>根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下：</p>\n<ul>\n<li>\n<p>外部Source的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。</p>\n</li>\n<li>\n<p>外部Sink的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>\n</li>\n</ul>\n<h1>小结</h1>\n<p>本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>实际问题</h1>\n<p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？\n在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。</p>\n<h1>什么是Fault Tolerance</h1>\n<p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。</p>\n<h1>传统数据库Fault Tolerance</h1>\n<p>我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下：</p>\n<ul>\n<li>Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别...</li>\n<li>Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别...</li>\n<li>Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别...</li>\n<li>Serializable  - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</li>\n</ul>\n<p>主备复制，事物控制都是传统数据库容错的机制。</p>\n<h1>流计算Fault Tolerance的挑战</h1>\n<p>流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。</p>\n<h1>Apache Flink的Fault Tolerance 机制</h1>\n<p>Apache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 <a href=\"https://arxiv.org/pdf/1506.08603.pdf\" target=\"_blank\" rel=\"noopener\">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf\" target=\"_blank\" rel=\"noopener\">Determining-Global-States-of-a-Distributed-System Paper</a>，该Paper描述了在分布式系统如何解决全局状态一致性问题。</p>\n<p>在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。</p>\n<h1>Checkpointing 的算法逻辑</h1>\n<p>上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/40FE5816-DA56-4222-ADDE-1B023EE71BCF.png\" alt></p>\n<p>上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点：</p>\n<ul>\n<li>barrier 由source节点发出；</li>\n<li>barrier会将流上event切分到不同的checkpoint中；</li>\n<li>汇聚到当前节点的多流的barrier要对齐；</li>\n<li>barrier对齐之后会进行Checkpointing，生成snapshot；</li>\n<li>完成snapshot之后向下游发出barrier，继续直到Sink节点；</li>\n</ul>\n<p>这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/66FB2539-A89C-4AE8-8639-F62F54B18E78.png\" alt></p>\n<p>生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。</p>\n<h1>Checkpointing 的控制</h1>\n<p>上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义）</p>\n<ul>\n<li>checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式；</li>\n<li>checkpointInterval - 检查点时间间隔，单位是毫秒。</li>\n<li>checkpointTimeout - 检查点超时时间，单位毫秒。</li>\n</ul>\n<p>在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下：</p>\n<ul>\n<li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。</li>\n<li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。</li>\n</ul>\n<h1>Apache Flink 如何做到Exactly-once</h1>\n<p>通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？</p>\n<h2>语义</h2>\n<ul>\n<li>At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据）</li>\n<li>Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复）</li>\n</ul>\n<p>从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。</p>\n<h2>实现</h2>\n<p>那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。</p>\n<p>我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/B0D184CA-95DE-40C7-AA80-C4FE44134D16.png\" alt></p>\n<p>上图示意了某个节点进行Checkpointing的过程：</p>\n<ul>\n<li>当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段；</li>\n<li>在进行对齐期间早到的input的数据会被缓存到buffer中；</li>\n<li>当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化；</li>\n<li>当完Checkpointing时候将barrier广播给下游Operator；</li>\n</ul>\n<p>多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。</p>\n<p>在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface CheckpointBarrierHandler &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent</span><br><span class=\"line\">     BufferOrEvent getNextNonBlocked() throws Exception;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下:</p>\n<ul>\n<li>Exactly-Once模式 - BarrierBuffer\nBarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。</li>\n</ul>\n<p>BarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。</p>\n<ul>\n<li>At-Least-Once模式 - BarrierTracker\nBarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。</li>\n</ul>\n<p>BarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。</p>\n<p>如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。</p>\n<h1>完整Job的Checkpointing过程</h1>\n<p>在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？</p>\n<h2>整体Checkpointing流程</h2>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/678508E9-D79F-438A-B236-17A942D31F73.png\" alt></p>\n<p>上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。</p>\n<h2>Incremental Checkpointing</h2>\n<p>对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png\" alt></p>\n<p>由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。</p>\n<h1>端到端exactly-once</h1>\n<p>根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下：</p>\n<ul>\n<li>\n<p>外部Source的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。</p>\n</li>\n<li>\n<p>外部Sink的容错要求\nApache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>\n</li>\n</ul>\n<h1>小结</h1>\n<p>本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。</p>\n"},{"title":"Apache Flink 漫谈系列 - JOIN LATERAL","date":"2019-03-11T10:18:18.000Z","_content":"# 聊什么\n上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。\n\n如下图所示：\n\n![](1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png)\n\n本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。\n\n# 实际问题\n假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下：\n* Customers\n```\nCREATE TABLE Customers (\n  customerid char(5) NOT NULL,\n  city varchar (10) NOT NULL\n)\n\ninsert into Customers values('C001','Beijing');\ninsert into Customers values('C002','Beijing');\ninsert into Customers values('C003','Beijing');\ninsert into Customers values('C004','HangZhou');\n```\n\n查看数据：\n![](3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png)\n\n* Orders\n```\nCREATE TABLE Orders(\n  orderid char(5) NOT NULL,\n  customerid char(5) NULL\n)\n\ninsert into Orders values('O001','C001');\ninsert into Orders values('O002','C001');\ninsert into Orders values('O003','C003');\ninsert into Orders values('O004','C001');\n```\n查看数据：\n![](5DA82C35-6E03-43AD-B695-35C85D85D641.png)\n\n## 问题示例\n假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是：\n![](E3709EBB-89AE-44EA-98C7-06A2794597D9.png)\n### 用INNER JOIN解决\n如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下：\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c JOIN Orders o \n\tON o.customerid = c.customerid\n```\n查询结果如下：\n![](ED960882-39B0-40FD-9400-60D0D66F366E.png)\n\n但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。\n\n## 用 Correlated subquery解决\nCorrelated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。\n* WHERE Clause\n用WHERE Clause实现上面的查询需求，SQL如下：\n```\nSELECT \n    c.customerid, c.city\nFROM Customers c WHERE c.customerid IN (\n    SELECT \n      o.customerid, o.orderid\n    FROM Orders o\n    WHERE o.customerid = c.customerid\n)\n```\n执行情况：\n![](28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png)\n\n上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对`o.orderid`进行投影, 上面查询我为什么要加一个`o.orderid`呢，因为查询需求是需要`o.orderid`的，去掉`o.orderid`查询能成功，但是拿到的结果并不是我们想要的，如下：\n```\nSELECT \n    c.customerid, c.city\nFROM Customers c WHERE c.customerid IN (\n    SELECT \n      o.customerid\n    FROM Orders o\n    WHERE o.customerid = c.customerid\n)\n```\n查询结果：\n![](D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png)\n\n可见上面查询结果缺少了`o.orderid`,不能满足我们的查询需求。\n\n* FROM Clause\n用FROM Clause实现上面的查询需求，SQL如下：\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c, (\n    SELECT \n      o.orderid, o.customerid \n    FROM Orders o\n    WHERE o.customerid = c.customerid\n) as o\n```\n我们会得到如下错误：\n![](37259B89-DD8F-4349-A378-000AAA886479.png)\n错误信息提示我们无法识别`c.customerid`。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，\n那么上面的查询需求除了`INNER JOIN` 我们还可以如何解决呢？\n\n\n# JOIN LATERAL\n我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了`LATERAL`关键字，也叫做 `lateral drive table`。\n\n## CROSS APPLY和LATERAL\n上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 `LATERAL` 的呢？SQL Server是用自己的方言 `CROSS APPLY` 来支持的。那么为啥不用ANSI-SQL的`LATERAL`而用`CROSS APPLY`呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时`LATERAL`是SQL2003引入的，而`CROSS APPLY`是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等`LATERAL`出来的时候，`CROSS APPLY`在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了`CROSS APPLY`，但`CROSS APPLY`的语义与`LATERAL`却完全一致，同时后续支持`LATERAL`的Oracle12和PostgreSQL94同时支持了`LATERAL`和`CROSS APPLY`。\n\n## 问题解决\n\n那么我们回到上面的问题，我们用SQL Server的`CROSS APPLY`来解决上面问题，SQL如下：\n![](42B8EBC9-CD5B-498C-B191-F4241C91483F.png)\n\n上面得到的结果完全满足查询需求。\n\n# JOIN LATERAL 与 INNER JOIN 关系\n上面的查询需求并没有体现`JOIN LATERAL`和`INNER JOIN`的区别，我们还是以SQL Server中两个查询执行Plan来观察一下：\n![](A3A08123-0490-41BA-9E07-95B4557A19F5.png)\n\n上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个`LATERAL` 出来呢？\n## 性能方面\n我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。\n* LATERAL 的写法\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c CROSS APPLY (\n    SELECT \n     TOP(1) o.orderid, o.customerid \n    FROM Orders o \n    WHERE o.customerid = c.customerid\n\tORDER BY o.customerid, o.orderid\n) as o\n```\n查询结果：\n![](C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png)\n我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。\n* JOIN 写法\n```\nSELECT  c.customerid, c.city, o.orderid\n FROM    Customers c\n  JOIN (\n   SELECT \n     o2.*, \n\t ROW_NUMBER() OVER (\n\t\tPARTITION BY customerid \n\t\tORDER BY orderid\n\t  ) AS rn\n   FROM    Orders o2\n ) o\nON c.customerid = o.customerid AND o.rn = 1\n```\n查询结果：\n![](90BC2538-6A12-4F5F-AE42-B87BA5C12871.png)\n\n如上我们都完成了查询需求，我们在来看一下执行Plan，如下：\n![](32183E30-E096-46DE-A855-932377D70BFE.png)\n我们直观发现完成相同功能，使用`CROSS APPLY`进行查询，执行Plan简单许多。\n\n## 功能方面\n在功能方面`INNER JOIN`本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入`CROSS APPLY`的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例：\n```\nSELECT \n   name, log_backup_time \nFROM sys.databases AS s\n CROSS APPLY sys.dm_db_log_stats(s.database_id); \n```\n查询结果：\n![](C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png)\n\n\n# Apache Flink对 LATERAL的支持\n\n前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持`LATERAL`的，接下来我们看看Apache Flink对`LATERAL`的支持情况。\n\n## Calcite \nApache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持`LATERAL`语法，示例如下：\n```\nSELECT \n    e.NAME, e.DEPTNO, d.NAME \nFROM EMPS e, LATERAL (\n    SELECT \n    *\n    FORM DEPTS d \n    WHERE e.DEPTNO=d.DEPTNO\n ) as d;\n```\n查询结果：\n![](DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png)\n我使用的是Calcite官方自带测试数据。\n\n## Flink\n截止到Flink-1.6.2，Apache Flink 中有两种场景使用`LATERAL`，如下：\n* UDTF(TVF) - User-defined Table Funciton \n* Temporal Table - 涉及内容会在后续篇章单独介绍。\n\n本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持`LATERAL`。\n### UDTF\nUDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下：\n* 基类\n```\n/**\n  * Base class for all user-defined functions such as scalar functions, table functions,\n  * or aggregation functions.\n  */\nabstract class UserDefinedFunction extends Serializable {\n  // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍）\n  def open(context: FunctionContext): Unit = {}\n  def close(): Unit = {}\n}\n```\n* TableFunction\n```\n/**\n  * Base class for a user-defined table function (UDTF). A user-defined table functions works on\n  * zero, one, or multiple scalar values as input and returns multiple rows as output.\n  *\n  * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation\n  * method. An evaluation method must be declared publicly, not static and named \"eval\".\n  * Evaluation methods can also be overloaded by implementing multiple methods named \"eval\".\n  *\n  * User-defined functions must have a default constructor and must be instantiable during runtime.\n  *\n  * By default the result type of an evaluation method is determined by Flink's type extraction\n  * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more\n  * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type\n  * can be manually defined by overriding [[getResultType()]].\n  */\nabstract class TableFunction[T] extends UserDefinedFunction {\n\n  // 对于泛型T，如果是基础类型那么Flink框架可以自动识别，\n  // 对于用户自定义的复杂对象，需要用户overwrite这个实现。\n  def getResultType: TypeInformation[T] = null\n}\n\n```\n上面定义的核心是要求用户实现`eval`方法，我们写一个具体示例。\n* 示例\n```\n// 定义一个简单的UDTF返回类型，对应接口上的 T \ncase class SimpleUser(name: String, age: Int)\n// 继承TableFunction，并实现evale方法\n// 核心功能是解析以#分割的字符串\nclass SplitTVF extends TableFunction[SimpleUser] {\n  // make sure input element's format is \"<string>#<int>\"\n  def eval(user: String): Unit = {\n    if (user.contains(\"#\")) {\n      val splits = user.split(\"#\")\n      collect(SimpleUser(splits(0), splits(1).toInt))\n    }\n  }\n}\n```\n\n### 示例(完整的ITCase)：\n* 测试数据\n我们构造一个只包含一个data字段的用户表，用户表数据如下：\n\n|  data| \n| --- |\n| Sunny#8 |\n|  Kevin#36| \n| Panpan#36 |\n* 查询需求\n查询的需求是将data字段flatten成为name和age两个字段的表，期望得到：\n\n| name | age |\n| --- | --- |\n| Sunny | 8 |\n| Kevin |36  |\n| Panpan |  36|\n\n* 查询示例\n我们以ITCase方式完成如上查询需求，完整代码如下：\n```\n@Test\n  def testLateralTVF(): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setStateBackend(getStateBackend)\n    StreamITCase.clear\n\n    val userData = new mutable.MutableList[(String)]\n    userData.+=((\"Sunny#8\"))\n    userData.+=((\"Kevin#36\"))\n    userData.+=((\"Panpan#36\"))\n\n    val SQLQuery = \"SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)\"\n\n    val users = env.fromCollection(userData).toTable(tEnv, 'data)\n\n    val tvf = new SplitTVF()\n    tEnv.registerTable(\"userTab\", users)\n    tEnv.registerFunction(\"splitTVF\", tvf)\n\n    val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row]\n    result.addSink(new StreamITCase.StringSink[Row])\n    env.execute()\n    StreamITCase.testResults.foreach(println(_))\n  }\n```\n运行结果：\n![](645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png)\n\n上面的核心语句是：\n```\nval SQLQuery = \"SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)\"\n```\n如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。\n\n# 小结\n本篇重点向大家介绍了一种新的`JOIN`类型 - `JOIN LATERAL`。并向大家介绍了SQL Server中对`LATERAL`的支持方式，详细分析了`JOIN LATERAL`和`INNER JOIN`的区别与联系，最后切入到Apache Flink中，以`UDTF`示例说明了Apache Flink中对`JOIN LATERAL`的支持，后续篇章会介绍Apache Flink中另一种使用`LATERAL`的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见!\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","source":"_posts/Apache Flink 漫谈系列 - JOIN LATERAL.md","raw":"---\ntitle: Apache Flink 漫谈系列 - JOIN LATERAL\ndate: 2019-03-11 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 聊什么\n上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。\n\n如下图所示：\n\n![](1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png)\n\n本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。\n\n# 实际问题\n假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下：\n* Customers\n```\nCREATE TABLE Customers (\n  customerid char(5) NOT NULL,\n  city varchar (10) NOT NULL\n)\n\ninsert into Customers values('C001','Beijing');\ninsert into Customers values('C002','Beijing');\ninsert into Customers values('C003','Beijing');\ninsert into Customers values('C004','HangZhou');\n```\n\n查看数据：\n![](3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png)\n\n* Orders\n```\nCREATE TABLE Orders(\n  orderid char(5) NOT NULL,\n  customerid char(5) NULL\n)\n\ninsert into Orders values('O001','C001');\ninsert into Orders values('O002','C001');\ninsert into Orders values('O003','C003');\ninsert into Orders values('O004','C001');\n```\n查看数据：\n![](5DA82C35-6E03-43AD-B695-35C85D85D641.png)\n\n## 问题示例\n假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是：\n![](E3709EBB-89AE-44EA-98C7-06A2794597D9.png)\n### 用INNER JOIN解决\n如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下：\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c JOIN Orders o \n\tON o.customerid = c.customerid\n```\n查询结果如下：\n![](ED960882-39B0-40FD-9400-60D0D66F366E.png)\n\n但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。\n\n## 用 Correlated subquery解决\nCorrelated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。\n* WHERE Clause\n用WHERE Clause实现上面的查询需求，SQL如下：\n```\nSELECT \n    c.customerid, c.city\nFROM Customers c WHERE c.customerid IN (\n    SELECT \n      o.customerid, o.orderid\n    FROM Orders o\n    WHERE o.customerid = c.customerid\n)\n```\n执行情况：\n![](28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png)\n\n上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对`o.orderid`进行投影, 上面查询我为什么要加一个`o.orderid`呢，因为查询需求是需要`o.orderid`的，去掉`o.orderid`查询能成功，但是拿到的结果并不是我们想要的，如下：\n```\nSELECT \n    c.customerid, c.city\nFROM Customers c WHERE c.customerid IN (\n    SELECT \n      o.customerid\n    FROM Orders o\n    WHERE o.customerid = c.customerid\n)\n```\n查询结果：\n![](D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png)\n\n可见上面查询结果缺少了`o.orderid`,不能满足我们的查询需求。\n\n* FROM Clause\n用FROM Clause实现上面的查询需求，SQL如下：\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c, (\n    SELECT \n      o.orderid, o.customerid \n    FROM Orders o\n    WHERE o.customerid = c.customerid\n) as o\n```\n我们会得到如下错误：\n![](37259B89-DD8F-4349-A378-000AAA886479.png)\n错误信息提示我们无法识别`c.customerid`。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，\n那么上面的查询需求除了`INNER JOIN` 我们还可以如何解决呢？\n\n\n# JOIN LATERAL\n我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了`LATERAL`关键字，也叫做 `lateral drive table`。\n\n## CROSS APPLY和LATERAL\n上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 `LATERAL` 的呢？SQL Server是用自己的方言 `CROSS APPLY` 来支持的。那么为啥不用ANSI-SQL的`LATERAL`而用`CROSS APPLY`呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时`LATERAL`是SQL2003引入的，而`CROSS APPLY`是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等`LATERAL`出来的时候，`CROSS APPLY`在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了`CROSS APPLY`，但`CROSS APPLY`的语义与`LATERAL`却完全一致，同时后续支持`LATERAL`的Oracle12和PostgreSQL94同时支持了`LATERAL`和`CROSS APPLY`。\n\n## 问题解决\n\n那么我们回到上面的问题，我们用SQL Server的`CROSS APPLY`来解决上面问题，SQL如下：\n![](42B8EBC9-CD5B-498C-B191-F4241C91483F.png)\n\n上面得到的结果完全满足查询需求。\n\n# JOIN LATERAL 与 INNER JOIN 关系\n上面的查询需求并没有体现`JOIN LATERAL`和`INNER JOIN`的区别，我们还是以SQL Server中两个查询执行Plan来观察一下：\n![](A3A08123-0490-41BA-9E07-95B4557A19F5.png)\n\n上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个`LATERAL` 出来呢？\n## 性能方面\n我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。\n* LATERAL 的写法\n```\nSELECT \n    c.customerid, c.city, o.orderid \nFROM Customers c CROSS APPLY (\n    SELECT \n     TOP(1) o.orderid, o.customerid \n    FROM Orders o \n    WHERE o.customerid = c.customerid\n\tORDER BY o.customerid, o.orderid\n) as o\n```\n查询结果：\n![](C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png)\n我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。\n* JOIN 写法\n```\nSELECT  c.customerid, c.city, o.orderid\n FROM    Customers c\n  JOIN (\n   SELECT \n     o2.*, \n\t ROW_NUMBER() OVER (\n\t\tPARTITION BY customerid \n\t\tORDER BY orderid\n\t  ) AS rn\n   FROM    Orders o2\n ) o\nON c.customerid = o.customerid AND o.rn = 1\n```\n查询结果：\n![](90BC2538-6A12-4F5F-AE42-B87BA5C12871.png)\n\n如上我们都完成了查询需求，我们在来看一下执行Plan，如下：\n![](32183E30-E096-46DE-A855-932377D70BFE.png)\n我们直观发现完成相同功能，使用`CROSS APPLY`进行查询，执行Plan简单许多。\n\n## 功能方面\n在功能方面`INNER JOIN`本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入`CROSS APPLY`的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例：\n```\nSELECT \n   name, log_backup_time \nFROM sys.databases AS s\n CROSS APPLY sys.dm_db_log_stats(s.database_id); \n```\n查询结果：\n![](C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png)\n\n\n# Apache Flink对 LATERAL的支持\n\n前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持`LATERAL`的，接下来我们看看Apache Flink对`LATERAL`的支持情况。\n\n## Calcite \nApache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持`LATERAL`语法，示例如下：\n```\nSELECT \n    e.NAME, e.DEPTNO, d.NAME \nFROM EMPS e, LATERAL (\n    SELECT \n    *\n    FORM DEPTS d \n    WHERE e.DEPTNO=d.DEPTNO\n ) as d;\n```\n查询结果：\n![](DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png)\n我使用的是Calcite官方自带测试数据。\n\n## Flink\n截止到Flink-1.6.2，Apache Flink 中有两种场景使用`LATERAL`，如下：\n* UDTF(TVF) - User-defined Table Funciton \n* Temporal Table - 涉及内容会在后续篇章单独介绍。\n\n本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持`LATERAL`。\n### UDTF\nUDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下：\n* 基类\n```\n/**\n  * Base class for all user-defined functions such as scalar functions, table functions,\n  * or aggregation functions.\n  */\nabstract class UserDefinedFunction extends Serializable {\n  // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍）\n  def open(context: FunctionContext): Unit = {}\n  def close(): Unit = {}\n}\n```\n* TableFunction\n```\n/**\n  * Base class for a user-defined table function (UDTF). A user-defined table functions works on\n  * zero, one, or multiple scalar values as input and returns multiple rows as output.\n  *\n  * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation\n  * method. An evaluation method must be declared publicly, not static and named \"eval\".\n  * Evaluation methods can also be overloaded by implementing multiple methods named \"eval\".\n  *\n  * User-defined functions must have a default constructor and must be instantiable during runtime.\n  *\n  * By default the result type of an evaluation method is determined by Flink's type extraction\n  * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more\n  * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type\n  * can be manually defined by overriding [[getResultType()]].\n  */\nabstract class TableFunction[T] extends UserDefinedFunction {\n\n  // 对于泛型T，如果是基础类型那么Flink框架可以自动识别，\n  // 对于用户自定义的复杂对象，需要用户overwrite这个实现。\n  def getResultType: TypeInformation[T] = null\n}\n\n```\n上面定义的核心是要求用户实现`eval`方法，我们写一个具体示例。\n* 示例\n```\n// 定义一个简单的UDTF返回类型，对应接口上的 T \ncase class SimpleUser(name: String, age: Int)\n// 继承TableFunction，并实现evale方法\n// 核心功能是解析以#分割的字符串\nclass SplitTVF extends TableFunction[SimpleUser] {\n  // make sure input element's format is \"<string>#<int>\"\n  def eval(user: String): Unit = {\n    if (user.contains(\"#\")) {\n      val splits = user.split(\"#\")\n      collect(SimpleUser(splits(0), splits(1).toInt))\n    }\n  }\n}\n```\n\n### 示例(完整的ITCase)：\n* 测试数据\n我们构造一个只包含一个data字段的用户表，用户表数据如下：\n\n|  data| \n| --- |\n| Sunny#8 |\n|  Kevin#36| \n| Panpan#36 |\n* 查询需求\n查询的需求是将data字段flatten成为name和age两个字段的表，期望得到：\n\n| name | age |\n| --- | --- |\n| Sunny | 8 |\n| Kevin |36  |\n| Panpan |  36|\n\n* 查询示例\n我们以ITCase方式完成如上查询需求，完整代码如下：\n```\n@Test\n  def testLateralTVF(): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setStateBackend(getStateBackend)\n    StreamITCase.clear\n\n    val userData = new mutable.MutableList[(String)]\n    userData.+=((\"Sunny#8\"))\n    userData.+=((\"Kevin#36\"))\n    userData.+=((\"Panpan#36\"))\n\n    val SQLQuery = \"SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)\"\n\n    val users = env.fromCollection(userData).toTable(tEnv, 'data)\n\n    val tvf = new SplitTVF()\n    tEnv.registerTable(\"userTab\", users)\n    tEnv.registerFunction(\"splitTVF\", tvf)\n\n    val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row]\n    result.addSink(new StreamITCase.StringSink[Row])\n    env.execute()\n    StreamITCase.testResults.foreach(println(_))\n  }\n```\n运行结果：\n![](645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png)\n\n上面的核心语句是：\n```\nval SQLQuery = \"SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)\"\n```\n如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。\n\n# 小结\n本篇重点向大家介绍了一种新的`JOIN`类型 - `JOIN LATERAL`。并向大家介绍了SQL Server中对`LATERAL`的支持方式，详细分析了`JOIN LATERAL`和`INNER JOIN`的区别与联系，最后切入到Apache Flink中，以`UDTF`示例说明了Apache Flink中对`JOIN LATERAL`的支持，后续篇章会介绍Apache Flink中另一种使用`LATERAL`的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见!\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","slug":"Apache Flink 漫谈系列 - JOIN LATERAL","published":1,"updated":"2019-07-13T13:09:34.845Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5f50003s94hsw6mkmgp","content":"<h1>聊什么</h1>\n<p>上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。</p>\n<p>如下图所示：</p>\n<p><img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png\" alt></p>\n<p>本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。</p>\n<h1>实际问题</h1>\n<p>假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下：</p>\n<ul>\n<li>Customers\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Customers (</span><br><span class=\"line\">  customerid char(5) NOT NULL,</span><br><span class=\"line\">  city varchar (10) NOT NULL</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">insert into Customers values(&apos;C001&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C002&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C003&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C004&apos;,&apos;HangZhou&apos;);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查看数据：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png\" alt></p>\n<ul>\n<li>Orders\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Orders(</span><br><span class=\"line\">  orderid char(5) NOT NULL,</span><br><span class=\"line\">  customerid char(5) NULL</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">insert into Orders values(&apos;O001&apos;,&apos;C001&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O002&apos;,&apos;C001&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O003&apos;,&apos;C003&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O004&apos;,&apos;C001&apos;);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查看数据：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/5DA82C35-6E03-43AD-B695-35C85D85D641.png\" alt></p>\n<h2>问题示例</h2>\n<p>假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/E3709EBB-89AE-44EA-98C7-06A2794597D9.png\" alt></p>\n<h3>用INNER JOIN解决</h3>\n<p>如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c JOIN Orders o </span><br><span class=\"line\">\tON o.customerid = c.customerid</span><br></pre></td></tr></table></figure></p>\n<p>查询结果如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/ED960882-39B0-40FD-9400-60D0D66F366E.png\" alt></p>\n<p>但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。</p>\n<h2>用 Correlated subquery解决</h2>\n<p>Correlated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。</p>\n<ul>\n<li>WHERE Clause\n用WHERE Clause实现上面的查询需求，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city</span><br><span class=\"line\">FROM Customers c WHERE c.customerid IN (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.customerid, o.orderid</span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行情况：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png\" alt></p>\n<p>上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对<code>o.orderid</code>进行投影, 上面查询我为什么要加一个<code>o.orderid</code>呢，因为查询需求是需要<code>o.orderid</code>的，去掉<code>o.orderid</code>查询能成功，但是拿到的结果并不是我们想要的，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city</span><br><span class=\"line\">FROM Customers c WHERE c.customerid IN (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.customerid</span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png\" alt></p>\n<p>可见上面查询结果缺少了<code>o.orderid</code>,不能满足我们的查询需求。</p>\n<ul>\n<li>FROM Clause\n用FROM Clause实现上面的查询需求，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c, (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.orderid, o.customerid </span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">) as o</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>我们会得到如下错误：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/37259B89-DD8F-4349-A378-000AAA886479.png\" alt>\n错误信息提示我们无法识别<code>c.customerid</code>。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，\n那么上面的查询需求除了<code>INNER JOIN</code> 我们还可以如何解决呢？</p>\n<h1>JOIN LATERAL</h1>\n<p>我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了<code>LATERAL</code>关键字，也叫做 <code>lateral drive table</code>。</p>\n<h2>CROSS APPLY和LATERAL</h2>\n<p>上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 <code>LATERAL</code> 的呢？SQL Server是用自己的方言 <code>CROSS APPLY</code> 来支持的。那么为啥不用ANSI-SQL的<code>LATERAL</code>而用<code>CROSS APPLY</code>呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时<code>LATERAL</code>是SQL2003引入的，而<code>CROSS APPLY</code>是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等<code>LATERAL</code>出来的时候，<code>CROSS APPLY</code>在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了<code>CROSS APPLY</code>，但<code>CROSS APPLY</code>的语义与<code>LATERAL</code>却完全一致，同时后续支持<code>LATERAL</code>的Oracle12和PostgreSQL94同时支持了<code>LATERAL</code>和<code>CROSS APPLY</code>。</p>\n<h2>问题解决</h2>\n<p>那么我们回到上面的问题，我们用SQL Server的<code>CROSS APPLY</code>来解决上面问题，SQL如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/42B8EBC9-CD5B-498C-B191-F4241C91483F.png\" alt></p>\n<p>上面得到的结果完全满足查询需求。</p>\n<h1>JOIN LATERAL 与 INNER JOIN 关系</h1>\n<p>上面的查询需求并没有体现<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别，我们还是以SQL Server中两个查询执行Plan来观察一下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/A3A08123-0490-41BA-9E07-95B4557A19F5.png\" alt></p>\n<p>上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个<code>LATERAL</code> 出来呢？</p>\n<h2>性能方面</h2>\n<p>我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。</p>\n<ul>\n<li>LATERAL 的写法\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c CROSS APPLY (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">     TOP(1) o.orderid, o.customerid </span><br><span class=\"line\">    FROM Orders o </span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">\tORDER BY o.customerid, o.orderid</span><br><span class=\"line\">) as o</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png\" alt>\n我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。</p>\n<ul>\n<li>JOIN 写法\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT  c.customerid, c.city, o.orderid</span><br><span class=\"line\"> FROM    Customers c</span><br><span class=\"line\">  JOIN (</span><br><span class=\"line\">   SELECT </span><br><span class=\"line\">     o2.*, </span><br><span class=\"line\">\t ROW_NUMBER() OVER (</span><br><span class=\"line\">\t\tPARTITION BY customerid </span><br><span class=\"line\">\t\tORDER BY orderid</span><br><span class=\"line\">\t  ) AS rn</span><br><span class=\"line\">   FROM    Orders o2</span><br><span class=\"line\"> ) o</span><br><span class=\"line\">ON c.customerid = o.customerid AND o.rn = 1</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/90BC2538-6A12-4F5F-AE42-B87BA5C12871.png\" alt></p>\n<p>如上我们都完成了查询需求，我们在来看一下执行Plan，如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/32183E30-E096-46DE-A855-932377D70BFE.png\" alt>\n我们直观发现完成相同功能，使用<code>CROSS APPLY</code>进行查询，执行Plan简单许多。</p>\n<h2>功能方面</h2>\n<p>在功能方面<code>INNER JOIN</code>本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入<code>CROSS APPLY</code>的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">   name, log_backup_time </span><br><span class=\"line\">FROM sys.databases AS s</span><br><span class=\"line\"> CROSS APPLY sys.dm_db_log_stats(s.database_id);</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png\" alt></p>\n<h1>Apache Flink对 LATERAL的支持</h1>\n<p>前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持<code>LATERAL</code>的，接下来我们看看Apache Flink对<code>LATERAL</code>的支持情况。</p>\n<h2>Calcite</h2>\n<p>Apache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持<code>LATERAL</code>语法，示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    e.NAME, e.DEPTNO, d.NAME </span><br><span class=\"line\">FROM EMPS e, LATERAL (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    *</span><br><span class=\"line\">    FORM DEPTS d </span><br><span class=\"line\">    WHERE e.DEPTNO=d.DEPTNO</span><br><span class=\"line\"> ) as d;</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png\" alt>\n我使用的是Calcite官方自带测试数据。</p>\n<h2>Flink</h2>\n<p>截止到Flink-1.6.2，Apache Flink 中有两种场景使用<code>LATERAL</code>，如下：</p>\n<ul>\n<li>UDTF(TVF) - User-defined Table Funciton</li>\n<li>Temporal Table - 涉及内容会在后续篇章单独介绍。</li>\n</ul>\n<p>本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持<code>LATERAL</code>。</p>\n<h3>UDTF</h3>\n<p>UDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下：</p>\n<ul>\n<li>\n<p>基类\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">  * Base class for all user-defined functions such as scalar functions, table functions,</span><br><span class=\"line\">  * or aggregation functions.</span><br><span class=\"line\">  */</span><br><span class=\"line\">abstract class UserDefinedFunction extends Serializable &#123;</span><br><span class=\"line\">  // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍）</span><br><span class=\"line\">  def open(context: FunctionContext): Unit = &#123;&#125;</span><br><span class=\"line\">  def close(): Unit = &#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>TableFunction\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">  * Base class for a user-defined table function (UDTF). A user-defined table functions works on</span><br><span class=\"line\">  * zero, one, or multiple scalar values as input and returns multiple rows as output.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation</span><br><span class=\"line\">  * method. An evaluation method must be declared publicly, not static and named &quot;eval&quot;.</span><br><span class=\"line\">  * Evaluation methods can also be overloaded by implementing multiple methods named &quot;eval&quot;.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * User-defined functions must have a default constructor and must be instantiable during runtime.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * By default the result type of an evaluation method is determined by Flink&apos;s type extraction</span><br><span class=\"line\">  * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more</span><br><span class=\"line\">  * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type</span><br><span class=\"line\">  * can be manually defined by overriding [[getResultType()]].</span><br><span class=\"line\">  */</span><br><span class=\"line\">abstract class TableFunction[T] extends UserDefinedFunction &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  // 对于泛型T，如果是基础类型那么Flink框架可以自动识别，</span><br><span class=\"line\">  // 对于用户自定义的复杂对象，需要用户overwrite这个实现。</span><br><span class=\"line\">  def getResultType: TypeInformation[T] = null</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>上面定义的核心是要求用户实现<code>eval</code>方法，我们写一个具体示例。</p>\n<ul>\n<li>示例\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 定义一个简单的UDTF返回类型，对应接口上的 T </span><br><span class=\"line\">case class SimpleUser(name: String, age: Int)</span><br><span class=\"line\">// 继承TableFunction，并实现evale方法</span><br><span class=\"line\">// 核心功能是解析以#分割的字符串</span><br><span class=\"line\">class SplitTVF extends TableFunction[SimpleUser] &#123;</span><br><span class=\"line\">  // make sure input element&apos;s format is &quot;&lt;string&gt;#&lt;int&gt;&quot;</span><br><span class=\"line\">  def eval(user: String): Unit = &#123;</span><br><span class=\"line\">    if (user.contains(&quot;#&quot;)) &#123;</span><br><span class=\"line\">      val splits = user.split(&quot;#&quot;)</span><br><span class=\"line\">      collect(SimpleUser(splits(0), splits(1).toInt))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3>示例(完整的ITCase)：</h3>\n<ul>\n<li>测试数据\n我们构造一个只包含一个data字段的用户表，用户表数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>data</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sunny#8</td>\n</tr>\n<tr>\n<td>Kevin#36</td>\n</tr>\n<tr>\n<td>Panpan#36</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查询需求\n查询的需求是将data字段flatten成为name和age两个字段的表，期望得到：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>name</th>\n<th>age</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sunny</td>\n<td>8</td>\n</tr>\n<tr>\n<td>Kevin</td>\n<td>36</td>\n</tr>\n<tr>\n<td>Panpan</td>\n<td>36</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查询示例\n我们以ITCase方式完成如上查询需求，完整代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">  def testLateralTVF(): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setStateBackend(getStateBackend)</span><br><span class=\"line\">    StreamITCase.clear</span><br><span class=\"line\"></span><br><span class=\"line\">    val userData = new mutable.MutableList[(String)]</span><br><span class=\"line\">    userData.+=((&quot;Sunny#8&quot;))</span><br><span class=\"line\">    userData.+=((&quot;Kevin#36&quot;))</span><br><span class=\"line\">    userData.+=((&quot;Panpan#36&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    val users = env.fromCollection(userData).toTable(tEnv, &apos;data)</span><br><span class=\"line\"></span><br><span class=\"line\">    val tvf = new SplitTVF()</span><br><span class=\"line\">    tEnv.registerTable(&quot;userTab&quot;, users)</span><br><span class=\"line\">    tEnv.registerFunction(&quot;splitTVF&quot;, tvf)</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row]</span><br><span class=\"line\">    result.addSink(new StreamITCase.StringSink[Row])</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">    StreamITCase.testResults.foreach(println(_))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>运行结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png\" alt></p>\n<p>上面的核心语句是：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br></pre></td></tr></table></figure></p>\n<p>如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。</p>\n<h1>小结</h1>\n<p>本篇重点向大家介绍了一种新的<code>JOIN</code>类型 - <code>JOIN LATERAL</code>。并向大家介绍了SQL Server中对<code>LATERAL</code>的支持方式，详细分析了<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别与联系，最后切入到Apache Flink中，以<code>UDTF</code>示例说明了Apache Flink中对<code>JOIN LATERAL</code>的支持，后续篇章会介绍Apache Flink中另一种使用<code>LATERAL</code>的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见!</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>聊什么</h1>\n<p>上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。</p>\n<p>如下图所示：</p>\n<p><img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png\" alt></p>\n<p>本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。</p>\n<h1>实际问题</h1>\n<p>假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下：</p>\n<ul>\n<li>Customers\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Customers (</span><br><span class=\"line\">  customerid char(5) NOT NULL,</span><br><span class=\"line\">  city varchar (10) NOT NULL</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">insert into Customers values(&apos;C001&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C002&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C003&apos;,&apos;Beijing&apos;);</span><br><span class=\"line\">insert into Customers values(&apos;C004&apos;,&apos;HangZhou&apos;);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查看数据：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png\" alt></p>\n<ul>\n<li>Orders\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Orders(</span><br><span class=\"line\">  orderid char(5) NOT NULL,</span><br><span class=\"line\">  customerid char(5) NULL</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">insert into Orders values(&apos;O001&apos;,&apos;C001&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O002&apos;,&apos;C001&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O003&apos;,&apos;C003&apos;);</span><br><span class=\"line\">insert into Orders values(&apos;O004&apos;,&apos;C001&apos;);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查看数据：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/5DA82C35-6E03-43AD-B695-35C85D85D641.png\" alt></p>\n<h2>问题示例</h2>\n<p>假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/E3709EBB-89AE-44EA-98C7-06A2794597D9.png\" alt></p>\n<h3>用INNER JOIN解决</h3>\n<p>如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c JOIN Orders o </span><br><span class=\"line\">\tON o.customerid = c.customerid</span><br></pre></td></tr></table></figure></p>\n<p>查询结果如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/ED960882-39B0-40FD-9400-60D0D66F366E.png\" alt></p>\n<p>但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。</p>\n<h2>用 Correlated subquery解决</h2>\n<p>Correlated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。</p>\n<ul>\n<li>WHERE Clause\n用WHERE Clause实现上面的查询需求，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city</span><br><span class=\"line\">FROM Customers c WHERE c.customerid IN (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.customerid, o.orderid</span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行情况：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png\" alt></p>\n<p>上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对<code>o.orderid</code>进行投影, 上面查询我为什么要加一个<code>o.orderid</code>呢，因为查询需求是需要<code>o.orderid</code>的，去掉<code>o.orderid</code>查询能成功，但是拿到的结果并不是我们想要的，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city</span><br><span class=\"line\">FROM Customers c WHERE c.customerid IN (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.customerid</span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png\" alt></p>\n<p>可见上面查询结果缺少了<code>o.orderid</code>,不能满足我们的查询需求。</p>\n<ul>\n<li>FROM Clause\n用FROM Clause实现上面的查询需求，SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c, (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">      o.orderid, o.customerid </span><br><span class=\"line\">    FROM Orders o</span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">) as o</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>我们会得到如下错误：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/37259B89-DD8F-4349-A378-000AAA886479.png\" alt>\n错误信息提示我们无法识别<code>c.customerid</code>。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，\n那么上面的查询需求除了<code>INNER JOIN</code> 我们还可以如何解决呢？</p>\n<h1>JOIN LATERAL</h1>\n<p>我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了<code>LATERAL</code>关键字，也叫做 <code>lateral drive table</code>。</p>\n<h2>CROSS APPLY和LATERAL</h2>\n<p>上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 <code>LATERAL</code> 的呢？SQL Server是用自己的方言 <code>CROSS APPLY</code> 来支持的。那么为啥不用ANSI-SQL的<code>LATERAL</code>而用<code>CROSS APPLY</code>呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时<code>LATERAL</code>是SQL2003引入的，而<code>CROSS APPLY</code>是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等<code>LATERAL</code>出来的时候，<code>CROSS APPLY</code>在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了<code>CROSS APPLY</code>，但<code>CROSS APPLY</code>的语义与<code>LATERAL</code>却完全一致，同时后续支持<code>LATERAL</code>的Oracle12和PostgreSQL94同时支持了<code>LATERAL</code>和<code>CROSS APPLY</code>。</p>\n<h2>问题解决</h2>\n<p>那么我们回到上面的问题，我们用SQL Server的<code>CROSS APPLY</code>来解决上面问题，SQL如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/42B8EBC9-CD5B-498C-B191-F4241C91483F.png\" alt></p>\n<p>上面得到的结果完全满足查询需求。</p>\n<h1>JOIN LATERAL 与 INNER JOIN 关系</h1>\n<p>上面的查询需求并没有体现<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别，我们还是以SQL Server中两个查询执行Plan来观察一下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/A3A08123-0490-41BA-9E07-95B4557A19F5.png\" alt></p>\n<p>上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个<code>LATERAL</code> 出来呢？</p>\n<h2>性能方面</h2>\n<p>我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。</p>\n<ul>\n<li>LATERAL 的写法\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    c.customerid, c.city, o.orderid </span><br><span class=\"line\">FROM Customers c CROSS APPLY (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">     TOP(1) o.orderid, o.customerid </span><br><span class=\"line\">    FROM Orders o </span><br><span class=\"line\">    WHERE o.customerid = c.customerid</span><br><span class=\"line\">\tORDER BY o.customerid, o.orderid</span><br><span class=\"line\">) as o</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png\" alt>\n我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。</p>\n<ul>\n<li>JOIN 写法\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT  c.customerid, c.city, o.orderid</span><br><span class=\"line\"> FROM    Customers c</span><br><span class=\"line\">  JOIN (</span><br><span class=\"line\">   SELECT </span><br><span class=\"line\">     o2.*, </span><br><span class=\"line\">\t ROW_NUMBER() OVER (</span><br><span class=\"line\">\t\tPARTITION BY customerid </span><br><span class=\"line\">\t\tORDER BY orderid</span><br><span class=\"line\">\t  ) AS rn</span><br><span class=\"line\">   FROM    Orders o2</span><br><span class=\"line\"> ) o</span><br><span class=\"line\">ON c.customerid = o.customerid AND o.rn = 1</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/90BC2538-6A12-4F5F-AE42-B87BA5C12871.png\" alt></p>\n<p>如上我们都完成了查询需求，我们在来看一下执行Plan，如下：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/32183E30-E096-46DE-A855-932377D70BFE.png\" alt>\n我们直观发现完成相同功能，使用<code>CROSS APPLY</code>进行查询，执行Plan简单许多。</p>\n<h2>功能方面</h2>\n<p>在功能方面<code>INNER JOIN</code>本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入<code>CROSS APPLY</code>的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">   name, log_backup_time </span><br><span class=\"line\">FROM sys.databases AS s</span><br><span class=\"line\"> CROSS APPLY sys.dm_db_log_stats(s.database_id);</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png\" alt></p>\n<h1>Apache Flink对 LATERAL的支持</h1>\n<p>前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持<code>LATERAL</code>的，接下来我们看看Apache Flink对<code>LATERAL</code>的支持情况。</p>\n<h2>Calcite</h2>\n<p>Apache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持<code>LATERAL</code>语法，示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    e.NAME, e.DEPTNO, d.NAME </span><br><span class=\"line\">FROM EMPS e, LATERAL (</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    *</span><br><span class=\"line\">    FORM DEPTS d </span><br><span class=\"line\">    WHERE e.DEPTNO=d.DEPTNO</span><br><span class=\"line\"> ) as d;</span><br></pre></td></tr></table></figure></p>\n<p>查询结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png\" alt>\n我使用的是Calcite官方自带测试数据。</p>\n<h2>Flink</h2>\n<p>截止到Flink-1.6.2，Apache Flink 中有两种场景使用<code>LATERAL</code>，如下：</p>\n<ul>\n<li>UDTF(TVF) - User-defined Table Funciton</li>\n<li>Temporal Table - 涉及内容会在后续篇章单独介绍。</li>\n</ul>\n<p>本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持<code>LATERAL</code>。</p>\n<h3>UDTF</h3>\n<p>UDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下：</p>\n<ul>\n<li>\n<p>基类\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">  * Base class for all user-defined functions such as scalar functions, table functions,</span><br><span class=\"line\">  * or aggregation functions.</span><br><span class=\"line\">  */</span><br><span class=\"line\">abstract class UserDefinedFunction extends Serializable &#123;</span><br><span class=\"line\">  // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍）</span><br><span class=\"line\">  def open(context: FunctionContext): Unit = &#123;&#125;</span><br><span class=\"line\">  def close(): Unit = &#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>TableFunction\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">  * Base class for a user-defined table function (UDTF). A user-defined table functions works on</span><br><span class=\"line\">  * zero, one, or multiple scalar values as input and returns multiple rows as output.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation</span><br><span class=\"line\">  * method. An evaluation method must be declared publicly, not static and named &quot;eval&quot;.</span><br><span class=\"line\">  * Evaluation methods can also be overloaded by implementing multiple methods named &quot;eval&quot;.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * User-defined functions must have a default constructor and must be instantiable during runtime.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * By default the result type of an evaluation method is determined by Flink&apos;s type extraction</span><br><span class=\"line\">  * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more</span><br><span class=\"line\">  * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type</span><br><span class=\"line\">  * can be manually defined by overriding [[getResultType()]].</span><br><span class=\"line\">  */</span><br><span class=\"line\">abstract class TableFunction[T] extends UserDefinedFunction &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  // 对于泛型T，如果是基础类型那么Flink框架可以自动识别，</span><br><span class=\"line\">  // 对于用户自定义的复杂对象，需要用户overwrite这个实现。</span><br><span class=\"line\">  def getResultType: TypeInformation[T] = null</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>上面定义的核心是要求用户实现<code>eval</code>方法，我们写一个具体示例。</p>\n<ul>\n<li>示例\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 定义一个简单的UDTF返回类型，对应接口上的 T </span><br><span class=\"line\">case class SimpleUser(name: String, age: Int)</span><br><span class=\"line\">// 继承TableFunction，并实现evale方法</span><br><span class=\"line\">// 核心功能是解析以#分割的字符串</span><br><span class=\"line\">class SplitTVF extends TableFunction[SimpleUser] &#123;</span><br><span class=\"line\">  // make sure input element&apos;s format is &quot;&lt;string&gt;#&lt;int&gt;&quot;</span><br><span class=\"line\">  def eval(user: String): Unit = &#123;</span><br><span class=\"line\">    if (user.contains(&quot;#&quot;)) &#123;</span><br><span class=\"line\">      val splits = user.split(&quot;#&quot;)</span><br><span class=\"line\">      collect(SimpleUser(splits(0), splits(1).toInt))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3>示例(完整的ITCase)：</h3>\n<ul>\n<li>测试数据\n我们构造一个只包含一个data字段的用户表，用户表数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>data</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sunny#8</td>\n</tr>\n<tr>\n<td>Kevin#36</td>\n</tr>\n<tr>\n<td>Panpan#36</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查询需求\n查询的需求是将data字段flatten成为name和age两个字段的表，期望得到：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>name</th>\n<th>age</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sunny</td>\n<td>8</td>\n</tr>\n<tr>\n<td>Kevin</td>\n<td>36</td>\n</tr>\n<tr>\n<td>Panpan</td>\n<td>36</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查询示例\n我们以ITCase方式完成如上查询需求，完整代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">  def testLateralTVF(): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setStateBackend(getStateBackend)</span><br><span class=\"line\">    StreamITCase.clear</span><br><span class=\"line\"></span><br><span class=\"line\">    val userData = new mutable.MutableList[(String)]</span><br><span class=\"line\">    userData.+=((&quot;Sunny#8&quot;))</span><br><span class=\"line\">    userData.+=((&quot;Kevin#36&quot;))</span><br><span class=\"line\">    userData.+=((&quot;Panpan#36&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    val users = env.fromCollection(userData).toTable(tEnv, &apos;data)</span><br><span class=\"line\"></span><br><span class=\"line\">    val tvf = new SplitTVF()</span><br><span class=\"line\">    tEnv.registerTable(&quot;userTab&quot;, users)</span><br><span class=\"line\">    tEnv.registerFunction(&quot;splitTVF&quot;, tvf)</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row]</span><br><span class=\"line\">    result.addSink(new StreamITCase.StringSink[Row])</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">    StreamITCase.testResults.foreach(println(_))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>运行结果：\n<img src=\"/2019/03/11/Apache Flink 漫谈系列 - JOIN LATERAL/645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png\" alt></p>\n<p>上面的核心语句是：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br></pre></td></tr></table></figure></p>\n<p>如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。</p>\n<h1>小结</h1>\n<p>本篇重点向大家介绍了一种新的<code>JOIN</code>类型 - <code>JOIN LATERAL</code>。并向大家介绍了SQL Server中对<code>LATERAL</code>的支持方式，详细分析了<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别与联系，最后切入到Apache Flink中，以<code>UDTF</code>示例说明了Apache Flink中对<code>JOIN LATERAL</code>的支持，后续篇章会介绍Apache Flink中另一种使用<code>LATERAL</code>的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见!</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n"},{"title":"Apache Flink 漫谈系列 - SQL 概览","date":"2019-04-01T10:18:18.000Z","_content":"# Apache Flink SQL 概览\n\n本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。\n\n# Apache Flink SQL Job的组成\n我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：\n![](E18BAB16-4094-48B3-92E5-D45EA18A62C1.png)\n如上所示，一个完整的Apache Flink SQL Job 由如下三部分：\n* Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。\n\n* Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。\n\n* Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。\n\n# Apache Flink SQL 核心算子\nSQL是Structured Quevy Language的缩写，最初是由美国计算机科学家[Donald D. Chamberlin](https://en.wikipedia.org/wiki/Donald_D._Chamberlin#/media/File:Don_Chamberlin.jpg)和Raymond F. Boyce在20世纪70年代早期从 [Early History of SQL](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6359709) 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great):\n\n![](59761691-219B-47D5-8DC8-FC1D9C3E949E.png)\n\n在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的\"数据库语言SQL\"语言定义。Apache Flink SQL 核心算子的语义设计也参考了[1992](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt) 、[2011](http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf)等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。\n\n## SELECT\nSELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:\n![](391D7615-63C5-4A18-85BA-540730B6B5E2.png)\n对应的SQL语句如下：\n```\nSELECT ColA, ColC FROME tab ;\n```\n## WHERE\nWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n![](9F6085AD-4792-4A30-9503-82DF82540304.png)\n对应的SQL语句如下：\n```\nSELECT * FROM tab WHERE ColA <> 'a2' ;\n```\n## GROUP BY\nGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n![](E3CA71CA-8B44-4802-8165-53684A152503.png)\n对应的SQL语句如下：\n```\nSELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ;\n```\n## UNION ALL\nUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n![](1D8D2D78-5454-4919-A7C3-48B2338BDC78.png)\n对应的SQL语句如下：\n```\nSELECT * FROM T1 UNION ALL SELECT * FROM T2\n```\n\n## UNION\n\nUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n![](CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png)\n对应的SQL语句如下：\n```\nSELECT * FROM T1 UNION SELECT * FROM T2\n```\n## JOIN \nJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：\n* JOIN - INNER JOIN\n* LEFT JOIN - LEFT OUTER JOIN\n* RIGHT JOIN - RIGHT OUTER JOIN  \n* FULL JOIN - FULL OUTER JOIN\n\nJOIN与关系代数的Join语义相同，具体如下：\n![](16080D31-EB87-48F9-A32E-A992CC6DCEAC.png)\n\n对应的SQL语句如下(INNER JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ; \n```\n\n`LEFT JOIN`与`INNER JOIN`的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补`NULL`输出，如下：\n![](05A64357-27DF-4714-BDDB-B390B7A0814A.png)\n对应的SQL语句如下(INNER JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; \n```\n\n**说明：** \n\n* 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。\n\n* `RIGHT JOIN` 相当于 `LEFT JOIN` 左右两个表交互一下位置。`FULL JOIN`相当于 `RIGHT JOIN` 和 `LEFT JOIN` 之后进行`UNION ALL`操作。\n\n## Window\n在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。\n\n### OverWindow\nOVER Window 目前支持由如下三个元素组合的8中类型：\n* 时间 - Processing Time 和 EventTime\n* 数据集 - Bounded 和 UnBounded\n* 划分方式 - ROWS 和 RANGE\n我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义\n\n#### Bounded ROWS Over Window\nBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n##### 语法\n```\nSELECT \n    agg1(col1) OVER(\n     [PARTITION BY (value_expression1,..., value_expressionN)] \n     ORDER BY timeCol\n     ROWS \n     BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, \n... \nFROM Tab1\n```\n* value_expression - 进行分区的字表达式；\n* timeCol - 用于元素排序的时间字段；\n* rowCount - 是定义根据当前行开始向前追溯几行元素；\n\n\n##### 语义\n我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n![](051876D7-5A89-4134-8ED0-E6939FC3F412.png)\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window.\n\n#### Bounded RANGE Over Window\n\nBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口；\n##### 语法\nBounded RANGE OVER Window的语法如下：\n```\nSELECT \n    agg1(col1) OVER(\n     [PARTITION BY (value_expression1,..., value_expressionN)] \n     ORDER BY timeCol\n     RANGE \n     BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, \n... \nFROM Tab1\n```\n* value_expression - 进行分区的字表达式；\n* timeCol - 用于元素排序的时间字段；\n* timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；\n##### 语义\n我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n![](95F7D755-E087-4808-83F7-2A20CA5C685F.png)\n\n注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window.\n\n### GroupWindow\n根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:\n\n* Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；\n* Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；\n* Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加；\n\n**说明：** Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。\n\nGroupWindow的语法如下：\n```\nSELECT \n    [gk], \n    agg1(col1),\n     ... \n    aggN(colN)\nFROM Tab1\nGROUP BY [WINDOW(definition)], [gk]\n```\n* [WINDOW(definition)] - 在具体窗口语义介绍中介绍。\n\n#### Tumble Window\nTumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n![](26FE2626-FDA4-4474-9B25-5505FEF5FB37.png)\n假设我们要写一个2分钟大小的Tumble，示例SQL如下：\n```\nSELECT gk, COUNT(*) AS pv \n  FROM tab \n    GROUP BY TUMBLE(rowtime, INTERVAL '2' MINUTE), gk\n```\n\n#### Hop Window\n\nHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：\n![](AE17D459-66B1-4946-A962-BFC3B9438E79.png)\n假设我们要写一个每5分钟统计近10分钟的页面访问量(PV).\n```\nSELECT gk, COUNT(*) AS pv \n  FROM tab \n    GROUP BY HOP(rowtime, INTERVAL '5' MINUTE, INTERVAL '10' MINUTE), gk\n```\n\n#### Session Window\n\nSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下：\n![](5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png)\n\n假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).\n```\nSELECT gk, COUNT(*) AS pv  \n  FROM pageAccessSession_tab\n    GROUP BY SESSION(rowtime, INTERVAL '3' MINUTE), gk\n```\n**说明:** 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数：\n* TUMBLE_START/TUMBLE_END\n* HOP_START/HOP_END\n* SESSION_START/SESSION_END\n\n这些辅助函数如何使用，请参考如下完整示例的使用方式。\n\n# 完整的 SQL Job 案例\n上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n## Source 定义\n自定义Apache Flink Stream Source需要实现`StreamTableSource`, `StreamTableSource`中通过`StreamExecutionEnvironment` 的`addSource`方法获取`DataStream`, 所以我们需要自定义一个 `SourceFunction`, 并且要支持产生WaterMark，也就是要实现`DefinedRowtimeAttributes`接口。\n\n### Source Function定义\n支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据:\n```\nclass MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) \n  extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n  override def cancel(): Unit = ???\n}\n```\n\n### 定义 StreamTableSource\n我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下:\n```\nclass MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes {\n\n  val fieldNames = Array(\"accessTime\", \"region\", \"userId\")\n  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))\n  val rowType = new RowTypeInfo(\n    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],\n    fieldNames)\n\n  // 页面访问表数据 rows with timestamps and watermarks\n  val data = Seq(\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"ShangHai\", \"U0010\")),\n    Right(1510365660000L),\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"BeiJing\", \"U1001\")),\n    Right(1510365660000L),\n    Left(1510366200000L, Row.of(new JLong(1510366200000L), \"BeiJing\", \"U2032\")),\n    Right(1510366200000L),\n    Left(1510366260000L, Row.of(new JLong(1510366260000L), \"BeiJing\", \"U1100\")),\n    Right(1510366260000L),\n    Left(1510373400000L, Row.of(new JLong(1510373400000L), \"ShangHai\", \"U0011\")),\n    Right(1510373400000L)\n  )\n\n  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = {\n    Collections.singletonList(new RowtimeAttributeDescriptor(\n      \"accessTime\",\n      new ExistingField(\"accessTime\"),\n      PreserveWatermarks.INSTANCE))\n  }\n\n  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n    execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType)\n  }\n\n  override def getReturnType: TypeInformation[Row] = rowType\n\n  override def getTableSchema: TableSchema = schema\n\n}\n  ```\n  \n## Sink 定义\n我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n```\ndef getCsvTableSink: TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    // 打印sink的文件路径，方便我们查看运行结果\n    println(\"Sink path : \" + tempFile)\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(\n      Array[String](\"region\", \"winStart\", \"winEnd\", \"pv\"),\n      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))\n  }\n```\n\n## 构建主程序\n主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    // 设置EventTime\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"mySource\"\n    // 创建自定义source数据结构\n    val tableSource = new MyTableSource\n\n    val sinkTableName = \"csvSink\"\n    // 创建CSV sink 数据结构\n    val tableSink = getCsvTableSink\n\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n    // 注册sink\n    tEnv.registerTableSink(sinkTableName, tableSink)\n\n    val sql =\n      \"SELECT  \" +\n      \"  region, \" +\n      \"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,\" +\n      \"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv \" +\n      \" FROM mySource \" +\n      \" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region\"\n\n    tEnv.sqlQuery(sql).insertInto(sinkTableName);\n    env.execute()\n  }\n```\n\n## 执行并查看运行结果\n执行主程序后我们会在控制台得到Sink的文件路径，如下：\n```\nSink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\n```\nCat 方式查看计算结果，如下：\n```\njinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\nShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2\nShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1\n```\n表格化如上结果：\n\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n# 小结\n本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.\n\n\n\n\n\n","source":"_posts/Apache Flink 漫谈系列 - SQL 概览.md","raw":"---\ntitle: Apache Flink 漫谈系列 - SQL 概览\ndate: 2019-04-01 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# Apache Flink SQL 概览\n\n本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。\n\n# Apache Flink SQL Job的组成\n我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：\n![](E18BAB16-4094-48B3-92E5-D45EA18A62C1.png)\n如上所示，一个完整的Apache Flink SQL Job 由如下三部分：\n* Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。\n\n* Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。\n\n* Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。\n\n# Apache Flink SQL 核心算子\nSQL是Structured Quevy Language的缩写，最初是由美国计算机科学家[Donald D. Chamberlin](https://en.wikipedia.org/wiki/Donald_D._Chamberlin#/media/File:Don_Chamberlin.jpg)和Raymond F. Boyce在20世纪70年代早期从 [Early History of SQL](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6359709) 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great):\n\n![](59761691-219B-47D5-8DC8-FC1D9C3E949E.png)\n\n在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的\"数据库语言SQL\"语言定义。Apache Flink SQL 核心算子的语义设计也参考了[1992](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt) 、[2011](http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf)等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。\n\n## SELECT\nSELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:\n![](391D7615-63C5-4A18-85BA-540730B6B5E2.png)\n对应的SQL语句如下：\n```\nSELECT ColA, ColC FROME tab ;\n```\n## WHERE\nWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n![](9F6085AD-4792-4A30-9503-82DF82540304.png)\n对应的SQL语句如下：\n```\nSELECT * FROM tab WHERE ColA <> 'a2' ;\n```\n## GROUP BY\nGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n![](E3CA71CA-8B44-4802-8165-53684A152503.png)\n对应的SQL语句如下：\n```\nSELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ;\n```\n## UNION ALL\nUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n![](1D8D2D78-5454-4919-A7C3-48B2338BDC78.png)\n对应的SQL语句如下：\n```\nSELECT * FROM T1 UNION ALL SELECT * FROM T2\n```\n\n## UNION\n\nUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n![](CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png)\n对应的SQL语句如下：\n```\nSELECT * FROM T1 UNION SELECT * FROM T2\n```\n## JOIN \nJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：\n* JOIN - INNER JOIN\n* LEFT JOIN - LEFT OUTER JOIN\n* RIGHT JOIN - RIGHT OUTER JOIN  \n* FULL JOIN - FULL OUTER JOIN\n\nJOIN与关系代数的Join语义相同，具体如下：\n![](16080D31-EB87-48F9-A32E-A992CC6DCEAC.png)\n\n对应的SQL语句如下(INNER JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ; \n```\n\n`LEFT JOIN`与`INNER JOIN`的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补`NULL`输出，如下：\n![](05A64357-27DF-4714-BDDB-B390B7A0814A.png)\n对应的SQL语句如下(INNER JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; \n```\n\n**说明：** \n\n* 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。\n\n* `RIGHT JOIN` 相当于 `LEFT JOIN` 左右两个表交互一下位置。`FULL JOIN`相当于 `RIGHT JOIN` 和 `LEFT JOIN` 之后进行`UNION ALL`操作。\n\n## Window\n在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。\n\n### OverWindow\nOVER Window 目前支持由如下三个元素组合的8中类型：\n* 时间 - Processing Time 和 EventTime\n* 数据集 - Bounded 和 UnBounded\n* 划分方式 - ROWS 和 RANGE\n我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义\n\n#### Bounded ROWS Over Window\nBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n##### 语法\n```\nSELECT \n    agg1(col1) OVER(\n     [PARTITION BY (value_expression1,..., value_expressionN)] \n     ORDER BY timeCol\n     ROWS \n     BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, \n... \nFROM Tab1\n```\n* value_expression - 进行分区的字表达式；\n* timeCol - 用于元素排序的时间字段；\n* rowCount - 是定义根据当前行开始向前追溯几行元素；\n\n\n##### 语义\n我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n![](051876D7-5A89-4134-8ED0-E6939FC3F412.png)\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window.\n\n#### Bounded RANGE Over Window\n\nBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口；\n##### 语法\nBounded RANGE OVER Window的语法如下：\n```\nSELECT \n    agg1(col1) OVER(\n     [PARTITION BY (value_expression1,..., value_expressionN)] \n     ORDER BY timeCol\n     RANGE \n     BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, \n... \nFROM Tab1\n```\n* value_expression - 进行分区的字表达式；\n* timeCol - 用于元素排序的时间字段；\n* timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；\n##### 语义\n我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n![](95F7D755-E087-4808-83F7-2A20CA5C685F.png)\n\n注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window.\n\n### GroupWindow\n根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:\n\n* Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；\n* Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；\n* Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加；\n\n**说明：** Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。\n\nGroupWindow的语法如下：\n```\nSELECT \n    [gk], \n    agg1(col1),\n     ... \n    aggN(colN)\nFROM Tab1\nGROUP BY [WINDOW(definition)], [gk]\n```\n* [WINDOW(definition)] - 在具体窗口语义介绍中介绍。\n\n#### Tumble Window\nTumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n![](26FE2626-FDA4-4474-9B25-5505FEF5FB37.png)\n假设我们要写一个2分钟大小的Tumble，示例SQL如下：\n```\nSELECT gk, COUNT(*) AS pv \n  FROM tab \n    GROUP BY TUMBLE(rowtime, INTERVAL '2' MINUTE), gk\n```\n\n#### Hop Window\n\nHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：\n![](AE17D459-66B1-4946-A962-BFC3B9438E79.png)\n假设我们要写一个每5分钟统计近10分钟的页面访问量(PV).\n```\nSELECT gk, COUNT(*) AS pv \n  FROM tab \n    GROUP BY HOP(rowtime, INTERVAL '5' MINUTE, INTERVAL '10' MINUTE), gk\n```\n\n#### Session Window\n\nSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下：\n![](5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png)\n\n假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).\n```\nSELECT gk, COUNT(*) AS pv  \n  FROM pageAccessSession_tab\n    GROUP BY SESSION(rowtime, INTERVAL '3' MINUTE), gk\n```\n**说明:** 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数：\n* TUMBLE_START/TUMBLE_END\n* HOP_START/HOP_END\n* SESSION_START/SESSION_END\n\n这些辅助函数如何使用，请参考如下完整示例的使用方式。\n\n# 完整的 SQL Job 案例\n上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n## Source 定义\n自定义Apache Flink Stream Source需要实现`StreamTableSource`, `StreamTableSource`中通过`StreamExecutionEnvironment` 的`addSource`方法获取`DataStream`, 所以我们需要自定义一个 `SourceFunction`, 并且要支持产生WaterMark，也就是要实现`DefinedRowtimeAttributes`接口。\n\n### Source Function定义\n支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据:\n```\nclass MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) \n  extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n  override def cancel(): Unit = ???\n}\n```\n\n### 定义 StreamTableSource\n我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下:\n```\nclass MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes {\n\n  val fieldNames = Array(\"accessTime\", \"region\", \"userId\")\n  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))\n  val rowType = new RowTypeInfo(\n    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],\n    fieldNames)\n\n  // 页面访问表数据 rows with timestamps and watermarks\n  val data = Seq(\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"ShangHai\", \"U0010\")),\n    Right(1510365660000L),\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"BeiJing\", \"U1001\")),\n    Right(1510365660000L),\n    Left(1510366200000L, Row.of(new JLong(1510366200000L), \"BeiJing\", \"U2032\")),\n    Right(1510366200000L),\n    Left(1510366260000L, Row.of(new JLong(1510366260000L), \"BeiJing\", \"U1100\")),\n    Right(1510366260000L),\n    Left(1510373400000L, Row.of(new JLong(1510373400000L), \"ShangHai\", \"U0011\")),\n    Right(1510373400000L)\n  )\n\n  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = {\n    Collections.singletonList(new RowtimeAttributeDescriptor(\n      \"accessTime\",\n      new ExistingField(\"accessTime\"),\n      PreserveWatermarks.INSTANCE))\n  }\n\n  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n    execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType)\n  }\n\n  override def getReturnType: TypeInformation[Row] = rowType\n\n  override def getTableSchema: TableSchema = schema\n\n}\n  ```\n  \n## Sink 定义\n我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n```\ndef getCsvTableSink: TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    // 打印sink的文件路径，方便我们查看运行结果\n    println(\"Sink path : \" + tempFile)\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(\n      Array[String](\"region\", \"winStart\", \"winEnd\", \"pv\"),\n      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))\n  }\n```\n\n## 构建主程序\n主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    // 设置EventTime\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"mySource\"\n    // 创建自定义source数据结构\n    val tableSource = new MyTableSource\n\n    val sinkTableName = \"csvSink\"\n    // 创建CSV sink 数据结构\n    val tableSink = getCsvTableSink\n\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n    // 注册sink\n    tEnv.registerTableSink(sinkTableName, tableSink)\n\n    val sql =\n      \"SELECT  \" +\n      \"  region, \" +\n      \"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,\" +\n      \"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv \" +\n      \" FROM mySource \" +\n      \" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region\"\n\n    tEnv.sqlQuery(sql).insertInto(sinkTableName);\n    env.execute()\n  }\n```\n\n## 执行并查看运行结果\n执行主程序后我们会在控制台得到Sink的文件路径，如下：\n```\nSink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\n```\nCat 方式查看计算结果，如下：\n```\njinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\nShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2\nShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1\n```\n表格化如上结果：\n\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n# 小结\n本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.\n\n\n\n\n\n","slug":"Apache Flink 漫谈系列 - SQL 概览","published":1,"updated":"2019-07-13T13:09:44.190Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5f90007s94h6qk1iaa4","content":"<h1>Apache Flink SQL 概览</h1>\n<p>本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。</p>\n<h1>Apache Flink SQL Job的组成</h1>\n<p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/E18BAB16-4094-48B3-92E5-D45EA18A62C1.png\" alt>\n如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p>\n<ul>\n<li>\n<p>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</p>\n</li>\n<li>\n<p>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</p>\n</li>\n<li>\n<p>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</p>\n</li>\n</ul>\n<h1>Apache Flink SQL 核心算子</h1>\n<p>SQL是Structured Quevy Language的缩写，最初是由美国计算机科学家<a href=\"https://en.wikipedia.org/wiki/Donald_D._Chamberlin#/media/File:Don_Chamberlin.jpg\" target=\"_blank\" rel=\"noopener\">Donald D. Chamberlin</a>和Raymond F. Boyce在20世纪70年代早期从 <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6359709\" target=\"_blank\" rel=\"noopener\">Early History of SQL</a> 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great):</p>\n<p><img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/59761691-219B-47D5-8DC8-FC1D9C3E949E.png\" alt></p>\n<p>在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的&quot;数据库语言SQL&quot;语言定义。Apache Flink SQL 核心算子的语义设计也参考了<a href=\"http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt\" target=\"_blank\" rel=\"noopener\">1992</a> 、<a href=\"http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf\" target=\"_blank\" rel=\"noopener\">2011</a>等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。</p>\n<h2>SELECT</h2>\n<p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/391D7615-63C5-4A18-85BA-540730B6B5E2.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColC FROME tab ;</span><br></pre></td></tr></table></figure></p>\n<h2>WHERE</h2>\n<p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/9F6085AD-4792-4A30-9503-82DF82540304.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM tab WHERE ColA &lt;&gt; &apos;a2&apos; ;</span><br></pre></td></tr></table></figure></p>\n<h2>GROUP BY</h2>\n<p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/E3CA71CA-8B44-4802-8165-53684A152503.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ;</span><br></pre></td></tr></table></figure></p>\n<h2>UNION ALL</h2>\n<p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/1D8D2D78-5454-4919-A7C3-48B2338BDC78.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM T1 UNION ALL SELECT * FROM T2</span><br></pre></td></tr></table></figure></p>\n<h2>UNION</h2>\n<p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM T1 UNION SELECT * FROM T2</span><br></pre></td></tr></table></figure></p>\n<h2>JOIN</h2>\n<p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>\n<ul>\n<li>JOIN - INNER JOIN</li>\n<li>LEFT JOIN - LEFT OUTER JOIN</li>\n<li>RIGHT JOIN - RIGHT OUTER JOIN</li>\n<li>FULL JOIN - FULL OUTER JOIN</li>\n</ul>\n<p>JOIN与关系代数的Join语义相同，具体如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/16080D31-EB87-48F9-A32E-A992CC6DCEAC.png\" alt></p>\n<p>对应的SQL语句如下(INNER JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/05A64357-27DF-4714-BDDB-B390B7A0814A.png\" alt>\n对应的SQL语句如下(INNER JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<p><strong>说明：</strong></p>\n<ul>\n<li>\n<p>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</p>\n</li>\n<li>\n<p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>\n</li>\n</ul>\n<h2>Window</h2>\n<p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>\n<h3>OverWindow</h3>\n<p>OVER Window 目前支持由如下三个元素组合的8中类型：</p>\n<ul>\n<li>时间 - Processing Time 和 EventTime</li>\n<li>数据集 - Bounded 和 UnBounded</li>\n<li>划分方式 - ROWS 和 RANGE\n我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义</li>\n</ul>\n<h4>Bounded ROWS Over Window</h4>\n<p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>\n<h5>语法</h5>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    agg1(col1) OVER(</span><br><span class=\"line\">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class=\"line\">     ORDER BY timeCol</span><br><span class=\"line\">     ROWS </span><br><span class=\"line\">     BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class=\"line\">... </span><br><span class=\"line\">FROM Tab1</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>value_expression - 进行分区的字表达式；</li>\n<li>timeCol - 用于元素排序的时间字段；</li>\n<li>rowCount - 是定义根据当前行开始向前追溯几行元素；</li>\n</ul>\n<h5>语义</h5>\n<p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/051876D7-5A89-4134-8ED0-E6939FC3F412.png\" alt>\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window.</p>\n<h4>Bounded RANGE Over Window</h4>\n<p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口；</p>\n<h5>语法</h5>\n<p>Bounded RANGE OVER Window的语法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    agg1(col1) OVER(</span><br><span class=\"line\">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class=\"line\">     ORDER BY timeCol</span><br><span class=\"line\">     RANGE </span><br><span class=\"line\">     BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class=\"line\">... </span><br><span class=\"line\">FROM Tab1</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>value_expression - 进行分区的字表达式；</li>\n<li>timeCol - 用于元素排序的时间字段；</li>\n<li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li>\n</ul>\n<h5>语义</h5>\n<p>我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/95F7D755-E087-4808-83F7-2A20CA5C685F.png\" alt></p>\n<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window.</p>\n<h3>GroupWindow</h3>\n<p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>\n<ul>\n<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>\n<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>\n<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加；</li>\n</ul>\n<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>\n<p>GroupWindow的语法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    [gk], </span><br><span class=\"line\">    agg1(col1),</span><br><span class=\"line\">     ... </span><br><span class=\"line\">    aggN(colN)</span><br><span class=\"line\">FROM Tab1</span><br><span class=\"line\">GROUP BY [WINDOW(definition)], [gk]</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>[WINDOW(definition)] - 在具体窗口语义介绍中介绍。</li>\n</ul>\n<h4>Tumble Window</h4>\n<p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/26FE2626-FDA4-4474-9B25-5505FEF5FB37.png\" alt>\n假设我们要写一个2分钟大小的Tumble，示例SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv </span><br><span class=\"line\">  FROM tab </span><br><span class=\"line\">    GROUP BY TUMBLE(rowtime, INTERVAL &apos;2&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<h4>Hop Window</h4>\n<p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/AE17D459-66B1-4946-A962-BFC3B9438E79.png\" alt>\n假设我们要写一个每5分钟统计近10分钟的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv </span><br><span class=\"line\">  FROM tab </span><br><span class=\"line\">    GROUP BY HOP(rowtime, INTERVAL &apos;5&apos; MINUTE, INTERVAL &apos;10&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<h4>Session Window</h4>\n<p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png\" alt></p>\n<p>假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv  </span><br><span class=\"line\">  FROM pageAccessSession_tab</span><br><span class=\"line\">    GROUP BY SESSION(rowtime, INTERVAL &apos;3&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<p><strong>说明:</strong> 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数：</p>\n<ul>\n<li>TUMBLE_START/TUMBLE_END</li>\n<li>HOP_START/HOP_END</li>\n<li>SESSION_START/SESSION_END</li>\n</ul>\n<p>这些辅助函数如何使用，请参考如下完整示例的使用方式。</p>\n<h1>完整的 SQL Job 案例</h1>\n<p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<h2>Source 定义</h2>\n<p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>\n<h3>Source Function定义</h3>\n<p>支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class=\"line\">  extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3>定义 StreamTableSource</h3>\n<p>我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class=\"line\">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class=\"line\">  val rowType = new RowTypeInfo(</span><br><span class=\"line\">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class=\"line\">    fieldNames)</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class=\"line\">  val data = Seq(</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class=\"line\">    Right(1510366200000L),</span><br><span class=\"line\">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class=\"line\">    Right(1510366260000L),</span><br><span class=\"line\">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class=\"line\">    Right(1510373400000L)</span><br><span class=\"line\">  )</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class=\"line\">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class=\"line\">      &quot;accessTime&quot;,</span><br><span class=\"line\">      new ExistingField(&quot;accessTime&quot;),</span><br><span class=\"line\">      PreserveWatermarks.INSTANCE))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class=\"line\">    execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getTableSchema: TableSchema = schema</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>Sink 定义</h2>\n<p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class=\"line\">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class=\"line\">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class=\"line\">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>构建主程序</h2>\n<p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 设置EventTime</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;mySource&quot;</span><br><span class=\"line\">    // 创建自定义source数据结构</span><br><span class=\"line\">    val tableSource = new MyTableSource</span><br><span class=\"line\"></span><br><span class=\"line\">    val sinkTableName = &quot;csvSink&quot;</span><br><span class=\"line\">    // 创建CSV sink 数据结构</span><br><span class=\"line\">    val tableSink = getCsvTableSink</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\">    // 注册sink</span><br><span class=\"line\">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sql =</span><br><span class=\"line\">      &quot;SELECT  &quot; +</span><br><span class=\"line\">      &quot;  region, &quot; +</span><br><span class=\"line\">      &quot;  TUMBLE_START(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winStart,&quot; +</span><br><span class=\"line\">      &quot;  TUMBLE_END(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winEnd, COUNT(region) AS pv &quot; +</span><br><span class=\"line\">      &quot; FROM mySource &quot; +</span><br><span class=\"line\">      &quot; GROUP BY TUMBLE(accessTime, INTERVAL &apos;2&apos; MINUTE), region&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>执行主程序后我们会在控制台得到Sink的文件路径，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure></p>\n<p>Cat 方式查看计算结果，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class=\"line\">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class=\"line\">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure></p>\n<p>表格化如上结果：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h1>小结</h1>\n<p>本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Apache Flink SQL 概览</h1>\n<p>本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。</p>\n<h1>Apache Flink SQL Job的组成</h1>\n<p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/E18BAB16-4094-48B3-92E5-D45EA18A62C1.png\" alt>\n如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p>\n<ul>\n<li>\n<p>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</p>\n</li>\n<li>\n<p>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</p>\n</li>\n<li>\n<p>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</p>\n</li>\n</ul>\n<h1>Apache Flink SQL 核心算子</h1>\n<p>SQL是Structured Quevy Language的缩写，最初是由美国计算机科学家<a href=\"https://en.wikipedia.org/wiki/Donald_D._Chamberlin#/media/File:Don_Chamberlin.jpg\" target=\"_blank\" rel=\"noopener\">Donald D. Chamberlin</a>和Raymond F. Boyce在20世纪70年代早期从 <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6359709\" target=\"_blank\" rel=\"noopener\">Early History of SQL</a> 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great):</p>\n<p><img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/59761691-219B-47D5-8DC8-FC1D9C3E949E.png\" alt></p>\n<p>在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的&quot;数据库语言SQL&quot;语言定义。Apache Flink SQL 核心算子的语义设计也参考了<a href=\"http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt\" target=\"_blank\" rel=\"noopener\">1992</a> 、<a href=\"http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf\" target=\"_blank\" rel=\"noopener\">2011</a>等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。</p>\n<h2>SELECT</h2>\n<p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/391D7615-63C5-4A18-85BA-540730B6B5E2.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColC FROME tab ;</span><br></pre></td></tr></table></figure></p>\n<h2>WHERE</h2>\n<p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/9F6085AD-4792-4A30-9503-82DF82540304.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM tab WHERE ColA &lt;&gt; &apos;a2&apos; ;</span><br></pre></td></tr></table></figure></p>\n<h2>GROUP BY</h2>\n<p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/E3CA71CA-8B44-4802-8165-53684A152503.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ;</span><br></pre></td></tr></table></figure></p>\n<h2>UNION ALL</h2>\n<p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/1D8D2D78-5454-4919-A7C3-48B2338BDC78.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM T1 UNION ALL SELECT * FROM T2</span><br></pre></td></tr></table></figure></p>\n<h2>UNION</h2>\n<p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png\" alt>\n对应的SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM T1 UNION SELECT * FROM T2</span><br></pre></td></tr></table></figure></p>\n<h2>JOIN</h2>\n<p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>\n<ul>\n<li>JOIN - INNER JOIN</li>\n<li>LEFT JOIN - LEFT OUTER JOIN</li>\n<li>RIGHT JOIN - RIGHT OUTER JOIN</li>\n<li>FULL JOIN - FULL OUTER JOIN</li>\n</ul>\n<p>JOIN与关系代数的Join语义相同，具体如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/16080D31-EB87-48F9-A32E-A992CC6DCEAC.png\" alt></p>\n<p>对应的SQL语句如下(INNER JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/05A64357-27DF-4714-BDDB-B390B7A0814A.png\" alt>\n对应的SQL语句如下(INNER JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<p><strong>说明：</strong></p>\n<ul>\n<li>\n<p>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</p>\n</li>\n<li>\n<p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>\n</li>\n</ul>\n<h2>Window</h2>\n<p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>\n<h3>OverWindow</h3>\n<p>OVER Window 目前支持由如下三个元素组合的8中类型：</p>\n<ul>\n<li>时间 - Processing Time 和 EventTime</li>\n<li>数据集 - Bounded 和 UnBounded</li>\n<li>划分方式 - ROWS 和 RANGE\n我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义</li>\n</ul>\n<h4>Bounded ROWS Over Window</h4>\n<p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>\n<h5>语法</h5>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    agg1(col1) OVER(</span><br><span class=\"line\">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class=\"line\">     ORDER BY timeCol</span><br><span class=\"line\">     ROWS </span><br><span class=\"line\">     BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class=\"line\">... </span><br><span class=\"line\">FROM Tab1</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>value_expression - 进行分区的字表达式；</li>\n<li>timeCol - 用于元素排序的时间字段；</li>\n<li>rowCount - 是定义根据当前行开始向前追溯几行元素；</li>\n</ul>\n<h5>语义</h5>\n<p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/051876D7-5A89-4134-8ED0-E6939FC3F412.png\" alt>\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window.</p>\n<h4>Bounded RANGE Over Window</h4>\n<p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口；</p>\n<h5>语法</h5>\n<p>Bounded RANGE OVER Window的语法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    agg1(col1) OVER(</span><br><span class=\"line\">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class=\"line\">     ORDER BY timeCol</span><br><span class=\"line\">     RANGE </span><br><span class=\"line\">     BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class=\"line\">... </span><br><span class=\"line\">FROM Tab1</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>value_expression - 进行分区的字表达式；</li>\n<li>timeCol - 用于元素排序的时间字段；</li>\n<li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li>\n</ul>\n<h5>语义</h5>\n<p>我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/95F7D755-E087-4808-83F7-2A20CA5C685F.png\" alt></p>\n<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window.</p>\n<h3>GroupWindow</h3>\n<p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>\n<ul>\n<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>\n<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>\n<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加；</li>\n</ul>\n<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>\n<p>GroupWindow的语法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">    [gk], </span><br><span class=\"line\">    agg1(col1),</span><br><span class=\"line\">     ... </span><br><span class=\"line\">    aggN(colN)</span><br><span class=\"line\">FROM Tab1</span><br><span class=\"line\">GROUP BY [WINDOW(definition)], [gk]</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>[WINDOW(definition)] - 在具体窗口语义介绍中介绍。</li>\n</ul>\n<h4>Tumble Window</h4>\n<p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/26FE2626-FDA4-4474-9B25-5505FEF5FB37.png\" alt>\n假设我们要写一个2分钟大小的Tumble，示例SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv </span><br><span class=\"line\">  FROM tab </span><br><span class=\"line\">    GROUP BY TUMBLE(rowtime, INTERVAL &apos;2&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<h4>Hop Window</h4>\n<p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/AE17D459-66B1-4946-A962-BFC3B9438E79.png\" alt>\n假设我们要写一个每5分钟统计近10分钟的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv </span><br><span class=\"line\">  FROM tab </span><br><span class=\"line\">    GROUP BY HOP(rowtime, INTERVAL &apos;5&apos; MINUTE, INTERVAL &apos;10&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<h4>Session Window</h4>\n<p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下：\n<img src=\"/2019/04/01/Apache Flink 漫谈系列 - SQL 概览/5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png\" alt></p>\n<p>假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT gk, COUNT(*) AS pv  </span><br><span class=\"line\">  FROM pageAccessSession_tab</span><br><span class=\"line\">    GROUP BY SESSION(rowtime, INTERVAL &apos;3&apos; MINUTE), gk</span><br></pre></td></tr></table></figure></p>\n<p><strong>说明:</strong> 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数：</p>\n<ul>\n<li>TUMBLE_START/TUMBLE_END</li>\n<li>HOP_START/HOP_END</li>\n<li>SESSION_START/SESSION_END</li>\n</ul>\n<p>这些辅助函数如何使用，请参考如下完整示例的使用方式。</p>\n<h1>完整的 SQL Job 案例</h1>\n<p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<h2>Source 定义</h2>\n<p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>\n<h3>Source Function定义</h3>\n<p>支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class=\"line\">  extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3>定义 StreamTableSource</h3>\n<p>我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class=\"line\">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class=\"line\">  val rowType = new RowTypeInfo(</span><br><span class=\"line\">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class=\"line\">    fieldNames)</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class=\"line\">  val data = Seq(</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class=\"line\">    Right(1510366200000L),</span><br><span class=\"line\">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class=\"line\">    Right(1510366260000L),</span><br><span class=\"line\">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class=\"line\">    Right(1510373400000L)</span><br><span class=\"line\">  )</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class=\"line\">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class=\"line\">      &quot;accessTime&quot;,</span><br><span class=\"line\">      new ExistingField(&quot;accessTime&quot;),</span><br><span class=\"line\">      PreserveWatermarks.INSTANCE))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class=\"line\">    execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getTableSchema: TableSchema = schema</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>Sink 定义</h2>\n<p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class=\"line\">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class=\"line\">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class=\"line\">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>构建主程序</h2>\n<p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 设置EventTime</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;mySource&quot;</span><br><span class=\"line\">    // 创建自定义source数据结构</span><br><span class=\"line\">    val tableSource = new MyTableSource</span><br><span class=\"line\"></span><br><span class=\"line\">    val sinkTableName = &quot;csvSink&quot;</span><br><span class=\"line\">    // 创建CSV sink 数据结构</span><br><span class=\"line\">    val tableSink = getCsvTableSink</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\">    // 注册sink</span><br><span class=\"line\">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sql =</span><br><span class=\"line\">      &quot;SELECT  &quot; +</span><br><span class=\"line\">      &quot;  region, &quot; +</span><br><span class=\"line\">      &quot;  TUMBLE_START(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winStart,&quot; +</span><br><span class=\"line\">      &quot;  TUMBLE_END(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winEnd, COUNT(region) AS pv &quot; +</span><br><span class=\"line\">      &quot; FROM mySource &quot; +</span><br><span class=\"line\">      &quot; GROUP BY TUMBLE(accessTime, INTERVAL &apos;2&apos; MINUTE), region&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>执行主程序后我们会在控制台得到Sink的文件路径，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure></p>\n<p>Cat 方式查看计算结果，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class=\"line\">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class=\"line\">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure></p>\n<p>表格化如上结果：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h1>小结</h1>\n<p>本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.</p>\n"},{"title":"Apache Flink 漫谈系列 - JOIN 算子","date":"2019-03-06T10:18:18.000Z","_content":"# 聊什么\n在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！\n \n\n# 什么是JOIN\n在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下：\n![](70173B48-E4B8-466C-9846-AD68FEB6F9B2.png)\n\n# 为啥需要JOIN\nJOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个：\n\n* 产生数据的源头可能不是一个系统；\n* 产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下： \n    * 1NF - 列不可再分；\n    * 2NF - 符合1NF，并且非主键属性全部依赖于主键属性；\n    * 3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来；\n    * BCNF - 符合3NF，并且主键属性之间无依赖关系。\n\n当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量）\n\n# JOIN种类\nJOIN 在传统数据库中有如下分类：\n* CROSS JOIN - 交叉连接，计算笛卡儿积；\n* INNER JOIN - 内连接，返回满足条件的记录；\n* OUTER JOIN\n    * LEFT - 返回左表所有行，右表不存在补NULL；\n    * RIGHT - 返回右表所有行，左边不存在补NULL；\n    * FULL -  返回左表和右表的并集，不存在一边补NULL;\n* SELF JOIN - 自连接，将表查询时候命名不同的别名。\n\n# JOIN语法\nJOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明：\n* SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定:\n```\nSELECT \n  a.colA, \n  b.colA\nFROM  \n  tab1 AS a , tab2 AS b\nWHERE a.id = b.id and a.other &gt; b.other\n```\n* SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰:\n\n```\nSELECT \n  a.colA, \n  b.colA\nFROM \n  tab1 AS a JOIN tab2 AS b ON a.id = b.id\nWHERE \n  a.other &gt; b.other\n  \n  \n```\n\n本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下：\n\n```\ntableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition]\n```\n\n# 语义示例说明\n在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。\n\n![](D9E93F8F-D39A-40BD-A78D-8E3404225B86.png)\n\n## CROSS JOIN\n\n交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下：\n```\nmysql&gt; SELECT * FROM student JOIN course;\n+------+-------+------+-----+-------+--------+\n| no   | name  | sex  | no  | name  | credit |\n+------+-------+------+-----+-------+--------+\n| S001 | Sunny | M    | C01 | Java  |      2 |\n| S002 | Tom   | F    | C01 | Java  |      2 |\n| S003 | Kevin | M    | C01 | Java  |      2 |\n| S001 | Sunny | M    | C02 | Blink |      3 |\n| S002 | Tom   | F    | C02 | Blink |      3 |\n| S003 | Kevin | M    | C02 | Blink |      3 |\n| S001 | Sunny | M    | C03 | Spark |      3 |\n| S002 | Tom   | F    | C03 | Spark |      3 |\n| S003 | Kevin | M    | C03 | Spark |      3 |\n+------+-------+------+-----+-------+--------+\n9 rows in set (0.00 sec)\n```\n如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。\n我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下：\n```\nmysql&gt; SELECT \n    -&gt;   student.name, course.name, score \n    -&gt; FROM student JOIN  score ON student.no = score.s_no \n    -&gt;              JOIN course ON score.c_no = course.no;\n+-------+-------+-------+\n| name  | name  | score |\n+-------+-------+-------+\n| Sunny | Java  |    80 |\n| Sunny | Blink |    98 |\n| Sunny | Spark |    76 |\n| Kevin | Java  |    78 |\n| Kevin | Blink |    88 |\n| Kevin | Spark |    68 |\n+-------+-------+-------+\n6 rows in set (0.00 sec)\n```\n如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们:\n\n* 第一步 student和course 进行交叉联接：\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, c.no, stu.name, c.name\n    -&gt; FROM student stu JOIN course c  笛卡尔积\n    -&gt; ORDER BY stu.no; -- 排序只是方便大家查看:)\n+------+-----+-------+-------+\n| no   | no  | name  | name  |\n+------+-----+-------+-------+\n| S001 | C03 | Sunny | Spark |\n| S001 | C01 | Sunny | Java  |\n| S001 | C02 | Sunny | Blink |\n| S002 | C03 | Tom   | Spark |\n| S002 | C01 | Tom   | Java  |\n| S002 | C02 | Tom   | Blink |\n| S003 | C02 | Kevin | Blink |\n| S003 | C03 | Kevin | Spark |\n| S003 | C01 | Kevin | Java  |\n+------+-----+-------+-------+\n9 rows in set (0.00 sec)\n```\n* 第二步 将交叉联接的结果与score表进行左外联接，如下：\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, c.no, stu.name, c.name,\n    -&gt;    CASE \n    -&gt;     WHEN s.score IS NULL THEN 0\n    -&gt;     ELSE s.score\n    -&gt;   END AS score \n    -&gt; FROM student stu JOIN course c  -- 迪卡尔积\n    -&gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN\n    -&gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)\n+------+-----+-------+-------+-------+\n| no   | no  | name  | name  | score |\n+------+-----+-------+-------+-------+\n| S001 | C03 | Sunny | Spark |    76 |\n| S001 | C01 | Sunny | Java  |    80 |\n| S001 | C02 | Sunny | Blink |    98 |\n| S002 | C02 | Tom   | Blink |     0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息\n| S002 | C03 | Tom   | Spark |     0 |\n| S002 | C01 | Tom   | Java  |     0 |\n| S003 | C02 | Kevin | Blink |    88 |\n| S003 | C03 | Kevin | Spark |    68 |\n| S003 | C01 | Kevin | Java  |    78 |\n+------+-----+-------+-------+-------+\n9 rows in set (0.00 sec)\n```\n经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息）\n\n## INNER JOIN\n内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩:\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, stu.name , s.score\n    -&gt; FROM student stu JOIN score s ON  stu.no = s.s_no \n    -&gt; WHERE s.score &gt; 80;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n上面按语义的逻辑是:\n\n* 第一步：先进行student和score的内连接，如下：\n\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, stu.name , s.score\n    -&gt; FROM student stu JOIN score s ON  stu.no = s.s_no ;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    80 |\n| S001 | Sunny |    98 |\n| S001 | Sunny |    76 |\n| S003 | Kevin |    78 |\n| S003 | Kevin |    88 |\n| S003 | Kevin |    68 |\n+------+-------+-------+\n6 rows in set (0.00 sec)\n```\n* 第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果：    \n```\n-&gt; WHERE s.score &gt; 80;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec) \n```\n\n上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下：\n```\nmysql&gt; SELECT \n    -&gt;   no, name , score\n    -&gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &gt;80) as sc ON no = s_no;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是：\n* 第一步：执行过滤子查询\n```\nmysql&gt; SELECT s_no, score FROM score s WHERE s.score &gt;80;\n+------+-------+\n| s_no | score |\n+------+-------+\n| S001 |    98 |\n| S003 |    88 |\n+------+-------+\n2 rows in set (0.00 sec)\n```\n* 第二步：执行内连接    \n```\n-&gt; ON no = s_no;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则：\n\n* filter push down\n* project push down\n\n如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动：\n![](181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png)\n\n## LEFT OUTER JOIN\n左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单：\n\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no\n    -&gt; WHERE s.score is NULL;\n+------+------+------+-------+\n| no   | name | c_no | score |\n+------+------+------+-------+\n| S002 | Tom  | NULL |  NULL |\n+------+------+------+-------+\n1 row in set (0.00 sec)\n```\n\n上面查询的执行逻辑上也是分成两步：\n* 第一步：左外联接查询\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;\n+------+-------+------+-------+\n| no   | name  | c_no | score |\n+------+-------+------+-------+\n| S001 | Sunny | C01  |    80 |\n| S001 | Sunny | C02  |    98 |\n| S001 | Sunny | C03  |    76 |\n| S002 | Tom   | NULL |  NULL | -- 右表不存在的补NULL\n| S003 | Kevin | C01  |    78 |\n| S003 | Kevin | C02  |    88 |\n| S003 | Kevin | C03  |    68 |\n+------+-------+------+-------+\n7 rows in set (0.00 sec)\n```\n* 第二步：过滤查询\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no\n    -&gt; WHERE s.score is NULL;\n+------+------+------+-------+\n| no   | name | c_no | score |\n+------+------+------+-------+\n| S002 | Tom  | NULL |  NULL |\n+------+------+------+-------+\n1 row in set (0.00 sec)\n```\n这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下：\n\n* 第一步：先进行过滤查询(获得一个空表）\n```\nmysql&gt; SELECT * FROM score s WHERE s.score is NULL;\nEmpty set (0.00 sec)\n\n```\n\n* 第二步： 进行左外链接\n\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;\n+------+-------+------+-------+\n| no   | name  | c_no | score |\n+------+-------+------+-------+\n| S001 | Sunny | NULL |  NULL |\n| S002 | Tom   | NULL |  NULL |\n| S003 | Kevin | NULL |  NULL |\n+------+-------+------+-------+\n3 rows in set (0.00 sec)\n\n```\n\n我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。\n\n## RIGHT OUTER JOIN\n右外链接语义是返回右表所有行，左边不存在补NULL，如下：\n```\nmysql&gt; SELECT \n    -&gt;   s.c_no, s.score, no, name\n    -&gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;\n+------+-------+------+-------+\n| c_no | score | no   | name  |\n+------+-------+------+-------+\n| C01  |    80 | S001 | Sunny |\n| C02  |    98 | S001 | Sunny |\n| C03  |    76 | S001 | Sunny |\n| NULL |  NULL | S002 | Tom   | -- 左边没有的进行补 NULL\n| C01  |    78 | S003 | Kevin |\n| C02  |    88 | S003 | Kevin |\n| C03  |    68 | S003 | Kevin |\n+------+-------+------+-------+\n7 rows in set (0.00 sec)\n```\n上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。\n\n## FULL OUTER JOIN\n全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。\n\n## SELF JOIN\n上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接：\n```\nmysql&gt; SELECT * FROM student l JOIN student r where l.no = r.no;\n+------+-------+------+------+-------+------+\n| no   | name  | sex  | no   | name  | sex  |\n+------+-------+------+------+-------+------+\n| S001 | Sunny | M    | S001 | Sunny | M    |\n| S002 | Tom   | F    | S002 | Tom   | F    |\n| S003 | Kevin | M    | S003 | Kevin | M    |\n+------+-------+------+------+-------+------+\n3 rows in set (0.00 sec) \n```\n\n## 不等值联接\n这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如：\n```\nmysql&gt; SELECT \n    -&gt;   s.c_no, s.score, no, name\n    -&gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;\n+------+-------+------+-------+\n| c_no | score | no   | name  |\n+------+-------+------+-------+\n| C01  |    80 | S001 | Sunny |\n| C01  |    80 | S002 | Tom   |\n| C01  |    80 | S003 | Kevin |\n| C02  |    98 | S001 | Sunny |\n| C02  |    98 | S002 | Tom   |\n| C02  |    98 | S003 | Kevin |\n| C03  |    76 | S001 | Sunny |\n| C03  |    76 | S002 | Tom   |\n| C03  |    76 | S003 | Kevin |\n| C01  |    78 | S001 | Sunny |\n| C01  |    78 | S002 | Tom   |\n| C01  |    78 | S003 | Kevin |\n| C02  |    88 | S001 | Sunny |\n| C02  |    88 | S002 | Tom   |\n| C02  |    88 | S003 | Kevin |\n| C03  |    68 | S001 | Sunny |\n| C03  |    68 | S002 | Tom   |\n| C03  |    68 | S003 | Kevin |\n+------+-------+------+-------+\n18 rows in set (0.00 sec)\n```\n上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。\n\n\n# Apache Flink双流JOIN\n\n| | CROSS | INNER | OUTER | SELF | ON | WHERE |\n| --- | --- | --- | --- | --- | --- | --- |\n| Apache Flink | N | Y  | Y |  Y| 必选 |  可选|\n \nApache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。\n\n## 双流JOIN与传统数据库表JOIN的区别\n传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别：\n\n* 左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入；\n* JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。\n* 查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。\n\n### 数据Shuffle\n分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。\n### 数据的保存\n不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作：\n* LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；\n\n* LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；\n* RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。\n\n![](3AE44275-DFE0-45D4-8259-EB03928B784B.png)\n\n## 简单场景介绍实现原理 \n### INNER JOIN 实现\nJOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：\n![](1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png)\n双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点：\n* INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；\n* INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。\n\n### LEFT OUTER JOIN 实现\nLEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：\n![](64F21679-7168-47B0-A0B2-31C876FF3830.png)\n下图也是表达LEFT JOIN的语义，只是展现方式不同：\n![](096879FE-B4A4-4802-8749-9784162D19D0.png)\n上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点：\n* 左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。\n* 在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。\n\n## RIGHT OUTER JOIN  和 FULL OUTER JOIN\nRIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。\n\n## 复杂场景介绍State结构\n上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？\n![](49829548-F09C-47E6-A245-02B7A92749CA.png)\n上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？\n根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：\n![](45343A7B-866B-4A70-A3A5-944C989E35BD.png)\n上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点：\n* 记录重复记录（完整记录重复记录或者记录相同记录的个数）\n* 记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数）\n* 记录哪一条事件是第一个可以与左边事件进行JOIN的事件\n\n### 双流JOIN的State数据结构\n在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途：\n#### 数据结构\n* Map&lt;JoinKey, Map&lt;rowData, count>>;\n    * 第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;\n    * 第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数\n#### 数据结构的利用\n* 记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取\n* 正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素\n* 判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回\n\n# 双流JOIN的应用优化\n## 构造更新流\n我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。\n## NULL造成的热点\n比如我们有A LEFT JOIN  B ON A.aCol = B.bCol LEFT JOIN  C ON B.cCol = C.cCol 的业务，JOB的DAG如下：\n![](82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png)\n假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？\n我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下：\n![](C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png)\n\n## JOIN ReOrder\n对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol  JOIN  C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图：\n![](D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png)\n\n# 小结\n本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。\n","source":"_posts/Apache Flink 漫谈系列 - JOIN 算子.md","raw":"---\ntitle: Apache Flink 漫谈系列 - JOIN 算子\ndate: 2019-03-06 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 聊什么\n在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！\n \n\n# 什么是JOIN\n在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下：\n![](70173B48-E4B8-466C-9846-AD68FEB6F9B2.png)\n\n# 为啥需要JOIN\nJOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个：\n\n* 产生数据的源头可能不是一个系统；\n* 产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下： \n    * 1NF - 列不可再分；\n    * 2NF - 符合1NF，并且非主键属性全部依赖于主键属性；\n    * 3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来；\n    * BCNF - 符合3NF，并且主键属性之间无依赖关系。\n\n当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量）\n\n# JOIN种类\nJOIN 在传统数据库中有如下分类：\n* CROSS JOIN - 交叉连接，计算笛卡儿积；\n* INNER JOIN - 内连接，返回满足条件的记录；\n* OUTER JOIN\n    * LEFT - 返回左表所有行，右表不存在补NULL；\n    * RIGHT - 返回右表所有行，左边不存在补NULL；\n    * FULL -  返回左表和右表的并集，不存在一边补NULL;\n* SELF JOIN - 自连接，将表查询时候命名不同的别名。\n\n# JOIN语法\nJOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明：\n* SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定:\n```\nSELECT \n  a.colA, \n  b.colA\nFROM  \n  tab1 AS a , tab2 AS b\nWHERE a.id = b.id and a.other &gt; b.other\n```\n* SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰:\n\n```\nSELECT \n  a.colA, \n  b.colA\nFROM \n  tab1 AS a JOIN tab2 AS b ON a.id = b.id\nWHERE \n  a.other &gt; b.other\n  \n  \n```\n\n本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下：\n\n```\ntableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition]\n```\n\n# 语义示例说明\n在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。\n\n![](D9E93F8F-D39A-40BD-A78D-8E3404225B86.png)\n\n## CROSS JOIN\n\n交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下：\n```\nmysql&gt; SELECT * FROM student JOIN course;\n+------+-------+------+-----+-------+--------+\n| no   | name  | sex  | no  | name  | credit |\n+------+-------+------+-----+-------+--------+\n| S001 | Sunny | M    | C01 | Java  |      2 |\n| S002 | Tom   | F    | C01 | Java  |      2 |\n| S003 | Kevin | M    | C01 | Java  |      2 |\n| S001 | Sunny | M    | C02 | Blink |      3 |\n| S002 | Tom   | F    | C02 | Blink |      3 |\n| S003 | Kevin | M    | C02 | Blink |      3 |\n| S001 | Sunny | M    | C03 | Spark |      3 |\n| S002 | Tom   | F    | C03 | Spark |      3 |\n| S003 | Kevin | M    | C03 | Spark |      3 |\n+------+-------+------+-----+-------+--------+\n9 rows in set (0.00 sec)\n```\n如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。\n我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下：\n```\nmysql&gt; SELECT \n    -&gt;   student.name, course.name, score \n    -&gt; FROM student JOIN  score ON student.no = score.s_no \n    -&gt;              JOIN course ON score.c_no = course.no;\n+-------+-------+-------+\n| name  | name  | score |\n+-------+-------+-------+\n| Sunny | Java  |    80 |\n| Sunny | Blink |    98 |\n| Sunny | Spark |    76 |\n| Kevin | Java  |    78 |\n| Kevin | Blink |    88 |\n| Kevin | Spark |    68 |\n+-------+-------+-------+\n6 rows in set (0.00 sec)\n```\n如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们:\n\n* 第一步 student和course 进行交叉联接：\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, c.no, stu.name, c.name\n    -&gt; FROM student stu JOIN course c  笛卡尔积\n    -&gt; ORDER BY stu.no; -- 排序只是方便大家查看:)\n+------+-----+-------+-------+\n| no   | no  | name  | name  |\n+------+-----+-------+-------+\n| S001 | C03 | Sunny | Spark |\n| S001 | C01 | Sunny | Java  |\n| S001 | C02 | Sunny | Blink |\n| S002 | C03 | Tom   | Spark |\n| S002 | C01 | Tom   | Java  |\n| S002 | C02 | Tom   | Blink |\n| S003 | C02 | Kevin | Blink |\n| S003 | C03 | Kevin | Spark |\n| S003 | C01 | Kevin | Java  |\n+------+-----+-------+-------+\n9 rows in set (0.00 sec)\n```\n* 第二步 将交叉联接的结果与score表进行左外联接，如下：\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, c.no, stu.name, c.name,\n    -&gt;    CASE \n    -&gt;     WHEN s.score IS NULL THEN 0\n    -&gt;     ELSE s.score\n    -&gt;   END AS score \n    -&gt; FROM student stu JOIN course c  -- 迪卡尔积\n    -&gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN\n    -&gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)\n+------+-----+-------+-------+-------+\n| no   | no  | name  | name  | score |\n+------+-----+-------+-------+-------+\n| S001 | C03 | Sunny | Spark |    76 |\n| S001 | C01 | Sunny | Java  |    80 |\n| S001 | C02 | Sunny | Blink |    98 |\n| S002 | C02 | Tom   | Blink |     0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息\n| S002 | C03 | Tom   | Spark |     0 |\n| S002 | C01 | Tom   | Java  |     0 |\n| S003 | C02 | Kevin | Blink |    88 |\n| S003 | C03 | Kevin | Spark |    68 |\n| S003 | C01 | Kevin | Java  |    78 |\n+------+-----+-------+-------+-------+\n9 rows in set (0.00 sec)\n```\n经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息）\n\n## INNER JOIN\n内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩:\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, stu.name , s.score\n    -&gt; FROM student stu JOIN score s ON  stu.no = s.s_no \n    -&gt; WHERE s.score &gt; 80;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n上面按语义的逻辑是:\n\n* 第一步：先进行student和score的内连接，如下：\n\n```\nmysql&gt; SELECT \n    -&gt;   stu.no, stu.name , s.score\n    -&gt; FROM student stu JOIN score s ON  stu.no = s.s_no ;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    80 |\n| S001 | Sunny |    98 |\n| S001 | Sunny |    76 |\n| S003 | Kevin |    78 |\n| S003 | Kevin |    88 |\n| S003 | Kevin |    68 |\n+------+-------+-------+\n6 rows in set (0.00 sec)\n```\n* 第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果：    \n```\n-&gt; WHERE s.score &gt; 80;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec) \n```\n\n上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下：\n```\nmysql&gt; SELECT \n    -&gt;   no, name , score\n    -&gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &gt;80) as sc ON no = s_no;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是：\n* 第一步：执行过滤子查询\n```\nmysql&gt; SELECT s_no, score FROM score s WHERE s.score &gt;80;\n+------+-------+\n| s_no | score |\n+------+-------+\n| S001 |    98 |\n| S003 |    88 |\n+------+-------+\n2 rows in set (0.00 sec)\n```\n* 第二步：执行内连接    \n```\n-&gt; ON no = s_no;\n+------+-------+-------+\n| no   | name  | score |\n+------+-------+-------+\n| S001 | Sunny |    98 |\n| S003 | Kevin |    88 |\n+------+-------+-------+\n2 rows in set (0.00 sec)\n```\n如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则：\n\n* filter push down\n* project push down\n\n如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动：\n![](181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png)\n\n## LEFT OUTER JOIN\n左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单：\n\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no\n    -&gt; WHERE s.score is NULL;\n+------+------+------+-------+\n| no   | name | c_no | score |\n+------+------+------+-------+\n| S002 | Tom  | NULL |  NULL |\n+------+------+------+-------+\n1 row in set (0.00 sec)\n```\n\n上面查询的执行逻辑上也是分成两步：\n* 第一步：左外联接查询\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;\n+------+-------+------+-------+\n| no   | name  | c_no | score |\n+------+-------+------+-------+\n| S001 | Sunny | C01  |    80 |\n| S001 | Sunny | C02  |    98 |\n| S001 | Sunny | C03  |    76 |\n| S002 | Tom   | NULL |  NULL | -- 右表不存在的补NULL\n| S003 | Kevin | C01  |    78 |\n| S003 | Kevin | C02  |    88 |\n| S003 | Kevin | C03  |    68 |\n+------+-------+------+-------+\n7 rows in set (0.00 sec)\n```\n* 第二步：过滤查询\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no\n    -&gt; WHERE s.score is NULL;\n+------+------+------+-------+\n| no   | name | c_no | score |\n+------+------+------+-------+\n| S002 | Tom  | NULL |  NULL |\n+------+------+------+-------+\n1 row in set (0.00 sec)\n```\n这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下：\n\n* 第一步：先进行过滤查询(获得一个空表）\n```\nmysql&gt; SELECT * FROM score s WHERE s.score is NULL;\nEmpty set (0.00 sec)\n\n```\n\n* 第二步： 进行左外链接\n\n```\nmysql&gt; SELECT \n    -&gt;   no, name , s.c_no, s.score\n    -&gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;\n+------+-------+------+-------+\n| no   | name  | c_no | score |\n+------+-------+------+-------+\n| S001 | Sunny | NULL |  NULL |\n| S002 | Tom   | NULL |  NULL |\n| S003 | Kevin | NULL |  NULL |\n+------+-------+------+-------+\n3 rows in set (0.00 sec)\n\n```\n\n我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。\n\n## RIGHT OUTER JOIN\n右外链接语义是返回右表所有行，左边不存在补NULL，如下：\n```\nmysql&gt; SELECT \n    -&gt;   s.c_no, s.score, no, name\n    -&gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;\n+------+-------+------+-------+\n| c_no | score | no   | name  |\n+------+-------+------+-------+\n| C01  |    80 | S001 | Sunny |\n| C02  |    98 | S001 | Sunny |\n| C03  |    76 | S001 | Sunny |\n| NULL |  NULL | S002 | Tom   | -- 左边没有的进行补 NULL\n| C01  |    78 | S003 | Kevin |\n| C02  |    88 | S003 | Kevin |\n| C03  |    68 | S003 | Kevin |\n+------+-------+------+-------+\n7 rows in set (0.00 sec)\n```\n上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。\n\n## FULL OUTER JOIN\n全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。\n\n## SELF JOIN\n上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接：\n```\nmysql&gt; SELECT * FROM student l JOIN student r where l.no = r.no;\n+------+-------+------+------+-------+------+\n| no   | name  | sex  | no   | name  | sex  |\n+------+-------+------+------+-------+------+\n| S001 | Sunny | M    | S001 | Sunny | M    |\n| S002 | Tom   | F    | S002 | Tom   | F    |\n| S003 | Kevin | M    | S003 | Kevin | M    |\n+------+-------+------+------+-------+------+\n3 rows in set (0.00 sec) \n```\n\n## 不等值联接\n这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如：\n```\nmysql&gt; SELECT \n    -&gt;   s.c_no, s.score, no, name\n    -&gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;\n+------+-------+------+-------+\n| c_no | score | no   | name  |\n+------+-------+------+-------+\n| C01  |    80 | S001 | Sunny |\n| C01  |    80 | S002 | Tom   |\n| C01  |    80 | S003 | Kevin |\n| C02  |    98 | S001 | Sunny |\n| C02  |    98 | S002 | Tom   |\n| C02  |    98 | S003 | Kevin |\n| C03  |    76 | S001 | Sunny |\n| C03  |    76 | S002 | Tom   |\n| C03  |    76 | S003 | Kevin |\n| C01  |    78 | S001 | Sunny |\n| C01  |    78 | S002 | Tom   |\n| C01  |    78 | S003 | Kevin |\n| C02  |    88 | S001 | Sunny |\n| C02  |    88 | S002 | Tom   |\n| C02  |    88 | S003 | Kevin |\n| C03  |    68 | S001 | Sunny |\n| C03  |    68 | S002 | Tom   |\n| C03  |    68 | S003 | Kevin |\n+------+-------+------+-------+\n18 rows in set (0.00 sec)\n```\n上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。\n\n\n# Apache Flink双流JOIN\n\n| | CROSS | INNER | OUTER | SELF | ON | WHERE |\n| --- | --- | --- | --- | --- | --- | --- |\n| Apache Flink | N | Y  | Y |  Y| 必选 |  可选|\n \nApache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。\n\n## 双流JOIN与传统数据库表JOIN的区别\n传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别：\n\n* 左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入；\n* JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。\n* 查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。\n\n### 数据Shuffle\n分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。\n### 数据的保存\n不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作：\n* LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；\n\n* LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；\n* RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。\n\n![](3AE44275-DFE0-45D4-8259-EB03928B784B.png)\n\n## 简单场景介绍实现原理 \n### INNER JOIN 实现\nJOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：\n![](1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png)\n双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点：\n* INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；\n* INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。\n\n### LEFT OUTER JOIN 实现\nLEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：\n![](64F21679-7168-47B0-A0B2-31C876FF3830.png)\n下图也是表达LEFT JOIN的语义，只是展现方式不同：\n![](096879FE-B4A4-4802-8749-9784162D19D0.png)\n上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点：\n* 左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。\n* 在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。\n\n## RIGHT OUTER JOIN  和 FULL OUTER JOIN\nRIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。\n\n## 复杂场景介绍State结构\n上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？\n![](49829548-F09C-47E6-A245-02B7A92749CA.png)\n上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？\n根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：\n![](45343A7B-866B-4A70-A3A5-944C989E35BD.png)\n上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点：\n* 记录重复记录（完整记录重复记录或者记录相同记录的个数）\n* 记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数）\n* 记录哪一条事件是第一个可以与左边事件进行JOIN的事件\n\n### 双流JOIN的State数据结构\n在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途：\n#### 数据结构\n* Map&lt;JoinKey, Map&lt;rowData, count>>;\n    * 第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;\n    * 第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数\n#### 数据结构的利用\n* 记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取\n* 正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素\n* 判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回\n\n# 双流JOIN的应用优化\n## 构造更新流\n我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。\n## NULL造成的热点\n比如我们有A LEFT JOIN  B ON A.aCol = B.bCol LEFT JOIN  C ON B.cCol = C.cCol 的业务，JOB的DAG如下：\n![](82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png)\n假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？\n我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下：\n![](C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png)\n\n## JOIN ReOrder\n对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol  JOIN  C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图：\n![](D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png)\n\n# 小结\n本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。\n","slug":"Apache Flink 漫谈系列 - JOIN 算子","published":1,"updated":"2019-07-13T13:09:39.586Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fa0008s94haoo37o4i","content":"<h1>聊什么</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！</p>\n<h1>什么是JOIN</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/70173B48-E4B8-466C-9846-AD68FEB6F9B2.png\" alt></p>\n<h1>为啥需要JOIN</h1>\n<p>JOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个：</p>\n<ul>\n<li>产生数据的源头可能不是一个系统；</li>\n<li>产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下：\n<ul>\n<li>1NF - 列不可再分；</li>\n<li>2NF - 符合1NF，并且非主键属性全部依赖于主键属性；</li>\n<li>3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来；</li>\n<li>BCNF - 符合3NF，并且主键属性之间无依赖关系。</li>\n</ul>\n</li>\n</ul>\n<p>当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量）</p>\n<h1>JOIN种类</h1>\n<p>JOIN 在传统数据库中有如下分类：</p>\n<ul>\n<li>CROSS JOIN - 交叉连接，计算笛卡儿积；</li>\n<li>INNER JOIN - 内连接，返回满足条件的记录；</li>\n<li>OUTER JOIN\n<ul>\n<li>LEFT - 返回左表所有行，右表不存在补NULL；</li>\n<li>RIGHT - 返回右表所有行，左边不存在补NULL；</li>\n<li>FULL -  返回左表和右表的并集，不存在一边补NULL;</li>\n</ul>\n</li>\n<li>SELF JOIN - 自连接，将表查询时候命名不同的别名。</li>\n</ul>\n<h1>JOIN语法</h1>\n<p>JOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明：</p>\n<ul>\n<li>\n<p>SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  a.colA, </span><br><span class=\"line\">  b.colA</span><br><span class=\"line\">FROM  </span><br><span class=\"line\">  tab1 AS a , tab2 AS b</span><br><span class=\"line\">WHERE a.id = b.id and a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰:</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  a.colA, </span><br><span class=\"line\">  b.colA</span><br><span class=\"line\">FROM </span><br><span class=\"line\">  tab1 AS a JOIN tab2 AS b ON a.id = b.id</span><br><span class=\"line\">WHERE </span><br><span class=\"line\">  a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure></p>\n<p>本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition]</span><br></pre></td></tr></table></figure></p>\n<h1>语义示例说明</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。</p>\n<p><img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/D9E93F8F-D39A-40BD-A78D-8E3404225B86.png\" alt></p>\n<h2>CROSS JOIN</h2>\n<p>交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM student JOIN course;</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">| no   | name  | sex  | no  | name  | credit |</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">| S001 | Sunny | M    | C01 | Java  |      2 |</span><br><span class=\"line\">| S002 | Tom   | F    | C01 | Java  |      2 |</span><br><span class=\"line\">| S003 | Kevin | M    | C01 | Java  |      2 |</span><br><span class=\"line\">| S001 | Sunny | M    | C02 | Blink |      3 |</span><br><span class=\"line\">| S002 | Tom   | F    | C02 | Blink |      3 |</span><br><span class=\"line\">| S003 | Kevin | M    | C02 | Blink |      3 |</span><br><span class=\"line\">| S001 | Sunny | M    | C03 | Spark |      3 |</span><br><span class=\"line\">| S002 | Tom   | F    | C03 | Spark |      3 |</span><br><span class=\"line\">| S003 | Kevin | M    | C03 | Spark |      3 |</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。\n我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   student.name, course.name, score </span><br><span class=\"line\">    -&amp;gt; FROM student JOIN  score ON student.no = score.s_no </span><br><span class=\"line\">    -&amp;gt;              JOIN course ON score.c_no = course.no;</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">| name  | name  | score |</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">| Sunny | Java  |    80 |</span><br><span class=\"line\">| Sunny | Blink |    98 |</span><br><span class=\"line\">| Sunny | Spark |    76 |</span><br><span class=\"line\">| Kevin | Java  |    78 |</span><br><span class=\"line\">| Kevin | Blink |    88 |</span><br><span class=\"line\">| Kevin | Spark |    68 |</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们:</p>\n<ul>\n<li>\n<p>第一步 student和course 进行交叉联接：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, c.no, stu.name, c.name</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN course c  笛卡尔积</span><br><span class=\"line\">    -&amp;gt; ORDER BY stu.no; -- 排序只是方便大家查看:)</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">| no   | no  | name  | name  |</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">| S001 | C03 | Sunny | Spark |</span><br><span class=\"line\">| S001 | C01 | Sunny | Java  |</span><br><span class=\"line\">| S001 | C02 | Sunny | Blink |</span><br><span class=\"line\">| S002 | C03 | Tom   | Spark |</span><br><span class=\"line\">| S002 | C01 | Tom   | Java  |</span><br><span class=\"line\">| S002 | C02 | Tom   | Blink |</span><br><span class=\"line\">| S003 | C02 | Kevin | Blink |</span><br><span class=\"line\">| S003 | C03 | Kevin | Spark |</span><br><span class=\"line\">| S003 | C01 | Kevin | Java  |</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步 将交叉联接的结果与score表进行左外联接，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, c.no, stu.name, c.name,</span><br><span class=\"line\">    -&amp;gt;    CASE </span><br><span class=\"line\">    -&amp;gt;     WHEN s.score IS NULL THEN 0</span><br><span class=\"line\">    -&amp;gt;     ELSE s.score</span><br><span class=\"line\">    -&amp;gt;   END AS score </span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN course c  -- 迪卡尔积</span><br><span class=\"line\">    -&amp;gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN</span><br><span class=\"line\">    -&amp;gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">| no   | no  | name  | name  | score |</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">| S001 | C03 | Sunny | Spark |    76 |</span><br><span class=\"line\">| S001 | C01 | Sunny | Java  |    80 |</span><br><span class=\"line\">| S001 | C02 | Sunny | Blink |    98 |</span><br><span class=\"line\">| S002 | C02 | Tom   | Blink |     0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息</span><br><span class=\"line\">| S002 | C03 | Tom   | Spark |     0 |</span><br><span class=\"line\">| S002 | C01 | Tom   | Java  |     0 |</span><br><span class=\"line\">| S003 | C02 | Kevin | Blink |    88 |</span><br><span class=\"line\">| S003 | C03 | Kevin | Spark |    68 |</span><br><span class=\"line\">| S003 | C01 | Kevin | Java  |    78 |</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息）</p>\n<h2>INNER JOIN</h2>\n<p>内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no </span><br><span class=\"line\">    -&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面按语义的逻辑是:</p>\n<ul>\n<li>第一步：先进行student和score的内连接，如下：</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no ;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    80 |</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S001 | Sunny |    76 |</span><br><span class=\"line\">| S003 | Kevin |    78 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">| S003 | Kevin |    68 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果：   \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &amp;gt;80) as sc ON no = s_no;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是：</p>\n<ul>\n<li>\n<p>第一步：执行过滤子查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT s_no, score FROM score s WHERE s.score &amp;gt;80;</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">| s_no | score |</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">| S001 |    98 |</span><br><span class=\"line\">| S003 |    88 |</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步：执行内连接   \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-&amp;gt; ON no = s_no;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则：</p>\n<ul>\n<li>filter push down</li>\n<li>project push down</li>\n</ul>\n<p>如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png\" alt></p>\n<h2>LEFT OUTER JOIN</h2>\n<p>左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class=\"line\">    -&amp;gt; WHERE s.score is NULL;</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| no   | name | c_no | score |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| S002 | Tom  | NULL |  NULL |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面查询的执行逻辑上也是分成两步：</p>\n<ul>\n<li>\n<p>第一步：左外联接查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| no   | name  | c_no | score |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| S001 | Sunny | C01  |    80 |</span><br><span class=\"line\">| S001 | Sunny | C02  |    98 |</span><br><span class=\"line\">| S001 | Sunny | C03  |    76 |</span><br><span class=\"line\">| S002 | Tom   | NULL |  NULL | -- 右表不存在的补NULL</span><br><span class=\"line\">| S003 | Kevin | C01  |    78 |</span><br><span class=\"line\">| S003 | Kevin | C02  |    88 |</span><br><span class=\"line\">| S003 | Kevin | C03  |    68 |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步：过滤查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class=\"line\">    -&amp;gt; WHERE s.score is NULL;</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| no   | name | c_no | score |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| S002 | Tom  | NULL |  NULL |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下：</p>\n<ul>\n<li>\n<p>第一步：先进行过滤查询(获得一个空表）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM score s WHERE s.score is NULL;</span><br><span class=\"line\">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步： 进行左外链接</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| no   | name  | c_no | score |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| S001 | Sunny | NULL |  NULL |</span><br><span class=\"line\">| S002 | Tom   | NULL |  NULL |</span><br><span class=\"line\">| S003 | Kevin | NULL |  NULL |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。</p>\n<h2>RIGHT OUTER JOIN</h2>\n<p>右外链接语义是返回右表所有行，左边不存在补NULL，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class=\"line\">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| c_no | score | no   | name  |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| C01  |    80 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    98 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    76 | S001 | Sunny |</span><br><span class=\"line\">| NULL |  NULL | S002 | Tom   | -- 左边没有的进行补 NULL</span><br><span class=\"line\">| C01  |    78 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    88 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    68 | S003 | Kevin |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。</p>\n<h2>FULL OUTER JOIN</h2>\n<p>全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。</p>\n<h2>SELF JOIN</h2>\n<p>上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM student l JOIN student r where l.no = r.no;</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">| no   | name  | sex  | no   | name  | sex  |</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">| S001 | Sunny | M    | S001 | Sunny | M    |</span><br><span class=\"line\">| S002 | Tom   | F    | S002 | Tom   | F    |</span><br><span class=\"line\">| S003 | Kevin | M    | S003 | Kevin | M    |</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<h2>不等值联接</h2>\n<p>这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class=\"line\">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| c_no | score | no   | name  |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| C01  |    80 | S001 | Sunny |</span><br><span class=\"line\">| C01  |    80 | S002 | Tom   |</span><br><span class=\"line\">| C01  |    80 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    98 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    98 | S002 | Tom   |</span><br><span class=\"line\">| C02  |    98 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    76 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    76 | S002 | Tom   |</span><br><span class=\"line\">| C03  |    76 | S003 | Kevin |</span><br><span class=\"line\">| C01  |    78 | S001 | Sunny |</span><br><span class=\"line\">| C01  |    78 | S002 | Tom   |</span><br><span class=\"line\">| C01  |    78 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    88 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    88 | S002 | Tom   |</span><br><span class=\"line\">| C02  |    88 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    68 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    68 | S002 | Tom   |</span><br><span class=\"line\">| C03  |    68 | S003 | Kevin |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">18 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。</p>\n<h1>Apache Flink双流JOIN</h1>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>CROSS</th>\n<th>INNER</th>\n<th>OUTER</th>\n<th>SELF</th>\n<th>ON</th>\n<th>WHERE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Apache Flink</td>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>必选</td>\n<td>可选</td>\n</tr>\n</tbody>\n</table>\n<p>Apache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。</p>\n<h2>双流JOIN与传统数据库表JOIN的区别</h2>\n<p>传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别：</p>\n<ul>\n<li>左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入；</li>\n<li>JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。</li>\n<li>查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。</li>\n</ul>\n<h3>数据Shuffle</h3>\n<p>分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。</p>\n<h3>数据的保存</h3>\n<p>不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作：</p>\n<ul>\n<li>\n<p>LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；</p>\n</li>\n<li>\n<p>LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；</p>\n</li>\n<li>\n<p>RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。</p>\n</li>\n</ul>\n<p><img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/3AE44275-DFE0-45D4-8259-EB03928B784B.png\" alt></p>\n<h2>简单场景介绍实现原理</h2>\n<h3>INNER JOIN 实现</h3>\n<p>JOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png\" alt>\n双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点：</p>\n<ul>\n<li>INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；</li>\n<li>INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。</li>\n</ul>\n<h3>LEFT OUTER JOIN 实现</h3>\n<p>LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/64F21679-7168-47B0-A0B2-31C876FF3830.png\" alt>\n下图也是表达LEFT JOIN的语义，只是展现方式不同：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/096879FE-B4A4-4802-8749-9784162D19D0.png\" alt>\n上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点：</p>\n<ul>\n<li>左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。</li>\n<li>在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。</li>\n</ul>\n<h2>RIGHT OUTER JOIN  和 FULL OUTER JOIN</h2>\n<p>RIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。</p>\n<h2>复杂场景介绍State结构</h2>\n<p>上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/49829548-F09C-47E6-A245-02B7A92749CA.png\" alt>\n上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？\n根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/45343A7B-866B-4A70-A3A5-944C989E35BD.png\" alt>\n上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点：</p>\n<ul>\n<li>记录重复记录（完整记录重复记录或者记录相同记录的个数）</li>\n<li>记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数）</li>\n<li>记录哪一条事件是第一个可以与左边事件进行JOIN的事件</li>\n</ul>\n<h3>双流JOIN的State数据结构</h3>\n<p>在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途：</p>\n<h4>数据结构</h4>\n<ul>\n<li>Map&lt;JoinKey, Map&lt;rowData, count&gt;&gt;;\n<ul>\n<li>第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;</li>\n<li>第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数</li>\n</ul>\n</li>\n</ul>\n<h4>数据结构的利用</h4>\n<ul>\n<li>记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取</li>\n<li>正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素</li>\n<li>判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回</li>\n</ul>\n<h1>双流JOIN的应用优化</h1>\n<h2>构造更新流</h2>\n<p>我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。</p>\n<h2>NULL造成的热点</h2>\n<p>比如我们有A LEFT JOIN  B ON A.aCol = B.bCol LEFT JOIN  C ON B.cCol = C.cCol 的业务，JOB的DAG如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png\" alt>\n假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？\n我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png\" alt></p>\n<h2>JOIN ReOrder</h2>\n<p>对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol  JOIN  C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png\" alt></p>\n<h1>小结</h1>\n<p>本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>聊什么</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！</p>\n<h1>什么是JOIN</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/70173B48-E4B8-466C-9846-AD68FEB6F9B2.png\" alt></p>\n<h1>为啥需要JOIN</h1>\n<p>JOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个：</p>\n<ul>\n<li>产生数据的源头可能不是一个系统；</li>\n<li>产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下：\n<ul>\n<li>1NF - 列不可再分；</li>\n<li>2NF - 符合1NF，并且非主键属性全部依赖于主键属性；</li>\n<li>3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来；</li>\n<li>BCNF - 符合3NF，并且主键属性之间无依赖关系。</li>\n</ul>\n</li>\n</ul>\n<p>当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量）</p>\n<h1>JOIN种类</h1>\n<p>JOIN 在传统数据库中有如下分类：</p>\n<ul>\n<li>CROSS JOIN - 交叉连接，计算笛卡儿积；</li>\n<li>INNER JOIN - 内连接，返回满足条件的记录；</li>\n<li>OUTER JOIN\n<ul>\n<li>LEFT - 返回左表所有行，右表不存在补NULL；</li>\n<li>RIGHT - 返回右表所有行，左边不存在补NULL；</li>\n<li>FULL -  返回左表和右表的并集，不存在一边补NULL;</li>\n</ul>\n</li>\n<li>SELF JOIN - 自连接，将表查询时候命名不同的别名。</li>\n</ul>\n<h1>JOIN语法</h1>\n<p>JOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明：</p>\n<ul>\n<li>\n<p>SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  a.colA, </span><br><span class=\"line\">  b.colA</span><br><span class=\"line\">FROM  </span><br><span class=\"line\">  tab1 AS a , tab2 AS b</span><br><span class=\"line\">WHERE a.id = b.id and a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰:</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  a.colA, </span><br><span class=\"line\">  b.colA</span><br><span class=\"line\">FROM </span><br><span class=\"line\">  tab1 AS a JOIN tab2 AS b ON a.id = b.id</span><br><span class=\"line\">WHERE </span><br><span class=\"line\">  a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure></p>\n<p>本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition]</span><br></pre></td></tr></table></figure></p>\n<h1>语义示例说明</h1>\n<p>在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。</p>\n<p><img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/D9E93F8F-D39A-40BD-A78D-8E3404225B86.png\" alt></p>\n<h2>CROSS JOIN</h2>\n<p>交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM student JOIN course;</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">| no   | name  | sex  | no  | name  | credit |</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">| S001 | Sunny | M    | C01 | Java  |      2 |</span><br><span class=\"line\">| S002 | Tom   | F    | C01 | Java  |      2 |</span><br><span class=\"line\">| S003 | Kevin | M    | C01 | Java  |      2 |</span><br><span class=\"line\">| S001 | Sunny | M    | C02 | Blink |      3 |</span><br><span class=\"line\">| S002 | Tom   | F    | C02 | Blink |      3 |</span><br><span class=\"line\">| S003 | Kevin | M    | C02 | Blink |      3 |</span><br><span class=\"line\">| S001 | Sunny | M    | C03 | Spark |      3 |</span><br><span class=\"line\">| S002 | Tom   | F    | C03 | Spark |      3 |</span><br><span class=\"line\">| S003 | Kevin | M    | C03 | Spark |      3 |</span><br><span class=\"line\">+------+-------+------+-----+-------+--------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。\n我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   student.name, course.name, score </span><br><span class=\"line\">    -&amp;gt; FROM student JOIN  score ON student.no = score.s_no </span><br><span class=\"line\">    -&amp;gt;              JOIN course ON score.c_no = course.no;</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">| name  | name  | score |</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">| Sunny | Java  |    80 |</span><br><span class=\"line\">| Sunny | Blink |    98 |</span><br><span class=\"line\">| Sunny | Spark |    76 |</span><br><span class=\"line\">| Kevin | Java  |    78 |</span><br><span class=\"line\">| Kevin | Blink |    88 |</span><br><span class=\"line\">| Kevin | Spark |    68 |</span><br><span class=\"line\">+-------+-------+-------+</span><br><span class=\"line\">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们:</p>\n<ul>\n<li>\n<p>第一步 student和course 进行交叉联接：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, c.no, stu.name, c.name</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN course c  笛卡尔积</span><br><span class=\"line\">    -&amp;gt; ORDER BY stu.no; -- 排序只是方便大家查看:)</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">| no   | no  | name  | name  |</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">| S001 | C03 | Sunny | Spark |</span><br><span class=\"line\">| S001 | C01 | Sunny | Java  |</span><br><span class=\"line\">| S001 | C02 | Sunny | Blink |</span><br><span class=\"line\">| S002 | C03 | Tom   | Spark |</span><br><span class=\"line\">| S002 | C01 | Tom   | Java  |</span><br><span class=\"line\">| S002 | C02 | Tom   | Blink |</span><br><span class=\"line\">| S003 | C02 | Kevin | Blink |</span><br><span class=\"line\">| S003 | C03 | Kevin | Spark |</span><br><span class=\"line\">| S003 | C01 | Kevin | Java  |</span><br><span class=\"line\">+------+-----+-------+-------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步 将交叉联接的结果与score表进行左外联接，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, c.no, stu.name, c.name,</span><br><span class=\"line\">    -&amp;gt;    CASE </span><br><span class=\"line\">    -&amp;gt;     WHEN s.score IS NULL THEN 0</span><br><span class=\"line\">    -&amp;gt;     ELSE s.score</span><br><span class=\"line\">    -&amp;gt;   END AS score </span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN course c  -- 迪卡尔积</span><br><span class=\"line\">    -&amp;gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN</span><br><span class=\"line\">    -&amp;gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">| no   | no  | name  | name  | score |</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">| S001 | C03 | Sunny | Spark |    76 |</span><br><span class=\"line\">| S001 | C01 | Sunny | Java  |    80 |</span><br><span class=\"line\">| S001 | C02 | Sunny | Blink |    98 |</span><br><span class=\"line\">| S002 | C02 | Tom   | Blink |     0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息</span><br><span class=\"line\">| S002 | C03 | Tom   | Spark |     0 |</span><br><span class=\"line\">| S002 | C01 | Tom   | Java  |     0 |</span><br><span class=\"line\">| S003 | C02 | Kevin | Blink |    88 |</span><br><span class=\"line\">| S003 | C03 | Kevin | Spark |    68 |</span><br><span class=\"line\">| S003 | C01 | Kevin | Java  |    78 |</span><br><span class=\"line\">+------+-----+-------+-------+-------+</span><br><span class=\"line\">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息）</p>\n<h2>INNER JOIN</h2>\n<p>内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no </span><br><span class=\"line\">    -&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面按语义的逻辑是:</p>\n<ul>\n<li>第一步：先进行student和score的内连接，如下：</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no ;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    80 |</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S001 | Sunny |    76 |</span><br><span class=\"line\">| S003 | Kevin |    78 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">| S003 | Kevin |    68 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果：   \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , score</span><br><span class=\"line\">    -&amp;gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &amp;gt;80) as sc ON no = s_no;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是：</p>\n<ul>\n<li>\n<p>第一步：执行过滤子查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT s_no, score FROM score s WHERE s.score &amp;gt;80;</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">| s_no | score |</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">| S001 |    98 |</span><br><span class=\"line\">| S003 |    88 |</span><br><span class=\"line\">+------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步：执行内连接   \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-&amp;gt; ON no = s_no;</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| no   | name  | score |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">| S001 | Sunny |    98 |</span><br><span class=\"line\">| S003 | Kevin |    88 |</span><br><span class=\"line\">+------+-------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则：</p>\n<ul>\n<li>filter push down</li>\n<li>project push down</li>\n</ul>\n<p>如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png\" alt></p>\n<h2>LEFT OUTER JOIN</h2>\n<p>左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class=\"line\">    -&amp;gt; WHERE s.score is NULL;</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| no   | name | c_no | score |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| S002 | Tom  | NULL |  NULL |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面查询的执行逻辑上也是分成两步：</p>\n<ul>\n<li>\n<p>第一步：左外联接查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| no   | name  | c_no | score |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| S001 | Sunny | C01  |    80 |</span><br><span class=\"line\">| S001 | Sunny | C02  |    98 |</span><br><span class=\"line\">| S001 | Sunny | C03  |    76 |</span><br><span class=\"line\">| S002 | Tom   | NULL |  NULL | -- 右表不存在的补NULL</span><br><span class=\"line\">| S003 | Kevin | C01  |    78 |</span><br><span class=\"line\">| S003 | Kevin | C02  |    88 |</span><br><span class=\"line\">| S003 | Kevin | C03  |    68 |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步：过滤查询\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class=\"line\">    -&amp;gt; WHERE s.score is NULL;</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| no   | name | c_no | score |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">| S002 | Tom  | NULL |  NULL |</span><br><span class=\"line\">+------+------+------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下：</p>\n<ul>\n<li>\n<p>第一步：先进行过滤查询(获得一个空表）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM score s WHERE s.score is NULL;</span><br><span class=\"line\">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>第二步： 进行左外链接</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class=\"line\">    -&amp;gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| no   | name  | c_no | score |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| S001 | Sunny | NULL |  NULL |</span><br><span class=\"line\">| S002 | Tom   | NULL |  NULL |</span><br><span class=\"line\">| S003 | Kevin | NULL |  NULL |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。</p>\n<h2>RIGHT OUTER JOIN</h2>\n<p>右外链接语义是返回右表所有行，左边不存在补NULL，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class=\"line\">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| c_no | score | no   | name  |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| C01  |    80 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    98 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    76 | S001 | Sunny |</span><br><span class=\"line\">| NULL |  NULL | S002 | Tom   | -- 左边没有的进行补 NULL</span><br><span class=\"line\">| C01  |    78 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    88 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    68 | S003 | Kevin |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。</p>\n<h2>FULL OUTER JOIN</h2>\n<p>全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。</p>\n<h2>SELF JOIN</h2>\n<p>上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT * FROM student l JOIN student r where l.no = r.no;</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">| no   | name  | sex  | no   | name  | sex  |</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">| S001 | Sunny | M    | S001 | Sunny | M    |</span><br><span class=\"line\">| S002 | Tom   | F    | S002 | Tom   | F    |</span><br><span class=\"line\">| S003 | Kevin | M    | S003 | Kevin | M    |</span><br><span class=\"line\">+------+-------+------+------+-------+------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<h2>不等值联接</h2>\n<p>这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&amp;gt; SELECT </span><br><span class=\"line\">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class=\"line\">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| c_no | score | no   | name  |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">| C01  |    80 | S001 | Sunny |</span><br><span class=\"line\">| C01  |    80 | S002 | Tom   |</span><br><span class=\"line\">| C01  |    80 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    98 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    98 | S002 | Tom   |</span><br><span class=\"line\">| C02  |    98 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    76 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    76 | S002 | Tom   |</span><br><span class=\"line\">| C03  |    76 | S003 | Kevin |</span><br><span class=\"line\">| C01  |    78 | S001 | Sunny |</span><br><span class=\"line\">| C01  |    78 | S002 | Tom   |</span><br><span class=\"line\">| C01  |    78 | S003 | Kevin |</span><br><span class=\"line\">| C02  |    88 | S001 | Sunny |</span><br><span class=\"line\">| C02  |    88 | S002 | Tom   |</span><br><span class=\"line\">| C02  |    88 | S003 | Kevin |</span><br><span class=\"line\">| C03  |    68 | S001 | Sunny |</span><br><span class=\"line\">| C03  |    68 | S002 | Tom   |</span><br><span class=\"line\">| C03  |    68 | S003 | Kevin |</span><br><span class=\"line\">+------+-------+------+-------+</span><br><span class=\"line\">18 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。</p>\n<h1>Apache Flink双流JOIN</h1>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>CROSS</th>\n<th>INNER</th>\n<th>OUTER</th>\n<th>SELF</th>\n<th>ON</th>\n<th>WHERE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Apache Flink</td>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>必选</td>\n<td>可选</td>\n</tr>\n</tbody>\n</table>\n<p>Apache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。</p>\n<h2>双流JOIN与传统数据库表JOIN的区别</h2>\n<p>传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别：</p>\n<ul>\n<li>左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入；</li>\n<li>JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。</li>\n<li>查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。</li>\n</ul>\n<h3>数据Shuffle</h3>\n<p>分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。</p>\n<h3>数据的保存</h3>\n<p>不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作：</p>\n<ul>\n<li>\n<p>LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；</p>\n</li>\n<li>\n<p>LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；</p>\n</li>\n<li>\n<p>RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。</p>\n</li>\n</ul>\n<p><img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/3AE44275-DFE0-45D4-8259-EB03928B784B.png\" alt></p>\n<h2>简单场景介绍实现原理</h2>\n<h3>INNER JOIN 实现</h3>\n<p>JOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png\" alt>\n双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点：</p>\n<ul>\n<li>INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；</li>\n<li>INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。</li>\n</ul>\n<h3>LEFT OUTER JOIN 实现</h3>\n<p>LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/64F21679-7168-47B0-A0B2-31C876FF3830.png\" alt>\n下图也是表达LEFT JOIN的语义，只是展现方式不同：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/096879FE-B4A4-4802-8749-9784162D19D0.png\" alt>\n上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点：</p>\n<ul>\n<li>左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。</li>\n<li>在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。</li>\n</ul>\n<h2>RIGHT OUTER JOIN  和 FULL OUTER JOIN</h2>\n<p>RIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。</p>\n<h2>复杂场景介绍State结构</h2>\n<p>上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/49829548-F09C-47E6-A245-02B7A92749CA.png\" alt>\n上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？\n根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/45343A7B-866B-4A70-A3A5-944C989E35BD.png\" alt>\n上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点：</p>\n<ul>\n<li>记录重复记录（完整记录重复记录或者记录相同记录的个数）</li>\n<li>记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数）</li>\n<li>记录哪一条事件是第一个可以与左边事件进行JOIN的事件</li>\n</ul>\n<h3>双流JOIN的State数据结构</h3>\n<p>在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途：</p>\n<h4>数据结构</h4>\n<ul>\n<li>Map&lt;JoinKey, Map&lt;rowData, count&gt;&gt;;\n<ul>\n<li>第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;</li>\n<li>第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数</li>\n</ul>\n</li>\n</ul>\n<h4>数据结构的利用</h4>\n<ul>\n<li>记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取</li>\n<li>正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素</li>\n<li>判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回</li>\n</ul>\n<h1>双流JOIN的应用优化</h1>\n<h2>构造更新流</h2>\n<p>我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。</p>\n<h2>NULL造成的热点</h2>\n<p>比如我们有A LEFT JOIN  B ON A.aCol = B.bCol LEFT JOIN  C ON B.cCol = C.cCol 的业务，JOB的DAG如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png\" alt>\n假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？\n我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png\" alt></p>\n<h2>JOIN ReOrder</h2>\n<p>对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol  JOIN  C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图：\n<img src=\"/2019/03/06/Apache Flink 漫谈系列 - JOIN 算子/D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png\" alt></p>\n<h1>小结</h1>\n<p>本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。</p>\n"},{"title":"Apache Flink 漫谈系列 -  Watermark","date":"2019-01-01T10:18:18.000Z","updated":"2019-01-11T10:18:18.000Z","_content":"# 实际问题（乱序）\n在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：\n![](7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png)\n那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？\n\n# Apache Flink的时间类型\n开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图：\n\n![](E5D7CBF0-A452-4907-8FCF-F97896C6993A.png)\n\n* ProcessingTime \n是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。\n\n* IngestionTime\nIngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。\n\n* EventTime\nEventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。\n\n开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。\n\n# 什么是Watermark\nWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: \n![](BC6D8C93-A068-4799-B2CC-6A75AD43549C.png)\n\n# Watermark的产生方式\n目前Apache Flink 有两种生产Watermark的方式，如下：\n\n* Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 \n在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。\n* Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。\n\n所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。\n\n# Watermark的接口定义\n对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下：\n\n* Periodic Watermarks - AssignerWithPeriodicWatermarks\n```\n/**\n * Returns the current watermark. This method is periodically called by the\n * system to retrieve the current watermark. The method may return {@code null} to\n * indicate that no new Watermark is available.\n *\n * &lt;p&gt;The returned watermark will be emitted only if it is non-null and itsTimestamp\n * is larger than that of the previously emitted watermark (to preserve the contract of\n * ascending watermarks). If the current watermark is still\n * identical to the previous one, no progress in EventTime has happened since\n * the previous call to this method. If a null value is returned, or theTimestamp\n * of the returned watermark is smaller than that of the last emitted one, then no\n * new watermark will be generated.\n *\n * &lt;p&gt;The interval in which this method is called and Watermarks are generated\n * depends on {@link ExecutionConfig#getAutoWatermarkInterval()}.\n *\n * @see org.Apache.flink.streaming.api.watermark.Watermark\n * @see ExecutionConfig#getAutoWatermarkInterval()\n *\n * @return {@code Null}, if no watermark should be emitted, or the next watermark to emit.\n */\n @Nullable\n Watermark getCurrentWatermark();\n```\n\n* Punctuated Watermarks - AssignerWithPunctuatedWatermarks \n\n```\npublic interface AssignerWithPunctuatedWatermarks&lt;T&gt; extendsTimestampAssigner&lt;T&gt; {\n\n/**\n * Asks this implementation if it wants to emit a watermark. This method is called right after\n * the {@link #extractTimestamp(Object, long)} method.\n *\n * &lt;p&gt;The returned watermark will be emitted only if it is non-null and itsTimestamp\n * is larger than that of the previously emitted watermark (to preserve the contract of\n * ascending watermarks). If a null value is returned, or theTimestamp of the returned\n * watermark is smaller than that of the last emitted one, then no new watermark will\n * be generated.\n *\n * &lt;p&gt;For an example how to use this method, see the documentation of\n * {@link AssignerWithPunctuatedWatermarks this class}.\n *\n * @return {@code Null}, if no watermark should be emitted, or the next watermark to emit.\n */\n @Nullable\nWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);\n}\n```\nAssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner\n\n```\npublic interfaceTimestampAssigner&lt;T&gt; extends Function {\n\n/**\n * Assigns aTimestamp to an element, in milliseconds since the Epoch.\n *\n * &lt;p&gt;The method is passed the previously assignedTimestamp of the element.\n * That previousTimestamp may have been assigned from a previous assigner,\n * by ingestionTime. If the element did not carry aTimestamp before, this value is\n * {@code Long.MIN_VALUE}.\n *\n * @param element The element that theTimestamp is wil be assigned to.\n * @param previousElementTimestamp The previous internalTimestamp of the element,\n *                                 or a negative value, if noTimestamp has been assigned, yet.\n * @return The newTimestamp.\n */\nlong extractTimestamp(T element, long previousElementTimestamp);\n}\n\n```\n从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。\n\n# Watermark解决如上问题\n从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。\n\n回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 \n\n* 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：![](FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png)\n 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：\n \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) \n) with (\n  ...\n);\n \n* 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下：![](C15AA404-D132-4D6C-BA35-B103CA53AE61.png)\n \n上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) \n) with (\n  ...\n);\n上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s.\n\n# 多流的Watermark处理\n在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示：\n\n![](237CED69-52AB-4206-BF02-F99183C7186E.png)\n \nApache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图:\n \n![](77C24B94-7F26-4F72-BCA3-D0F20A791772.png)\n \n# 小结\n本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。\n\n","source":"_posts/Apache Flink 漫谈系列 -  Watermark.md","raw":"---\ntitle: Apache Flink 漫谈系列 -  Watermark\ndate: 2019-01-01 18:18:18\nupdated: 2019-01-11 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 实际问题（乱序）\n在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：\n![](7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png)\n那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？\n\n# Apache Flink的时间类型\n开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图：\n\n![](E5D7CBF0-A452-4907-8FCF-F97896C6993A.png)\n\n* ProcessingTime \n是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。\n\n* IngestionTime\nIngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。\n\n* EventTime\nEventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。\n\n开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。\n\n# 什么是Watermark\nWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: \n![](BC6D8C93-A068-4799-B2CC-6A75AD43549C.png)\n\n# Watermark的产生方式\n目前Apache Flink 有两种生产Watermark的方式，如下：\n\n* Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 \n在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。\n* Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。\n\n所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。\n\n# Watermark的接口定义\n对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下：\n\n* Periodic Watermarks - AssignerWithPeriodicWatermarks\n```\n/**\n * Returns the current watermark. This method is periodically called by the\n * system to retrieve the current watermark. The method may return {@code null} to\n * indicate that no new Watermark is available.\n *\n * &lt;p&gt;The returned watermark will be emitted only if it is non-null and itsTimestamp\n * is larger than that of the previously emitted watermark (to preserve the contract of\n * ascending watermarks). If the current watermark is still\n * identical to the previous one, no progress in EventTime has happened since\n * the previous call to this method. If a null value is returned, or theTimestamp\n * of the returned watermark is smaller than that of the last emitted one, then no\n * new watermark will be generated.\n *\n * &lt;p&gt;The interval in which this method is called and Watermarks are generated\n * depends on {@link ExecutionConfig#getAutoWatermarkInterval()}.\n *\n * @see org.Apache.flink.streaming.api.watermark.Watermark\n * @see ExecutionConfig#getAutoWatermarkInterval()\n *\n * @return {@code Null}, if no watermark should be emitted, or the next watermark to emit.\n */\n @Nullable\n Watermark getCurrentWatermark();\n```\n\n* Punctuated Watermarks - AssignerWithPunctuatedWatermarks \n\n```\npublic interface AssignerWithPunctuatedWatermarks&lt;T&gt; extendsTimestampAssigner&lt;T&gt; {\n\n/**\n * Asks this implementation if it wants to emit a watermark. This method is called right after\n * the {@link #extractTimestamp(Object, long)} method.\n *\n * &lt;p&gt;The returned watermark will be emitted only if it is non-null and itsTimestamp\n * is larger than that of the previously emitted watermark (to preserve the contract of\n * ascending watermarks). If a null value is returned, or theTimestamp of the returned\n * watermark is smaller than that of the last emitted one, then no new watermark will\n * be generated.\n *\n * &lt;p&gt;For an example how to use this method, see the documentation of\n * {@link AssignerWithPunctuatedWatermarks this class}.\n *\n * @return {@code Null}, if no watermark should be emitted, or the next watermark to emit.\n */\n @Nullable\nWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);\n}\n```\nAssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner\n\n```\npublic interfaceTimestampAssigner&lt;T&gt; extends Function {\n\n/**\n * Assigns aTimestamp to an element, in milliseconds since the Epoch.\n *\n * &lt;p&gt;The method is passed the previously assignedTimestamp of the element.\n * That previousTimestamp may have been assigned from a previous assigner,\n * by ingestionTime. If the element did not carry aTimestamp before, this value is\n * {@code Long.MIN_VALUE}.\n *\n * @param element The element that theTimestamp is wil be assigned to.\n * @param previousElementTimestamp The previous internalTimestamp of the element,\n *                                 or a negative value, if noTimestamp has been assigned, yet.\n * @return The newTimestamp.\n */\nlong extractTimestamp(T element, long previousElementTimestamp);\n}\n\n```\n从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。\n\n# Watermark解决如上问题\n从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。\n\n回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 \n\n* 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：![](FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png)\n 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：\n \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) \n) with (\n  ...\n);\n \n* 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下：![](C15AA404-D132-4D6C-BA35-B103CA53AE61.png)\n \n上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) \n) with (\n  ...\n);\n上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s.\n\n# 多流的Watermark处理\n在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示：\n\n![](237CED69-52AB-4206-BF02-F99183C7186E.png)\n \nApache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图:\n \n![](77C24B94-7F26-4F72-BCA3-D0F20A791772.png)\n \n# 小结\n本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。\n\n","slug":"Apache Flink 漫谈系列 -  Watermark","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fb0009s94h3odv0vds","content":"<h1>实际问题（乱序）</h1>\n<p>在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png\" alt>\n那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？</p>\n<h1>Apache Flink的时间类型</h1>\n<p>开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/E5D7CBF0-A452-4907-8FCF-F97896C6993A.png\" alt></p>\n<ul>\n<li>\n<p>ProcessingTime\n是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。</p>\n</li>\n<li>\n<p>IngestionTime\nIngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。</p>\n</li>\n<li>\n<p>EventTime\nEventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。</p>\n</li>\n</ul>\n<p>开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。</p>\n<h1>什么是Watermark</h1>\n<p>Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/BC6D8C93-A068-4799-B2CC-6A75AD43549C.png\" alt></p>\n<h1>Watermark的产生方式</h1>\n<p>目前Apache Flink 有两种生产Watermark的方式，如下：</p>\n<ul>\n<li>Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。\n在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。</li>\n<li>Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。</li>\n</ul>\n<p>所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。</p>\n<h1>Watermark的接口定义</h1>\n<p>对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下：</p>\n<ul>\n<li>\n<p>Periodic Watermarks - AssignerWithPeriodicWatermarks\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * Returns the current watermark. This method is periodically called by the</span><br><span class=\"line\"> * system to retrieve the current watermark. The method may return &#123;@code null&#125; to</span><br><span class=\"line\"> * indicate that no new Watermark is available.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class=\"line\"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class=\"line\"> * ascending watermarks). If the current watermark is still</span><br><span class=\"line\"> * identical to the previous one, no progress in EventTime has happened since</span><br><span class=\"line\"> * the previous call to this method. If a null value is returned, or theTimestamp</span><br><span class=\"line\"> * of the returned watermark is smaller than that of the last emitted one, then no</span><br><span class=\"line\"> * new watermark will be generated.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated</span><br><span class=\"line\"> * depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @see org.Apache.flink.streaming.api.watermark.Watermark</span><br><span class=\"line\"> * @see ExecutionConfig#getAutoWatermarkInterval()</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class=\"line\"> */</span><br><span class=\"line\"> @Nullable</span><br><span class=\"line\"> Watermark getCurrentWatermark();</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>Punctuated Watermarks - AssignerWithPunctuatedWatermarks</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * Asks this implementation if it wants to emit a watermark. This method is called right after</span><br><span class=\"line\"> * the &#123;@link #extractTimestamp(Object, long)&#125; method.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class=\"line\"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class=\"line\"> * ascending watermarks). If a null value is returned, or theTimestamp of the returned</span><br><span class=\"line\"> * watermark is smaller than that of the last emitted one, then no new watermark will</span><br><span class=\"line\"> * be generated.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of</span><br><span class=\"line\"> * &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class=\"line\"> */</span><br><span class=\"line\"> @Nullable</span><br><span class=\"line\">Watermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * Assigns aTimestamp to an element, in milliseconds since the Epoch.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element.</span><br><span class=\"line\"> * That previousTimestamp may have been assigned from a previous assigner,</span><br><span class=\"line\"> * by ingestionTime. If the element did not carry aTimestamp before, this value is</span><br><span class=\"line\"> * &#123;@code Long.MIN_VALUE&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @param element The element that theTimestamp is wil be assigned to.</span><br><span class=\"line\"> * @param previousElementTimestamp The previous internalTimestamp of the element,</span><br><span class=\"line\"> *                                 or a negative value, if noTimestamp has been assigned, yet.</span><br><span class=\"line\"> * @return The newTimestamp.</span><br><span class=\"line\"> */</span><br><span class=\"line\">long extractTimestamp(T element, long previousElementTimestamp);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。</p>\n<h1>Watermark解决如上问题</h1>\n<p>从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。</p>\n<p>回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。</p>\n<ul>\n<li>当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png\" alt>\n 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：\n \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0)\n) with (\n  ...\n);</li>\n<li>如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下：<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/C15AA404-D132-4D6C-BA35-B103CA53AE61.png\" alt>\n \n上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000)\n) with (\n  ...\n);\n上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s.</li>\n</ul>\n<h1>多流的Watermark处理</h1>\n<p>在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/237CED69-52AB-4206-BF02-F99183C7186E.png\" alt>\n \nApache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图:\n \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/77C24B94-7F26-4F72-BCA3-D0F20A791772.png\" alt></p>\n<h1>小结</h1>\n<p>本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>实际问题（乱序）</h1>\n<p>在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png\" alt>\n那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？</p>\n<h1>Apache Flink的时间类型</h1>\n<p>开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/E5D7CBF0-A452-4907-8FCF-F97896C6993A.png\" alt></p>\n<ul>\n<li>\n<p>ProcessingTime\n是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。</p>\n</li>\n<li>\n<p>IngestionTime\nIngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。</p>\n</li>\n<li>\n<p>EventTime\nEventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。</p>\n</li>\n</ul>\n<p>开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。</p>\n<h1>什么是Watermark</h1>\n<p>Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/BC6D8C93-A068-4799-B2CC-6A75AD43549C.png\" alt></p>\n<h1>Watermark的产生方式</h1>\n<p>目前Apache Flink 有两种生产Watermark的方式，如下：</p>\n<ul>\n<li>Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。\n在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。</li>\n<li>Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。</li>\n</ul>\n<p>所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。</p>\n<h1>Watermark的接口定义</h1>\n<p>对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下：</p>\n<ul>\n<li>\n<p>Periodic Watermarks - AssignerWithPeriodicWatermarks\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * Returns the current watermark. This method is periodically called by the</span><br><span class=\"line\"> * system to retrieve the current watermark. The method may return &#123;@code null&#125; to</span><br><span class=\"line\"> * indicate that no new Watermark is available.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class=\"line\"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class=\"line\"> * ascending watermarks). If the current watermark is still</span><br><span class=\"line\"> * identical to the previous one, no progress in EventTime has happened since</span><br><span class=\"line\"> * the previous call to this method. If a null value is returned, or theTimestamp</span><br><span class=\"line\"> * of the returned watermark is smaller than that of the last emitted one, then no</span><br><span class=\"line\"> * new watermark will be generated.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated</span><br><span class=\"line\"> * depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @see org.Apache.flink.streaming.api.watermark.Watermark</span><br><span class=\"line\"> * @see ExecutionConfig#getAutoWatermarkInterval()</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class=\"line\"> */</span><br><span class=\"line\"> @Nullable</span><br><span class=\"line\"> Watermark getCurrentWatermark();</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>Punctuated Watermarks - AssignerWithPunctuatedWatermarks</p>\n</li>\n</ul>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * Asks this implementation if it wants to emit a watermark. This method is called right after</span><br><span class=\"line\"> * the &#123;@link #extractTimestamp(Object, long)&#125; method.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class=\"line\"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class=\"line\"> * ascending watermarks). If a null value is returned, or theTimestamp of the returned</span><br><span class=\"line\"> * watermark is smaller than that of the last emitted one, then no new watermark will</span><br><span class=\"line\"> * be generated.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of</span><br><span class=\"line\"> * &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class=\"line\"> */</span><br><span class=\"line\"> @Nullable</span><br><span class=\"line\">Watermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * Assigns aTimestamp to an element, in milliseconds since the Epoch.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element.</span><br><span class=\"line\"> * That previousTimestamp may have been assigned from a previous assigner,</span><br><span class=\"line\"> * by ingestionTime. If the element did not carry aTimestamp before, this value is</span><br><span class=\"line\"> * &#123;@code Long.MIN_VALUE&#125;.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @param element The element that theTimestamp is wil be assigned to.</span><br><span class=\"line\"> * @param previousElementTimestamp The previous internalTimestamp of the element,</span><br><span class=\"line\"> *                                 or a negative value, if noTimestamp has been assigned, yet.</span><br><span class=\"line\"> * @return The newTimestamp.</span><br><span class=\"line\"> */</span><br><span class=\"line\">long extractTimestamp(T element, long previousElementTimestamp);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。</p>\n<h1>Watermark解决如上问题</h1>\n<p>从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。</p>\n<p>回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。</p>\n<ul>\n<li>当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png\" alt>\n 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：\n \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0)\n) with (\n  ...\n);</li>\n<li>如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下：<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/C15AA404-D132-4D6C-BA35-B103CA53AE61.png\" alt>\n \n上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： \nCREATE TABLE source(\n  ...,\n  Event_timeTimeStamp,\n  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000)\n) with (\n  ...\n);\n上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s.</li>\n</ul>\n<h1>多流的Watermark处理</h1>\n<p>在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/237CED69-52AB-4206-BF02-F99183C7186E.png\" alt>\n \nApache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图:\n \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 -  Watermark/77C24B94-7F26-4F72-BCA3-D0F20A791772.png\" alt></p>\n<h1>小结</h1>\n<p>本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。</p>\n"},{"title":"Apache Flink 漫谈系列 - Table API 概览","date":"2019-03-26T10:18:18.000Z","_content":"# 什么是Table API\n在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象.\n\n如下图所示：\n\n![](6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png)\n\nApache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。\n\n# Table API的特点\n\nTable API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下：\n* 声明式 - 用户只关心做什么，不用关心怎么做；\n* 高性能 - 支持查询优化，可以获取最好的执行性能；\n* 流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行；\n* 标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。\n\n当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点：\n* 表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等\n* 功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能\n* 编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。\n\n**说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 [FLIP-29](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739) 中规划的新功能。**\n\n# HelloWorld\n在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。\n\n## Maven 依赖\n在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。\n```\n<properties>\n    <table.version>1.7.0</table.version>\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-table_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-scala_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-streaming-scala_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-streaming-java_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n  </dependencies>\n```\n## 程序结构\n在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示：\n![](6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png)\n1. 外部数据源，比如Kafka, Rabbitmq, CSV 等等；\n2. 查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等；\n3. 外部结果存储，比如Kafka，Cassandra，CSV等。\n\n**说明：1和3 在Apache Flink中统称为Connector。**\n\n## 主程序\n我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。\n根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码：\n\n- TableSoruce Code - 用于创建数据源的代码\n- Table API Query - 用于进行word count统计的Table API 查询逻辑\n- TableSink Code - 用于保存word count计算结果的结果表代码 \n\n### 运行模式选择\n一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。\n```\n// Stream运行环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n```\n### 构建测试Source\n我们用最简单的构建Source方式进行本次测试，代码如下：\n```\n // 测试数据\n val data = Seq(\"Flink\", \"Bob\", \"Bob\", \"something\", \"Hello\", \"Flink\", \"Bob\")\n // 最简单的获取Source方式\nval source = env.fromCollection(data).toTable(tEnv, 'word)\n```\n\n### WordCount 统计逻辑\nWordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下：\n```\n // 单词统计核心逻辑\n val result = source\n   .groupBy('word) // 单词分组\n   .select('word, 'word.count) // 单词统计\n```\n### 定义Sink\n将WordCount的统计结果写入Sink中，代码如下：\n```\n// 自定义Sink\n    val sink = new RetractSink // 自定义Sink（下面有完整代码）\n    // 计算结果写入sink\n    result.toRetractStream[(String, Long)].addSink(sink)\n```\n### 完整的HelloWord代码\n为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下:\n```\nimport org.apache.flink.api.scala._\nimport org.apache.flink.configuration.Configuration\nimport org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.table.api.scala._\n\nimport scala.collection.mutable\n\nobject HelloWord {\n\n  def main(args: Array[String]): Unit = {\n    // 测试数据\n    val data = Seq(\"Flink\", \"Bob\", \"Bob\", \"something\", \"Hello\", \"Flink\", \"Bob\")\n\n    // Stream运行环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    // 最简单的获取Source方式\n    val source = env.fromCollection(data).toTable(tEnv, 'word)\n\n    // 单词统计核心逻辑\n    val result = source\n      .groupBy('word) // 单词分组\n      .select('word, 'word.count) // 单词统计\n\n    // 自定义Sink\n    val sink = new RetractSink\n    // 计算结果写入sink\n    result.toRetractStream[(String, Long)].addSink(sink)\n\n    env.execute\n  }\n}\n\nclass RetractSink extends RichSinkFunction[(Boolean, (String, Long))] {\n  private var resultSet: mutable.Set[(String, Long)] = _\n\n  override def open(parameters: Configuration): Unit = {\n    // 初始化内存存储结构\n    resultSet = new mutable.HashSet[(String, Long)]\n  }\n\n  override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = {\n    if (v._1) {\n      // 计算数据\n      resultSet.add(v._2)\n    }\n    else {\n      // 撤回数据\n      resultSet.remove(v._2)\n    }\n  }\n\n  override def close(): Unit = {\n    // 打印写入sink的结果数据\n    resultSet.foreach(println)\n  }\n}\n```\n\n运行结果如下：\n![](E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png)\n\n虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码: \n` source.groupBy('word).select('word, 'word.count)`\n所以Table API开发技术任务非常的简洁高效。\n\n# Table API 算子\n虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。\n\n## 示例数据及测试类\n### 测试数据\n\n* customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n* order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：\n\n| o_id | c_id | o_time|o_desc |\n| --- | --- | --- | --- |\n| o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n| o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n\n* Item_tab \n  商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：\n  \n| itemID | itemType | onSellTime | price |\n| :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 |\n| ITEM003 | Electronic | _**2017-11-11 10:03:00**_ | 30 |\n| ITEM004 | Electronic | _**2017-11-11 10:03:00**_ | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 |\n\n* PageAccess_tab\n页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n* PageAccessCount_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userCount | accessTime |\n| --- | --- | --- |\n| ShangHai | 100 | 2017.11.11 10:01:00 |\n| BeiJing |  86| 2017.11.11 10:01:00 |\n| BeiJing | 210 | 2017.11.11 10:06:00 |\n| BeiJing  | 33 | 2017.11.11 10:10:00 |\n| ShangHai | 129 | 2017.11.11 12:10:00 |\n\n* PageAccessSession_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n|ShangHai|U0011|2017-11-11 10:01:00\n|ShangHai|U0012|2017-11-11 10:02:00\n|ShangHai|U0013|2017-11-11 10:03:00\n|ShangHai|U0015|2017-11-11 10:05:00\n|ShangHai|U0011|2017-11-11 10:10:00\n|BeiJing|U0110|2017-11-11 10:10:00\n|ShangHai|U2010|2017-11-11 10:11:00\n|ShangHai|U0410|2017-11-11 12:16:00\n\n### 测试类\n我们创建一个`TableAPIOverviewITCase.scala` 用于接下来介绍Flink Table API算子的功能体验。代码如下：\n```\nimport org.apache.flink.api.scala._\nimport org.apache.flink.runtime.state.StateBackend\nimport org.apache.flink.runtime.state.memory.MemoryStateBackend\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction\nimport org.apache.flink.streaming.api.functions.source.SourceFunction\nimport org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.api.watermark.Watermark\nimport org.apache.flink.table.api.{Table, TableEnvironment}\nimport org.apache.flink.table.api.scala._\nimport org.apache.flink.types.Row\nimport org.junit.rules.TemporaryFolder\nimport org.junit.{Rule, Test}\n\nimport scala.collection.mutable\nimport scala.collection.mutable.ArrayBuffer\n\nclass Table APIOverviewITCase {\n\n  // 客户表数据\n  val customer_data = new mutable.MutableList[(String, String, String)]\n  customer_data.+=((\"c_001\", \"Kevin\", \"from JinLin\"))\n  customer_data.+=((\"c_002\", \"Sunny\", \"from JinLin\"))\n  customer_data.+=((\"c_003\", \"JinCheng\", \"from HeBei\"))\n\n\n  // 订单表数据\n  val order_data = new mutable.MutableList[(String, String, String, String)]\n  order_data.+=((\"o_001\", \"c_002\", \"2018-11-05 10:01:01\", \"iphone\"))\n  order_data.+=((\"o_002\", \"c_001\", \"2018-11-05 10:01:55\", \"ipad\"))\n  order_data.+=((\"o_003\", \"c_001\", \"2018-11-05 10:03:44\", \"flink book\"))\n\n  // 商品销售表数据\n  val item_data = Seq(\n    Left((1510365660000L, (1510365660000L, 20, \"ITEM001\", \"Electronic\"))),\n    Right((1510365660000L)),\n    Left((1510365720000L, (1510365720000L, 50, \"ITEM002\", \"Electronic\"))),\n    Right((1510365720000L)),\n    Left((1510365780000L, (1510365780000L, 30, \"ITEM003\", \"Electronic\"))),\n    Left((1510365780000L, (1510365780000L, 60, \"ITEM004\", \"Electronic\"))),\n    Right((1510365780000L)),\n    Left((1510365900000L, (1510365900000L, 40, \"ITEM005\", \"Electronic\"))),\n    Right((1510365900000L)),\n    Left((1510365960000L, (1510365960000L, 20, \"ITEM006\", \"Electronic\"))),\n    Right((1510365960000L)),\n    Left((1510366020000L, (1510366020000L, 70, \"ITEM007\", \"Electronic\"))),\n    Right((1510366020000L)),\n    Left((1510366080000L, (1510366080000L, 20, \"ITEM008\", \"Clothes\"))),\n    Right((151036608000L)))\n\n  // 页面访问表数据\n  val pageAccess_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", \"U0010\"))),\n    Right((1510365660000L)),\n    Left((1510365660000L, (1510365660000L, \"BeiJing\", \"U1001\"))),\n    Right((1510365660000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", \"U2032\"))),\n    Right((1510366200000L)),\n    Left((1510366260000L, (1510366260000L, \"BeiJing\", \"U1100\"))),\n    Right((1510366260000L)),\n    Left((1510373400000L, (1510373400000L, \"ShangHai\", \"U0011\"))),\n    Right((1510373400000L)))\n\n  // 页面访问量表数据2\n  val pageAccessCount_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", 100))),\n    Right((1510365660000L)),\n    Left((1510365660000L, (1510365660000L, \"BeiJing\", 86))),\n    Right((1510365660000L)),\n    Left((1510365960000L, (1510365960000L, \"BeiJing\", 210))),\n    Right((1510366200000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", 33))),\n    Right((1510366200000L)),\n    Left((1510373400000L, (1510373400000L, \"ShangHai\", 129))),\n    Right((1510373400000L)))\n\n  // 页面访问表数据3\n  val pageAccessSession_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", \"U0011\"))),\n    Right((1510365660000L)),\n    Left((1510365720000L, (1510365720000L, \"ShangHai\", \"U0012\"))),\n    Right((1510365720000L)),\n    Left((1510365720000L, (1510365720000L, \"ShangHai\", \"U0013\"))),\n    Right((1510365720000L)),\n    Left((1510365900000L, (1510365900000L, \"ShangHai\", \"U0015\"))),\n    Right((1510365900000L)),\n    Left((1510366200000L, (1510366200000L, \"ShangHai\", \"U0011\"))),\n    Right((1510366200000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", \"U2010\"))),\n    Right((1510366200000L)),\n    Left((1510366260000L, (1510366260000L, \"ShangHai\", \"U0011\"))),\n    Right((1510366260000L)),\n    Left((1510373760000L, (1510373760000L, \"ShangHai\", \"U0410\"))),\n    Right((1510373760000L)))\n\n  val _tempFolder = new TemporaryFolder\n\n  // Streaming 环境\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n  val tEnv = TableEnvironment.getTableEnvironment(env)\n  env.setParallelism(1)\n  env.setStateBackend(getStateBackend)\n\n  def getProcTimeTables(): (Table, Table) = {\n    // 将order_tab, customer_tab 注册到catalog\n    val customer = env.fromCollection(customer_data).toTable(tEnv).as('c_id, 'c_name, 'c_desc)\n    val order = env.fromCollection(order_data).toTable(tEnv).as('o_id, 'c_id, 'o_time, 'o_desc)\n    (customer, order)\n  }\n\n\n  def getEventTimeTables():  (Table, Table, Table, Table) = {\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 将item_tab, pageAccess_tab 注册到catalog\n    val item =\n      env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data))\n      .toTable(tEnv, 'onSellTime, 'price, 'itemID, 'itemType, 'rowtime.rowtime)\n\n    val pageAccess =\n      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data))\n      .toTable(tEnv, 'accessTime, 'region, 'userId, 'rowtime.rowtime)\n\n    val pageAccessCount =\n      env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data))\n      .toTable(tEnv, 'accessTime, 'region, 'accessCount, 'rowtime.rowtime)\n\n    val pageAccessSession =\n      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data))\n      .toTable(tEnv, 'accessTime, 'region, 'userId, 'rowtime.rowtime)\n\n    (item, pageAccess, pageAccessCount, pageAccessSession)\n  }\n\n\n  @Rule\n  def tempFolder: TemporaryFolder = _tempFolder\n\n  def getStateBackend: StateBackend = {\n    new MemoryStateBackend()\n  }\n\n  def procTimePrint(result: Table): Unit = {\n    val sink = new RetractingSink\n    result.toRetractStream[Row].addSink(sink)\n    env.execute()\n  }\n\n  def rowTimePrint(result: Table): Unit = {\n    val sink = new RetractingSink\n    result.toRetractStream[Row].addSink(sink)\n    env.execute()\n  }\n\n  @Test\n  def testProc(): Unit = {\n    val (customer, order) = getProcTimeTables()\n    val result = ...// 测试的查询逻辑\n    procTimePrint(result)\n  }\n\n  @Test\n  def testEvent(): Unit = {\n    val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables()\n    val result = ...// 测试的查询逻辑\n    procTimePrint(result)\n  }\n\n}\n\n// 自定义Sink\nfinal class RetractingSink extends RichSinkFunction[(Boolean, Row)] {\n  var retractedResults: ArrayBuffer[String] = null\n\n\n  override def open(parameters: Configuration): Unit = {\n    super.open(parameters)\n    retractedResults = mutable.ArrayBuffer.empty[String]\n  }\n\n  def invoke(v: (Boolean, Row)) {\n    retractedResults.synchronized {\n      val value = v._2.toString\n      if (v._1) {\n        retractedResults += value\n      } else {\n        val idx = retractedResults.indexOf(value)\n        if (idx >= 0) {\n          retractedResults.remove(idx)\n        } else {\n          throw new RuntimeException(\"Tried to retract a value that wasn't added first. \" +\n                                       \"This is probably an incorrectly implemented test. \" +\n                                       \"Try to set the parallelism of the sink to 1.\")\n        }\n      }\n    }\n\n  }\n\n  override def close(): Unit = {\n    super.close()\n    retractedResults.sorted.foreach(println(_))\n  }\n}\n\n// Water mark 生成器\nclass EventTimeSourceFunction[T](\n  dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n\n  override def cancel(): Unit = ???\n}\n```\n\n## SELECT\nSELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示:\n![](EEED7340-FE9D-4791-961C-9996D35E148F.png)\n\n### Table API 示例\n从`customer_tab`选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：\n```\nval result = customer\n  .select('c_name, concat_ws('c_name, \" come \", 'c_desc))\n```\n### Result\n\n| c_name | desc |\n| --- | --- |\n| Kevin | Kevin come from JinLin |\n| Sunny | Sunny come from JinLin |\n| Jincheng|Jincheng come from HeBei|\n\n### 特别说明\n大家看到在 `SELECT` 不仅可以使用普通的字段选择，还可以使用`ScalarFunction`,当然也包括`User-Defined Function`，同时还可以进行字段的`alias`设置。其实`SELECT`可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下：\n#### Table API示例\n在订单表查询所有的客户id，消除重复客户id, 如下：\n```\nval result = order\n  .groupBy('c_id)\n  .select('c_id)\n```\n#### Result\n\n| c_id |\n| --- |\n| c_001 |  \n| c_002 |\n\n## WHERE\nWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n\n![](15657C5E-DD6E-4339-824E-6B17C30886C9.png)\n\n### Table API 示例\n在`customer_tab`查询客户id为`c_001`和`c_003`的客户信息，如下：\n```\n val result = customer\n   .where(\"c_id = 'c_001' || c_id = 'c_003'\")\n   .select( 'c_id, 'c_name, 'c_desc)\n```\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\n我们发现`WHERE`是对满足一定条件的数据进行过滤，`WHERE`支持=, <, >, <>, >=, <=以及`&&`， `||`等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的`IN`和`NOT IN`在Table API里面用`intersect` 和 `minus`描述(flink-1.7.0版本)。\n\n#### Intersect 示例 \n`Intersect`只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在`customer_tab`查询已经下过订单的客户信息，如下：\n```\n // 计算客户id，并去重\n val distinct_cids = order\n   .groupBy('c_id) // 去重\n   .select('c_id as 'o_c_id)\n\n val result = customer\n   .join(distinct_cids, 'c_id === 'o_c_id)\n   .select('c_id, 'c_name, 'c_desc)\n```\n#### Result\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n\n#### Minus 示例 \n`Minus`只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在`customer_tab`查询没有下过订单的客户信息，如下：\n```\n // 查询下过订单的客户id，并去重\n val distinct_cids = order\n   .groupBy('c_id)\n   .select('c_id as 'o_c_id)\n   \n// 查询没有下过订单的客户信息\nval result = customer\n  .leftOuterJoin(distinct_cids, 'c_id === 'o_c_id)\n  .where('o_c_id isNull)\n  .select('c_id, 'c_name, 'c_desc)\n  \n```\n说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。\n#### Result\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n|c_003|JinCheng|from HeBei|\n\n\n#### Intersect/Minus与关系代数\n如上介绍`Intersect`是关系代数中的Intersection， `Minus`是关系代数的Difference， 如下图示意：\n* Intersect(Intersection)\n![](CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png)\n* Minus(Difference)\n![](CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png)\n\n## GROUP BY\nGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n![](D56FE35E-0212-444C-8B0F-C650F1300C3C.png)\n### Table API 示例\n将order_tab信息按`c_id`分组统计订单数量，简单示例如下：\n```\nval result = order\n  .groupBy('c_id)\n  .select('c_id, 'o_id.count)\n```\n### Result\n| c_id |o_count  |\n| --- | --- |\n| c_001 | 2 |\n| c_002 | 1 |\n\n### 特别说明\n  在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：\n  \n#### Table API 示例\n按时间进行分组，查询每分钟的订单数量，如下：\n\n    ```\n    val result = order\n      .select('o_id, 'c_id, 'o_time.substring(1, 16) as 'o_time_min)\n      .groupBy('o_time_min)\n      .select('o_time_min, 'o_id.count)\n    ```\n    \n#### Result\n    \n| o_time_min | o_count |\n| --- | --- |\n| 2018-11-05 10:01|  2|\n| 2018-11-05 10:03|  1|\n\n说明：如果我们时间字段是timestamp类型，建议使用内置的 `DATE_FORMAT` 函数。\n    \n## UNION ALL\nUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n![](3402EE52-F2D5-452E-A0A5-66F0EC14C878.png)\n\n\n### Table API 示例\n我们简单的将`customer_tab`查询2次，将查询结果合并起来，如下：\n```\nval result = customer.unionAll(customer)\n```\n\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\nUNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。\n\n## UNION\nUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n![](AE6DCCF9-32A4-4E7A-B7C5-334983354240.png)\n\n### Table API 示例\n我们简单的将`customer_tab`查询2次，将查询结果合并起来，如下：\n```\nval result = customer.union(customer)\n```\n我们发现完全一样的表数据进行 `UNION`之后，数据是被去重的，`UNION`之后的数据并没有增加。\n\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\nUNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。\n\n## JOIN \nJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：\n* JOIN - INNER JOIN\n* LEFT JOIN - LEFT OUTER JOIN\n* RIGHT JOIN - RIGHT OUTER JOIN  \n* FULL JOIN - FULL OUTER JOIN\nJOIN与关系代数的Join语义相同，具体如下：\n![](97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png)\n\n### Table API 示例 (JOIN)\n`INNER JOIN`只选择满足`ON`条件的记录，我们查询`customer_tab` 和 `order_tab`表，将有订单的客户和订单信息选择出来，如下：\n```\nval result = customer\n  .join(order.select('o_id, 'c_id as 'o_c_id, 'o_time, 'o_desc), 'c_id === 'o_c_id)\n```\n### Result\n\n| c_id |  c_name|c_desc  | o_id | o_c_id | o_time|o_desc |\n| --- | --- | --- | --- | --- | --- | --- |\n| c_001 | Kevin | from JinLin | o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| c_001 | Kevin | from JinLin | o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n| c_002| Sunny | from JinLin | o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n\n### Table API 示例 (LEFT JOIN)\n`LEFT JOIN`与`INNER JOIN`的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补`NULL`输出，语义如下：\n![](AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png)\n对应的SQL语句如下(LEFT JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; \n```\n* 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。\n\n我们查询`customer_tab` 和 `order_tab`表，将客户和订单信息选择出来如下：\n```\n val result = customer\n   .leftOuterJoin(order.select('o_id, 'c_id as 'o_c_id, 'o_time, 'o_desc), 'c_id === 'o_c_id)\n```\n### Result\n| c_id |  c_name|c_desc  | o_id | c_id | o_time|o_desc |\n| --- | --- | --- | --- | --- | --- | --- |\n| c_001 | Kevin | from JinLin | o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| c_001 | Kevin | from JinLin | o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n| c_002| Sunny | from JinLin | o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n| c_003| JinCheng | from HeBei | NULL|NULL|NULL|NULL\n\n\n### 特别说明\n`RIGHT JOIN` 相当于 `LEFT JOIN` 左右两个表交互一下位置。`FULL JOIN`相当于 `RIGHT JOIN` 和 `LEFT JOIN` 之后进行`UNION ALL`操作。\n\n## Time-Interval JOIN\nTime-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下：\n```\n...\nval result = left\n  .join(right)\n  // 定义Time Interval\n  .where('a === 'd && 'c >= 'f - 5.seconds && 'c < 'f + 6.seconds)\n  ...\n```\n\n## Lateral JOIN\nApache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下：\n```\n...\nval udtf = new UDTF\nval result = source.join(udtf('c) as ('d, 'e))\n...\n```\n## Temporal Table JOIN\nTemporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下：\n\n```\n...\nval rates = tEnv.scan(\"versonedTable\").createTemporalTableFunction('rowtime, 'r_currency)\nval result = left.join(rates('o_rowtime), 'r_currency === 'o_currency)\n...\n```\n\n## Window\n在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。\n\n### Over Window\nApache Flink中对OVER Window的定义遵循标准SQL的定义语法。\n按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:\n* ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n* RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。\n\n#### Bounded ROWS OVER Window\nBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n##### 语义\n我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n![](FC821998-4A3E-4F10-9351-C7B3060E7349.png)\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。\n\n##### Table API 示例\n利用`item_tab`测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。\n```\nval result = item\n  .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.rows following CURRENT_ROW as 'w)\n  .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)\n```\n#### Result\n\n| itemID | itemType | onSellTime | price | maxPrice |\n| :--- | :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 | 50 |\n| ITEM003 | Electronic | 2017-11-11 10:03:00 | 30 | 50 |\n| ITEM004 | Electronic | 2017-11-11 10:03:00 | 60 | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 | 60 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 | 60 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 | 20 |\n\n#### Bounded RANGE OVER Window\nBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。\n##### 语义\n我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n![](79DC098C-1171-4C0E-A735-C4F63B6010FD.png)\n\n注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。\n\n\n##### Tabel API 示例\n我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。\n```sql\n val result = item\n   .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w)\n   .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)\n```\n##### Result（Bounded RANGE OVER Window）\n| itemID | itemType | onSellTime | price | maxPrice |\n| :--- | :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 | 50 |\n| ITEM003 | Electronic | _**2017-11-11 10:03:00**_ | 30 | 60 |\n| ITEM004 | Electronic | _**2017-11-11 10:03:00**_ | 60 | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 | 60 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 | 40 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 | 20 |\n\n#### 特别说明\nOverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在`SELECT`中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，`SELECT`可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:\n* GROUP BY - `tab.groupBy('d).select(d, MAX(c))`\n* OVER Window = `tab.window(Over.. as 'w).select('a, 'b, 'c, 'd, c.max over 'w)`\n如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。\n\n### Group Window\n根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:\n\n* Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；\n* Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；\n* Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。\n\n**说明：** Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。\n\n#### Tumble\n##### 语义\nTumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n\n![](C2A514D5-61F1-4464-95ED-17EB2A030EC2.png)\n\n##### Table API 示例\n利用`pageAccess_tab`测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。\n```\n val result = pageAccess\n   .window(Tumble over 2.minute on 'rowtime as 'w)\n   .groupBy('w, 'region)\n   .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n```\n##### Result\n\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n#### Hop\nHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。\n##### 语义\nHop 滑动窗口语义如下所示：\n![](3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png)\n\n##### Table API 示例\n利用`pageAccessCount_tab`测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).\n```\nval result = pageAccessCount\n  .window(Slide over 10.minute every 5.minute on 'rowtime as 'w)\n  .groupBy('w)\n  .select('w.start, 'w.end, 'accessCount.sum as 'accessCount)\n```\n\n##### Result\n\n| winStart | winEnd | accessCount |\n| --- | --- | --- |\n|2017-11-11 01:55:00.0|2017-11-11 02:05:00.0|186\n|2017-11-11 02:00:00.0|2017-11-11 02:10:00.0|396\n|2017-11-11 02:05:00.0|2017-11-11 02:15:00.0|243\n|2017-11-11 02:10:00.0|2017-11-11 02:20:00.0|33\n|2017-11-11 04:05:00.0|2017-11-11 04:15:00.0|129\n|2017-11-11 04:10:00.0|2017-11-11 04:20:00.0|129\n\n#### Session\nSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.\n##### 语义\nSession 会话窗口语义如下所示：\n\n![](D1197B0B-2DCF-434E-A7BB-A250187079F0.png)\n\n```\nval result = pageAccessSession\n  .window(Session withGap 3.minute on 'rowtime as 'w)\n  .groupBy('w, 'region)\n  .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n```\n##### Result\n| region | winStart | winEnd | pv |\n| --- | --- | --- | --- |\n|BeiJing|2017-11-11 02:10:00.0|2017-11-11 02:13:00.0|1\n|ShangHai|2017-11-11 02:01:00.0|2017-11-11 02:08:00.0|4\n|ShangHai|2017-11-11 02:10:00.0|2017-11-11 02:14:00.0|2\n|ShangHai|2017-11-11 04:16:00.0|2017-11-11 04:19:00.0|1\n\n#### 嵌套Window\n在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用`'w.rowtime`来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下:\n\n```\n ... \n val result = pageAccess\n   .window(Tumble over 2.minute on 'rowtime as 'w1)\n   .groupBy('w1)\n   .select('w1.rowtime as 'rowtime, 'col1.count as 'cnt)\n   .window(Session withGap 3.minute on 'rowtime as 'w2)\n   .groupBy('w2)\n   .select('cnt.sum)\n...\n```\n\n# Source&Sink\n上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n## Source 定义\n自定义Apache Flink Stream Source需要实现`StreamTableSource`, `StreamTableSource`中通过`StreamExecutionEnvironment` 的`addSource`方法获取`DataStream`, 所以我们需要自定义一个 `SourceFunction`, 并且要支持产生WaterMark，也就是要实现`DefinedRowtimeAttributes`接口。\n\n### Source Function定义\n支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:\n```\nclass MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) \n  extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n  override def cancel(): Unit = ???\n}\n```\n\n### 定义 StreamTableSource\n我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:\n```\nclass MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes {\n\n  val fieldNames = Array(\"accessTime\", \"region\", \"userId\")\n  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))\n  val rowType = new RowTypeInfo(\n    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],\n    fieldNames)\n\n  // 页面访问表数据 rows with timestamps and watermarks\n  val data = Seq(\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"ShangHai\", \"U0010\")),\n    Right(1510365660000L),\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"BeiJing\", \"U1001\")),\n    Right(1510365660000L),\n    Left(1510366200000L, Row.of(new JLong(1510366200000L), \"BeiJing\", \"U2032\")),\n    Right(1510366200000L),\n    Left(1510366260000L, Row.of(new JLong(1510366260000L), \"BeiJing\", \"U1100\")),\n    Right(1510366260000L),\n    Left(1510373400000L, Row.of(new JLong(1510373400000L), \"ShangHai\", \"U0011\")),\n    Right(1510373400000L)\n  )\n\n  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = {\n    Collections.singletonList(new RowtimeAttributeDescriptor(\n      \"accessTime\",\n      new ExistingField(\"accessTime\"),\n      PreserveWatermarks.INSTANCE))\n  }\n\n  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n     execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1)\n  }\n\n  override def getReturnType: TypeInformation[Row] = rowType\n\n  override def getTableSchema: TableSchema = schema\n\n}\n```\n  \n## Sink 定义\n我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n```\ndef getCsvTableSink: TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    // 打印sink的文件路径，方便我们查看运行结果\n    println(\"Sink path : \" + tempFile)\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(\n      Array[String](\"region\", \"winStart\", \"winEnd\", \"pv\"),\n      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))\n  }\n```\n\n## 构建主程序\n主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    // 设置EventTime\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"mySource\"\n    // 创建自定义source数据结构\n    val tableSource = new MyTableSource\n\n    val sinkTableName = \"csvSink\"\n    // 创建CSV sink 数据结构\n    val tableSink = getCsvTableSink\n\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n    // 注册sink\n    tEnv.registerTableSink(sinkTableName, tableSink)\n\n    val result = tEnv.scan(sourceTableName)\n      .window(Tumble over 2.minute on 'accessTime as 'w)\n      .groupBy('w, 'region)\n      .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n\n    result.insertInto(sinkTableName)\n    env.execute()\n  }\n  \n```\n\n## 执行并查看运行结果\n执行主程序后我们会在控制台得到Sink的文件路径，如下：\n```\nSink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\n\n```\n\nCat 方式查看计算结果，如下：\n```\njinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\nShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2\nShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1\n```\n\n表格化如上结果：\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！\n\n\n# 小结\n本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。\n\n# 关于点赞和评论\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!\n","source":"_posts/Apache Flink 漫谈系列 - Table API 概述.md","raw":"---\ntitle: Apache Flink 漫谈系列 - Table API 概览\ndate: 2019-03-26 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 什么是Table API\n在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象.\n\n如下图所示：\n\n![](6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png)\n\nApache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。\n\n# Table API的特点\n\nTable API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下：\n* 声明式 - 用户只关心做什么，不用关心怎么做；\n* 高性能 - 支持查询优化，可以获取最好的执行性能；\n* 流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行；\n* 标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。\n\n当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点：\n* 表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等\n* 功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能\n* 编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。\n\n**说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 [FLIP-29](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739) 中规划的新功能。**\n\n# HelloWorld\n在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。\n\n## Maven 依赖\n在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。\n```\n<properties>\n    <table.version>1.7.0</table.version>\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-table_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-scala_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-streaming-scala_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-streaming-java_2.11</artifactId>\n      <version>${table.version}</version>\n    </dependency>\n\n  </dependencies>\n```\n## 程序结构\n在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示：\n![](6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png)\n1. 外部数据源，比如Kafka, Rabbitmq, CSV 等等；\n2. 查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等；\n3. 外部结果存储，比如Kafka，Cassandra，CSV等。\n\n**说明：1和3 在Apache Flink中统称为Connector。**\n\n## 主程序\n我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。\n根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码：\n\n- TableSoruce Code - 用于创建数据源的代码\n- Table API Query - 用于进行word count统计的Table API 查询逻辑\n- TableSink Code - 用于保存word count计算结果的结果表代码 \n\n### 运行模式选择\n一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。\n```\n// Stream运行环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n```\n### 构建测试Source\n我们用最简单的构建Source方式进行本次测试，代码如下：\n```\n // 测试数据\n val data = Seq(\"Flink\", \"Bob\", \"Bob\", \"something\", \"Hello\", \"Flink\", \"Bob\")\n // 最简单的获取Source方式\nval source = env.fromCollection(data).toTable(tEnv, 'word)\n```\n\n### WordCount 统计逻辑\nWordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下：\n```\n // 单词统计核心逻辑\n val result = source\n   .groupBy('word) // 单词分组\n   .select('word, 'word.count) // 单词统计\n```\n### 定义Sink\n将WordCount的统计结果写入Sink中，代码如下：\n```\n// 自定义Sink\n    val sink = new RetractSink // 自定义Sink（下面有完整代码）\n    // 计算结果写入sink\n    result.toRetractStream[(String, Long)].addSink(sink)\n```\n### 完整的HelloWord代码\n为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下:\n```\nimport org.apache.flink.api.scala._\nimport org.apache.flink.configuration.Configuration\nimport org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.table.api.scala._\n\nimport scala.collection.mutable\n\nobject HelloWord {\n\n  def main(args: Array[String]): Unit = {\n    // 测试数据\n    val data = Seq(\"Flink\", \"Bob\", \"Bob\", \"something\", \"Hello\", \"Flink\", \"Bob\")\n\n    // Stream运行环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    // 最简单的获取Source方式\n    val source = env.fromCollection(data).toTable(tEnv, 'word)\n\n    // 单词统计核心逻辑\n    val result = source\n      .groupBy('word) // 单词分组\n      .select('word, 'word.count) // 单词统计\n\n    // 自定义Sink\n    val sink = new RetractSink\n    // 计算结果写入sink\n    result.toRetractStream[(String, Long)].addSink(sink)\n\n    env.execute\n  }\n}\n\nclass RetractSink extends RichSinkFunction[(Boolean, (String, Long))] {\n  private var resultSet: mutable.Set[(String, Long)] = _\n\n  override def open(parameters: Configuration): Unit = {\n    // 初始化内存存储结构\n    resultSet = new mutable.HashSet[(String, Long)]\n  }\n\n  override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = {\n    if (v._1) {\n      // 计算数据\n      resultSet.add(v._2)\n    }\n    else {\n      // 撤回数据\n      resultSet.remove(v._2)\n    }\n  }\n\n  override def close(): Unit = {\n    // 打印写入sink的结果数据\n    resultSet.foreach(println)\n  }\n}\n```\n\n运行结果如下：\n![](E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png)\n\n虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码: \n` source.groupBy('word).select('word, 'word.count)`\n所以Table API开发技术任务非常的简洁高效。\n\n# Table API 算子\n虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。\n\n## 示例数据及测试类\n### 测试数据\n\n* customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n* order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：\n\n| o_id | c_id | o_time|o_desc |\n| --- | --- | --- | --- |\n| o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n| o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n\n* Item_tab \n  商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：\n  \n| itemID | itemType | onSellTime | price |\n| :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 |\n| ITEM003 | Electronic | _**2017-11-11 10:03:00**_ | 30 |\n| ITEM004 | Electronic | _**2017-11-11 10:03:00**_ | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 |\n\n* PageAccess_tab\n页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n* PageAccessCount_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userCount | accessTime |\n| --- | --- | --- |\n| ShangHai | 100 | 2017.11.11 10:01:00 |\n| BeiJing |  86| 2017.11.11 10:01:00 |\n| BeiJing | 210 | 2017.11.11 10:06:00 |\n| BeiJing  | 33 | 2017.11.11 10:10:00 |\n| ShangHai | 129 | 2017.11.11 12:10:00 |\n\n* PageAccessSession_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n|ShangHai|U0011|2017-11-11 10:01:00\n|ShangHai|U0012|2017-11-11 10:02:00\n|ShangHai|U0013|2017-11-11 10:03:00\n|ShangHai|U0015|2017-11-11 10:05:00\n|ShangHai|U0011|2017-11-11 10:10:00\n|BeiJing|U0110|2017-11-11 10:10:00\n|ShangHai|U2010|2017-11-11 10:11:00\n|ShangHai|U0410|2017-11-11 12:16:00\n\n### 测试类\n我们创建一个`TableAPIOverviewITCase.scala` 用于接下来介绍Flink Table API算子的功能体验。代码如下：\n```\nimport org.apache.flink.api.scala._\nimport org.apache.flink.runtime.state.StateBackend\nimport org.apache.flink.runtime.state.memory.MemoryStateBackend\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction\nimport org.apache.flink.streaming.api.functions.source.SourceFunction\nimport org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.api.watermark.Watermark\nimport org.apache.flink.table.api.{Table, TableEnvironment}\nimport org.apache.flink.table.api.scala._\nimport org.apache.flink.types.Row\nimport org.junit.rules.TemporaryFolder\nimport org.junit.{Rule, Test}\n\nimport scala.collection.mutable\nimport scala.collection.mutable.ArrayBuffer\n\nclass Table APIOverviewITCase {\n\n  // 客户表数据\n  val customer_data = new mutable.MutableList[(String, String, String)]\n  customer_data.+=((\"c_001\", \"Kevin\", \"from JinLin\"))\n  customer_data.+=((\"c_002\", \"Sunny\", \"from JinLin\"))\n  customer_data.+=((\"c_003\", \"JinCheng\", \"from HeBei\"))\n\n\n  // 订单表数据\n  val order_data = new mutable.MutableList[(String, String, String, String)]\n  order_data.+=((\"o_001\", \"c_002\", \"2018-11-05 10:01:01\", \"iphone\"))\n  order_data.+=((\"o_002\", \"c_001\", \"2018-11-05 10:01:55\", \"ipad\"))\n  order_data.+=((\"o_003\", \"c_001\", \"2018-11-05 10:03:44\", \"flink book\"))\n\n  // 商品销售表数据\n  val item_data = Seq(\n    Left((1510365660000L, (1510365660000L, 20, \"ITEM001\", \"Electronic\"))),\n    Right((1510365660000L)),\n    Left((1510365720000L, (1510365720000L, 50, \"ITEM002\", \"Electronic\"))),\n    Right((1510365720000L)),\n    Left((1510365780000L, (1510365780000L, 30, \"ITEM003\", \"Electronic\"))),\n    Left((1510365780000L, (1510365780000L, 60, \"ITEM004\", \"Electronic\"))),\n    Right((1510365780000L)),\n    Left((1510365900000L, (1510365900000L, 40, \"ITEM005\", \"Electronic\"))),\n    Right((1510365900000L)),\n    Left((1510365960000L, (1510365960000L, 20, \"ITEM006\", \"Electronic\"))),\n    Right((1510365960000L)),\n    Left((1510366020000L, (1510366020000L, 70, \"ITEM007\", \"Electronic\"))),\n    Right((1510366020000L)),\n    Left((1510366080000L, (1510366080000L, 20, \"ITEM008\", \"Clothes\"))),\n    Right((151036608000L)))\n\n  // 页面访问表数据\n  val pageAccess_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", \"U0010\"))),\n    Right((1510365660000L)),\n    Left((1510365660000L, (1510365660000L, \"BeiJing\", \"U1001\"))),\n    Right((1510365660000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", \"U2032\"))),\n    Right((1510366200000L)),\n    Left((1510366260000L, (1510366260000L, \"BeiJing\", \"U1100\"))),\n    Right((1510366260000L)),\n    Left((1510373400000L, (1510373400000L, \"ShangHai\", \"U0011\"))),\n    Right((1510373400000L)))\n\n  // 页面访问量表数据2\n  val pageAccessCount_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", 100))),\n    Right((1510365660000L)),\n    Left((1510365660000L, (1510365660000L, \"BeiJing\", 86))),\n    Right((1510365660000L)),\n    Left((1510365960000L, (1510365960000L, \"BeiJing\", 210))),\n    Right((1510366200000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", 33))),\n    Right((1510366200000L)),\n    Left((1510373400000L, (1510373400000L, \"ShangHai\", 129))),\n    Right((1510373400000L)))\n\n  // 页面访问表数据3\n  val pageAccessSession_data = Seq(\n    Left((1510365660000L, (1510365660000L, \"ShangHai\", \"U0011\"))),\n    Right((1510365660000L)),\n    Left((1510365720000L, (1510365720000L, \"ShangHai\", \"U0012\"))),\n    Right((1510365720000L)),\n    Left((1510365720000L, (1510365720000L, \"ShangHai\", \"U0013\"))),\n    Right((1510365720000L)),\n    Left((1510365900000L, (1510365900000L, \"ShangHai\", \"U0015\"))),\n    Right((1510365900000L)),\n    Left((1510366200000L, (1510366200000L, \"ShangHai\", \"U0011\"))),\n    Right((1510366200000L)),\n    Left((1510366200000L, (1510366200000L, \"BeiJing\", \"U2010\"))),\n    Right((1510366200000L)),\n    Left((1510366260000L, (1510366260000L, \"ShangHai\", \"U0011\"))),\n    Right((1510366260000L)),\n    Left((1510373760000L, (1510373760000L, \"ShangHai\", \"U0410\"))),\n    Right((1510373760000L)))\n\n  val _tempFolder = new TemporaryFolder\n\n  // Streaming 环境\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n  val tEnv = TableEnvironment.getTableEnvironment(env)\n  env.setParallelism(1)\n  env.setStateBackend(getStateBackend)\n\n  def getProcTimeTables(): (Table, Table) = {\n    // 将order_tab, customer_tab 注册到catalog\n    val customer = env.fromCollection(customer_data).toTable(tEnv).as('c_id, 'c_name, 'c_desc)\n    val order = env.fromCollection(order_data).toTable(tEnv).as('o_id, 'c_id, 'o_time, 'o_desc)\n    (customer, order)\n  }\n\n\n  def getEventTimeTables():  (Table, Table, Table, Table) = {\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 将item_tab, pageAccess_tab 注册到catalog\n    val item =\n      env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data))\n      .toTable(tEnv, 'onSellTime, 'price, 'itemID, 'itemType, 'rowtime.rowtime)\n\n    val pageAccess =\n      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data))\n      .toTable(tEnv, 'accessTime, 'region, 'userId, 'rowtime.rowtime)\n\n    val pageAccessCount =\n      env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data))\n      .toTable(tEnv, 'accessTime, 'region, 'accessCount, 'rowtime.rowtime)\n\n    val pageAccessSession =\n      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data))\n      .toTable(tEnv, 'accessTime, 'region, 'userId, 'rowtime.rowtime)\n\n    (item, pageAccess, pageAccessCount, pageAccessSession)\n  }\n\n\n  @Rule\n  def tempFolder: TemporaryFolder = _tempFolder\n\n  def getStateBackend: StateBackend = {\n    new MemoryStateBackend()\n  }\n\n  def procTimePrint(result: Table): Unit = {\n    val sink = new RetractingSink\n    result.toRetractStream[Row].addSink(sink)\n    env.execute()\n  }\n\n  def rowTimePrint(result: Table): Unit = {\n    val sink = new RetractingSink\n    result.toRetractStream[Row].addSink(sink)\n    env.execute()\n  }\n\n  @Test\n  def testProc(): Unit = {\n    val (customer, order) = getProcTimeTables()\n    val result = ...// 测试的查询逻辑\n    procTimePrint(result)\n  }\n\n  @Test\n  def testEvent(): Unit = {\n    val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables()\n    val result = ...// 测试的查询逻辑\n    procTimePrint(result)\n  }\n\n}\n\n// 自定义Sink\nfinal class RetractingSink extends RichSinkFunction[(Boolean, Row)] {\n  var retractedResults: ArrayBuffer[String] = null\n\n\n  override def open(parameters: Configuration): Unit = {\n    super.open(parameters)\n    retractedResults = mutable.ArrayBuffer.empty[String]\n  }\n\n  def invoke(v: (Boolean, Row)) {\n    retractedResults.synchronized {\n      val value = v._2.toString\n      if (v._1) {\n        retractedResults += value\n      } else {\n        val idx = retractedResults.indexOf(value)\n        if (idx >= 0) {\n          retractedResults.remove(idx)\n        } else {\n          throw new RuntimeException(\"Tried to retract a value that wasn't added first. \" +\n                                       \"This is probably an incorrectly implemented test. \" +\n                                       \"Try to set the parallelism of the sink to 1.\")\n        }\n      }\n    }\n\n  }\n\n  override def close(): Unit = {\n    super.close()\n    retractedResults.sorted.foreach(println(_))\n  }\n}\n\n// Water mark 生成器\nclass EventTimeSourceFunction[T](\n  dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n\n  override def cancel(): Unit = ???\n}\n```\n\n## SELECT\nSELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示:\n![](EEED7340-FE9D-4791-961C-9996D35E148F.png)\n\n### Table API 示例\n从`customer_tab`选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：\n```\nval result = customer\n  .select('c_name, concat_ws('c_name, \" come \", 'c_desc))\n```\n### Result\n\n| c_name | desc |\n| --- | --- |\n| Kevin | Kevin come from JinLin |\n| Sunny | Sunny come from JinLin |\n| Jincheng|Jincheng come from HeBei|\n\n### 特别说明\n大家看到在 `SELECT` 不仅可以使用普通的字段选择，还可以使用`ScalarFunction`,当然也包括`User-Defined Function`，同时还可以进行字段的`alias`设置。其实`SELECT`可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下：\n#### Table API示例\n在订单表查询所有的客户id，消除重复客户id, 如下：\n```\nval result = order\n  .groupBy('c_id)\n  .select('c_id)\n```\n#### Result\n\n| c_id |\n| --- |\n| c_001 |  \n| c_002 |\n\n## WHERE\nWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：\n\n![](15657C5E-DD6E-4339-824E-6B17C30886C9.png)\n\n### Table API 示例\n在`customer_tab`查询客户id为`c_001`和`c_003`的客户信息，如下：\n```\n val result = customer\n   .where(\"c_id = 'c_001' || c_id = 'c_003'\")\n   .select( 'c_id, 'c_name, 'c_desc)\n```\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\n我们发现`WHERE`是对满足一定条件的数据进行过滤，`WHERE`支持=, <, >, <>, >=, <=以及`&&`， `||`等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的`IN`和`NOT IN`在Table API里面用`intersect` 和 `minus`描述(flink-1.7.0版本)。\n\n#### Intersect 示例 \n`Intersect`只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在`customer_tab`查询已经下过订单的客户信息，如下：\n```\n // 计算客户id，并去重\n val distinct_cids = order\n   .groupBy('c_id) // 去重\n   .select('c_id as 'o_c_id)\n\n val result = customer\n   .join(distinct_cids, 'c_id === 'o_c_id)\n   .select('c_id, 'c_name, 'c_desc)\n```\n#### Result\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n\n#### Minus 示例 \n`Minus`只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在`customer_tab`查询没有下过订单的客户信息，如下：\n```\n // 查询下过订单的客户id，并去重\n val distinct_cids = order\n   .groupBy('c_id)\n   .select('c_id as 'o_c_id)\n   \n// 查询没有下过订单的客户信息\nval result = customer\n  .leftOuterJoin(distinct_cids, 'c_id === 'o_c_id)\n  .where('o_c_id isNull)\n  .select('c_id, 'c_name, 'c_desc)\n  \n```\n说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。\n#### Result\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n|c_003|JinCheng|from HeBei|\n\n\n#### Intersect/Minus与关系代数\n如上介绍`Intersect`是关系代数中的Intersection， `Minus`是关系代数的Difference， 如下图示意：\n* Intersect(Intersection)\n![](CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png)\n* Minus(Difference)\n![](CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png)\n\n## GROUP BY\nGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n![](D56FE35E-0212-444C-8B0F-C650F1300C3C.png)\n### Table API 示例\n将order_tab信息按`c_id`分组统计订单数量，简单示例如下：\n```\nval result = order\n  .groupBy('c_id)\n  .select('c_id, 'o_id.count)\n```\n### Result\n| c_id |o_count  |\n| --- | --- |\n| c_001 | 2 |\n| c_002 | 1 |\n\n### 特别说明\n  在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：\n  \n#### Table API 示例\n按时间进行分组，查询每分钟的订单数量，如下：\n\n    ```\n    val result = order\n      .select('o_id, 'c_id, 'o_time.substring(1, 16) as 'o_time_min)\n      .groupBy('o_time_min)\n      .select('o_time_min, 'o_id.count)\n    ```\n    \n#### Result\n    \n| o_time_min | o_count |\n| --- | --- |\n| 2018-11-05 10:01|  2|\n| 2018-11-05 10:03|  1|\n\n说明：如果我们时间字段是timestamp类型，建议使用内置的 `DATE_FORMAT` 函数。\n    \n## UNION ALL\nUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n![](3402EE52-F2D5-452E-A0A5-66F0EC14C878.png)\n\n\n### Table API 示例\n我们简单的将`customer_tab`查询2次，将查询结果合并起来，如下：\n```\nval result = customer.unionAll(customer)\n```\n\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\nUNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。\n\n## UNION\nUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n![](AE6DCCF9-32A4-4E7A-B7C5-334983354240.png)\n\n### Table API 示例\n我们简单的将`customer_tab`查询2次，将查询结果合并起来，如下：\n```\nval result = customer.union(customer)\n```\n我们发现完全一样的表数据进行 `UNION`之后，数据是被去重的，`UNION`之后的数据并没有增加。\n\n### Result\n\n| c_id |  c_name|c_desc  |\n| --- | --- | --- |\n| c_001 | Kevin | from JinLin |\n| c_002| Sunny | from JinLin |\n| c_003| JinCheng | from HeBei |\n\n### 特别说明\nUNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。\n\n## JOIN \nJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：\n* JOIN - INNER JOIN\n* LEFT JOIN - LEFT OUTER JOIN\n* RIGHT JOIN - RIGHT OUTER JOIN  \n* FULL JOIN - FULL OUTER JOIN\nJOIN与关系代数的Join语义相同，具体如下：\n![](97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png)\n\n### Table API 示例 (JOIN)\n`INNER JOIN`只选择满足`ON`条件的记录，我们查询`customer_tab` 和 `order_tab`表，将有订单的客户和订单信息选择出来，如下：\n```\nval result = customer\n  .join(order.select('o_id, 'c_id as 'o_c_id, 'o_time, 'o_desc), 'c_id === 'o_c_id)\n```\n### Result\n\n| c_id |  c_name|c_desc  | o_id | o_c_id | o_time|o_desc |\n| --- | --- | --- | --- | --- | --- | --- |\n| c_001 | Kevin | from JinLin | o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| c_001 | Kevin | from JinLin | o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n| c_002| Sunny | from JinLin | o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n\n### Table API 示例 (LEFT JOIN)\n`LEFT JOIN`与`INNER JOIN`的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补`NULL`输出，语义如下：\n![](AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png)\n对应的SQL语句如下(LEFT JOIN)：\n```\nSELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; \n```\n* 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。\n\n我们查询`customer_tab` 和 `order_tab`表，将客户和订单信息选择出来如下：\n```\n val result = customer\n   .leftOuterJoin(order.select('o_id, 'c_id as 'o_c_id, 'o_time, 'o_desc), 'c_id === 'o_c_id)\n```\n### Result\n| c_id |  c_name|c_desc  | o_id | c_id | o_time|o_desc |\n| --- | --- | --- | --- | --- | --- | --- |\n| c_001 | Kevin | from JinLin | o_002 |  c_001| 2018-11-05 10:01:55 |ipad|\n| c_001 | Kevin | from JinLin | o_003| c_001 | 2018-11-05 10:03:44  | flink book|\n| c_002| Sunny | from JinLin | o_oo1 | c_002 | 2018-11-05 10:01:01|iphone |\n| c_003| JinCheng | from HeBei | NULL|NULL|NULL|NULL\n\n\n### 特别说明\n`RIGHT JOIN` 相当于 `LEFT JOIN` 左右两个表交互一下位置。`FULL JOIN`相当于 `RIGHT JOIN` 和 `LEFT JOIN` 之后进行`UNION ALL`操作。\n\n## Time-Interval JOIN\nTime-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下：\n```\n...\nval result = left\n  .join(right)\n  // 定义Time Interval\n  .where('a === 'd && 'c >= 'f - 5.seconds && 'c < 'f + 6.seconds)\n  ...\n```\n\n## Lateral JOIN\nApache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下：\n```\n...\nval udtf = new UDTF\nval result = source.join(udtf('c) as ('d, 'e))\n...\n```\n## Temporal Table JOIN\nTemporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下：\n\n```\n...\nval rates = tEnv.scan(\"versonedTable\").createTemporalTableFunction('rowtime, 'r_currency)\nval result = left.join(rates('o_rowtime), 'r_currency === 'o_currency)\n...\n```\n\n## Window\n在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。\n\n### Over Window\nApache Flink中对OVER Window的定义遵循标准SQL的定义语法。\n按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:\n* ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n* RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。\n\n#### Bounded ROWS OVER Window\nBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。\n##### 语义\n我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n![](FC821998-4A3E-4F10-9351-C7B3060E7349.png)\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。\n\n##### Table API 示例\n利用`item_tab`测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。\n```\nval result = item\n  .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.rows following CURRENT_ROW as 'w)\n  .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)\n```\n#### Result\n\n| itemID | itemType | onSellTime | price | maxPrice |\n| :--- | :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 | 50 |\n| ITEM003 | Electronic | 2017-11-11 10:03:00 | 30 | 50 |\n| ITEM004 | Electronic | 2017-11-11 10:03:00 | 60 | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 | 60 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 | 60 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 | 20 |\n\n#### Bounded RANGE OVER Window\nBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。\n##### 语义\n我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n![](79DC098C-1171-4C0E-A735-C4F63B6010FD.png)\n\n注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。\n\n\n##### Tabel API 示例\n我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。\n```sql\n val result = item\n   .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w)\n   .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)\n```\n##### Result（Bounded RANGE OVER Window）\n| itemID | itemType | onSellTime | price | maxPrice |\n| :--- | :--- | :--- | :--- | :--- |\n| ITEM001 | Electronic | 2017-11-11 10:01:00 | 20 | 20 |\n| ITEM002 | Electronic | 2017-11-11 10:02:00 | 50 | 50 |\n| ITEM003 | Electronic | _**2017-11-11 10:03:00**_ | 30 | 60 |\n| ITEM004 | Electronic | _**2017-11-11 10:03:00**_ | 60 | 60 |\n| ITEM005 | Electronic | 2017-11-11 10:05:00 | 40 | 60 |\n| ITEM006 | Electronic | 2017-11-11 10:06:00 | 20 | 40 |\n| ITEM007 | Electronic | 2017-11-11 10:07:00 | 70 | 70 |\n| ITEM008 | Clothes | 2017-11-11 10:08:00 | 20 | 20 |\n\n#### 特别说明\nOverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在`SELECT`中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，`SELECT`可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:\n* GROUP BY - `tab.groupBy('d).select(d, MAX(c))`\n* OVER Window = `tab.window(Over.. as 'w).select('a, 'b, 'c, 'd, c.max over 'w)`\n如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。\n\n### Group Window\n根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:\n\n* Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；\n* Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；\n* Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。\n\n**说明：** Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。\n\n#### Tumble\n##### 语义\nTumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：\n\n![](C2A514D5-61F1-4464-95ED-17EB2A030EC2.png)\n\n##### Table API 示例\n利用`pageAccess_tab`测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。\n```\n val result = pageAccess\n   .window(Tumble over 2.minute on 'rowtime as 'w)\n   .groupBy('w, 'region)\n   .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n```\n##### Result\n\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n#### Hop\nHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。\n##### 语义\nHop 滑动窗口语义如下所示：\n![](3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png)\n\n##### Table API 示例\n利用`pageAccessCount_tab`测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).\n```\nval result = pageAccessCount\n  .window(Slide over 10.minute every 5.minute on 'rowtime as 'w)\n  .groupBy('w)\n  .select('w.start, 'w.end, 'accessCount.sum as 'accessCount)\n```\n\n##### Result\n\n| winStart | winEnd | accessCount |\n| --- | --- | --- |\n|2017-11-11 01:55:00.0|2017-11-11 02:05:00.0|186\n|2017-11-11 02:00:00.0|2017-11-11 02:10:00.0|396\n|2017-11-11 02:05:00.0|2017-11-11 02:15:00.0|243\n|2017-11-11 02:10:00.0|2017-11-11 02:20:00.0|33\n|2017-11-11 04:05:00.0|2017-11-11 04:15:00.0|129\n|2017-11-11 04:10:00.0|2017-11-11 04:20:00.0|129\n\n#### Session\nSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.\n##### 语义\nSession 会话窗口语义如下所示：\n\n![](D1197B0B-2DCF-434E-A7BB-A250187079F0.png)\n\n```\nval result = pageAccessSession\n  .window(Session withGap 3.minute on 'rowtime as 'w)\n  .groupBy('w, 'region)\n  .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n```\n##### Result\n| region | winStart | winEnd | pv |\n| --- | --- | --- | --- |\n|BeiJing|2017-11-11 02:10:00.0|2017-11-11 02:13:00.0|1\n|ShangHai|2017-11-11 02:01:00.0|2017-11-11 02:08:00.0|4\n|ShangHai|2017-11-11 02:10:00.0|2017-11-11 02:14:00.0|2\n|ShangHai|2017-11-11 04:16:00.0|2017-11-11 04:19:00.0|1\n\n#### 嵌套Window\n在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用`'w.rowtime`来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下:\n\n```\n ... \n val result = pageAccess\n   .window(Tumble over 2.minute on 'rowtime as 'w1)\n   .groupBy('w1)\n   .select('w1.rowtime as 'rowtime, 'col1.count as 'cnt)\n   .window(Session withGap 3.minute on 'rowtime as 'w2)\n   .groupBy('w2)\n   .select('cnt.sum)\n...\n```\n\n# Source&Sink\n上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下：\n\n| region | userId | accessTime |\n| --- | --- | --- |\n| ShangHai | U0010 | 2017-11-11 10:01:00 |\n| BeiJing |  U1001| 2017-11-11 10:01:00 |\n| BeiJing | U2032 | 2017-11-11 10:10:00 |\n| BeiJing  | U1100 | 2017-11-11 10:11:00 |\n| ShangHai | U0011 | 2017-11-11 12:10:00 |\n\n## Source 定义\n自定义Apache Flink Stream Source需要实现`StreamTableSource`, `StreamTableSource`中通过`StreamExecutionEnvironment` 的`addSource`方法获取`DataStream`, 所以我们需要自定义一个 `SourceFunction`, 并且要支持产生WaterMark，也就是要实现`DefinedRowtimeAttributes`接口。\n\n### Source Function定义\n支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:\n```\nclass MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) \n  extends SourceFunction[T] {\n  override def run(ctx: SourceContext[T]): Unit = {\n    dataWithTimestampList.foreach {\n      case Left(t) => ctx.collectWithTimestamp(t._2, t._1)\n      case Right(w) => ctx.emitWatermark(new Watermark(w))\n    }\n  }\n  override def cancel(): Unit = ???\n}\n```\n\n### 定义 StreamTableSource\n我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:\n```\nclass MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes {\n\n  val fieldNames = Array(\"accessTime\", \"region\", \"userId\")\n  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))\n  val rowType = new RowTypeInfo(\n    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],\n    fieldNames)\n\n  // 页面访问表数据 rows with timestamps and watermarks\n  val data = Seq(\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"ShangHai\", \"U0010\")),\n    Right(1510365660000L),\n    Left(1510365660000L, Row.of(new JLong(1510365660000L), \"BeiJing\", \"U1001\")),\n    Right(1510365660000L),\n    Left(1510366200000L, Row.of(new JLong(1510366200000L), \"BeiJing\", \"U2032\")),\n    Right(1510366200000L),\n    Left(1510366260000L, Row.of(new JLong(1510366260000L), \"BeiJing\", \"U1100\")),\n    Right(1510366260000L),\n    Left(1510373400000L, Row.of(new JLong(1510373400000L), \"ShangHai\", \"U0011\")),\n    Right(1510373400000L)\n  )\n\n  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = {\n    Collections.singletonList(new RowtimeAttributeDescriptor(\n      \"accessTime\",\n      new ExistingField(\"accessTime\"),\n      PreserveWatermarks.INSTANCE))\n  }\n\n  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = {\n     execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1)\n  }\n\n  override def getReturnType: TypeInformation[Row] = rowType\n\n  override def getTableSchema: TableSchema = schema\n\n}\n```\n  \n## Sink 定义\n我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n```\ndef getCsvTableSink: TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    // 打印sink的文件路径，方便我们查看运行结果\n    println(\"Sink path : \" + tempFile)\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(\n      Array[String](\"region\", \"winStart\", \"winEnd\", \"pv\"),\n      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))\n  }\n```\n\n## 构建主程序\n主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：\n\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    // 设置EventTime\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"mySource\"\n    // 创建自定义source数据结构\n    val tableSource = new MyTableSource\n\n    val sinkTableName = \"csvSink\"\n    // 创建CSV sink 数据结构\n    val tableSink = getCsvTableSink\n\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n    // 注册sink\n    tEnv.registerTableSink(sinkTableName, tableSink)\n\n    val result = tEnv.scan(sourceTableName)\n      .window(Tumble over 2.minute on 'accessTime as 'w)\n      .groupBy('w, 'region)\n      .select('region, 'w.start, 'w.end, 'region.count as 'pv)\n\n    result.insertInto(sinkTableName)\n    env.execute()\n  }\n  \n```\n\n## 执行并查看运行结果\n执行主程序后我们会在控制台得到Sink的文件路径，如下：\n```\nSink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\n\n```\n\nCat 方式查看计算结果，如下：\n```\njinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem\nShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1\nBeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2\nShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1\n```\n\n表格化如上结果：\n| region |winStart  | winEnd | pv|\n| --- | --- | --- | --- |\n| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2\n| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1\n| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1\n\n上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！\n\n\n# 小结\n本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。\n\n# 关于点赞和评论\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!\n","slug":"Apache Flink 漫谈系列 - Table API 概述","published":1,"updated":"2019-07-13T13:20:57.786Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5ff000ds94h6weiamkx","content":"<h1>什么是Table API</h1>\n<p>在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象.</p>\n<p>如下图所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png\" alt></p>\n<p>Apache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。</p>\n<h1>Table API的特点</h1>\n<p>Table API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下：</p>\n<ul>\n<li>声明式 - 用户只关心做什么，不用关心怎么做；</li>\n<li>高性能 - 支持查询优化，可以获取最好的执行性能；</li>\n<li>流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行；</li>\n<li>标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。</li>\n</ul>\n<p>当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点：</p>\n<ul>\n<li>表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等</li>\n<li>功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能</li>\n<li>编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。</li>\n</ul>\n<p><strong>说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 <a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739\" target=\"_blank\" rel=\"noopener\">FLIP-29</a> 中规划的新功能。</strong></p>\n<h1>HelloWorld</h1>\n<p>在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。</p>\n<h2>Maven 依赖</h2>\n<p>在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt;</span><br><span class=\"line\">    &lt;table.version&gt;1.7.0&lt;/table.version&gt;</span><br><span class=\"line\">  &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;dependencies&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-table_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></p>\n<h2>程序结构</h2>\n<p>在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png\" alt></p>\n<ol>\n<li>外部数据源，比如Kafka, Rabbitmq, CSV 等等；</li>\n<li>查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等；</li>\n<li>外部结果存储，比如Kafka，Cassandra，CSV等。</li>\n</ol>\n<p><strong>说明：1和3 在Apache Flink中统称为Connector。</strong></p>\n<h2>主程序</h2>\n<p>我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。\n根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码：</p>\n<ul>\n<li>TableSoruce Code - 用于创建数据源的代码</li>\n<li>Table API Query - 用于进行word count统计的Table API 查询逻辑</li>\n<li>TableSink Code - 用于保存word count计算结果的结果表代码</li>\n</ul>\n<h3>运行模式选择</h3>\n<p>一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Stream运行环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br></pre></td></tr></table></figure></p>\n<h3>构建测试Source</h3>\n<p>我们用最简单的构建Source方式进行本次测试，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> // 测试数据</span><br><span class=\"line\"> val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class=\"line\"> // 最简单的获取Source方式</span><br><span class=\"line\">val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br></pre></td></tr></table></figure></p>\n<h3>WordCount 统计逻辑</h3>\n<p>WordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 单词统计核心逻辑</span><br><span class=\"line\">val result = source</span><br><span class=\"line\">  .groupBy(&apos;word) // 单词分组</span><br><span class=\"line\">  .select(&apos;word, &apos;word.count) // 单词统计</span><br></pre></td></tr></table></figure></p>\n<h3>定义Sink</h3>\n<p>将WordCount的统计结果写入Sink中，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 自定义Sink</span><br><span class=\"line\">    val sink = new RetractSink // 自定义Sink（下面有完整代码）</span><br><span class=\"line\">    // 计算结果写入sink</span><br><span class=\"line\">    result.toRetractStream[(String, Long)].addSink(sink)</span><br></pre></td></tr></table></figure></p>\n<h3>完整的HelloWord代码</h3>\n<p>为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.configuration.Configuration</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.TableEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\"></span><br><span class=\"line\">object HelloWord &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // 测试数据</span><br><span class=\"line\">    val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Stream运行环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    // 最简单的获取Source方式</span><br><span class=\"line\">    val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 单词统计核心逻辑</span><br><span class=\"line\">    val result = source</span><br><span class=\"line\">      .groupBy(&apos;word) // 单词分组</span><br><span class=\"line\">      .select(&apos;word, &apos;word.count) // 单词统计</span><br><span class=\"line\"></span><br><span class=\"line\">    // 自定义Sink</span><br><span class=\"line\">    val sink = new RetractSink</span><br><span class=\"line\">    // 计算结果写入sink</span><br><span class=\"line\">    result.toRetractStream[(String, Long)].addSink(sink)</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class RetractSink extends RichSinkFunction[(Boolean, (String, Long))] &#123;</span><br><span class=\"line\">  private var resultSet: mutable.Set[(String, Long)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    // 初始化内存存储结构</span><br><span class=\"line\">    resultSet = new mutable.HashSet[(String, Long)]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = &#123;</span><br><span class=\"line\">    if (v._1) &#123;</span><br><span class=\"line\">      // 计算数据</span><br><span class=\"line\">      resultSet.add(v._2)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    else &#123;</span><br><span class=\"line\">      // 撤回数据</span><br><span class=\"line\">      resultSet.remove(v._2)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def close(): Unit = &#123;</span><br><span class=\"line\">    // 打印写入sink的结果数据</span><br><span class=\"line\">    resultSet.foreach(println)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png\" alt></p>\n<p>虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码:\n<code>source.groupBy('word).select('word, 'word.count)</code>\n所以Table API开发技术任务非常的简洁高效。</p>\n<h1>Table API 算子</h1>\n<p>虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。</p>\n<h2>示例数据及测试类</h2>\n<h3>测试数据</h3>\n<ul>\n<li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>o_id</th>\n<th>c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n<tr>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Item_tab\n商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">30</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccess_tab\n页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccessCount_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userCount</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>100</td>\n<td>2017.11.11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>86</td>\n<td>2017.11.11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>210</td>\n<td>2017.11.11 10:06:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>33</td>\n<td>2017.11.11 10:10:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>129</td>\n<td>2017.11.11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccessSession_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0012</td>\n<td>2017-11-11 10:02:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0013</td>\n<td>2017-11-11 10:03:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0015</td>\n<td>2017-11-11 10:05:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U0110</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U2010</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0410</td>\n<td>2017-11-11 12:16:00</td>\n</tr>\n</tbody>\n</table>\n<h3>测试类</h3>\n<p>我们创建一个<code>TableAPIOverviewITCase.scala</code> 用于接下来介绍Flink Table API算子的功能体验。代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.runtime.state.StateBackend</span><br><span class=\"line\">import org.apache.flink.runtime.state.memory.MemoryStateBackend</span><br><span class=\"line\">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.source.SourceFunction</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.streaming.api.watermark.Watermark</span><br><span class=\"line\">import org.apache.flink.table.api.&#123;Table, TableEnvironment&#125;</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\">import org.junit.rules.TemporaryFolder</span><br><span class=\"line\">import org.junit.&#123;Rule, Test&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\">import scala.collection.mutable.ArrayBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">class Table APIOverviewITCase &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  // 客户表数据</span><br><span class=\"line\">  val customer_data = new mutable.MutableList[(String, String, String)]</span><br><span class=\"line\">  customer_data.+=((&quot;c_001&quot;, &quot;Kevin&quot;, &quot;from JinLin&quot;))</span><br><span class=\"line\">  customer_data.+=((&quot;c_002&quot;, &quot;Sunny&quot;, &quot;from JinLin&quot;))</span><br><span class=\"line\">  customer_data.+=((&quot;c_003&quot;, &quot;JinCheng&quot;, &quot;from HeBei&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  // 订单表数据</span><br><span class=\"line\">  val order_data = new mutable.MutableList[(String, String, String, String)]</span><br><span class=\"line\">  order_data.+=((&quot;o_001&quot;, &quot;c_002&quot;, &quot;2018-11-05 10:01:01&quot;, &quot;iphone&quot;))</span><br><span class=\"line\">  order_data.+=((&quot;o_002&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:01:55&quot;, &quot;ipad&quot;))</span><br><span class=\"line\">  order_data.+=((&quot;o_003&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:03:44&quot;, &quot;flink book&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 商品销售表数据</span><br><span class=\"line\">  val item_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, 20, &quot;ITEM001&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, 50, &quot;ITEM002&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365780000L, (1510365780000L, 30, &quot;ITEM003&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Left((1510365780000L, (1510365780000L, 60, &quot;ITEM004&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365780000L)),</span><br><span class=\"line\">    Left((1510365900000L, (1510365900000L, 40, &quot;ITEM005&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365900000L)),</span><br><span class=\"line\">    Left((1510365960000L, (1510365960000L, 20, &quot;ITEM006&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365960000L)),</span><br><span class=\"line\">    Left((1510366020000L, (1510366020000L, 70, &quot;ITEM007&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510366020000L)),</span><br><span class=\"line\">    Left((1510366080000L, (1510366080000L, 20, &quot;ITEM008&quot;, &quot;Clothes&quot;))),</span><br><span class=\"line\">    Right((151036608000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据</span><br><span class=\"line\">  val pageAccess_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0010&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, &quot;U1001&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2032&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366260000L, (1510366260000L, &quot;BeiJing&quot;, &quot;U1100&quot;))),</span><br><span class=\"line\">    Right((1510366260000L)),</span><br><span class=\"line\">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510373400000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问量表数据2</span><br><span class=\"line\">  val pageAccessCount_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, 100))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, 86))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365960000L, (1510365960000L, &quot;BeiJing&quot;, 210))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, 33))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, 129))),</span><br><span class=\"line\">    Right((1510373400000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据3</span><br><span class=\"line\">  val pageAccessSession_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0012&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0013&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365900000L, (1510365900000L, &quot;ShangHai&quot;, &quot;U0015&quot;))),</span><br><span class=\"line\">    Right((1510365900000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2010&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366260000L, (1510366260000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510366260000L)),</span><br><span class=\"line\">    Left((1510373760000L, (1510373760000L, &quot;ShangHai&quot;, &quot;U0410&quot;))),</span><br><span class=\"line\">    Right((1510373760000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  val _tempFolder = new TemporaryFolder</span><br><span class=\"line\"></span><br><span class=\"line\">  // Streaming 环境</span><br><span class=\"line\">  val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">  val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">  env.setParallelism(1)</span><br><span class=\"line\">  env.setStateBackend(getStateBackend)</span><br><span class=\"line\"></span><br><span class=\"line\">  def getProcTimeTables(): (Table, Table) = &#123;</span><br><span class=\"line\">    // 将order_tab, customer_tab 注册到catalog</span><br><span class=\"line\">    val customer = env.fromCollection(customer_data).toTable(tEnv).as(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br><span class=\"line\">    val order = env.fromCollection(order_data).toTable(tEnv).as(&apos;o_id, &apos;c_id, &apos;o_time, &apos;o_desc)</span><br><span class=\"line\">    (customer, order)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  def getEventTimeTables():  (Table, Table, Table, Table) = &#123;</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 将item_tab, pageAccess_tab 注册到catalog</span><br><span class=\"line\">    val item =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;onSellTime, &apos;price, &apos;itemID, &apos;itemType, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccess =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccessCount =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;accessCount, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccessSession =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    (item, pageAccess, pageAccessCount, pageAccessSession)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  @Rule</span><br><span class=\"line\">  def tempFolder: TemporaryFolder = _tempFolder</span><br><span class=\"line\"></span><br><span class=\"line\">  def getStateBackend: StateBackend = &#123;</span><br><span class=\"line\">    new MemoryStateBackend()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def procTimePrint(result: Table): Unit = &#123;</span><br><span class=\"line\">    val sink = new RetractingSink</span><br><span class=\"line\">    result.toRetractStream[Row].addSink(sink)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def rowTimePrint(result: Table): Unit = &#123;</span><br><span class=\"line\">    val sink = new RetractingSink</span><br><span class=\"line\">    result.toRetractStream[Row].addSink(sink)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Test</span><br><span class=\"line\">  def testProc(): Unit = &#123;</span><br><span class=\"line\">    val (customer, order) = getProcTimeTables()</span><br><span class=\"line\">    val result = ...// 测试的查询逻辑</span><br><span class=\"line\">    procTimePrint(result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Test</span><br><span class=\"line\">  def testEvent(): Unit = &#123;</span><br><span class=\"line\">    val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables()</span><br><span class=\"line\">    val result = ...// 测试的查询逻辑</span><br><span class=\"line\">    procTimePrint(result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 自定义Sink</span><br><span class=\"line\">final class RetractingSink extends RichSinkFunction[(Boolean, Row)] &#123;</span><br><span class=\"line\">  var retractedResults: ArrayBuffer[String] = null</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    super.open(parameters)</span><br><span class=\"line\">    retractedResults = mutable.ArrayBuffer.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def invoke(v: (Boolean, Row)) &#123;</span><br><span class=\"line\">    retractedResults.synchronized &#123;</span><br><span class=\"line\">      val value = v._2.toString</span><br><span class=\"line\">      if (v._1) &#123;</span><br><span class=\"line\">        retractedResults += value</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        val idx = retractedResults.indexOf(value)</span><br><span class=\"line\">        if (idx &gt;= 0) &#123;</span><br><span class=\"line\">          retractedResults.remove(idx)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          throw new RuntimeException(&quot;Tried to retract a value that wasn&apos;t added first. &quot; +</span><br><span class=\"line\">                                       &quot;This is probably an incorrectly implemented test. &quot; +</span><br><span class=\"line\">                                       &quot;Try to set the parallelism of the sink to 1.&quot;)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def close(): Unit = &#123;</span><br><span class=\"line\">    super.close()</span><br><span class=\"line\">    retractedResults.sorted.foreach(println(_))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Water mark 生成器</span><br><span class=\"line\">class EventTimeSourceFunction[T](</span><br><span class=\"line\">  dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>SELECT</h2>\n<p>SELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示:\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/EEED7340-FE9D-4791-961C-9996D35E148F.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .select(&apos;c_name, concat_ws(&apos;c_name, &quot; come &quot;, &apos;c_desc))</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_name</th>\n<th>desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Kevin</td>\n<td>Kevin come from JinLin</td>\n</tr>\n<tr>\n<td>Sunny</td>\n<td>Sunny come from JinLin</td>\n</tr>\n<tr>\n<td>Jincheng</td>\n<td>Jincheng come from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下：</p>\n<h4>Table API示例</h4>\n<p>在订单表查询所有的客户id，消除重复客户id, 如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .groupBy(&apos;c_id)</span><br><span class=\"line\">  .select(&apos;c_id)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n</tr>\n<tr>\n<td>c_002</td>\n</tr>\n</tbody>\n</table>\n<h2>WHERE</h2>\n<p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/15657C5E-DD6E-4339-824E-6B17C30886C9.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .where(&quot;c_id = &apos;c_001&apos; || c_id = &apos;c_003&apos;&quot;)</span><br><span class=\"line\">  .select( &apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>&amp;&amp;</code>， <code>||</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的<code>IN</code>和<code>NOT IN</code>在Table API里面用<code>intersect</code> 和 <code>minus</code>描述(flink-1.7.0版本)。</p>\n<h4>Intersect 示例</h4>\n<p><code>Intersect</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询已经下过订单的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 计算客户id，并去重</span><br><span class=\"line\">val distinct_cids = order</span><br><span class=\"line\">  .groupBy(&apos;c_id) // 去重</span><br><span class=\"line\">  .select(&apos;c_id as &apos;o_c_id)</span><br><span class=\"line\"></span><br><span class=\"line\">val result = customer</span><br><span class=\"line\">  .join(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n</tbody>\n</table>\n<h4>Minus 示例</h4>\n<p><code>Minus</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询没有下过订单的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> // 查询下过订单的客户id，并去重</span><br><span class=\"line\"> val distinct_cids = order</span><br><span class=\"line\">   .groupBy(&apos;c_id)</span><br><span class=\"line\">   .select(&apos;c_id as &apos;o_c_id)</span><br><span class=\"line\">   </span><br><span class=\"line\">// 查询没有下过订单的客户信息</span><br><span class=\"line\">val result = customer</span><br><span class=\"line\">  .leftOuterJoin(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class=\"line\">  .where(&apos;o_c_id isNull)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<p>说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。</p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h4>Intersect/Minus与关系代数</h4>\n<p>如上介绍<code>Intersect</code>是关系代数中的Intersection， <code>Minus</code>是关系代数的Difference， 如下图示意：</p>\n<ul>\n<li>Intersect(Intersection)\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png\" alt></li>\n<li>Minus(Difference)\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png\" alt></li>\n</ul>\n<h2>GROUP BY</h2>\n<p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/D56FE35E-0212-444C-8B0F-C650F1300C3C.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>将order_tab信息按<code>c_id</code>分组统计订单数量，简单示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .groupBy(&apos;c_id)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;o_id.count)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>o_count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>2</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p>\n<h4>Table API 示例</h4>\n<p>按时间进行分组，查询每分钟的订单数量，如下：</p>\n<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .select(&apos;o_id, &apos;c_id, &apos;o_time.substring(1, 16) as &apos;o_time_min)</span><br><span class=\"line\">  .groupBy(&apos;o_time_min)</span><br><span class=\"line\">  .select(&apos;o_time_min, &apos;o_id.count)</span><br></pre></td></tr></table></figure>\n</code></pre>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>o_time_min</th>\n<th>o_count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2018-11-05 10:01</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2018-11-05 10:03</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p>\n<h2>UNION ALL</h2>\n<p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/3402EE52-F2D5-452E-A0A5-66F0EC14C878.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer.unionAll(customer)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p>\n<h2>UNION</h2>\n<p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/AE6DCCF9-32A4-4E7A-B7C5-334983354240.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer.union(customer)</span><br></pre></td></tr></table></figure></p>\n<p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p>\n<h2>JOIN</h2>\n<p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>\n<ul>\n<li>JOIN - INNER JOIN</li>\n<li>LEFT JOIN - LEFT OUTER JOIN</li>\n<li>RIGHT JOIN - RIGHT OUTER JOIN</li>\n<li>FULL JOIN - FULL OUTER JOIN\nJOIN与关系代数的Join语义相同，具体如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png\" alt></li>\n</ul>\n<h3>Table API 示例 (JOIN)</h3>\n<p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .join(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n<th>o_id</th>\n<th>o_c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n</tbody>\n</table>\n<h3>Table API 示例 (LEFT JOIN)</h3>\n<p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png\" alt>\n对应的SQL语句如下(LEFT JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li>\n</ul>\n<p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .leftOuterJoin(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n<th>o_id</th>\n<th>c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n<td>NULL</td>\n<td>NULL</td>\n<td>NULL</td>\n<td>NULL</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>\n<h2>Time-Interval JOIN</h2>\n<p>Time-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val result = left</span><br><span class=\"line\">  .join(right)</span><br><span class=\"line\">  // 定义Time Interval</span><br><span class=\"line\">  .where(&apos;a === &apos;d &amp;&amp; &apos;c &gt;= &apos;f - 5.seconds &amp;&amp; &apos;c &lt; &apos;f + 6.seconds)</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure></p>\n<h2>Lateral JOIN</h2>\n<p>Apache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val udtf = new UDTF</span><br><span class=\"line\">val result = source.join(udtf(&apos;c) as (&apos;d, &apos;e))</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>Temporal Table JOIN</h2>\n<p>Temporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val rates = tEnv.scan(&quot;versonedTable&quot;).createTemporalTableFunction(&apos;rowtime, &apos;r_currency)</span><br><span class=\"line\">val result = left.join(rates(&apos;o_rowtime), &apos;r_currency === &apos;o_currency)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>Window</h2>\n<p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>\n<h3>Over Window</h3>\n<p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。\n按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p>\n<ul>\n<li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li>\n<li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li>\n</ul>\n<h4>Bounded ROWS OVER Window</h4>\n<p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>\n<h5>语义</h5>\n<p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/FC821998-4A3E-4F10-9351-C7B3060E7349.png\" alt>\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p>\n<h5>Table API 示例</h5>\n<p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = item</span><br><span class=\"line\">  .window(Over partitionBy &apos;itemType orderBy &apos;rowtime preceding 2.rows following CURRENT_ROW as &apos;w)</span><br><span class=\"line\">  .select(&apos;itemID, &apos;itemType, &apos;onSellTime, &apos;price, &apos;price.max over &apos;w as &apos;maxPrice)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n<th style=\"text-align:left\">maxPrice</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:03:00</td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:03:00</td>\n<td style=\"text-align:left\">60</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<h4>Bounded RANGE OVER Window</h4>\n<p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p>\n<h5>语义</h5>\n<p>我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/79DC098C-1171-4C0E-A735-C4F63B6010FD.png\" alt></p>\n<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p>\n<h5>Tabel API 示例</h5>\n<p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = item</span><br><span class=\"line\">  .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w)</span><br><span class=\"line\">  .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)</span><br></pre></td></tr></table></figure></p>\n<h5>Result（Bounded RANGE OVER Window）</h5>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n<th style=\"text-align:left\">maxPrice</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">60</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">40</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<h4>特别说明</h4>\n<p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p>\n<ul>\n<li>GROUP BY - <code>tab.groupBy('d).select(d, MAX(c))</code></li>\n<li>OVER Window = <code>tab.window(Over.. as 'w).select('a, 'b, 'c, 'd, c.max over 'w)</code>\n如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li>\n</ul>\n<h3>Group Window</h3>\n<p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>\n<ul>\n<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>\n<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>\n<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li>\n</ul>\n<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>\n<h4>Tumble</h4>\n<h5>语义</h5>\n<p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/C2A514D5-61F1-4464-95ED-17EB2A030EC2.png\" alt></p>\n<h5>Table API 示例</h5>\n<p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccess</span><br><span class=\"line\">  .window(Tumble over 2.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h4>Hop</h4>\n<p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p>\n<h5>语义</h5>\n<p>Hop 滑动窗口语义如下所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png\" alt></p>\n<h5>Table API 示例</h5>\n<p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccessCount</span><br><span class=\"line\">  .window(Slide over 10.minute every 5.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w)</span><br><span class=\"line\">  .select(&apos;w.start, &apos;w.end, &apos;accessCount.sum as &apos;accessCount)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>winStart</th>\n<th>winEnd</th>\n<th>accessCount</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2017-11-11 01:55:00.0</td>\n<td>2017-11-11 02:05:00.0</td>\n<td>186</td>\n</tr>\n<tr>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>396</td>\n</tr>\n<tr>\n<td>2017-11-11 02:05:00.0</td>\n<td>2017-11-11 02:15:00.0</td>\n<td>243</td>\n</tr>\n<tr>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:20:00.0</td>\n<td>33</td>\n</tr>\n<tr>\n<td>2017-11-11 04:05:00.0</td>\n<td>2017-11-11 04:15:00.0</td>\n<td>129</td>\n</tr>\n<tr>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:20:00.0</td>\n<td>129</td>\n</tr>\n</tbody>\n</table>\n<h4>Session</h4>\n<p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p>\n<h5>语义</h5>\n<p>Session 会话窗口语义如下所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/D1197B0B-2DCF-434E-A7BB-A250187079F0.png\" alt></p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccessSession</span><br><span class=\"line\">  .window(Session withGap 3.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:13:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:01:00.0</td>\n<td>2017-11-11 02:08:00.0</td>\n<td>4</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:14:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:16:00.0</td>\n<td>2017-11-11 04:19:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h4>嵌套Window</h4>\n<p>在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用<code>'w.rowtime</code>来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下:</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> ... </span><br><span class=\"line\"> val result = pageAccess</span><br><span class=\"line\">   .window(Tumble over 2.minute on &apos;rowtime as &apos;w1)</span><br><span class=\"line\">   .groupBy(&apos;w1)</span><br><span class=\"line\">   .select(&apos;w1.rowtime as &apos;rowtime, &apos;col1.count as &apos;cnt)</span><br><span class=\"line\">   .window(Session withGap 3.minute on &apos;rowtime as &apos;w2)</span><br><span class=\"line\">   .groupBy(&apos;w2)</span><br><span class=\"line\">   .select(&apos;cnt.sum)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h1>Source&amp;Sink</h1>\n<p>上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<h2>Source 定义</h2>\n<p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>\n<h3>Source Function定义</h3>\n<p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class=\"line\">  extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3>定义 StreamTableSource</h3>\n<p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class=\"line\">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class=\"line\">  val rowType = new RowTypeInfo(</span><br><span class=\"line\">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class=\"line\">    fieldNames)</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class=\"line\">  val data = Seq(</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class=\"line\">    Right(1510366200000L),</span><br><span class=\"line\">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class=\"line\">    Right(1510366260000L),</span><br><span class=\"line\">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class=\"line\">    Right(1510373400000L)</span><br><span class=\"line\">  )</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class=\"line\">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class=\"line\">      &quot;accessTime&quot;,</span><br><span class=\"line\">      new ExistingField(&quot;accessTime&quot;),</span><br><span class=\"line\">      PreserveWatermarks.INSTANCE))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class=\"line\">     execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getTableSchema: TableSchema = schema</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>Sink 定义</h2>\n<p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class=\"line\">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class=\"line\">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class=\"line\">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>构建主程序</h2>\n<p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 设置EventTime</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;mySource&quot;</span><br><span class=\"line\">    // 创建自定义source数据结构</span><br><span class=\"line\">    val tableSource = new MyTableSource</span><br><span class=\"line\"></span><br><span class=\"line\">    val sinkTableName = &quot;csvSink&quot;</span><br><span class=\"line\">    // 创建CSV sink 数据结构</span><br><span class=\"line\">    val tableSink = getCsvTableSink</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\">    // 注册sink</span><br><span class=\"line\">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(sourceTableName)</span><br><span class=\"line\">      .window(Tumble over 2.minute on &apos;accessTime as &apos;w)</span><br><span class=\"line\">      .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">      .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br><span class=\"line\"></span><br><span class=\"line\">    result.insertInto(sinkTableName)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>执行主程序后我们会在控制台得到Sink的文件路径，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure></p>\n<p>Cat 方式查看计算结果，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class=\"line\">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class=\"line\">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure></p>\n<p>表格化如上结果：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p>\n<h1>小结</h1>\n<p>本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>什么是Table API</h1>\n<p>在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象.</p>\n<p>如下图所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png\" alt></p>\n<p>Apache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。</p>\n<h1>Table API的特点</h1>\n<p>Table API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下：</p>\n<ul>\n<li>声明式 - 用户只关心做什么，不用关心怎么做；</li>\n<li>高性能 - 支持查询优化，可以获取最好的执行性能；</li>\n<li>流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行；</li>\n<li>标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。</li>\n</ul>\n<p>当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点：</p>\n<ul>\n<li>表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等</li>\n<li>功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能</li>\n<li>编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。</li>\n</ul>\n<p><strong>说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 <a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739\" target=\"_blank\" rel=\"noopener\">FLIP-29</a> 中规划的新功能。</strong></p>\n<h1>HelloWorld</h1>\n<p>在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。</p>\n<h2>Maven 依赖</h2>\n<p>在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt;</span><br><span class=\"line\">    &lt;table.version&gt;1.7.0&lt;/table.version&gt;</span><br><span class=\"line\">  &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;dependencies&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-table_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></p>\n<h2>程序结构</h2>\n<p>在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png\" alt></p>\n<ol>\n<li>外部数据源，比如Kafka, Rabbitmq, CSV 等等；</li>\n<li>查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等；</li>\n<li>外部结果存储，比如Kafka，Cassandra，CSV等。</li>\n</ol>\n<p><strong>说明：1和3 在Apache Flink中统称为Connector。</strong></p>\n<h2>主程序</h2>\n<p>我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。\n根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码：</p>\n<ul>\n<li>TableSoruce Code - 用于创建数据源的代码</li>\n<li>Table API Query - 用于进行word count统计的Table API 查询逻辑</li>\n<li>TableSink Code - 用于保存word count计算结果的结果表代码</li>\n</ul>\n<h3>运行模式选择</h3>\n<p>一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Stream运行环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br></pre></td></tr></table></figure></p>\n<h3>构建测试Source</h3>\n<p>我们用最简单的构建Source方式进行本次测试，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> // 测试数据</span><br><span class=\"line\"> val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class=\"line\"> // 最简单的获取Source方式</span><br><span class=\"line\">val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br></pre></td></tr></table></figure></p>\n<h3>WordCount 统计逻辑</h3>\n<p>WordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 单词统计核心逻辑</span><br><span class=\"line\">val result = source</span><br><span class=\"line\">  .groupBy(&apos;word) // 单词分组</span><br><span class=\"line\">  .select(&apos;word, &apos;word.count) // 单词统计</span><br></pre></td></tr></table></figure></p>\n<h3>定义Sink</h3>\n<p>将WordCount的统计结果写入Sink中，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 自定义Sink</span><br><span class=\"line\">    val sink = new RetractSink // 自定义Sink（下面有完整代码）</span><br><span class=\"line\">    // 计算结果写入sink</span><br><span class=\"line\">    result.toRetractStream[(String, Long)].addSink(sink)</span><br></pre></td></tr></table></figure></p>\n<h3>完整的HelloWord代码</h3>\n<p>为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.configuration.Configuration</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.TableEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\"></span><br><span class=\"line\">object HelloWord &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // 测试数据</span><br><span class=\"line\">    val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Stream运行环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    // 最简单的获取Source方式</span><br><span class=\"line\">    val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 单词统计核心逻辑</span><br><span class=\"line\">    val result = source</span><br><span class=\"line\">      .groupBy(&apos;word) // 单词分组</span><br><span class=\"line\">      .select(&apos;word, &apos;word.count) // 单词统计</span><br><span class=\"line\"></span><br><span class=\"line\">    // 自定义Sink</span><br><span class=\"line\">    val sink = new RetractSink</span><br><span class=\"line\">    // 计算结果写入sink</span><br><span class=\"line\">    result.toRetractStream[(String, Long)].addSink(sink)</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class RetractSink extends RichSinkFunction[(Boolean, (String, Long))] &#123;</span><br><span class=\"line\">  private var resultSet: mutable.Set[(String, Long)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    // 初始化内存存储结构</span><br><span class=\"line\">    resultSet = new mutable.HashSet[(String, Long)]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = &#123;</span><br><span class=\"line\">    if (v._1) &#123;</span><br><span class=\"line\">      // 计算数据</span><br><span class=\"line\">      resultSet.add(v._2)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    else &#123;</span><br><span class=\"line\">      // 撤回数据</span><br><span class=\"line\">      resultSet.remove(v._2)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def close(): Unit = &#123;</span><br><span class=\"line\">    // 打印写入sink的结果数据</span><br><span class=\"line\">    resultSet.foreach(println)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png\" alt></p>\n<p>虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码:\n<code>source.groupBy('word).select('word, 'word.count)</code>\n所以Table API开发技术任务非常的简洁高效。</p>\n<h1>Table API 算子</h1>\n<p>虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。</p>\n<h2>示例数据及测试类</h2>\n<h3>测试数据</h3>\n<ul>\n<li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>o_id</th>\n<th>c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n<tr>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Item_tab\n商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">30</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccess_tab\n页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccessCount_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userCount</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>100</td>\n<td>2017.11.11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>86</td>\n<td>2017.11.11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>210</td>\n<td>2017.11.11 10:06:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>33</td>\n<td>2017.11.11 10:10:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>129</td>\n<td>2017.11.11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>PageAccessSession_tab\n页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0012</td>\n<td>2017-11-11 10:02:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0013</td>\n<td>2017-11-11 10:03:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0015</td>\n<td>2017-11-11 10:05:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U0110</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U2010</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0410</td>\n<td>2017-11-11 12:16:00</td>\n</tr>\n</tbody>\n</table>\n<h3>测试类</h3>\n<p>我们创建一个<code>TableAPIOverviewITCase.scala</code> 用于接下来介绍Flink Table API算子的功能体验。代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.runtime.state.StateBackend</span><br><span class=\"line\">import org.apache.flink.runtime.state.memory.MemoryStateBackend</span><br><span class=\"line\">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.source.SourceFunction</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.streaming.api.watermark.Watermark</span><br><span class=\"line\">import org.apache.flink.table.api.&#123;Table, TableEnvironment&#125;</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\">import org.junit.rules.TemporaryFolder</span><br><span class=\"line\">import org.junit.&#123;Rule, Test&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\">import scala.collection.mutable.ArrayBuffer</span><br><span class=\"line\"></span><br><span class=\"line\">class Table APIOverviewITCase &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  // 客户表数据</span><br><span class=\"line\">  val customer_data = new mutable.MutableList[(String, String, String)]</span><br><span class=\"line\">  customer_data.+=((&quot;c_001&quot;, &quot;Kevin&quot;, &quot;from JinLin&quot;))</span><br><span class=\"line\">  customer_data.+=((&quot;c_002&quot;, &quot;Sunny&quot;, &quot;from JinLin&quot;))</span><br><span class=\"line\">  customer_data.+=((&quot;c_003&quot;, &quot;JinCheng&quot;, &quot;from HeBei&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  // 订单表数据</span><br><span class=\"line\">  val order_data = new mutable.MutableList[(String, String, String, String)]</span><br><span class=\"line\">  order_data.+=((&quot;o_001&quot;, &quot;c_002&quot;, &quot;2018-11-05 10:01:01&quot;, &quot;iphone&quot;))</span><br><span class=\"line\">  order_data.+=((&quot;o_002&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:01:55&quot;, &quot;ipad&quot;))</span><br><span class=\"line\">  order_data.+=((&quot;o_003&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:03:44&quot;, &quot;flink book&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 商品销售表数据</span><br><span class=\"line\">  val item_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, 20, &quot;ITEM001&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, 50, &quot;ITEM002&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365780000L, (1510365780000L, 30, &quot;ITEM003&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Left((1510365780000L, (1510365780000L, 60, &quot;ITEM004&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365780000L)),</span><br><span class=\"line\">    Left((1510365900000L, (1510365900000L, 40, &quot;ITEM005&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365900000L)),</span><br><span class=\"line\">    Left((1510365960000L, (1510365960000L, 20, &quot;ITEM006&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510365960000L)),</span><br><span class=\"line\">    Left((1510366020000L, (1510366020000L, 70, &quot;ITEM007&quot;, &quot;Electronic&quot;))),</span><br><span class=\"line\">    Right((1510366020000L)),</span><br><span class=\"line\">    Left((1510366080000L, (1510366080000L, 20, &quot;ITEM008&quot;, &quot;Clothes&quot;))),</span><br><span class=\"line\">    Right((151036608000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据</span><br><span class=\"line\">  val pageAccess_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0010&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, &quot;U1001&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2032&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366260000L, (1510366260000L, &quot;BeiJing&quot;, &quot;U1100&quot;))),</span><br><span class=\"line\">    Right((1510366260000L)),</span><br><span class=\"line\">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510373400000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问量表数据2</span><br><span class=\"line\">  val pageAccessCount_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, 100))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, 86))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365960000L, (1510365960000L, &quot;BeiJing&quot;, 210))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, 33))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, 129))),</span><br><span class=\"line\">    Right((1510373400000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据3</span><br><span class=\"line\">  val pageAccessSession_data = Seq(</span><br><span class=\"line\">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510365660000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0012&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0013&quot;))),</span><br><span class=\"line\">    Right((1510365720000L)),</span><br><span class=\"line\">    Left((1510365900000L, (1510365900000L, &quot;ShangHai&quot;, &quot;U0015&quot;))),</span><br><span class=\"line\">    Right((1510365900000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2010&quot;))),</span><br><span class=\"line\">    Right((1510366200000L)),</span><br><span class=\"line\">    Left((1510366260000L, (1510366260000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class=\"line\">    Right((1510366260000L)),</span><br><span class=\"line\">    Left((1510373760000L, (1510373760000L, &quot;ShangHai&quot;, &quot;U0410&quot;))),</span><br><span class=\"line\">    Right((1510373760000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">  val _tempFolder = new TemporaryFolder</span><br><span class=\"line\"></span><br><span class=\"line\">  // Streaming 环境</span><br><span class=\"line\">  val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">  val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">  env.setParallelism(1)</span><br><span class=\"line\">  env.setStateBackend(getStateBackend)</span><br><span class=\"line\"></span><br><span class=\"line\">  def getProcTimeTables(): (Table, Table) = &#123;</span><br><span class=\"line\">    // 将order_tab, customer_tab 注册到catalog</span><br><span class=\"line\">    val customer = env.fromCollection(customer_data).toTable(tEnv).as(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br><span class=\"line\">    val order = env.fromCollection(order_data).toTable(tEnv).as(&apos;o_id, &apos;c_id, &apos;o_time, &apos;o_desc)</span><br><span class=\"line\">    (customer, order)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  def getEventTimeTables():  (Table, Table, Table, Table) = &#123;</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 将item_tab, pageAccess_tab 注册到catalog</span><br><span class=\"line\">    val item =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;onSellTime, &apos;price, &apos;itemID, &apos;itemType, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccess =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccessCount =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;accessCount, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    val pageAccessSession =</span><br><span class=\"line\">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data))</span><br><span class=\"line\">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    (item, pageAccess, pageAccessCount, pageAccessSession)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  @Rule</span><br><span class=\"line\">  def tempFolder: TemporaryFolder = _tempFolder</span><br><span class=\"line\"></span><br><span class=\"line\">  def getStateBackend: StateBackend = &#123;</span><br><span class=\"line\">    new MemoryStateBackend()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def procTimePrint(result: Table): Unit = &#123;</span><br><span class=\"line\">    val sink = new RetractingSink</span><br><span class=\"line\">    result.toRetractStream[Row].addSink(sink)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def rowTimePrint(result: Table): Unit = &#123;</span><br><span class=\"line\">    val sink = new RetractingSink</span><br><span class=\"line\">    result.toRetractStream[Row].addSink(sink)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Test</span><br><span class=\"line\">  def testProc(): Unit = &#123;</span><br><span class=\"line\">    val (customer, order) = getProcTimeTables()</span><br><span class=\"line\">    val result = ...// 测试的查询逻辑</span><br><span class=\"line\">    procTimePrint(result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Test</span><br><span class=\"line\">  def testEvent(): Unit = &#123;</span><br><span class=\"line\">    val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables()</span><br><span class=\"line\">    val result = ...// 测试的查询逻辑</span><br><span class=\"line\">    procTimePrint(result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 自定义Sink</span><br><span class=\"line\">final class RetractingSink extends RichSinkFunction[(Boolean, Row)] &#123;</span><br><span class=\"line\">  var retractedResults: ArrayBuffer[String] = null</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    super.open(parameters)</span><br><span class=\"line\">    retractedResults = mutable.ArrayBuffer.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def invoke(v: (Boolean, Row)) &#123;</span><br><span class=\"line\">    retractedResults.synchronized &#123;</span><br><span class=\"line\">      val value = v._2.toString</span><br><span class=\"line\">      if (v._1) &#123;</span><br><span class=\"line\">        retractedResults += value</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        val idx = retractedResults.indexOf(value)</span><br><span class=\"line\">        if (idx &gt;= 0) &#123;</span><br><span class=\"line\">          retractedResults.remove(idx)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          throw new RuntimeException(&quot;Tried to retract a value that wasn&apos;t added first. &quot; +</span><br><span class=\"line\">                                       &quot;This is probably an incorrectly implemented test. &quot; +</span><br><span class=\"line\">                                       &quot;Try to set the parallelism of the sink to 1.&quot;)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def close(): Unit = &#123;</span><br><span class=\"line\">    super.close()</span><br><span class=\"line\">    retractedResults.sorted.foreach(println(_))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Water mark 生成器</span><br><span class=\"line\">class EventTimeSourceFunction[T](</span><br><span class=\"line\">  dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>SELECT</h2>\n<p>SELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示:\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/EEED7340-FE9D-4791-961C-9996D35E148F.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .select(&apos;c_name, concat_ws(&apos;c_name, &quot; come &quot;, &apos;c_desc))</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_name</th>\n<th>desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Kevin</td>\n<td>Kevin come from JinLin</td>\n</tr>\n<tr>\n<td>Sunny</td>\n<td>Sunny come from JinLin</td>\n</tr>\n<tr>\n<td>Jincheng</td>\n<td>Jincheng come from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下：</p>\n<h4>Table API示例</h4>\n<p>在订单表查询所有的客户id，消除重复客户id, 如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .groupBy(&apos;c_id)</span><br><span class=\"line\">  .select(&apos;c_id)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n</tr>\n<tr>\n<td>c_002</td>\n</tr>\n</tbody>\n</table>\n<h2>WHERE</h2>\n<p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/15657C5E-DD6E-4339-824E-6B17C30886C9.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .where(&quot;c_id = &apos;c_001&apos; || c_id = &apos;c_003&apos;&quot;)</span><br><span class=\"line\">  .select( &apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>&amp;&amp;</code>， <code>||</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的<code>IN</code>和<code>NOT IN</code>在Table API里面用<code>intersect</code> 和 <code>minus</code>描述(flink-1.7.0版本)。</p>\n<h4>Intersect 示例</h4>\n<p><code>Intersect</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询已经下过订单的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 计算客户id，并去重</span><br><span class=\"line\">val distinct_cids = order</span><br><span class=\"line\">  .groupBy(&apos;c_id) // 去重</span><br><span class=\"line\">  .select(&apos;c_id as &apos;o_c_id)</span><br><span class=\"line\"></span><br><span class=\"line\">val result = customer</span><br><span class=\"line\">  .join(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n</tbody>\n</table>\n<h4>Minus 示例</h4>\n<p><code>Minus</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询没有下过订单的客户信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> // 查询下过订单的客户id，并去重</span><br><span class=\"line\"> val distinct_cids = order</span><br><span class=\"line\">   .groupBy(&apos;c_id)</span><br><span class=\"line\">   .select(&apos;c_id as &apos;o_c_id)</span><br><span class=\"line\">   </span><br><span class=\"line\">// 查询没有下过订单的客户信息</span><br><span class=\"line\">val result = customer</span><br><span class=\"line\">  .leftOuterJoin(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class=\"line\">  .where(&apos;o_c_id isNull)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure></p>\n<p>说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。</p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h4>Intersect/Minus与关系代数</h4>\n<p>如上介绍<code>Intersect</code>是关系代数中的Intersection， <code>Minus</code>是关系代数的Difference， 如下图示意：</p>\n<ul>\n<li>Intersect(Intersection)\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png\" alt></li>\n<li>Minus(Difference)\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png\" alt></li>\n</ul>\n<h2>GROUP BY</h2>\n<p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/D56FE35E-0212-444C-8B0F-C650F1300C3C.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>将order_tab信息按<code>c_id</code>分组统计订单数量，简单示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .groupBy(&apos;c_id)</span><br><span class=\"line\">  .select(&apos;c_id, &apos;o_id.count)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>o_count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>2</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p>\n<h4>Table API 示例</h4>\n<p>按时间进行分组，查询每分钟的订单数量，如下：</p>\n<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = order</span><br><span class=\"line\">  .select(&apos;o_id, &apos;c_id, &apos;o_time.substring(1, 16) as &apos;o_time_min)</span><br><span class=\"line\">  .groupBy(&apos;o_time_min)</span><br><span class=\"line\">  .select(&apos;o_time_min, &apos;o_id.count)</span><br></pre></td></tr></table></figure>\n</code></pre>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th>o_time_min</th>\n<th>o_count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2018-11-05 10:01</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2018-11-05 10:03</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p>\n<h2>UNION ALL</h2>\n<p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/3402EE52-F2D5-452E-A0A5-66F0EC14C878.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer.unionAll(customer)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p>\n<h2>UNION</h2>\n<p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/AE6DCCF9-32A4-4E7A-B7C5-334983354240.png\" alt></p>\n<h3>Table API 示例</h3>\n<p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer.union(customer)</span><br></pre></td></tr></table></figure></p>\n<p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p>\n<h2>JOIN</h2>\n<p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>\n<ul>\n<li>JOIN - INNER JOIN</li>\n<li>LEFT JOIN - LEFT OUTER JOIN</li>\n<li>RIGHT JOIN - RIGHT OUTER JOIN</li>\n<li>FULL JOIN - FULL OUTER JOIN\nJOIN与关系代数的Join语义相同，具体如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png\" alt></li>\n</ul>\n<h3>Table API 示例 (JOIN)</h3>\n<p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .join(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n<th>o_id</th>\n<th>o_c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n</tbody>\n</table>\n<h3>Table API 示例 (LEFT JOIN)</h3>\n<p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png\" alt>\n对应的SQL语句如下(LEFT JOIN)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li>\n</ul>\n<p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = customer</span><br><span class=\"line\">  .leftOuterJoin(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure></p>\n<h3>Result</h3>\n<table>\n<thead>\n<tr>\n<th>c_id</th>\n<th>c_name</th>\n<th>c_desc</th>\n<th>o_id</th>\n<th>c_id</th>\n<th>o_time</th>\n<th>o_desc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_002</td>\n<td>c_001</td>\n<td>2018-11-05 10:01:55</td>\n<td>ipad</td>\n</tr>\n<tr>\n<td>c_001</td>\n<td>Kevin</td>\n<td>from JinLin</td>\n<td>o_003</td>\n<td>c_001</td>\n<td>2018-11-05 10:03:44</td>\n<td>flink book</td>\n</tr>\n<tr>\n<td>c_002</td>\n<td>Sunny</td>\n<td>from JinLin</td>\n<td>o_oo1</td>\n<td>c_002</td>\n<td>2018-11-05 10:01:01</td>\n<td>iphone</td>\n</tr>\n<tr>\n<td>c_003</td>\n<td>JinCheng</td>\n<td>from HeBei</td>\n<td>NULL</td>\n<td>NULL</td>\n<td>NULL</td>\n<td>NULL</td>\n</tr>\n</tbody>\n</table>\n<h3>特别说明</h3>\n<p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>\n<h2>Time-Interval JOIN</h2>\n<p>Time-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val result = left</span><br><span class=\"line\">  .join(right)</span><br><span class=\"line\">  // 定义Time Interval</span><br><span class=\"line\">  .where(&apos;a === &apos;d &amp;&amp; &apos;c &gt;= &apos;f - 5.seconds &amp;&amp; &apos;c &lt; &apos;f + 6.seconds)</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure></p>\n<h2>Lateral JOIN</h2>\n<p>Apache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val udtf = new UDTF</span><br><span class=\"line\">val result = source.join(udtf(&apos;c) as (&apos;d, &apos;e))</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>Temporal Table JOIN</h2>\n<p>Temporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">val rates = tEnv.scan(&quot;versonedTable&quot;).createTemporalTableFunction(&apos;rowtime, &apos;r_currency)</span><br><span class=\"line\">val result = left.join(rates(&apos;o_rowtime), &apos;r_currency === &apos;o_currency)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>Window</h2>\n<p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>\n<h3>Over Window</h3>\n<p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。\n按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p>\n<ul>\n<li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li>\n<li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li>\n</ul>\n<h4>Bounded ROWS OVER Window</h4>\n<p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>\n<h5>语义</h5>\n<p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/FC821998-4A3E-4F10-9351-C7B3060E7349.png\" alt>\n上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p>\n<h5>Table API 示例</h5>\n<p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = item</span><br><span class=\"line\">  .window(Over partitionBy &apos;itemType orderBy &apos;rowtime preceding 2.rows following CURRENT_ROW as &apos;w)</span><br><span class=\"line\">  .select(&apos;itemID, &apos;itemType, &apos;onSellTime, &apos;price, &apos;price.max over &apos;w as &apos;maxPrice)</span><br></pre></td></tr></table></figure></p>\n<h4>Result</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n<th style=\"text-align:left\">maxPrice</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:03:00</td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:03:00</td>\n<td style=\"text-align:left\">60</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<h4>Bounded RANGE OVER Window</h4>\n<p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p>\n<h5>语义</h5>\n<p>我们以3秒中数据(INTERVAL '2' SECOND)的窗口为例，如下图：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/79DC098C-1171-4C0E-A735-C4F63B6010FD.png\" alt></p>\n<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p>\n<h5>Tabel API 示例</h5>\n<p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = item</span><br><span class=\"line\">  .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w)</span><br><span class=\"line\">  .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)</span><br></pre></td></tr></table></figure></p>\n<h5>Result（Bounded RANGE OVER Window）</h5>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">itemID</th>\n<th style=\"text-align:left\">itemType</th>\n<th style=\"text-align:left\">onSellTime</th>\n<th style=\"text-align:left\">price</th>\n<th style=\"text-align:left\">maxPrice</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">ITEM001</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:01:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM002</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:02:00</td>\n<td style=\"text-align:left\">50</td>\n<td style=\"text-align:left\">50</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM003</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM004</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\"><em><strong>2017-11-11 10:03:00</strong></em></td>\n<td style=\"text-align:left\">60</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM005</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:05:00</td>\n<td style=\"text-align:left\">40</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM006</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:06:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">40</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM007</td>\n<td style=\"text-align:left\">Electronic</td>\n<td style=\"text-align:left\">2017-11-11 10:07:00</td>\n<td style=\"text-align:left\">70</td>\n<td style=\"text-align:left\">70</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ITEM008</td>\n<td style=\"text-align:left\">Clothes</td>\n<td style=\"text-align:left\">2017-11-11 10:08:00</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">20</td>\n</tr>\n</tbody>\n</table>\n<h4>特别说明</h4>\n<p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p>\n<ul>\n<li>GROUP BY - <code>tab.groupBy('d).select(d, MAX(c))</code></li>\n<li>OVER Window = <code>tab.window(Over.. as 'w).select('a, 'b, 'c, 'd, c.max over 'w)</code>\n如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li>\n</ul>\n<h3>Group Window</h3>\n<p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>\n<ul>\n<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>\n<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>\n<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li>\n</ul>\n<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>\n<h4>Tumble</h4>\n<h5>语义</h5>\n<p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/C2A514D5-61F1-4464-95ED-17EB2A030EC2.png\" alt></p>\n<h5>Table API 示例</h5>\n<p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccess</span><br><span class=\"line\">  .window(Tumble over 2.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h4>Hop</h4>\n<p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p>\n<h5>语义</h5>\n<p>Hop 滑动窗口语义如下所示：\n<img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png\" alt></p>\n<h5>Table API 示例</h5>\n<p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccessCount</span><br><span class=\"line\">  .window(Slide over 10.minute every 5.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w)</span><br><span class=\"line\">  .select(&apos;w.start, &apos;w.end, &apos;accessCount.sum as &apos;accessCount)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>winStart</th>\n<th>winEnd</th>\n<th>accessCount</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2017-11-11 01:55:00.0</td>\n<td>2017-11-11 02:05:00.0</td>\n<td>186</td>\n</tr>\n<tr>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>396</td>\n</tr>\n<tr>\n<td>2017-11-11 02:05:00.0</td>\n<td>2017-11-11 02:15:00.0</td>\n<td>243</td>\n</tr>\n<tr>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:20:00.0</td>\n<td>33</td>\n</tr>\n<tr>\n<td>2017-11-11 04:05:00.0</td>\n<td>2017-11-11 04:15:00.0</td>\n<td>129</td>\n</tr>\n<tr>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:20:00.0</td>\n<td>129</td>\n</tr>\n</tbody>\n</table>\n<h4>Session</h4>\n<p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p>\n<h5>语义</h5>\n<p>Session 会话窗口语义如下所示：</p>\n<p><img src=\"/2019/03/26/Apache Flink 漫谈系列 - Table API 概述/D1197B0B-2DCF-434E-A7BB-A250187079F0.png\" alt></p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val result = pageAccessSession</span><br><span class=\"line\">  .window(Session withGap 3.minute on &apos;rowtime as &apos;w)</span><br><span class=\"line\">  .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure></p>\n<h5>Result</h5>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:13:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:01:00.0</td>\n<td>2017-11-11 02:08:00.0</td>\n<td>4</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:14:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:16:00.0</td>\n<td>2017-11-11 04:19:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h4>嵌套Window</h4>\n<p>在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用<code>'w.rowtime</code>来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下:</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> ... </span><br><span class=\"line\"> val result = pageAccess</span><br><span class=\"line\">   .window(Tumble over 2.minute on &apos;rowtime as &apos;w1)</span><br><span class=\"line\">   .groupBy(&apos;w1)</span><br><span class=\"line\">   .select(&apos;w1.rowtime as &apos;rowtime, &apos;col1.count as &apos;cnt)</span><br><span class=\"line\">   .window(Session withGap 3.minute on &apos;rowtime as &apos;w2)</span><br><span class=\"line\">   .groupBy(&apos;w2)</span><br><span class=\"line\">   .select(&apos;cnt.sum)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h1>Source&amp;Sink</h1>\n<p>上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>userId</th>\n<th>accessTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ShangHai</td>\n<td>U0010</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1001</td>\n<td>2017-11-11 10:01:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U2032</td>\n<td>2017-11-11 10:10:00</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>U1100</td>\n<td>2017-11-11 10:11:00</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>U0011</td>\n<td>2017-11-11 12:10:00</td>\n</tr>\n</tbody>\n</table>\n<h2>Source 定义</h2>\n<p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>\n<h3>Source Function定义</h3>\n<p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class=\"line\">  extends SourceFunction[T] &#123;</span><br><span class=\"line\">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class=\"line\">    dataWithTimestampList.foreach &#123;</span><br><span class=\"line\">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class=\"line\">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def cancel(): Unit = ???</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3>定义 StreamTableSource</h3>\n<p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class=\"line\">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class=\"line\">  val rowType = new RowTypeInfo(</span><br><span class=\"line\">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class=\"line\">    fieldNames)</span><br><span class=\"line\"></span><br><span class=\"line\">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class=\"line\">  val data = Seq(</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class=\"line\">    Right(1510365660000L),</span><br><span class=\"line\">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class=\"line\">    Right(1510366200000L),</span><br><span class=\"line\">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class=\"line\">    Right(1510366260000L),</span><br><span class=\"line\">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class=\"line\">    Right(1510373400000L)</span><br><span class=\"line\">  )</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class=\"line\">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class=\"line\">      &quot;accessTime&quot;,</span><br><span class=\"line\">      new ExistingField(&quot;accessTime&quot;),</span><br><span class=\"line\">      PreserveWatermarks.INSTANCE))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class=\"line\">     execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class=\"line\"></span><br><span class=\"line\">  override def getTableSchema: TableSchema = schema</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2>Sink 定义</h2>\n<p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class=\"line\">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class=\"line\">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class=\"line\">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>构建主程序</h2>\n<p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 设置EventTime</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;mySource&quot;</span><br><span class=\"line\">    // 创建自定义source数据结构</span><br><span class=\"line\">    val tableSource = new MyTableSource</span><br><span class=\"line\"></span><br><span class=\"line\">    val sinkTableName = &quot;csvSink&quot;</span><br><span class=\"line\">    // 创建CSV sink 数据结构</span><br><span class=\"line\">    val tableSink = getCsvTableSink</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\">    // 注册sink</span><br><span class=\"line\">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(sourceTableName)</span><br><span class=\"line\">      .window(Tumble over 2.minute on &apos;accessTime as &apos;w)</span><br><span class=\"line\">      .groupBy(&apos;w, &apos;region)</span><br><span class=\"line\">      .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br><span class=\"line\"></span><br><span class=\"line\">    result.insertInto(sinkTableName)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>执行主程序后我们会在控制台得到Sink的文件路径，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure></p>\n<p>Cat 方式查看计算结果，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class=\"line\">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class=\"line\">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class=\"line\">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure></p>\n<p>表格化如上结果：</p>\n<table>\n<thead>\n<tr>\n<th>region</th>\n<th>winStart</th>\n<th>winEnd</th>\n<th>pv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>BeiJing</td>\n<td>2017-11-11 02:10:00.0</td>\n<td>2017-11-11 02:12:00.0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 02:00:00.0</td>\n<td>2017-11-11 02:02:00.0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>ShangHai</td>\n<td>2017-11-11 04:10:00.0</td>\n<td>2017-11-11 04:12:00.0</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p>\n<h1>小结</h1>\n<p>本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n"},{"title":"Apache Flink 漫谈系列 - Time Interval JOIN","date":"2019-03-22T10:18:18.000Z","_content":"\n# 说什么\nJOIN 算子是数据处理的核心算子，前面我们在《Apache Flink 漫谈系列 - JOIN 算子》介绍了UnBounded的双流JOIN，在《Apache Flink 漫谈系列 - JOIN LATERAL》介绍了单流与UDTF的JOIN操作，在《Apache Flink 漫谈系列 - Temporal Table JOIN》又介绍了单流与版本表的JOIN，本篇将介绍在UnBounded数据流上按时间维度进行数据划分进行JOIN操作 - Time Interval(Time-windowed)JOIN, 后面我们叫做Interval JOIN。\n\n# 实际问题\n前面章节我们介绍了Flink中对各种JOIN的支持，那么想想下面的查询需求之前介绍的JOIN能否满足？需求描述如下:\n\n比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计下单一小时内付款的订单信息。\n\n## 传统数据库解决方式\n  在传统刘数据库中完成上面的需求非常简单，查询sql如下:：\n  ```\n  SELECT \n    o.orderId,\n    o.productName,\n    p.payType,\n    o.orderTime，\n    payTime\n  FROM\n    Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId AND p.payTime >= orderTime AND p.payTime < orderTime + 3600 // 秒\n```\n上面查询可以完美的完成查询需求，那么在Apache Flink里面应该如何完成上面的需求呢？\n\n## Apache Flink解决方式\n### UnBounded 双流 JOIN\n上面查询需求我们很容易想到利用《Apache Flink 漫谈系列(09) - JOIN 算子》介绍了UnBounded的双流JOIN，SQL语句如下：\n\n```\n SELECT \n    o.orderId,\n    o.productName,\n    p.payType,\n    o.orderTime，\n    payTime \n  FROM\n    Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId AND p.payTime >= orderTime AND p.payTime as timestamp < TIMESTAMPADD(SECOND, 3600, orderTime)\n```\nUnBounded双流JOIN可以解决上面问题，这个示例和本篇要介绍的Interval JOIN有什么关系呢？\n\n### 性能问题\n虽然我们利用UnBounded的JOIN能解决上面的问题，但是仔细分析用户需求，会发现这个需求场景订单信息和付款信息并不需要长期存储，比如`2018-12-27 14:22:22`的订单只需要保持1小时，因为超过1个小时的订单如果没有被付款就是无效订单了。同样付款信息也不需要长期保持，`2018-12-27 14:22:22`的订单付款信息如果是`2018-12-27 15:22:22`以后到达的那么我们也没有必要保存到State中。 而对于UnBounded的双流JOIN我们会一直将数据保存到State中，如下示意图：\n\n![](b26733d7d193bbea8d594f098bcd98cbea926a88.png)\n\n\n这样的底层实现，对于当前需求有不必要的性能损失。所以我们有必要开发一种新的可以清除State的JOIN方式(Interval JOIN)来高性能的完成上面的查询需求。\n\n### 功能扩展\n目前的UnBounded的双流JOIN是后面是没有办法再进行Event-Time的Window Aggregate的。也就是下面的语句在Apache Flink上面是无法支持的：\n```\n SELECT COUNT(*) FROM (\n  SELECT \n   ...,\n   payTime\n   FROM Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId \n  ) GROUP BY TUMBLE(payTime, INTERVAL '15' MINUTE)\n```\n因为在UnBounded的双流JOIN中无法保证payTime的值一定大于WaterMark（WaterMark相关可以查阅<<Apache Flink 漫谈系列(03) - Watermark>>). Apache Flink的Interval JOIN之后可以进行Event-Time的Window Aggregate。\n\n## Interval JOIN \n为了完成上面需求，并且解决性能和功能扩展的问题，Apache Flink在1.4开始开发了Time-windowed Join，也就是本文所说的Interval JOIN。接下来我们详细介绍Interval JOIN的语法，语义和实现原理。\n\n# 什么是Interval JOIN\nInterval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 \n\n## Interval JOIN 语法\n```\nSELECT ... FROM t1 JOIN t2  ON t1.key = t2.key AND TIMEBOUND_EXPRESSION\n```\nTIMEBOUND_EXPRESSION 有两种写法，如下：\n\n* L.time between LowerBound(R.time) and UpperBound(R.time)\n* R.time between LowerBound(L.time) and UpperBound(L.time)\n* 带有时间属性(L.time/R.time)的比较表达式。\n\n## Interval JOIN 语义\n\nInterval JOIN 的语义就是每条数据对应一个 Interval 的数据区间，比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计在下单一小时内付款的订单信息。SQL查询如下:\n```\nSELECT \n  o.orderId,\n  o.productName,\n  p.payType,\n  o.orderTime，\n  cast(payTime as timestamp) as payTime\nFROM\n  Orders AS o JOIN Payment AS p ON \n  o.orderId = p.orderId AND \n  p.payTime BETWEEN orderTime AND \n  orderTime + INTERVAL '1' HOUR\n```\n* Orders订单数据\n\n| orderId | productName |orderTime  |\n| --- | --- | --- |\n|001|iphone|2018-12-26 04:53:22.0\n|002|mac|2018-12-26 04:53:23.0\n|003|book|**2018-12-26 04:53:24.0**\n|004|cup|2018-12-26 04:53:38.0\n\n* Payment付款数据\n\n| orderId | payType |payTime  |\n| --- | --- | --- |\n|001|alipay|2018-12-26 05:51:41.0\n|002|card|2018-12-26 05:53:22.0\n|003|card|**2018-12-26 05:53:30.0**\n|004|alipay|2018-12-26 05:53:31.0\n\n符合语义的预期结果是 订单id为003的信息不出现在结果表中，因为下单时间`2018-12-26 04:53:24.0`, 付款时间是 `2018-12-26 05:53:30.0`超过了1小时付款。\n那么预期的结果信息如下：\n\n| orderId | productName | payType | orderTime |payTime  |\n| --- | --- | --- | --- | --- |\n|001|iphone|alipay|2018-12-26 04:53:22.0|2018-12-26 05:51:41.0\n|002|mac|card|2018-12-26 04:53:23.0|2018-12-26 05:53:22.0\n|004|cup|alipay|2018-12-26 04:53:38.0|2018-12-26 05:53:31.0\n\n这样Id为003的订单是无效订单，可以更新库存继续售卖。\n\n接下来我们以图示的方式直观说明Interval JOIN的语义，**我们对上面的示例需求稍微变化一下：** 订单可以预付款(不管是否合理，我们只是为了说明语义）也就是订单 **前后** 1小时的付款都是有效的。SQL语句如下：\n```\nSELECT\n  ...\nFROM\n  Orders AS o JOIN Payment AS p ON\n  o.orderId = p.orderId AND\n  p.payTime BETWEEN orderTime - INTERVAL '1' HOUR AND\n  orderTime + INTERVAL '1' HOUR\n```\n\n这样的查询语义示意图如下：\n![](452ada49a233d68c6e614e05e61ba158aa180f83.png)\n\n\n上图有几个关键点，如下：\n* 数据JOIN的区间 - 比如Order时间为3的订单会在付款时间为[2, 4]区间进行JOIN。\n* WaterMark - 比如图示Order最后一条数据时间是3，Payment最后一条数据时间是5，那么WaterMark是根据实际最小值减去UpperBound生成，即：Min(3,5)-1 = 2\n* 过期数据 - 出于性能和存储的考虑，要将过期数据清除，如图当WaterMark是2的时候时间为2以前的数据过期了，可以被清除。\n\n## Interval JOIN 实现原理\n\n由于Interval JOIN和双流JOIN类似都要存储左右两边的数据，所以底层实现中仍然是利用State进行数据的存储。流计算的特点是数据不停的流入，我们可以不停的进行增量计算，也就是我们每条数据流入都可以进行JOIN计算。我们还是以具体示例和图示来说明内部计算逻辑，如下图：\n\n![](e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png)\n\n\n简单解释一下每条记录的处理逻辑如下：\n\n![](eef2a485ae6f7de29d5965457a1ae0da89055a58.png)\n\n\n实际的内部逻辑会比描述的复杂的多，大家可以根据如上简述理解内部原理即可。\n\n# 示例代码\n我们还是以订单和付款示例，将完整代码分享给大家，具体如下(代码基于flink-1.7.0)：\n```\nimport java.sql.Timestamp\n\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.table.api.scala._\nimport org.apache.flink.types.Row\n\nimport scala.collection.mutable\n\nobject SimpleTimeIntervalJoin {\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setParallelism(1)\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 构造订单数据\n    val ordersData = new mutable.MutableList[(String, String, Timestamp)]\n    ordersData.+=((\"001\", \"iphone\", new Timestamp(1545800002000L)))\n    ordersData.+=((\"002\", \"mac\", new Timestamp(1545800003000L)))\n    ordersData.+=((\"003\", \"book\", new Timestamp(1545800004000L)))\n    ordersData.+=((\"004\", \"cup\", new Timestamp(1545800018000L)))\n\n    // 构造付款表\n    val paymentData = new mutable.MutableList[(String, String, Timestamp)]\n    paymentData.+=((\"001\", \"alipay\", new Timestamp(1545803501000L)))\n    paymentData.+=((\"002\", \"card\", new Timestamp(1545803602000L)))\n    paymentData.+=((\"003\", \"card\", new Timestamp(1545803610000L)))\n    paymentData.+=((\"004\", \"alipay\", new Timestamp(1545803611000L)))\n    val orders = env\n      .fromCollection(ordersData)\n      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())\n      .toTable(tEnv, 'orderId, 'productName, 'orderTime.rowtime)\n    val ratesHistory = env\n      .fromCollection(paymentData)\n      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())\n      .toTable(tEnv, 'orderId, 'payType, 'payTime.rowtime)\n\n    tEnv.registerTable(\"Orders\", orders)\n    tEnv.registerTable(\"Payment\", ratesHistory)\n\n    var sqlQuery =\n      \"\"\"\n        |SELECT\n        |  o.orderId,\n        |  o.productName,\n        |  p.payType,\n        |  o.orderTime,\n        |  cast(payTime as timestamp) as payTime\n        |FROM\n        |  Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND\n        | p.payTime BETWEEN orderTime AND orderTime + INTERVAL '1' HOUR\n        |\"\"\".stripMargin\n    tEnv.registerTable(\"TemporalJoinResult\", tEnv.sqlQuery(sqlQuery))\n\n    val result = tEnv.scan(\"TemporalJoinResult\").toAppendStream[Row]\n    result.print()\n    env.execute()\n  }\n\n}\n\nclass TimestampExtractor[T1, T2]\n  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) {\n  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = {\n    element._3.getTime\n  }\n}\n\n```\n运行结果如下：\n\n![](AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png)\n\n# 小节\n本篇由实际业务需求场景切入，介绍了相同业务需求既可以利用Unbounded 双流JOIN实现，也可以利用Time Interval JOIN来实现，Time Interval JOIN 性能优于UnBounded的双流JOIN，并且Interval JOIN之后可以进行Window Aggregate算子计算。然后介绍了Interval JOIN的语法，语义和实现原理，最后将订单和付款的完整示例代码分享给大家。期望本篇能够让大家对Apache Flink Time Interval JOIN有一个具体的了解！\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","source":"_posts/Apache Flink 漫谈系列 - Time Interval JOIN.md","raw":"---\ntitle: Apache Flink 漫谈系列 - Time Interval JOIN\ndate: 2019-03-22 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n\n# 说什么\nJOIN 算子是数据处理的核心算子，前面我们在《Apache Flink 漫谈系列 - JOIN 算子》介绍了UnBounded的双流JOIN，在《Apache Flink 漫谈系列 - JOIN LATERAL》介绍了单流与UDTF的JOIN操作，在《Apache Flink 漫谈系列 - Temporal Table JOIN》又介绍了单流与版本表的JOIN，本篇将介绍在UnBounded数据流上按时间维度进行数据划分进行JOIN操作 - Time Interval(Time-windowed)JOIN, 后面我们叫做Interval JOIN。\n\n# 实际问题\n前面章节我们介绍了Flink中对各种JOIN的支持，那么想想下面的查询需求之前介绍的JOIN能否满足？需求描述如下:\n\n比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计下单一小时内付款的订单信息。\n\n## 传统数据库解决方式\n  在传统刘数据库中完成上面的需求非常简单，查询sql如下:：\n  ```\n  SELECT \n    o.orderId,\n    o.productName,\n    p.payType,\n    o.orderTime，\n    payTime\n  FROM\n    Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId AND p.payTime >= orderTime AND p.payTime < orderTime + 3600 // 秒\n```\n上面查询可以完美的完成查询需求，那么在Apache Flink里面应该如何完成上面的需求呢？\n\n## Apache Flink解决方式\n### UnBounded 双流 JOIN\n上面查询需求我们很容易想到利用《Apache Flink 漫谈系列(09) - JOIN 算子》介绍了UnBounded的双流JOIN，SQL语句如下：\n\n```\n SELECT \n    o.orderId,\n    o.productName,\n    p.payType,\n    o.orderTime，\n    payTime \n  FROM\n    Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId AND p.payTime >= orderTime AND p.payTime as timestamp < TIMESTAMPADD(SECOND, 3600, orderTime)\n```\nUnBounded双流JOIN可以解决上面问题，这个示例和本篇要介绍的Interval JOIN有什么关系呢？\n\n### 性能问题\n虽然我们利用UnBounded的JOIN能解决上面的问题，但是仔细分析用户需求，会发现这个需求场景订单信息和付款信息并不需要长期存储，比如`2018-12-27 14:22:22`的订单只需要保持1小时，因为超过1个小时的订单如果没有被付款就是无效订单了。同样付款信息也不需要长期保持，`2018-12-27 14:22:22`的订单付款信息如果是`2018-12-27 15:22:22`以后到达的那么我们也没有必要保存到State中。 而对于UnBounded的双流JOIN我们会一直将数据保存到State中，如下示意图：\n\n![](b26733d7d193bbea8d594f098bcd98cbea926a88.png)\n\n\n这样的底层实现，对于当前需求有不必要的性能损失。所以我们有必要开发一种新的可以清除State的JOIN方式(Interval JOIN)来高性能的完成上面的查询需求。\n\n### 功能扩展\n目前的UnBounded的双流JOIN是后面是没有办法再进行Event-Time的Window Aggregate的。也就是下面的语句在Apache Flink上面是无法支持的：\n```\n SELECT COUNT(*) FROM (\n  SELECT \n   ...,\n   payTime\n   FROM Orders AS o JOIN Payment AS p ON \n    o.orderId = p.orderId \n  ) GROUP BY TUMBLE(payTime, INTERVAL '15' MINUTE)\n```\n因为在UnBounded的双流JOIN中无法保证payTime的值一定大于WaterMark（WaterMark相关可以查阅<<Apache Flink 漫谈系列(03) - Watermark>>). Apache Flink的Interval JOIN之后可以进行Event-Time的Window Aggregate。\n\n## Interval JOIN \n为了完成上面需求，并且解决性能和功能扩展的问题，Apache Flink在1.4开始开发了Time-windowed Join，也就是本文所说的Interval JOIN。接下来我们详细介绍Interval JOIN的语法，语义和实现原理。\n\n# 什么是Interval JOIN\nInterval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 \n\n## Interval JOIN 语法\n```\nSELECT ... FROM t1 JOIN t2  ON t1.key = t2.key AND TIMEBOUND_EXPRESSION\n```\nTIMEBOUND_EXPRESSION 有两种写法，如下：\n\n* L.time between LowerBound(R.time) and UpperBound(R.time)\n* R.time between LowerBound(L.time) and UpperBound(L.time)\n* 带有时间属性(L.time/R.time)的比较表达式。\n\n## Interval JOIN 语义\n\nInterval JOIN 的语义就是每条数据对应一个 Interval 的数据区间，比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计在下单一小时内付款的订单信息。SQL查询如下:\n```\nSELECT \n  o.orderId,\n  o.productName,\n  p.payType,\n  o.orderTime，\n  cast(payTime as timestamp) as payTime\nFROM\n  Orders AS o JOIN Payment AS p ON \n  o.orderId = p.orderId AND \n  p.payTime BETWEEN orderTime AND \n  orderTime + INTERVAL '1' HOUR\n```\n* Orders订单数据\n\n| orderId | productName |orderTime  |\n| --- | --- | --- |\n|001|iphone|2018-12-26 04:53:22.0\n|002|mac|2018-12-26 04:53:23.0\n|003|book|**2018-12-26 04:53:24.0**\n|004|cup|2018-12-26 04:53:38.0\n\n* Payment付款数据\n\n| orderId | payType |payTime  |\n| --- | --- | --- |\n|001|alipay|2018-12-26 05:51:41.0\n|002|card|2018-12-26 05:53:22.0\n|003|card|**2018-12-26 05:53:30.0**\n|004|alipay|2018-12-26 05:53:31.0\n\n符合语义的预期结果是 订单id为003的信息不出现在结果表中，因为下单时间`2018-12-26 04:53:24.0`, 付款时间是 `2018-12-26 05:53:30.0`超过了1小时付款。\n那么预期的结果信息如下：\n\n| orderId | productName | payType | orderTime |payTime  |\n| --- | --- | --- | --- | --- |\n|001|iphone|alipay|2018-12-26 04:53:22.0|2018-12-26 05:51:41.0\n|002|mac|card|2018-12-26 04:53:23.0|2018-12-26 05:53:22.0\n|004|cup|alipay|2018-12-26 04:53:38.0|2018-12-26 05:53:31.0\n\n这样Id为003的订单是无效订单，可以更新库存继续售卖。\n\n接下来我们以图示的方式直观说明Interval JOIN的语义，**我们对上面的示例需求稍微变化一下：** 订单可以预付款(不管是否合理，我们只是为了说明语义）也就是订单 **前后** 1小时的付款都是有效的。SQL语句如下：\n```\nSELECT\n  ...\nFROM\n  Orders AS o JOIN Payment AS p ON\n  o.orderId = p.orderId AND\n  p.payTime BETWEEN orderTime - INTERVAL '1' HOUR AND\n  orderTime + INTERVAL '1' HOUR\n```\n\n这样的查询语义示意图如下：\n![](452ada49a233d68c6e614e05e61ba158aa180f83.png)\n\n\n上图有几个关键点，如下：\n* 数据JOIN的区间 - 比如Order时间为3的订单会在付款时间为[2, 4]区间进行JOIN。\n* WaterMark - 比如图示Order最后一条数据时间是3，Payment最后一条数据时间是5，那么WaterMark是根据实际最小值减去UpperBound生成，即：Min(3,5)-1 = 2\n* 过期数据 - 出于性能和存储的考虑，要将过期数据清除，如图当WaterMark是2的时候时间为2以前的数据过期了，可以被清除。\n\n## Interval JOIN 实现原理\n\n由于Interval JOIN和双流JOIN类似都要存储左右两边的数据，所以底层实现中仍然是利用State进行数据的存储。流计算的特点是数据不停的流入，我们可以不停的进行增量计算，也就是我们每条数据流入都可以进行JOIN计算。我们还是以具体示例和图示来说明内部计算逻辑，如下图：\n\n![](e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png)\n\n\n简单解释一下每条记录的处理逻辑如下：\n\n![](eef2a485ae6f7de29d5965457a1ae0da89055a58.png)\n\n\n实际的内部逻辑会比描述的复杂的多，大家可以根据如上简述理解内部原理即可。\n\n# 示例代码\n我们还是以订单和付款示例，将完整代码分享给大家，具体如下(代码基于flink-1.7.0)：\n```\nimport java.sql.Timestamp\n\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.table.api.TableEnvironment\nimport org.apache.flink.table.api.scala._\nimport org.apache.flink.types.Row\n\nimport scala.collection.mutable\n\nobject SimpleTimeIntervalJoin {\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setParallelism(1)\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 构造订单数据\n    val ordersData = new mutable.MutableList[(String, String, Timestamp)]\n    ordersData.+=((\"001\", \"iphone\", new Timestamp(1545800002000L)))\n    ordersData.+=((\"002\", \"mac\", new Timestamp(1545800003000L)))\n    ordersData.+=((\"003\", \"book\", new Timestamp(1545800004000L)))\n    ordersData.+=((\"004\", \"cup\", new Timestamp(1545800018000L)))\n\n    // 构造付款表\n    val paymentData = new mutable.MutableList[(String, String, Timestamp)]\n    paymentData.+=((\"001\", \"alipay\", new Timestamp(1545803501000L)))\n    paymentData.+=((\"002\", \"card\", new Timestamp(1545803602000L)))\n    paymentData.+=((\"003\", \"card\", new Timestamp(1545803610000L)))\n    paymentData.+=((\"004\", \"alipay\", new Timestamp(1545803611000L)))\n    val orders = env\n      .fromCollection(ordersData)\n      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())\n      .toTable(tEnv, 'orderId, 'productName, 'orderTime.rowtime)\n    val ratesHistory = env\n      .fromCollection(paymentData)\n      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())\n      .toTable(tEnv, 'orderId, 'payType, 'payTime.rowtime)\n\n    tEnv.registerTable(\"Orders\", orders)\n    tEnv.registerTable(\"Payment\", ratesHistory)\n\n    var sqlQuery =\n      \"\"\"\n        |SELECT\n        |  o.orderId,\n        |  o.productName,\n        |  p.payType,\n        |  o.orderTime,\n        |  cast(payTime as timestamp) as payTime\n        |FROM\n        |  Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND\n        | p.payTime BETWEEN orderTime AND orderTime + INTERVAL '1' HOUR\n        |\"\"\".stripMargin\n    tEnv.registerTable(\"TemporalJoinResult\", tEnv.sqlQuery(sqlQuery))\n\n    val result = tEnv.scan(\"TemporalJoinResult\").toAppendStream[Row]\n    result.print()\n    env.execute()\n  }\n\n}\n\nclass TimestampExtractor[T1, T2]\n  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) {\n  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = {\n    element._3.getTime\n  }\n}\n\n```\n运行结果如下：\n\n![](AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png)\n\n# 小节\n本篇由实际业务需求场景切入，介绍了相同业务需求既可以利用Unbounded 双流JOIN实现，也可以利用Time Interval JOIN来实现，Time Interval JOIN 性能优于UnBounded的双流JOIN，并且Interval JOIN之后可以进行Window Aggregate算子计算。然后介绍了Interval JOIN的语法，语义和实现原理，最后将订单和付款的完整示例代码分享给大家。期望本篇能够让大家对Apache Flink Time Interval JOIN有一个具体的了解！\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","slug":"Apache Flink 漫谈系列 - Time Interval JOIN","published":1,"updated":"2019-07-13T13:10:05.847Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fg000fs94hsga2rtfu","content":"<h1>说什么</h1>\n<p>JOIN 算子是数据处理的核心算子，前面我们在《Apache Flink 漫谈系列 - JOIN 算子》介绍了UnBounded的双流JOIN，在《Apache Flink 漫谈系列 - JOIN LATERAL》介绍了单流与UDTF的JOIN操作，在《Apache Flink 漫谈系列 - Temporal Table JOIN》又介绍了单流与版本表的JOIN，本篇将介绍在UnBounded数据流上按时间维度进行数据划分进行JOIN操作 - Time Interval(Time-windowed)JOIN, 后面我们叫做Interval JOIN。</p>\n<h1>实际问题</h1>\n<p>前面章节我们介绍了Flink中对各种JOIN的支持，那么想想下面的查询需求之前介绍的JOIN能否满足？需求描述如下:</p>\n<p>比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计下单一小时内付款的订单信息。</p>\n<h2>传统数据库解决方式</h2>\n<p>在传统刘数据库中完成上面的需求非常简单，查询sql如下:：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  o.orderId,</span><br><span class=\"line\">  o.productName,</span><br><span class=\"line\">  p.payType,</span><br><span class=\"line\">  o.orderTime，</span><br><span class=\"line\">  payTime</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">  o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime &lt; orderTime + 3600 // 秒</span><br></pre></td></tr></table></figure></p>\n<p>上面查询可以完美的完成查询需求，那么在Apache Flink里面应该如何完成上面的需求呢？</p>\n<h2>Apache Flink解决方式</h2>\n<h3>UnBounded 双流 JOIN</h3>\n<p>上面查询需求我们很容易想到利用《Apache Flink 漫谈系列(09) - JOIN 算子》介绍了UnBounded的双流JOIN，SQL语句如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">   o.orderId,</span><br><span class=\"line\">   o.productName,</span><br><span class=\"line\">   p.payType,</span><br><span class=\"line\">   o.orderTime，</span><br><span class=\"line\">   payTime </span><br><span class=\"line\"> FROM</span><br><span class=\"line\">   Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">   o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime as timestamp &lt; TIMESTAMPADD(SECOND, 3600, orderTime)</span><br></pre></td></tr></table></figure></p>\n<p>UnBounded双流JOIN可以解决上面问题，这个示例和本篇要介绍的Interval JOIN有什么关系呢？</p>\n<h3>性能问题</h3>\n<p>虽然我们利用UnBounded的JOIN能解决上面的问题，但是仔细分析用户需求，会发现这个需求场景订单信息和付款信息并不需要长期存储，比如<code>2018-12-27 14:22:22</code>的订单只需要保持1小时，因为超过1个小时的订单如果没有被付款就是无效订单了。同样付款信息也不需要长期保持，<code>2018-12-27 14:22:22</code>的订单付款信息如果是<code>2018-12-27 15:22:22</code>以后到达的那么我们也没有必要保存到State中。 而对于UnBounded的双流JOIN我们会一直将数据保存到State中，如下示意图：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/b26733d7d193bbea8d594f098bcd98cbea926a88.png\" alt></p>\n<p>这样的底层实现，对于当前需求有不必要的性能损失。所以我们有必要开发一种新的可以清除State的JOIN方式(Interval JOIN)来高性能的完成上面的查询需求。</p>\n<h3>功能扩展</h3>\n<p>目前的UnBounded的双流JOIN是后面是没有办法再进行Event-Time的Window Aggregate的。也就是下面的语句在Apache Flink上面是无法支持的：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT COUNT(*) FROM (</span><br><span class=\"line\"> SELECT </span><br><span class=\"line\">  ...,</span><br><span class=\"line\">  payTime</span><br><span class=\"line\">  FROM Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">   o.orderId = p.orderId </span><br><span class=\"line\"> ) GROUP BY TUMBLE(payTime, INTERVAL &apos;15&apos; MINUTE)</span><br></pre></td></tr></table></figure></p>\n<p>因为在UnBounded的双流JOIN中无法保证payTime的值一定大于WaterMark（WaterMark相关可以查阅&lt;&lt;Apache Flink 漫谈系列(03) - Watermark&gt;&gt;). Apache Flink的Interval JOIN之后可以进行Event-Time的Window Aggregate。</p>\n<h2>Interval JOIN</h2>\n<p>为了完成上面需求，并且解决性能和功能扩展的问题，Apache Flink在1.4开始开发了Time-windowed Join，也就是本文所说的Interval JOIN。接下来我们详细介绍Interval JOIN的语法，语义和实现原理。</p>\n<h1>什么是Interval JOIN</h1>\n<p>Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。</p>\n<h2>Interval JOIN 语法</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ... FROM t1 JOIN t2  ON t1.key = t2.key AND TIMEBOUND_EXPRESSION</span><br></pre></td></tr></table></figure></p>\n<p>TIMEBOUND_EXPRESSION 有两种写法，如下：</p>\n<ul>\n<li>L.time between LowerBound(R.time) and UpperBound(R.time)</li>\n<li>R.time between LowerBound(L.time) and UpperBound(L.time)</li>\n<li>带有时间属性(L.time/R.time)的比较表达式。</li>\n</ul>\n<h2>Interval JOIN 语义</h2>\n<p>Interval JOIN 的语义就是每条数据对应一个 Interval 的数据区间，比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计在下单一小时内付款的订单信息。SQL查询如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  o.orderId,</span><br><span class=\"line\">  o.productName,</span><br><span class=\"line\">  p.payType,</span><br><span class=\"line\">  o.orderTime，</span><br><span class=\"line\">  cast(payTime as timestamp) as payTime</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">  o.orderId = p.orderId AND </span><br><span class=\"line\">  p.payTime BETWEEN orderTime AND </span><br><span class=\"line\">  orderTime + INTERVAL &apos;1&apos; HOUR</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>Orders订单数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>productName</th>\n<th>orderTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>iphone</td>\n<td>2018-12-26 04:53:22.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>mac</td>\n<td>2018-12-26 04:53:23.0</td>\n</tr>\n<tr>\n<td>003</td>\n<td>book</td>\n<td><strong>2018-12-26 04:53:24.0</strong></td>\n</tr>\n<tr>\n<td>004</td>\n<td>cup</td>\n<td>2018-12-26 04:53:38.0</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Payment付款数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>payType</th>\n<th>payTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>alipay</td>\n<td>2018-12-26 05:51:41.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>card</td>\n<td>2018-12-26 05:53:22.0</td>\n</tr>\n<tr>\n<td>003</td>\n<td>card</td>\n<td><strong>2018-12-26 05:53:30.0</strong></td>\n</tr>\n<tr>\n<td>004</td>\n<td>alipay</td>\n<td>2018-12-26 05:53:31.0</td>\n</tr>\n</tbody>\n</table>\n<p>符合语义的预期结果是 订单id为003的信息不出现在结果表中，因为下单时间<code>2018-12-26 04:53:24.0</code>, 付款时间是 <code>2018-12-26 05:53:30.0</code>超过了1小时付款。\n那么预期的结果信息如下：</p>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>productName</th>\n<th>payType</th>\n<th>orderTime</th>\n<th>payTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>iphone</td>\n<td>alipay</td>\n<td>2018-12-26 04:53:22.0</td>\n<td>2018-12-26 05:51:41.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>mac</td>\n<td>card</td>\n<td>2018-12-26 04:53:23.0</td>\n<td>2018-12-26 05:53:22.0</td>\n</tr>\n<tr>\n<td>004</td>\n<td>cup</td>\n<td>alipay</td>\n<td>2018-12-26 04:53:38.0</td>\n<td>2018-12-26 05:53:31.0</td>\n</tr>\n</tbody>\n</table>\n<p>这样Id为003的订单是无效订单，可以更新库存继续售卖。</p>\n<p>接下来我们以图示的方式直观说明Interval JOIN的语义，<strong>我们对上面的示例需求稍微变化一下：</strong> 订单可以预付款(不管是否合理，我们只是为了说明语义）也就是订单 <strong>前后</strong> 1小时的付款都是有效的。SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT</span><br><span class=\"line\">  ...</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON</span><br><span class=\"line\">  o.orderId = p.orderId AND</span><br><span class=\"line\">  p.payTime BETWEEN orderTime - INTERVAL &apos;1&apos; HOUR AND</span><br><span class=\"line\">  orderTime + INTERVAL &apos;1&apos; HOUR</span><br></pre></td></tr></table></figure></p>\n<p>这样的查询语义示意图如下：\n<img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/452ada49a233d68c6e614e05e61ba158aa180f83.png\" alt></p>\n<p>上图有几个关键点，如下：</p>\n<ul>\n<li>数据JOIN的区间 - 比如Order时间为3的订单会在付款时间为[2, 4]区间进行JOIN。</li>\n<li>WaterMark - 比如图示Order最后一条数据时间是3，Payment最后一条数据时间是5，那么WaterMark是根据实际最小值减去UpperBound生成，即：Min(3,5)-1 = 2</li>\n<li>过期数据 - 出于性能和存储的考虑，要将过期数据清除，如图当WaterMark是2的时候时间为2以前的数据过期了，可以被清除。</li>\n</ul>\n<h2>Interval JOIN 实现原理</h2>\n<p>由于Interval JOIN和双流JOIN类似都要存储左右两边的数据，所以底层实现中仍然是利用State进行数据的存储。流计算的特点是数据不停的流入，我们可以不停的进行增量计算，也就是我们每条数据流入都可以进行JOIN计算。我们还是以具体示例和图示来说明内部计算逻辑，如下图：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png\" alt></p>\n<p>简单解释一下每条记录的处理逻辑如下：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/eef2a485ae6f7de29d5965457a1ae0da89055a58.png\" alt></p>\n<p>实际的内部逻辑会比描述的复杂的多，大家可以根据如上简述理解内部原理即可。</p>\n<h1>示例代码</h1>\n<p>我们还是以订单和付款示例，将完整代码分享给大家，具体如下(代码基于flink-1.7.0)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.sql.Timestamp</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class=\"line\">import org.apache.flink.table.api.TableEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\"></span><br><span class=\"line\">object SimpleTimeIntervalJoin &#123;</span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 构造订单数据</span><br><span class=\"line\">    val ordersData = new mutable.MutableList[(String, String, Timestamp)]</span><br><span class=\"line\">    ordersData.+=((&quot;001&quot;, &quot;iphone&quot;, new Timestamp(1545800002000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;002&quot;, &quot;mac&quot;, new Timestamp(1545800003000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;003&quot;, &quot;book&quot;, new Timestamp(1545800004000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;004&quot;, &quot;cup&quot;, new Timestamp(1545800018000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">    // 构造付款表</span><br><span class=\"line\">    val paymentData = new mutable.MutableList[(String, String, Timestamp)]</span><br><span class=\"line\">    paymentData.+=((&quot;001&quot;, &quot;alipay&quot;, new Timestamp(1545803501000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;002&quot;, &quot;card&quot;, new Timestamp(1545803602000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;003&quot;, &quot;card&quot;, new Timestamp(1545803610000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;004&quot;, &quot;alipay&quot;, new Timestamp(1545803611000L)))</span><br><span class=\"line\">    val orders = env</span><br><span class=\"line\">      .fromCollection(ordersData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;orderId, &apos;productName, &apos;orderTime.rowtime)</span><br><span class=\"line\">    val ratesHistory = env</span><br><span class=\"line\">      .fromCollection(paymentData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;orderId, &apos;payType, &apos;payTime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTable(&quot;Orders&quot;, orders)</span><br><span class=\"line\">    tEnv.registerTable(&quot;Payment&quot;, ratesHistory)</span><br><span class=\"line\"></span><br><span class=\"line\">    var sqlQuery =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT</span><br><span class=\"line\">        |  o.orderId,</span><br><span class=\"line\">        |  o.productName,</span><br><span class=\"line\">        |  p.payType,</span><br><span class=\"line\">        |  o.orderTime,</span><br><span class=\"line\">        |  cast(payTime as timestamp) as payTime</span><br><span class=\"line\">        |FROM</span><br><span class=\"line\">        |  Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND</span><br><span class=\"line\">        | p.payTime BETWEEN orderTime AND orderTime + INTERVAL &apos;1&apos; HOUR</span><br><span class=\"line\">        |&quot;&quot;&quot;.stripMargin</span><br><span class=\"line\">    tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.sqlQuery(sqlQuery))</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row]</span><br><span class=\"line\">    result.print()</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class TimestampExtractor[T1, T2]</span><br><span class=\"line\">  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123;</span><br><span class=\"line\">  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123;</span><br><span class=\"line\">    element._3.getTime</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png\" alt></p>\n<h1>小节</h1>\n<p>本篇由实际业务需求场景切入，介绍了相同业务需求既可以利用Unbounded 双流JOIN实现，也可以利用Time Interval JOIN来实现，Time Interval JOIN 性能优于UnBounded的双流JOIN，并且Interval JOIN之后可以进行Window Aggregate算子计算。然后介绍了Interval JOIN的语法，语义和实现原理，最后将订单和付款的完整示例代码分享给大家。期望本篇能够让大家对Apache Flink Time Interval JOIN有一个具体的了解！</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>说什么</h1>\n<p>JOIN 算子是数据处理的核心算子，前面我们在《Apache Flink 漫谈系列 - JOIN 算子》介绍了UnBounded的双流JOIN，在《Apache Flink 漫谈系列 - JOIN LATERAL》介绍了单流与UDTF的JOIN操作，在《Apache Flink 漫谈系列 - Temporal Table JOIN》又介绍了单流与版本表的JOIN，本篇将介绍在UnBounded数据流上按时间维度进行数据划分进行JOIN操作 - Time Interval(Time-windowed)JOIN, 后面我们叫做Interval JOIN。</p>\n<h1>实际问题</h1>\n<p>前面章节我们介绍了Flink中对各种JOIN的支持，那么想想下面的查询需求之前介绍的JOIN能否满足？需求描述如下:</p>\n<p>比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计下单一小时内付款的订单信息。</p>\n<h2>传统数据库解决方式</h2>\n<p>在传统刘数据库中完成上面的需求非常简单，查询sql如下:：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  o.orderId,</span><br><span class=\"line\">  o.productName,</span><br><span class=\"line\">  p.payType,</span><br><span class=\"line\">  o.orderTime，</span><br><span class=\"line\">  payTime</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">  o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime &lt; orderTime + 3600 // 秒</span><br></pre></td></tr></table></figure></p>\n<p>上面查询可以完美的完成查询需求，那么在Apache Flink里面应该如何完成上面的需求呢？</p>\n<h2>Apache Flink解决方式</h2>\n<h3>UnBounded 双流 JOIN</h3>\n<p>上面查询需求我们很容易想到利用《Apache Flink 漫谈系列(09) - JOIN 算子》介绍了UnBounded的双流JOIN，SQL语句如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">   o.orderId,</span><br><span class=\"line\">   o.productName,</span><br><span class=\"line\">   p.payType,</span><br><span class=\"line\">   o.orderTime，</span><br><span class=\"line\">   payTime </span><br><span class=\"line\"> FROM</span><br><span class=\"line\">   Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">   o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime as timestamp &lt; TIMESTAMPADD(SECOND, 3600, orderTime)</span><br></pre></td></tr></table></figure></p>\n<p>UnBounded双流JOIN可以解决上面问题，这个示例和本篇要介绍的Interval JOIN有什么关系呢？</p>\n<h3>性能问题</h3>\n<p>虽然我们利用UnBounded的JOIN能解决上面的问题，但是仔细分析用户需求，会发现这个需求场景订单信息和付款信息并不需要长期存储，比如<code>2018-12-27 14:22:22</code>的订单只需要保持1小时，因为超过1个小时的订单如果没有被付款就是无效订单了。同样付款信息也不需要长期保持，<code>2018-12-27 14:22:22</code>的订单付款信息如果是<code>2018-12-27 15:22:22</code>以后到达的那么我们也没有必要保存到State中。 而对于UnBounded的双流JOIN我们会一直将数据保存到State中，如下示意图：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/b26733d7d193bbea8d594f098bcd98cbea926a88.png\" alt></p>\n<p>这样的底层实现，对于当前需求有不必要的性能损失。所以我们有必要开发一种新的可以清除State的JOIN方式(Interval JOIN)来高性能的完成上面的查询需求。</p>\n<h3>功能扩展</h3>\n<p>目前的UnBounded的双流JOIN是后面是没有办法再进行Event-Time的Window Aggregate的。也就是下面的语句在Apache Flink上面是无法支持的：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT COUNT(*) FROM (</span><br><span class=\"line\"> SELECT </span><br><span class=\"line\">  ...,</span><br><span class=\"line\">  payTime</span><br><span class=\"line\">  FROM Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">   o.orderId = p.orderId </span><br><span class=\"line\"> ) GROUP BY TUMBLE(payTime, INTERVAL &apos;15&apos; MINUTE)</span><br></pre></td></tr></table></figure></p>\n<p>因为在UnBounded的双流JOIN中无法保证payTime的值一定大于WaterMark（WaterMark相关可以查阅&lt;&lt;Apache Flink 漫谈系列(03) - Watermark&gt;&gt;). Apache Flink的Interval JOIN之后可以进行Event-Time的Window Aggregate。</p>\n<h2>Interval JOIN</h2>\n<p>为了完成上面需求，并且解决性能和功能扩展的问题，Apache Flink在1.4开始开发了Time-windowed Join，也就是本文所说的Interval JOIN。接下来我们详细介绍Interval JOIN的语法，语义和实现原理。</p>\n<h1>什么是Interval JOIN</h1>\n<p>Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。</p>\n<h2>Interval JOIN 语法</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ... FROM t1 JOIN t2  ON t1.key = t2.key AND TIMEBOUND_EXPRESSION</span><br></pre></td></tr></table></figure></p>\n<p>TIMEBOUND_EXPRESSION 有两种写法，如下：</p>\n<ul>\n<li>L.time between LowerBound(R.time) and UpperBound(R.time)</li>\n<li>R.time between LowerBound(L.time) and UpperBound(L.time)</li>\n<li>带有时间属性(L.time/R.time)的比较表达式。</li>\n</ul>\n<h2>Interval JOIN 语义</h2>\n<p>Interval JOIN 的语义就是每条数据对应一个 Interval 的数据区间，比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计在下单一小时内付款的订单信息。SQL查询如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">  o.orderId,</span><br><span class=\"line\">  o.productName,</span><br><span class=\"line\">  p.payType,</span><br><span class=\"line\">  o.orderTime，</span><br><span class=\"line\">  cast(payTime as timestamp) as payTime</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON </span><br><span class=\"line\">  o.orderId = p.orderId AND </span><br><span class=\"line\">  p.payTime BETWEEN orderTime AND </span><br><span class=\"line\">  orderTime + INTERVAL &apos;1&apos; HOUR</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>Orders订单数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>productName</th>\n<th>orderTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>iphone</td>\n<td>2018-12-26 04:53:22.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>mac</td>\n<td>2018-12-26 04:53:23.0</td>\n</tr>\n<tr>\n<td>003</td>\n<td>book</td>\n<td><strong>2018-12-26 04:53:24.0</strong></td>\n</tr>\n<tr>\n<td>004</td>\n<td>cup</td>\n<td>2018-12-26 04:53:38.0</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Payment付款数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>payType</th>\n<th>payTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>alipay</td>\n<td>2018-12-26 05:51:41.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>card</td>\n<td>2018-12-26 05:53:22.0</td>\n</tr>\n<tr>\n<td>003</td>\n<td>card</td>\n<td><strong>2018-12-26 05:53:30.0</strong></td>\n</tr>\n<tr>\n<td>004</td>\n<td>alipay</td>\n<td>2018-12-26 05:53:31.0</td>\n</tr>\n</tbody>\n</table>\n<p>符合语义的预期结果是 订单id为003的信息不出现在结果表中，因为下单时间<code>2018-12-26 04:53:24.0</code>, 付款时间是 <code>2018-12-26 05:53:30.0</code>超过了1小时付款。\n那么预期的结果信息如下：</p>\n<table>\n<thead>\n<tr>\n<th>orderId</th>\n<th>productName</th>\n<th>payType</th>\n<th>orderTime</th>\n<th>payTime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>001</td>\n<td>iphone</td>\n<td>alipay</td>\n<td>2018-12-26 04:53:22.0</td>\n<td>2018-12-26 05:51:41.0</td>\n</tr>\n<tr>\n<td>002</td>\n<td>mac</td>\n<td>card</td>\n<td>2018-12-26 04:53:23.0</td>\n<td>2018-12-26 05:53:22.0</td>\n</tr>\n<tr>\n<td>004</td>\n<td>cup</td>\n<td>alipay</td>\n<td>2018-12-26 04:53:38.0</td>\n<td>2018-12-26 05:53:31.0</td>\n</tr>\n</tbody>\n</table>\n<p>这样Id为003的订单是无效订单，可以更新库存继续售卖。</p>\n<p>接下来我们以图示的方式直观说明Interval JOIN的语义，<strong>我们对上面的示例需求稍微变化一下：</strong> 订单可以预付款(不管是否合理，我们只是为了说明语义）也就是订单 <strong>前后</strong> 1小时的付款都是有效的。SQL语句如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT</span><br><span class=\"line\">  ...</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o JOIN Payment AS p ON</span><br><span class=\"line\">  o.orderId = p.orderId AND</span><br><span class=\"line\">  p.payTime BETWEEN orderTime - INTERVAL &apos;1&apos; HOUR AND</span><br><span class=\"line\">  orderTime + INTERVAL &apos;1&apos; HOUR</span><br></pre></td></tr></table></figure></p>\n<p>这样的查询语义示意图如下：\n<img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/452ada49a233d68c6e614e05e61ba158aa180f83.png\" alt></p>\n<p>上图有几个关键点，如下：</p>\n<ul>\n<li>数据JOIN的区间 - 比如Order时间为3的订单会在付款时间为[2, 4]区间进行JOIN。</li>\n<li>WaterMark - 比如图示Order最后一条数据时间是3，Payment最后一条数据时间是5，那么WaterMark是根据实际最小值减去UpperBound生成，即：Min(3,5)-1 = 2</li>\n<li>过期数据 - 出于性能和存储的考虑，要将过期数据清除，如图当WaterMark是2的时候时间为2以前的数据过期了，可以被清除。</li>\n</ul>\n<h2>Interval JOIN 实现原理</h2>\n<p>由于Interval JOIN和双流JOIN类似都要存储左右两边的数据，所以底层实现中仍然是利用State进行数据的存储。流计算的特点是数据不停的流入，我们可以不停的进行增量计算，也就是我们每条数据流入都可以进行JOIN计算。我们还是以具体示例和图示来说明内部计算逻辑，如下图：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png\" alt></p>\n<p>简单解释一下每条记录的处理逻辑如下：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/eef2a485ae6f7de29d5965457a1ae0da89055a58.png\" alt></p>\n<p>实际的内部逻辑会比描述的复杂的多，大家可以根据如上简述理解内部原理即可。</p>\n<h1>示例代码</h1>\n<p>我们还是以订单和付款示例，将完整代码分享给大家，具体如下(代码基于flink-1.7.0)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.sql.Timestamp</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.api.scala._</span><br><span class=\"line\">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class=\"line\">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class=\"line\">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class=\"line\">import org.apache.flink.table.api.TableEnvironment</span><br><span class=\"line\">import org.apache.flink.table.api.scala._</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.collection.mutable</span><br><span class=\"line\"></span><br><span class=\"line\">object SimpleTimeIntervalJoin &#123;</span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 构造订单数据</span><br><span class=\"line\">    val ordersData = new mutable.MutableList[(String, String, Timestamp)]</span><br><span class=\"line\">    ordersData.+=((&quot;001&quot;, &quot;iphone&quot;, new Timestamp(1545800002000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;002&quot;, &quot;mac&quot;, new Timestamp(1545800003000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;003&quot;, &quot;book&quot;, new Timestamp(1545800004000L)))</span><br><span class=\"line\">    ordersData.+=((&quot;004&quot;, &quot;cup&quot;, new Timestamp(1545800018000L)))</span><br><span class=\"line\"></span><br><span class=\"line\">    // 构造付款表</span><br><span class=\"line\">    val paymentData = new mutable.MutableList[(String, String, Timestamp)]</span><br><span class=\"line\">    paymentData.+=((&quot;001&quot;, &quot;alipay&quot;, new Timestamp(1545803501000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;002&quot;, &quot;card&quot;, new Timestamp(1545803602000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;003&quot;, &quot;card&quot;, new Timestamp(1545803610000L)))</span><br><span class=\"line\">    paymentData.+=((&quot;004&quot;, &quot;alipay&quot;, new Timestamp(1545803611000L)))</span><br><span class=\"line\">    val orders = env</span><br><span class=\"line\">      .fromCollection(ordersData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;orderId, &apos;productName, &apos;orderTime.rowtime)</span><br><span class=\"line\">    val ratesHistory = env</span><br><span class=\"line\">      .fromCollection(paymentData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;orderId, &apos;payType, &apos;payTime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTable(&quot;Orders&quot;, orders)</span><br><span class=\"line\">    tEnv.registerTable(&quot;Payment&quot;, ratesHistory)</span><br><span class=\"line\"></span><br><span class=\"line\">    var sqlQuery =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT</span><br><span class=\"line\">        |  o.orderId,</span><br><span class=\"line\">        |  o.productName,</span><br><span class=\"line\">        |  p.payType,</span><br><span class=\"line\">        |  o.orderTime,</span><br><span class=\"line\">        |  cast(payTime as timestamp) as payTime</span><br><span class=\"line\">        |FROM</span><br><span class=\"line\">        |  Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND</span><br><span class=\"line\">        | p.payTime BETWEEN orderTime AND orderTime + INTERVAL &apos;1&apos; HOUR</span><br><span class=\"line\">        |&quot;&quot;&quot;.stripMargin</span><br><span class=\"line\">    tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.sqlQuery(sqlQuery))</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row]</span><br><span class=\"line\">    result.print()</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class TimestampExtractor[T1, T2]</span><br><span class=\"line\">  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123;</span><br><span class=\"line\">  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123;</span><br><span class=\"line\">    element._3.getTime</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>运行结果如下：</p>\n<p><img src=\"/2019/03/22/Apache Flink 漫谈系列 - Time Interval JOIN/AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png\" alt></p>\n<h1>小节</h1>\n<p>本篇由实际业务需求场景切入，介绍了相同业务需求既可以利用Unbounded 双流JOIN实现，也可以利用Time Interval JOIN来实现，Time Interval JOIN 性能优于UnBounded的双流JOIN，并且Interval JOIN之后可以进行Window Aggregate算子计算。然后介绍了Interval JOIN的语法，语义和实现原理，最后将订单和付款的完整示例代码分享给大家。期望本篇能够让大家对Apache Flink Time Interval JOIN有一个具体的了解！</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n"},{"title":"Apache Flink 漫谈系列 - State","date":"2019-01-01T10:18:18.000Z","updated":"2019-01-16T10:18:18.000Z","_content":"# 实际问题\n在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: \"上一次的计算结果保存在哪里，保存在内存可以吗？\"，\n\n答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。\n\n# 什么是State\n这个问题似乎有些\"弱智\"？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。\n# 为什么需要State\n与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。\n# State 实现\nApache Flink内部有四种state的存储实现，具体如下：\n* 基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；\n* 基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；\n* 基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；\n* 还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；\n# State 持久化逻辑\nApache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。\n![](4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png)\n# State 分类\nApache Flink 内部按照算子和数据分组角度将State划分为如下两类：\n* KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的；\n* OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。  \n\n# State 扩容重新分配\nApache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。\n\nApache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。\n\n如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示:\n![](AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png)\n\n在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。\n\n## OperatorState对扩容的处理\n我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？\nstate 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下：\n```\npublic interface ListCheckpointed&lt;T extends Serializable&gt; {\n    List&lt;T&gt; snapshotState(long var1, long var3) throws Exception;\n\n    void restoreState(List&lt;T&gt; var1) throws Exception;\n}\n```\n我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下：\n```\npublic interface InputSplit extends Serializable {\n    int getSplitNumber();\n}\n```\n也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下：\n![](7230573C-621C-4792-BB1C-7EB96A61DBEE.png)\n\n如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下：\n```\nList&lt;Integer&gt; assignedPartitions = new LinkedList&lt;&gt;();\nfor (int i = 0; i &lt; partitions; i++) {\n        if (i % consumerCount == consumerIndex) {\n                assignedPartitions.add(i);\n        }\n}\n```\n这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下：\n![](93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png)\n\n那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。\n## KeyedState对扩容的处理\n对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。\n\n### 什么是Key-Groups\nKey-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下：\n```\npublic class KeyGroupRange implements KeyGroupsList, Serializable {\n        ...\n        ...\n        private final int startKeyGroup;\n        private final int endKeyGroup;\n        ...\n        ...\n}\n```\nKeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。\n\n### 什么决定Key-Groups的个数\nkey-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。\n \n`GroupRange.of(0, maxParallelism)`如何决定key属于哪个Key-Group\n确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下：\n```\npublic static int assignToKeyGroup(Object key, int maxParallelism) {\n      return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);\n}\n\npublic static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) {\n      return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);\n}\n\n@Override\npublic int partition(T key, int numPartitions) {\n      return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;\n}\n```\n如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示：\n \n![](989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png)\n\n如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。\n每个Operator实例如何获取Key-Groups\n 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法：\n ```\npublic static KeyGroupRange computeKeyGroupRangeForOperatorIndex(\n      int maxParallelism,\n      int parallelism,\n      int operatorIndex) {\n        GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex);\n        int startGroup = splitRange.getStartGroup();\n        int endGroup = splitRange.getEndGroup();\n   return new KeyGroupRange(startGroup, endGroup - 1);\n}\n\npublic GroupRange getSplitRange(int numSplits, int splitIndex) {\n        ...\n        final int numGroupsPerSplit = getNumGroups() / numSplits;\n        final int numFatSplits = getNumGroups() % numSplits;\n\n        int startGroupForThisSplit;\n        int endGroupForThisSplit;\n        if (splitIndex &lt; numFatSplits) {\n            startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1);\n            endGroupForThisSplit =   startGroupForThisSplit + numGroupsPerSplit + 1;\n        } else {\n            startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits;\n            endGroupForThisSplit =  startGroupForThisSplit + numGroupsPerSplit;\n        }\n        if (startGroupForThisSplit &gt;= endGroupForThisSplit) {\n                return GroupRange.emptyGroupRange();\n        } else {\n                return new GroupRange(startGroupForThisSplit, endGroupForThisSplit);\n        }\n}\n```\n上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。\n假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：\n![](1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png)\n如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。\n\n# 小结\n本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。\n","source":"_posts/Apache Flink 漫谈系列 - State.md","raw":"---\ntitle: Apache Flink 漫谈系列 - State\ndate: 2019-01-01 18:18:18\nupdated: 2019-01-16 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 实际问题\n在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: \"上一次的计算结果保存在哪里，保存在内存可以吗？\"，\n\n答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。\n\n# 什么是State\n这个问题似乎有些\"弱智\"？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。\n# 为什么需要State\n与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。\n# State 实现\nApache Flink内部有四种state的存储实现，具体如下：\n* 基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；\n* 基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；\n* 基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；\n* 还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；\n# State 持久化逻辑\nApache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。\n![](4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png)\n# State 分类\nApache Flink 内部按照算子和数据分组角度将State划分为如下两类：\n* KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的；\n* OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。  \n\n# State 扩容重新分配\nApache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。\n\nApache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。\n\n如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示:\n![](AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png)\n\n在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。\n\n## OperatorState对扩容的处理\n我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？\nstate 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下：\n```\npublic interface ListCheckpointed&lt;T extends Serializable&gt; {\n    List&lt;T&gt; snapshotState(long var1, long var3) throws Exception;\n\n    void restoreState(List&lt;T&gt; var1) throws Exception;\n}\n```\n我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下：\n```\npublic interface InputSplit extends Serializable {\n    int getSplitNumber();\n}\n```\n也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下：\n![](7230573C-621C-4792-BB1C-7EB96A61DBEE.png)\n\n如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下：\n```\nList&lt;Integer&gt; assignedPartitions = new LinkedList&lt;&gt;();\nfor (int i = 0; i &lt; partitions; i++) {\n        if (i % consumerCount == consumerIndex) {\n                assignedPartitions.add(i);\n        }\n}\n```\n这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下：\n![](93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png)\n\n那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。\n## KeyedState对扩容的处理\n对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。\n\n### 什么是Key-Groups\nKey-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下：\n```\npublic class KeyGroupRange implements KeyGroupsList, Serializable {\n        ...\n        ...\n        private final int startKeyGroup;\n        private final int endKeyGroup;\n        ...\n        ...\n}\n```\nKeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。\n\n### 什么决定Key-Groups的个数\nkey-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。\n \n`GroupRange.of(0, maxParallelism)`如何决定key属于哪个Key-Group\n确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下：\n```\npublic static int assignToKeyGroup(Object key, int maxParallelism) {\n      return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);\n}\n\npublic static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) {\n      return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);\n}\n\n@Override\npublic int partition(T key, int numPartitions) {\n      return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;\n}\n```\n如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示：\n \n![](989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png)\n\n如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。\n每个Operator实例如何获取Key-Groups\n 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法：\n ```\npublic static KeyGroupRange computeKeyGroupRangeForOperatorIndex(\n      int maxParallelism,\n      int parallelism,\n      int operatorIndex) {\n        GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex);\n        int startGroup = splitRange.getStartGroup();\n        int endGroup = splitRange.getEndGroup();\n   return new KeyGroupRange(startGroup, endGroup - 1);\n}\n\npublic GroupRange getSplitRange(int numSplits, int splitIndex) {\n        ...\n        final int numGroupsPerSplit = getNumGroups() / numSplits;\n        final int numFatSplits = getNumGroups() % numSplits;\n\n        int startGroupForThisSplit;\n        int endGroupForThisSplit;\n        if (splitIndex &lt; numFatSplits) {\n            startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1);\n            endGroupForThisSplit =   startGroupForThisSplit + numGroupsPerSplit + 1;\n        } else {\n            startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits;\n            endGroupForThisSplit =  startGroupForThisSplit + numGroupsPerSplit;\n        }\n        if (startGroupForThisSplit &gt;= endGroupForThisSplit) {\n                return GroupRange.emptyGroupRange();\n        } else {\n                return new GroupRange(startGroupForThisSplit, endGroupForThisSplit);\n        }\n}\n```\n上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。\n假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：\n![](1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png)\n如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。\n\n# 小结\n本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。\n","slug":"Apache Flink 漫谈系列 - State","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fi000ks94hctfz1eri","content":"<h1>实际问题</h1>\n<p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: &quot;上一次的计算结果保存在哪里，保存在内存可以吗？&quot;，</p>\n<p>答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。</p>\n<h1>什么是State</h1>\n<p>这个问题似乎有些&quot;弱智&quot;？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。</p>\n<h1>为什么需要State</h1>\n<p>与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。</p>\n<h1>State 实现</h1>\n<p>Apache Flink内部有四种state的存储实现，具体如下：</p>\n<ul>\n<li>基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；</li>\n<li>基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li>\n<li>基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；</li>\n<li>还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；</li>\n</ul>\n<h1>State 持久化逻辑</h1>\n<p>Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png\" alt></p>\n<h1>State 分类</h1>\n<p>Apache Flink 内部按照算子和数据分组角度将State划分为如下两类：</p>\n<ul>\n<li>KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的；</li>\n<li>OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。</li>\n</ul>\n<h1>State 扩容重新分配</h1>\n<p>Apache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。</p>\n<p>Apache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。</p>\n<p>如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示:\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png\" alt></p>\n<p>在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。</p>\n<h2>OperatorState对扩容的处理</h2>\n<p>我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？\nstate 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface ListCheckpointed&amp;lt;T extends Serializable&amp;gt; &#123;</span><br><span class=\"line\">    List&amp;lt;T&amp;gt; snapshotState(long var1, long var3) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">    void restoreState(List&amp;lt;T&amp;gt; var1) throws Exception;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface InputSplit extends Serializable &#123;</span><br><span class=\"line\">    int getSplitNumber();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/7230573C-621C-4792-BB1C-7EB96A61DBEE.png\" alt></p>\n<p>如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&amp;lt;Integer&amp;gt; assignedPartitions = new LinkedList&amp;lt;&amp;gt;();</span><br><span class=\"line\">for (int i = 0; i &amp;lt; partitions; i++) &#123;</span><br><span class=\"line\">        if (i % consumerCount == consumerIndex) &#123;</span><br><span class=\"line\">                assignedPartitions.add(i);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png\" alt></p>\n<p>那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。</p>\n<h2>KeyedState对扩容的处理</h2>\n<p>对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。</p>\n<h3>什么是Key-Groups</h3>\n<p>Key-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class KeyGroupRange implements KeyGroupsList, Serializable &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        private final int startKeyGroup;</span><br><span class=\"line\">        private final int endKeyGroup;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>KeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。</p>\n<h3>什么决定Key-Groups的个数</h3>\n<p>key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。\n \n<code>GroupRange.of(0, maxParallelism)</code>如何决定key属于哪个Key-Group\n确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static int assignToKeyGroup(Object key, int maxParallelism) &#123;</span><br><span class=\"line\">      return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) &#123;</span><br><span class=\"line\">      return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">public int partition(T key, int numPartitions) &#123;</span><br><span class=\"line\">      return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示：\n \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png\" alt></p>\n<p>如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。\n每个Operator实例如何获取Key-Groups\n 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static KeyGroupRange computeKeyGroupRangeForOperatorIndex(</span><br><span class=\"line\">      int maxParallelism,</span><br><span class=\"line\">      int parallelism,</span><br><span class=\"line\">      int operatorIndex) &#123;</span><br><span class=\"line\">        GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex);</span><br><span class=\"line\">        int startGroup = splitRange.getStartGroup();</span><br><span class=\"line\">        int endGroup = splitRange.getEndGroup();</span><br><span class=\"line\">   return new KeyGroupRange(startGroup, endGroup - 1);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public GroupRange getSplitRange(int numSplits, int splitIndex) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        final int numGroupsPerSplit = getNumGroups() / numSplits;</span><br><span class=\"line\">        final int numFatSplits = getNumGroups() % numSplits;</span><br><span class=\"line\"></span><br><span class=\"line\">        int startGroupForThisSplit;</span><br><span class=\"line\">        int endGroupForThisSplit;</span><br><span class=\"line\">        if (splitIndex &amp;lt; numFatSplits) &#123;</span><br><span class=\"line\">            startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1);</span><br><span class=\"line\">            endGroupForThisSplit =   startGroupForThisSplit + numGroupsPerSplit + 1;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits;</span><br><span class=\"line\">            endGroupForThisSplit =  startGroupForThisSplit + numGroupsPerSplit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (startGroupForThisSplit &amp;gt;= endGroupForThisSplit) &#123;</span><br><span class=\"line\">                return GroupRange.emptyGroupRange();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">                return new GroupRange(startGroupForThisSplit, endGroupForThisSplit);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。\n假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png\" alt>\n如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。</p>\n<h1>小结</h1>\n<p>本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>实际问题</h1>\n<p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: &quot;上一次的计算结果保存在哪里，保存在内存可以吗？&quot;，</p>\n<p>答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。</p>\n<h1>什么是State</h1>\n<p>这个问题似乎有些&quot;弱智&quot;？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。</p>\n<h1>为什么需要State</h1>\n<p>与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。</p>\n<h1>State 实现</h1>\n<p>Apache Flink内部有四种state的存储实现，具体如下：</p>\n<ul>\n<li>基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；</li>\n<li>基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li>\n<li>基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；</li>\n<li>还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；</li>\n</ul>\n<h1>State 持久化逻辑</h1>\n<p>Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png\" alt></p>\n<h1>State 分类</h1>\n<p>Apache Flink 内部按照算子和数据分组角度将State划分为如下两类：</p>\n<ul>\n<li>KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的；</li>\n<li>OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。</li>\n</ul>\n<h1>State 扩容重新分配</h1>\n<p>Apache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。</p>\n<p>Apache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。</p>\n<p>如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示:\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png\" alt></p>\n<p>在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。</p>\n<h2>OperatorState对扩容的处理</h2>\n<p>我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？\nstate 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface ListCheckpointed&amp;lt;T extends Serializable&amp;gt; &#123;</span><br><span class=\"line\">    List&amp;lt;T&amp;gt; snapshotState(long var1, long var3) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">    void restoreState(List&amp;lt;T&amp;gt; var1) throws Exception;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface InputSplit extends Serializable &#123;</span><br><span class=\"line\">    int getSplitNumber();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/7230573C-621C-4792-BB1C-7EB96A61DBEE.png\" alt></p>\n<p>如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&amp;lt;Integer&amp;gt; assignedPartitions = new LinkedList&amp;lt;&amp;gt;();</span><br><span class=\"line\">for (int i = 0; i &amp;lt; partitions; i++) &#123;</span><br><span class=\"line\">        if (i % consumerCount == consumerIndex) &#123;</span><br><span class=\"line\">                assignedPartitions.add(i);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png\" alt></p>\n<p>那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。</p>\n<h2>KeyedState对扩容的处理</h2>\n<p>对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。</p>\n<h3>什么是Key-Groups</h3>\n<p>Key-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class KeyGroupRange implements KeyGroupsList, Serializable &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        private final int startKeyGroup;</span><br><span class=\"line\">        private final int endKeyGroup;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>KeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。</p>\n<h3>什么决定Key-Groups的个数</h3>\n<p>key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。\n \n<code>GroupRange.of(0, maxParallelism)</code>如何决定key属于哪个Key-Group\n确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static int assignToKeyGroup(Object key, int maxParallelism) &#123;</span><br><span class=\"line\">      return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) &#123;</span><br><span class=\"line\">      return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">public int partition(T key, int numPartitions) &#123;</span><br><span class=\"line\">      return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示：\n \n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png\" alt></p>\n<p>如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。\n每个Operator实例如何获取Key-Groups\n 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static KeyGroupRange computeKeyGroupRangeForOperatorIndex(</span><br><span class=\"line\">      int maxParallelism,</span><br><span class=\"line\">      int parallelism,</span><br><span class=\"line\">      int operatorIndex) &#123;</span><br><span class=\"line\">        GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex);</span><br><span class=\"line\">        int startGroup = splitRange.getStartGroup();</span><br><span class=\"line\">        int endGroup = splitRange.getEndGroup();</span><br><span class=\"line\">   return new KeyGroupRange(startGroup, endGroup - 1);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public GroupRange getSplitRange(int numSplits, int splitIndex) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        final int numGroupsPerSplit = getNumGroups() / numSplits;</span><br><span class=\"line\">        final int numFatSplits = getNumGroups() % numSplits;</span><br><span class=\"line\"></span><br><span class=\"line\">        int startGroupForThisSplit;</span><br><span class=\"line\">        int endGroupForThisSplit;</span><br><span class=\"line\">        if (splitIndex &amp;lt; numFatSplits) &#123;</span><br><span class=\"line\">            startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1);</span><br><span class=\"line\">            endGroupForThisSplit =   startGroupForThisSplit + numGroupsPerSplit + 1;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits;</span><br><span class=\"line\">            endGroupForThisSplit =  startGroupForThisSplit + numGroupsPerSplit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (startGroupForThisSplit &amp;gt;= endGroupForThisSplit) &#123;</span><br><span class=\"line\">                return GroupRange.emptyGroupRange();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">                return new GroupRange(startGroupForThisSplit, endGroupForThisSplit);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。\n假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - State/1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png\" alt>\n如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。</p>\n<h1>小结</h1>\n<p>本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。</p>\n"},{"title":"Apache Flink 漫谈系列 - Temporal Table JOIN","date":"2019-03-18T10:18:18.000Z","_content":"# 什么是Temporal Table\n在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。\n\n在[ANSI-SQL 2011](https://sigmodrecord.org/publications/sigmodRecord/1209/pdfs/07.industry.kulkarni.pdf) 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：\n![](D05B8A19-6D97-4809-8723-B4B95FE075A7.png)\n上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。\n# ANSI-SQL 2011 Temporal Table示例\n我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：\n* DDL 示例\n```\nCREATE TABLE Emp\nENo INTEGER,\nSys_Start TIMESTAMP(12) GENERATED\nALWAYS AS ROW Start,\nSys_end TIMESTAMP(12) GENERATED\nALWAYS AS ROW END,\nEName VARCHAR(30),\nPERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)\n) WITH SYSTEM VERSIONING\n```\n* DML 示例\n1. INSERT\n```\nINSERT INTO Emp (ENo, EName) VALUES (22217, 'Joe')\n```\n![](BF868DDE-D746-4529-878D-91F1A0D7486F.png)\n**说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。**\n\n2. UPDATE\n\n```\nUPDATE Emp SET EName = 'Tom' WHERE ENo = 22217 \n```\n![](364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png)\n \n**说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值\"Joe\"的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值\"Tom\"生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。**\n\n3. DELETE (假设执行DELETE之前的表内容如下)\n![](BF868DDE-D746-4529-878D-91F1A0D7486F.png)\n```\nDELETE FROM Emp WHERE ENo = 22217\n```\n![](172A63B1-C30D-43EA-B04E-B9BE8706838B.png)\n\n**说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。**\n4. SELECT\n```\nSELECT ENo,EName,Sys_Start,Sys_End FROM Emp \nFOR SYSTEM_TIME AS OF TIMESTAMP '2011-01-02 00:00:00'\n```\n**说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。**\n# SQLServer Temporal Table 示例\n## DDL\n```\nCREATE TABLE Department\n(\nDeptID int NOT NULL PRIMARY KEY CLUSTERED\n, DeptName varchar(50) NOT NULL\n, ManagerID INT NULL\n, ParentDeptID int NULL\n, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL\n, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL\n, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)\n)\nWITH (SYSTEM_VERSIONING = ON);\n```\n执行上面的语句，在数据库会创建当前表和历史表，如下图：\n![](6EAC4329-C402-4497-BAE2-7E7B778E613B.png)\nDepartment 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：`SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。`\n## DML\n* INSERT - 插入列不包含SysStartTime和SysEndTime列\n```\nINSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])\nVALUES(10, 'Marketing', 101, 1);\n```\n执行之后我们分别查询当前表和历史表，如下图：![](3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png)\n我们第一条INSERT语句数据值的有效时间是操作那一刻`2018-06-06 05:50:20.7913985` 到永远 `9999-12-31 23:59:59.9999999`，但这时刻历史表还没有任何信息。我们接下来进行更新操作。\n* UPDATE\n```\nUPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10\n```\n执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：![](75AC8012-59C7-4105-AE66-4453384E762B.png)\n注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次：\n```\nUPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10\n```\n![](274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png)\n到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。\n* SELECT \n```\nSELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]\nFROM [dbo].[Department]\nFOR SYSTEM_TIME AS OF '2018-06-06 05:50:21.0000000' ;\n```\n![](A94E2FAD-F04A-4691-8C34-3C697FF81476.png)\nSELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 `SysStartTime <= '2018-06-06 05:50:21.0000000' and SysEndTime > '2018-06-06 05:50:21.0000000'` 。\n\n# Apache Flink Temporal Table\n我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用`FOR SYSTEM_TIME AS OF`的语法，目前Apache Flink中使用 `LATERAL TABLE(TemporalTableFunction)`的语法。这一点后续需要推动社区进行改进。\n\n## 为啥需要 Temporal Table\n我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下：\n\n| rowtime(ts) | currency(pk) | rate |\n| --- | --- | --- |\n|09:00:00   |US Dollar   |102|\n|09:00:00   |Euro        |114|\n|09:00:00   |Yen         |  1|\n|10:45:00   |Euro        |116|\n|11:15:00   |Euro        |119|\n|11:49:00   |Pounds      |108|\n\nRatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。\n\n假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表：\n```\nSELECT *\nFROM RatesHistory AS r\nWHERE r.rowtime = (\n  SELECT MAX(rowtime)\n  FROM RatesHistory AS r2\n  WHERE r2.currency = r.currency\n  AND r2.rowtime <= '10:58');\n```\n相应Flink代码如下：\n* 定义数据源-genRatesHistorySource \n```\ndef genRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"rowtime ,currency   ,rate\",\n    \"09:00:00   ,US Dollar  , 102\",\n    \"09:00:00   ,Euro       , 114\",\n    \"09:00:00  ,Yen        ,   1\",\n    \"10:45:00   ,Euro       , 116\",\n    \"11:15:00   ,Euro       , 119\",\n    \"11:49:00   ,Pounds     , 108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"rowtime\",\"currency\",\"rate\"),\n      Array(\n        Types.STRING,Types.STRING,Types.STRING\n      ),\n      fieldDelim = \",\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n  \n  def writeToTempFile(\n    contents: String,\n    filePrefix: String,\n    fileSuffix: String,\n    charset: String = \"UTF-8\"): String = {\n    val tempFile = File.createTempFile(filePrefix, fileSuffix)\n    val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset)\n    tmpWriter.write(contents)\n    tmpWriter.close()\n    tempFile.getAbsolutePath\n  }\n```\n* 主程序代码\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"RatesHistory\"\n    // 创建CSV source数据结构\n    val tableSource = CsvTableSourceUtils.genRatesHistorySource\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n\n    // 注册retract sink\n    val sinkTableName = \"retractSink\"\n    val fieldNames = Array(\"rowtime\", \"currency\", \"rate\")\n    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING)\n\n    tEnv.registerTableSink(\n      sinkTableName,\n      fieldNames,\n      fieldTypes,\n      new MemoryRetractSink)\n\n    val SQL =\n      \"\"\"\n        |SELECT *\n        |FROM RatesHistory AS r\n        |WHERE r.rowtime = (\n        |  SELECT MAX(rowtime)\n        |  FROM RatesHistory AS r2\n        |  WHERE r2.currency = r.currency\n        |  AND r2.rowtime <= '10:58:00'  )\n      \"\"\".stripMargin\n\n    // 执行查询\n    val result = tEnv.SQLQuery(SQL)\n\n    // 将结果插入sink\n    result.insertInto(sinkTableName)\n    env.execute()\n  }\n```\n* 执行结果如下图：\n![](82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png)\n结果表格化一下：\n\n| rowtime(ts) | currency(pk) | rate |\n| --- | --- | --- |\n|09:00:00   |US Dollar   |102|\n|09:00:00   |Yen         |  1|\n|10:45:00   |Euro        |116|\n\nTemporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append  Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。\n\n在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。\n\n## 如何定义Temporal Table\n在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。\n\n* 内部TemporalTableFunction定义如下：\n```\nclass TemporalTableFunction private(\n    @transient private val underlyingHistoryTable: Table,\n    // 时间属性，相当于版本信息\n    private val timeAttribute: Expression,\n    // 主键定义\n    private val primaryKey: String,\n    private val resultType: RowTypeInfo)\n  extends TableFunction[Row] {\n  ...\n}\n```\n* 用户创建TemporalTableFunction方式\n在`Table`中添加了`createTemporalTableFunction`方法，该方法需要传入时间属性和主键，接口定义如下：\n```\n// Creates TemporalTableFunction backed up by this table as a history table.\n\ndef createTemporalTableFunction(\n      timeAttribute: Expression,\n      primaryKey: Expression): TemporalTableFunction = {\n   ...\n}\n```\n用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下：\n```\nval tab = ...\nval temporalTableFunction = tab.createTemporalTableFunction('time, 'pk)\n...\n```\n\n## 案例代码\n* 需求描述\n假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。\n* Orders 数据\n\n| amount | currency | order_time |\n| --- | --- | --- |\n| 2 | Euro |  2|\n| 1 | US Dollar | 3 |\n| 50 | Yen | 4 |\n| 3 | Euro | 5 |\n\n* Rates 数据\n\n| currency | rate| rate_time |\n| --- | --- | --- |\n|US Dollar  |102  | 1 |\n| Euro | 114 | 1 |\n| Yen |1  | 1 |\n| Euro | 116 | 5 |\n| Euro |117  |7  |\n\n* 统计需求对应的SQL\n```\nSELECT o.currency, o.amount, r.rate\n  o.amount * r.rate AS yen_amount\nFROM\n  Orders AS o,\n  LATERAL TABLE (Rates(o.rowtime)) AS r\nWHERE r.currency = o.currency\n```\n* 预期结果\n\n| currency | amount |  rate|yen_amount|\n| --- | --- | --- |---|\n|US Dollar |1 |102|102 |\n|Yen |50  |1|50|\n|Euro |2 |114|228 |\n|Euro |3  |116|348|\n\n### Without connnector 实现代码\n```\nobject TemporalTableJoinTest {\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setParallelism(1)\n// 设置时间类型是 event-time  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 构造订单数据\n    val ordersData = new mutable.MutableList[(Long, String, Timestamp)]\n    ordersData.+=((2L, \"Euro\", new Timestamp(2L)))\n    ordersData.+=((1L, \"US Dollar\", new Timestamp(3L)))\n    ordersData.+=((50L, \"Yen\", new Timestamp(4L)))\n    ordersData.+=((3L, \"Euro\", new Timestamp(5L)))\n\n    //构造汇率数据\n    val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)]\n    ratesHistoryData.+=((\"US Dollar\", 102L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Euro\", 114L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Yen\", 1L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Euro\", 116L, new Timestamp(5L)))\n    ratesHistoryData.+=((\"Euro\", 119L, new Timestamp(7L)))\n\n// 进行订单表 event-time 的提取\n    val orders = env\n      .fromCollection(ordersData)\n      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]())\n      .toTable(tEnv, 'amount, 'currency, 'rowtime.rowtime)\n\n// 进行汇率表 event-time 的提取\n    val ratesHistory = env\n      .fromCollection(ratesHistoryData)\n      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]())\n      .toTable(tEnv, 'currency, 'rate, 'rowtime.rowtime)\n\n// 注册订单表和汇率表\n    tEnv.registerTable(\"Orders\", orders)\n    tEnv.registerTable(\"RatesHistory\", ratesHistory)\n    val tab = tEnv.scan(\"RatesHistory\");\n// 创建TemporalTableFunction\n    val temporalTableFunction = tab.createTemporalTableFunction('rowtime, 'currency)\n//注册TemporalTableFunction\ntEnv.registerFunction(\"Rates\",temporalTableFunction)\n\n    val SQLQuery =\n      \"\"\"\n        |SELECT o.currency, o.amount, r.rate,\n        |  o.amount * r.rate AS yen_amount\n        |FROM\n        |  Orders AS o,\n        |  LATERAL TABLE (Rates(o.rowtime)) AS r\n        |WHERE r.currency = o.currency\n        |\"\"\".stripMargin\n\n    tEnv.registerTable(\"TemporalJoinResult\", tEnv.SQLQuery(SQLQuery))\n\n    val result = tEnv.scan(\"TemporalJoinResult\").toAppendStream[Row]\n    // 打印查询结果\n    result.print()\n    env.execute()\n  }\n\n}\n```\n在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的`TimeCharacteristic.EventTime` 模式，需要调用`assignTimestampsAndWatermarks`方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) -  Watermark》。 在本示例中提取EventTime的完整代码如下：\n```\nimport java.SQL.Timestamp\n\nimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor\nimport org.apache.flink.streaming.api.windowing.time.Time\n\nclass OrderTimestampExtractor[T1, T2]\n  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) {\n  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = {\n    element._3.getTime\n  }\n}\n```\n\n查看运行结果:\n![](44FDFE22-3B21-4514-8E91-E339F9188E4E.png)\n\n\n### With CSVConnector 实现代码\n在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。\n* genEventRatesHistorySource\n```\ndef genEventRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#rate\",\n      \"1#US Dollar#102\",\n      \"1#Euro#114\",\n      \"1#Yen#1\",\n      \"3#Euro#116\",\n      \"5#Euro#119\",\n      \"7#Pounds#108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_rate\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\",\"rate\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n```\n* genRatesOrderSource\n```\ndef genRatesOrderSource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#amount\",\n      \"2#Euro#10\",\n      \"4#Euro#10\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_order\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\", \"amount\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n```\n* 主程序代码\n```\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.flink.book.connectors\n\nimport java.io.File\n\nimport org.apache.flink.api.common.typeinfo.{TypeInformation, Types}\nimport org.apache.flink.book.utils.{CommonUtils, FileUtils}\nimport org.apache.flink.table.sinks.{CsvTableSink, TableSink}\nimport org.apache.flink.table.sources.CsvTableSource\nimport org.apache.flink.types.Row\n\nobject CsvTableSourceUtils {\n\n  def genWordCountSource: CsvTableSource = {\n    val csvRecords = Seq(\n      \"words\",\n      \"Hello Flink\",\n      \"Hi, Apache Flink\",\n      \"Apache FlinkBook\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"words\"),\n      Array(\n        Types.STRING\n      ),\n      fieldDelim = \"#\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n\n  def genRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"rowtime ,currency   ,rate\",\n    \"09:00:00   ,US Dollar  , 102\",\n    \"09:00:00   ,Euro       , 114\",\n    \"09:00:00  ,Yen        ,   1\",\n    \"10:45:00   ,Euro       , 116\",\n    \"11:15:00   ,Euro       , 119\",\n    \"11:49:00   ,Pounds     , 108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"rowtime\",\"currency\",\"rate\"),\n      Array(\n        Types.STRING,Types.STRING,Types.STRING\n      ),\n      fieldDelim = \",\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n  def genEventRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#rate\",\n      \"1#US Dollar#102\",\n      \"1#Euro#114\",\n      \"1#Yen#1\",\n      \"3#Euro#116\",\n      \"5#Euro#119\",\n      \"7#Pounds#108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_rate\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\",\"rate\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n  def genRatesOrderSource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#amount\",\n      \"2#Euro#10\",\n      \"4#Euro#10\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_order\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\", \"amount\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n\n  /**\n    * Example:\n    * genCsvSink(\n    *   Array[String](\"word\", \"count\"),\n    *   Array[TypeInformation[_] ](Types.STRING, Types.LONG))\n    */\n  def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes)\n  }\n\n}\n```\n运行结果如下 ：\n![](A05E658F-C511-45E2-870F-5E3227A94570.png)\n\n## 内部实现原理\n我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示:\n\n![](07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png)\n\n# Temporal Table JOIN vs 双流JOIN vs Lateral JOIN\n在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？\n\n* 双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。 \n* LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。\n* Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table  或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点\n单流驱动，Temporal Table 是被动查询。\n\n## Temporal Table JOIN vs  LATERAL JOIN\n从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。\n\n## Temporal Table JOIN vs 双流 JOIN\nTemporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。\n\n# Temporal Table JOIN改进\n个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的`FOR SYSTEM_TIME AS OF`标准语法。改进后的处理逻辑示意图:\n![](A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png)\n其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。\n\n# 小结\n本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","source":"_posts/Apache Flink 漫谈系列 - Temporal Table JOIN.md","raw":"---\ntitle: Apache Flink 漫谈系列 - Temporal Table JOIN\ndate: 2019-03-18 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 什么是Temporal Table\n在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。\n\n在[ANSI-SQL 2011](https://sigmodrecord.org/publications/sigmodRecord/1209/pdfs/07.industry.kulkarni.pdf) 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：\n![](D05B8A19-6D97-4809-8723-B4B95FE075A7.png)\n上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。\n# ANSI-SQL 2011 Temporal Table示例\n我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：\n* DDL 示例\n```\nCREATE TABLE Emp\nENo INTEGER,\nSys_Start TIMESTAMP(12) GENERATED\nALWAYS AS ROW Start,\nSys_end TIMESTAMP(12) GENERATED\nALWAYS AS ROW END,\nEName VARCHAR(30),\nPERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)\n) WITH SYSTEM VERSIONING\n```\n* DML 示例\n1. INSERT\n```\nINSERT INTO Emp (ENo, EName) VALUES (22217, 'Joe')\n```\n![](BF868DDE-D746-4529-878D-91F1A0D7486F.png)\n**说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。**\n\n2. UPDATE\n\n```\nUPDATE Emp SET EName = 'Tom' WHERE ENo = 22217 \n```\n![](364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png)\n \n**说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值\"Joe\"的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值\"Tom\"生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。**\n\n3. DELETE (假设执行DELETE之前的表内容如下)\n![](BF868DDE-D746-4529-878D-91F1A0D7486F.png)\n```\nDELETE FROM Emp WHERE ENo = 22217\n```\n![](172A63B1-C30D-43EA-B04E-B9BE8706838B.png)\n\n**说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。**\n4. SELECT\n```\nSELECT ENo,EName,Sys_Start,Sys_End FROM Emp \nFOR SYSTEM_TIME AS OF TIMESTAMP '2011-01-02 00:00:00'\n```\n**说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。**\n# SQLServer Temporal Table 示例\n## DDL\n```\nCREATE TABLE Department\n(\nDeptID int NOT NULL PRIMARY KEY CLUSTERED\n, DeptName varchar(50) NOT NULL\n, ManagerID INT NULL\n, ParentDeptID int NULL\n, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL\n, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL\n, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)\n)\nWITH (SYSTEM_VERSIONING = ON);\n```\n执行上面的语句，在数据库会创建当前表和历史表，如下图：\n![](6EAC4329-C402-4497-BAE2-7E7B778E613B.png)\nDepartment 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：`SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。`\n## DML\n* INSERT - 插入列不包含SysStartTime和SysEndTime列\n```\nINSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])\nVALUES(10, 'Marketing', 101, 1);\n```\n执行之后我们分别查询当前表和历史表，如下图：![](3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png)\n我们第一条INSERT语句数据值的有效时间是操作那一刻`2018-06-06 05:50:20.7913985` 到永远 `9999-12-31 23:59:59.9999999`，但这时刻历史表还没有任何信息。我们接下来进行更新操作。\n* UPDATE\n```\nUPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10\n```\n执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：![](75AC8012-59C7-4105-AE66-4453384E762B.png)\n注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次：\n```\nUPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10\n```\n![](274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png)\n到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。\n* SELECT \n```\nSELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]\nFROM [dbo].[Department]\nFOR SYSTEM_TIME AS OF '2018-06-06 05:50:21.0000000' ;\n```\n![](A94E2FAD-F04A-4691-8C34-3C697FF81476.png)\nSELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 `SysStartTime <= '2018-06-06 05:50:21.0000000' and SysEndTime > '2018-06-06 05:50:21.0000000'` 。\n\n# Apache Flink Temporal Table\n我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用`FOR SYSTEM_TIME AS OF`的语法，目前Apache Flink中使用 `LATERAL TABLE(TemporalTableFunction)`的语法。这一点后续需要推动社区进行改进。\n\n## 为啥需要 Temporal Table\n我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下：\n\n| rowtime(ts) | currency(pk) | rate |\n| --- | --- | --- |\n|09:00:00   |US Dollar   |102|\n|09:00:00   |Euro        |114|\n|09:00:00   |Yen         |  1|\n|10:45:00   |Euro        |116|\n|11:15:00   |Euro        |119|\n|11:49:00   |Pounds      |108|\n\nRatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。\n\n假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表：\n```\nSELECT *\nFROM RatesHistory AS r\nWHERE r.rowtime = (\n  SELECT MAX(rowtime)\n  FROM RatesHistory AS r2\n  WHERE r2.currency = r.currency\n  AND r2.rowtime <= '10:58');\n```\n相应Flink代码如下：\n* 定义数据源-genRatesHistorySource \n```\ndef genRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"rowtime ,currency   ,rate\",\n    \"09:00:00   ,US Dollar  , 102\",\n    \"09:00:00   ,Euro       , 114\",\n    \"09:00:00  ,Yen        ,   1\",\n    \"10:45:00   ,Euro       , 116\",\n    \"11:15:00   ,Euro       , 119\",\n    \"11:49:00   ,Pounds     , 108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"rowtime\",\"currency\",\"rate\"),\n      Array(\n        Types.STRING,Types.STRING,Types.STRING\n      ),\n      fieldDelim = \",\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n  \n  def writeToTempFile(\n    contents: String,\n    filePrefix: String,\n    fileSuffix: String,\n    charset: String = \"UTF-8\"): String = {\n    val tempFile = File.createTempFile(filePrefix, fileSuffix)\n    val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset)\n    tmpWriter.write(contents)\n    tmpWriter.close()\n    tempFile.getAbsolutePath\n  }\n```\n* 主程序代码\n```\ndef main(args: Array[String]): Unit = {\n    // Streaming 环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n\n    //方便我们查出输出数据\n    env.setParallelism(1)\n\n    val sourceTableName = \"RatesHistory\"\n    // 创建CSV source数据结构\n    val tableSource = CsvTableSourceUtils.genRatesHistorySource\n    // 注册source\n    tEnv.registerTableSource(sourceTableName, tableSource)\n\n    // 注册retract sink\n    val sinkTableName = \"retractSink\"\n    val fieldNames = Array(\"rowtime\", \"currency\", \"rate\")\n    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING)\n\n    tEnv.registerTableSink(\n      sinkTableName,\n      fieldNames,\n      fieldTypes,\n      new MemoryRetractSink)\n\n    val SQL =\n      \"\"\"\n        |SELECT *\n        |FROM RatesHistory AS r\n        |WHERE r.rowtime = (\n        |  SELECT MAX(rowtime)\n        |  FROM RatesHistory AS r2\n        |  WHERE r2.currency = r.currency\n        |  AND r2.rowtime <= '10:58:00'  )\n      \"\"\".stripMargin\n\n    // 执行查询\n    val result = tEnv.SQLQuery(SQL)\n\n    // 将结果插入sink\n    result.insertInto(sinkTableName)\n    env.execute()\n  }\n```\n* 执行结果如下图：\n![](82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png)\n结果表格化一下：\n\n| rowtime(ts) | currency(pk) | rate |\n| --- | --- | --- |\n|09:00:00   |US Dollar   |102|\n|09:00:00   |Yen         |  1|\n|10:45:00   |Euro        |116|\n\nTemporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append  Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。\n\n在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。\n\n## 如何定义Temporal Table\n在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。\n\n* 内部TemporalTableFunction定义如下：\n```\nclass TemporalTableFunction private(\n    @transient private val underlyingHistoryTable: Table,\n    // 时间属性，相当于版本信息\n    private val timeAttribute: Expression,\n    // 主键定义\n    private val primaryKey: String,\n    private val resultType: RowTypeInfo)\n  extends TableFunction[Row] {\n  ...\n}\n```\n* 用户创建TemporalTableFunction方式\n在`Table`中添加了`createTemporalTableFunction`方法，该方法需要传入时间属性和主键，接口定义如下：\n```\n// Creates TemporalTableFunction backed up by this table as a history table.\n\ndef createTemporalTableFunction(\n      timeAttribute: Expression,\n      primaryKey: Expression): TemporalTableFunction = {\n   ...\n}\n```\n用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下：\n```\nval tab = ...\nval temporalTableFunction = tab.createTemporalTableFunction('time, 'pk)\n...\n```\n\n## 案例代码\n* 需求描述\n假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。\n* Orders 数据\n\n| amount | currency | order_time |\n| --- | --- | --- |\n| 2 | Euro |  2|\n| 1 | US Dollar | 3 |\n| 50 | Yen | 4 |\n| 3 | Euro | 5 |\n\n* Rates 数据\n\n| currency | rate| rate_time |\n| --- | --- | --- |\n|US Dollar  |102  | 1 |\n| Euro | 114 | 1 |\n| Yen |1  | 1 |\n| Euro | 116 | 5 |\n| Euro |117  |7  |\n\n* 统计需求对应的SQL\n```\nSELECT o.currency, o.amount, r.rate\n  o.amount * r.rate AS yen_amount\nFROM\n  Orders AS o,\n  LATERAL TABLE (Rates(o.rowtime)) AS r\nWHERE r.currency = o.currency\n```\n* 预期结果\n\n| currency | amount |  rate|yen_amount|\n| --- | --- | --- |---|\n|US Dollar |1 |102|102 |\n|Yen |50  |1|50|\n|Euro |2 |114|228 |\n|Euro |3  |116|348|\n\n### Without connnector 实现代码\n```\nobject TemporalTableJoinTest {\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    val tEnv = TableEnvironment.getTableEnvironment(env)\n    env.setParallelism(1)\n// 设置时间类型是 event-time  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    // 构造订单数据\n    val ordersData = new mutable.MutableList[(Long, String, Timestamp)]\n    ordersData.+=((2L, \"Euro\", new Timestamp(2L)))\n    ordersData.+=((1L, \"US Dollar\", new Timestamp(3L)))\n    ordersData.+=((50L, \"Yen\", new Timestamp(4L)))\n    ordersData.+=((3L, \"Euro\", new Timestamp(5L)))\n\n    //构造汇率数据\n    val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)]\n    ratesHistoryData.+=((\"US Dollar\", 102L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Euro\", 114L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Yen\", 1L, new Timestamp(1L)))\n    ratesHistoryData.+=((\"Euro\", 116L, new Timestamp(5L)))\n    ratesHistoryData.+=((\"Euro\", 119L, new Timestamp(7L)))\n\n// 进行订单表 event-time 的提取\n    val orders = env\n      .fromCollection(ordersData)\n      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]())\n      .toTable(tEnv, 'amount, 'currency, 'rowtime.rowtime)\n\n// 进行汇率表 event-time 的提取\n    val ratesHistory = env\n      .fromCollection(ratesHistoryData)\n      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]())\n      .toTable(tEnv, 'currency, 'rate, 'rowtime.rowtime)\n\n// 注册订单表和汇率表\n    tEnv.registerTable(\"Orders\", orders)\n    tEnv.registerTable(\"RatesHistory\", ratesHistory)\n    val tab = tEnv.scan(\"RatesHistory\");\n// 创建TemporalTableFunction\n    val temporalTableFunction = tab.createTemporalTableFunction('rowtime, 'currency)\n//注册TemporalTableFunction\ntEnv.registerFunction(\"Rates\",temporalTableFunction)\n\n    val SQLQuery =\n      \"\"\"\n        |SELECT o.currency, o.amount, r.rate,\n        |  o.amount * r.rate AS yen_amount\n        |FROM\n        |  Orders AS o,\n        |  LATERAL TABLE (Rates(o.rowtime)) AS r\n        |WHERE r.currency = o.currency\n        |\"\"\".stripMargin\n\n    tEnv.registerTable(\"TemporalJoinResult\", tEnv.SQLQuery(SQLQuery))\n\n    val result = tEnv.scan(\"TemporalJoinResult\").toAppendStream[Row]\n    // 打印查询结果\n    result.print()\n    env.execute()\n  }\n\n}\n```\n在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的`TimeCharacteristic.EventTime` 模式，需要调用`assignTimestampsAndWatermarks`方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) -  Watermark》。 在本示例中提取EventTime的完整代码如下：\n```\nimport java.SQL.Timestamp\n\nimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor\nimport org.apache.flink.streaming.api.windowing.time.Time\n\nclass OrderTimestampExtractor[T1, T2]\n  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) {\n  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = {\n    element._3.getTime\n  }\n}\n```\n\n查看运行结果:\n![](44FDFE22-3B21-4514-8E91-E339F9188E4E.png)\n\n\n### With CSVConnector 实现代码\n在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。\n* genEventRatesHistorySource\n```\ndef genEventRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#rate\",\n      \"1#US Dollar#102\",\n      \"1#Euro#114\",\n      \"1#Yen#1\",\n      \"3#Euro#116\",\n      \"5#Euro#119\",\n      \"7#Pounds#108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_rate\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\",\"rate\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n```\n* genRatesOrderSource\n```\ndef genRatesOrderSource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#amount\",\n      \"2#Euro#10\",\n      \"4#Euro#10\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_order\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\", \"amount\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n```\n* 主程序代码\n```\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.flink.book.connectors\n\nimport java.io.File\n\nimport org.apache.flink.api.common.typeinfo.{TypeInformation, Types}\nimport org.apache.flink.book.utils.{CommonUtils, FileUtils}\nimport org.apache.flink.table.sinks.{CsvTableSink, TableSink}\nimport org.apache.flink.table.sources.CsvTableSource\nimport org.apache.flink.types.Row\n\nobject CsvTableSourceUtils {\n\n  def genWordCountSource: CsvTableSource = {\n    val csvRecords = Seq(\n      \"words\",\n      \"Hello Flink\",\n      \"Hi, Apache Flink\",\n      \"Apache FlinkBook\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"words\"),\n      Array(\n        Types.STRING\n      ),\n      fieldDelim = \"#\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n\n  def genRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"rowtime ,currency   ,rate\",\n    \"09:00:00   ,US Dollar  , 102\",\n    \"09:00:00   ,Euro       , 114\",\n    \"09:00:00  ,Yen        ,   1\",\n    \"10:45:00   ,Euro       , 116\",\n    \"11:15:00   ,Euro       , 119\",\n    \"11:49:00   ,Pounds     , 108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(\"$\"), \"csv_source_\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"rowtime\",\"currency\",\"rate\"),\n      Array(\n        Types.STRING,Types.STRING,Types.STRING\n      ),\n      fieldDelim = \",\",\n      rowDelim = \"$\",\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n  def genEventRatesHistorySource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#rate\",\n      \"1#US Dollar#102\",\n      \"1#Euro#114\",\n      \"1#Yen#1\",\n      \"3#Euro#116\",\n      \"5#Euro#119\",\n      \"7#Pounds#108\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_rate\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\",\"rate\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n  def genRatesOrderSource: CsvTableSource = {\n\n    val csvRecords = Seq(\n      \"ts#currency#amount\",\n      \"2#Euro#10\",\n      \"4#Euro#10\"\n    )\n    // 测试数据写入临时文件\n    val tempFilePath =\n      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), \"csv_source_order\", \"tmp\")\n\n    // 创建Source connector\n    new CsvTableSource(\n      tempFilePath,\n      Array(\"ts\",\"currency\", \"amount\"),\n      Array(\n        Types.LONG,Types.STRING,Types.LONG\n      ),\n      fieldDelim = \"#\",\n      rowDelim = CommonUtils.line,\n      ignoreFirstLine = true,\n      ignoreComments = \"%\"\n    )\n  }\n\n\n  /**\n    * Example:\n    * genCsvSink(\n    *   Array[String](\"word\", \"count\"),\n    *   Array[TypeInformation[_] ](Types.STRING, Types.LONG))\n    */\n  def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = {\n    val tempFile = File.createTempFile(\"csv_sink_\", \"tem\")\n    if (tempFile.exists()) {\n      tempFile.delete()\n    }\n    new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes)\n  }\n\n}\n```\n运行结果如下 ：\n![](A05E658F-C511-45E2-870F-5E3227A94570.png)\n\n## 内部实现原理\n我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示:\n\n![](07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png)\n\n# Temporal Table JOIN vs 双流JOIN vs Lateral JOIN\n在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？\n\n* 双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。 \n* LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。\n* Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table  或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点\n单流驱动，Temporal Table 是被动查询。\n\n## Temporal Table JOIN vs  LATERAL JOIN\n从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。\n\n## Temporal Table JOIN vs 双流 JOIN\nTemporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。\n\n# Temporal Table JOIN改进\n个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的`FOR SYSTEM_TIME AS OF`标准语法。改进后的处理逻辑示意图:\n![](A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png)\n其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。\n\n# 小结\n本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。\n\n# 关于点赞和评论\n\n本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!","slug":"Apache Flink 漫谈系列 - Temporal Table JOIN","published":1,"updated":"2019-07-13T13:09:56.043Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fl000ns94hhnc4x3rq","content":"<h1>什么是Temporal Table</h1>\n<p>在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。</p>\n<p>在<a href=\"https://sigmodrecord.org/publications/sigmodRecord/1209/pdfs/07.industry.kulkarni.pdf\" target=\"_blank\" rel=\"noopener\">ANSI-SQL 2011</a> 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/D05B8A19-6D97-4809-8723-B4B95FE075A7.png\" alt>\n上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。</p>\n<h1>ANSI-SQL 2011 Temporal Table示例</h1>\n<p>我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：</p>\n<ul>\n<li>\n<p>DDL 示例\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Emp</span><br><span class=\"line\">ENo INTEGER,</span><br><span class=\"line\">Sys_Start TIMESTAMP(12) GENERATED</span><br><span class=\"line\">ALWAYS AS ROW Start,</span><br><span class=\"line\">Sys_end TIMESTAMP(12) GENERATED</span><br><span class=\"line\">ALWAYS AS ROW END,</span><br><span class=\"line\">EName VARCHAR(30),</span><br><span class=\"line\">PERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)</span><br><span class=\"line\">) WITH SYSTEM VERSIONING</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>DML 示例</p>\n</li>\n</ul>\n<ol>\n<li>INSERT\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO Emp (ENo, EName) VALUES (22217, &apos;Joe&apos;)</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png\" alt>\n<strong>说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。</strong></p>\n<ol start=\"2\">\n<li>UPDATE</li>\n</ol>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE Emp SET EName = &apos;Tom&apos; WHERE ENo = 22217</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png\" alt>\n \n<strong>说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值&quot;Joe&quot;的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值&quot;Tom&quot;生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。</strong></p>\n<ol start=\"3\">\n<li>DELETE (假设执行DELETE之前的表内容如下)\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png\" alt>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DELETE FROM Emp WHERE ENo = 22217</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/172A63B1-C30D-43EA-B04E-B9BE8706838B.png\" alt></p>\n<p><strong>说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。</strong></p>\n<ol start=\"4\">\n<li>SELECT\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ENo,EName,Sys_Start,Sys_End FROM Emp </span><br><span class=\"line\">FOR SYSTEM_TIME AS OF TIMESTAMP &apos;2011-01-02 00:00:00&apos;</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><strong>说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。</strong></p>\n<h1>SQLServer Temporal Table 示例</h1>\n<h2>DDL</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Department</span><br><span class=\"line\">(</span><br><span class=\"line\">DeptID int NOT NULL PRIMARY KEY CLUSTERED</span><br><span class=\"line\">, DeptName varchar(50) NOT NULL</span><br><span class=\"line\">, ManagerID INT NULL</span><br><span class=\"line\">, ParentDeptID int NULL</span><br><span class=\"line\">, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL</span><br><span class=\"line\">, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL</span><br><span class=\"line\">, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)</span><br><span class=\"line\">)</span><br><span class=\"line\">WITH (SYSTEM_VERSIONING = ON);</span><br></pre></td></tr></table></figure></p>\n<p>执行上面的语句，在数据库会创建当前表和历史表，如下图：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/6EAC4329-C402-4497-BAE2-7E7B778E613B.png\" alt>\nDepartment 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：<code>SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。</code></p>\n<h2>DML</h2>\n<ul>\n<li>INSERT - 插入列不包含SysStartTime和SysEndTime列\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])</span><br><span class=\"line\">VALUES(10, &apos;Marketing&apos;, 101, 1);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行之后我们分别查询当前表和历史表，如下图：<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png\" alt>\n我们第一条INSERT语句数据值的有效时间是操作那一刻<code>2018-06-06 05:50:20.7913985</code> 到永远 <code>9999-12-31 23:59:59.9999999</code>，但这时刻历史表还没有任何信息。我们接下来进行更新操作。</p>\n<ul>\n<li>UPDATE\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/75AC8012-59C7-4105-AE66-4453384E762B.png\" alt>\n注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png\" alt>\n到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。</p>\n<ul>\n<li>SELECT \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]</span><br><span class=\"line\">FROM [dbo].[Department]</span><br><span class=\"line\">FOR SYSTEM_TIME AS OF &apos;2018-06-06 05:50:21.0000000&apos; ;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A94E2FAD-F04A-4691-8C34-3C697FF81476.png\" alt>\nSELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 <code>SysStartTime &lt;= '2018-06-06 05:50:21.0000000' and SysEndTime &gt; '2018-06-06 05:50:21.0000000'</code> 。</p>\n<h1>Apache Flink Temporal Table</h1>\n<p>我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用<code>FOR SYSTEM_TIME AS OF</code>的语法，目前Apache Flink中使用 <code>LATERAL TABLE(TemporalTableFunction)</code>的语法。这一点后续需要推动社区进行改进。</p>\n<h2>为啥需要 Temporal Table</h2>\n<p>我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下：</p>\n<table>\n<thead>\n<tr>\n<th>rowtime(ts)</th>\n<th>currency(pk)</th>\n<th>rate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>09:00:00</td>\n<td>US Dollar</td>\n<td>102</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Euro</td>\n<td>114</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Yen</td>\n<td>1</td>\n</tr>\n<tr>\n<td>10:45:00</td>\n<td>Euro</td>\n<td>116</td>\n</tr>\n<tr>\n<td>11:15:00</td>\n<td>Euro</td>\n<td>119</td>\n</tr>\n<tr>\n<td>11:49:00</td>\n<td>Pounds</td>\n<td>108</td>\n</tr>\n</tbody>\n</table>\n<p>RatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。</p>\n<p>假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT *</span><br><span class=\"line\">FROM RatesHistory AS r</span><br><span class=\"line\">WHERE r.rowtime = (</span><br><span class=\"line\">  SELECT MAX(rowtime)</span><br><span class=\"line\">  FROM RatesHistory AS r2</span><br><span class=\"line\">  WHERE r2.currency = r.currency</span><br><span class=\"line\">  AND r2.rowtime &lt;= &apos;10:58&apos;);</span><br></pre></td></tr></table></figure></p>\n<p>相应Flink代码如下：</p>\n<ul>\n<li>\n<p>定义数据源-genRatesHistorySource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class=\"line\">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class=\"line\">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class=\"line\">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class=\"line\">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\"> writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING,Types.STRING,Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;,&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  def writeToTempFile(</span><br><span class=\"line\">    contents: String,</span><br><span class=\"line\">    filePrefix: String,</span><br><span class=\"line\">    fileSuffix: String,</span><br><span class=\"line\">    charset: String = &quot;UTF-8&quot;): String = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(filePrefix, fileSuffix)</span><br><span class=\"line\">    val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset)</span><br><span class=\"line\">    tmpWriter.write(contents)</span><br><span class=\"line\">    tmpWriter.close()</span><br><span class=\"line\">    tempFile.getAbsolutePath</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>主程序代码\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;RatesHistory&quot;</span><br><span class=\"line\">    // 创建CSV source数据结构</span><br><span class=\"line\">    val tableSource = CsvTableSourceUtils.genRatesHistorySource</span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册retract sink</span><br><span class=\"line\">    val sinkTableName = &quot;retractSink&quot;</span><br><span class=\"line\">    val fieldNames = Array(&quot;rowtime&quot;, &quot;currency&quot;, &quot;rate&quot;)</span><br><span class=\"line\">    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING)</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTableSink(</span><br><span class=\"line\">      sinkTableName,</span><br><span class=\"line\">      fieldNames,</span><br><span class=\"line\">      fieldTypes,</span><br><span class=\"line\">      new MemoryRetractSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQL =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT *</span><br><span class=\"line\">        |FROM RatesHistory AS r</span><br><span class=\"line\">        |WHERE r.rowtime = (</span><br><span class=\"line\">        |  SELECT MAX(rowtime)</span><br><span class=\"line\">        |  FROM RatesHistory AS r2</span><br><span class=\"line\">        |  WHERE r2.currency = r.currency</span><br><span class=\"line\">        |  AND r2.rowtime &lt;= &apos;10:58:00&apos;  )</span><br><span class=\"line\">      &quot;&quot;&quot;.stripMargin</span><br><span class=\"line\"></span><br><span class=\"line\">    // 执行查询</span><br><span class=\"line\">    val result = tEnv.SQLQuery(SQL)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 将结果插入sink</span><br><span class=\"line\">    result.insertInto(sinkTableName)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>执行结果如下图：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png\" alt>\n结果表格化一下：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>rowtime(ts)</th>\n<th>currency(pk)</th>\n<th>rate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>09:00:00</td>\n<td>US Dollar</td>\n<td>102</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Yen</td>\n<td>1</td>\n</tr>\n<tr>\n<td>10:45:00</td>\n<td>Euro</td>\n<td>116</td>\n</tr>\n</tbody>\n</table>\n<p>Temporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append  Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。</p>\n<p>在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。</p>\n<h2>如何定义Temporal Table</h2>\n<p>在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。</p>\n<ul>\n<li>\n<p>内部TemporalTableFunction定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class TemporalTableFunction private(</span><br><span class=\"line\">    @transient private val underlyingHistoryTable: Table,</span><br><span class=\"line\">    // 时间属性，相当于版本信息</span><br><span class=\"line\">    private val timeAttribute: Expression,</span><br><span class=\"line\">    // 主键定义</span><br><span class=\"line\">    private val primaryKey: String,</span><br><span class=\"line\">    private val resultType: RowTypeInfo)</span><br><span class=\"line\">  extends TableFunction[Row] &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>用户创建TemporalTableFunction方式\n在<code>Table</code>中添加了<code>createTemporalTableFunction</code>方法，该方法需要传入时间属性和主键，接口定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Creates TemporalTableFunction backed up by this table as a history table.</span><br><span class=\"line\"></span><br><span class=\"line\">def createTemporalTableFunction(</span><br><span class=\"line\">      timeAttribute: Expression,</span><br><span class=\"line\">      primaryKey: Expression): TemporalTableFunction = &#123;</span><br><span class=\"line\">   ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val tab = ...</span><br><span class=\"line\">val temporalTableFunction = tab.createTemporalTableFunction(&apos;time, &apos;pk)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>案例代码</h2>\n<ul>\n<li>需求描述\n假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。</li>\n<li>Orders 数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>amount</th>\n<th>currency</th>\n<th>order_time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2</td>\n<td>Euro</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1</td>\n<td>US Dollar</td>\n<td>3</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Yen</td>\n<td>4</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Euro</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Rates 数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>currency</th>\n<th>rate</th>\n<th>rate_time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>US Dollar</td>\n<td>102</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>114</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Yen</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>116</td>\n<td>5</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>117</td>\n<td>7</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p>统计需求对应的SQL\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT o.currency, o.amount, r.rate</span><br><span class=\"line\">  o.amount * r.rate AS yen_amount</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o,</span><br><span class=\"line\">  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class=\"line\">WHERE r.currency = o.currency</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>预期结果</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>currency</th>\n<th>amount</th>\n<th>rate</th>\n<th>yen_amount</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>US Dollar</td>\n<td>1</td>\n<td>102</td>\n<td>102</td>\n</tr>\n<tr>\n<td>Yen</td>\n<td>50</td>\n<td>1</td>\n<td>50</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>2</td>\n<td>114</td>\n<td>228</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>3</td>\n<td>116</td>\n<td>348</td>\n</tr>\n</tbody>\n</table>\n<h3>Without connnector 实现代码</h3>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object TemporalTableJoinTest &#123;</span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\">// 设置时间类型是 event-time  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 构造订单数据</span><br><span class=\"line\">    val ordersData = new mutable.MutableList[(Long, String, Timestamp)]</span><br><span class=\"line\">    ordersData.+=((2L, &quot;Euro&quot;, new Timestamp(2L)))</span><br><span class=\"line\">    ordersData.+=((1L, &quot;US Dollar&quot;, new Timestamp(3L)))</span><br><span class=\"line\">    ordersData.+=((50L, &quot;Yen&quot;, new Timestamp(4L)))</span><br><span class=\"line\">    ordersData.+=((3L, &quot;Euro&quot;, new Timestamp(5L)))</span><br><span class=\"line\"></span><br><span class=\"line\">    //构造汇率数据</span><br><span class=\"line\">    val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)]</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;US Dollar&quot;, 102L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 114L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Yen&quot;, 1L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 116L, new Timestamp(5L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 119L, new Timestamp(7L)))</span><br><span class=\"line\"></span><br><span class=\"line\">// 进行订单表 event-time 的提取</span><br><span class=\"line\">    val orders = env</span><br><span class=\"line\">      .fromCollection(ordersData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;amount, &apos;currency, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">// 进行汇率表 event-time 的提取</span><br><span class=\"line\">    val ratesHistory = env</span><br><span class=\"line\">      .fromCollection(ratesHistoryData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;currency, &apos;rate, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">// 注册订单表和汇率表</span><br><span class=\"line\">    tEnv.registerTable(&quot;Orders&quot;, orders)</span><br><span class=\"line\">    tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory)</span><br><span class=\"line\">    val tab = tEnv.scan(&quot;RatesHistory&quot;);</span><br><span class=\"line\">// 创建TemporalTableFunction</span><br><span class=\"line\">    val temporalTableFunction = tab.createTemporalTableFunction(&apos;rowtime, &apos;currency)</span><br><span class=\"line\">//注册TemporalTableFunction</span><br><span class=\"line\">tEnv.registerFunction(&quot;Rates&quot;,temporalTableFunction)</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQLQuery =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT o.currency, o.amount, r.rate,</span><br><span class=\"line\">        |  o.amount * r.rate AS yen_amount</span><br><span class=\"line\">        |FROM</span><br><span class=\"line\">        |  Orders AS o,</span><br><span class=\"line\">        |  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class=\"line\">        |WHERE r.currency = o.currency</span><br><span class=\"line\">        |&quot;&quot;&quot;.stripMargin</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.SQLQuery(SQLQuery))</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row]</span><br><span class=\"line\">    // 打印查询结果</span><br><span class=\"line\">    result.print()</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的<code>TimeCharacteristic.EventTime</code> 模式，需要调用<code>assignTimestampsAndWatermarks</code>方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) -  Watermark》。 在本示例中提取EventTime的完整代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.SQL.Timestamp</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class=\"line\">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class=\"line\"></span><br><span class=\"line\">class OrderTimestampExtractor[T1, T2]</span><br><span class=\"line\">  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123;</span><br><span class=\"line\">  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123;</span><br><span class=\"line\">    element._3.getTime</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>查看运行结果:\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/44FDFE22-3B21-4514-8E91-E339F9188E4E.png\" alt></p>\n<h3>With CSVConnector 实现代码</h3>\n<p>在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。</p>\n<ul>\n<li>\n<p>genEventRatesHistorySource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#rate&quot;,</span><br><span class=\"line\">      &quot;1#US Dollar#102&quot;,</span><br><span class=\"line\">      &quot;1#Euro#114&quot;,</span><br><span class=\"line\">      &quot;1#Yen#1&quot;,</span><br><span class=\"line\">      &quot;3#Euro#116&quot;,</span><br><span class=\"line\">      &quot;5#Euro#119&quot;,</span><br><span class=\"line\">      &quot;7#Pounds#108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>genRatesOrderSource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#amount&quot;,</span><br><span class=\"line\">      &quot;2#Euro#10&quot;,</span><br><span class=\"line\">      &quot;4#Euro#10&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>主程序代码\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class=\"line\"> * or more contributor license agreements.  See the NOTICE file</span><br><span class=\"line\"> * distributed with this work for additional information</span><br><span class=\"line\"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class=\"line\"> * to you under the Apache License, Version 2.0 (the</span><br><span class=\"line\"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class=\"line\"> * with the License.  You may obtain a copy of the License at</span><br><span class=\"line\"> *</span><br><span class=\"line\"> *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * Unless required by applicable law or agreed to in writing, software</span><br><span class=\"line\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class=\"line\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class=\"line\"> * See the License for the specific language governing permissions and</span><br><span class=\"line\"> * limitations under the License.</span><br><span class=\"line\"> */</span><br><span class=\"line\"></span><br><span class=\"line\">package org.apache.flink.book.connectors</span><br><span class=\"line\"></span><br><span class=\"line\">import java.io.File</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.api.common.typeinfo.&#123;TypeInformation, Types&#125;</span><br><span class=\"line\">import org.apache.flink.book.utils.&#123;CommonUtils, FileUtils&#125;</span><br><span class=\"line\">import org.apache.flink.table.sinks.&#123;CsvTableSink, TableSink&#125;</span><br><span class=\"line\">import org.apache.flink.table.sources.CsvTableSource</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\"></span><br><span class=\"line\">object CsvTableSourceUtils &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genWordCountSource: CsvTableSource = &#123;</span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;words&quot;,</span><br><span class=\"line\">      &quot;Hello Flink&quot;,</span><br><span class=\"line\">      &quot;Hi, Apache Flink&quot;,</span><br><span class=\"line\">      &quot;Apache FlinkBook&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;words&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class=\"line\">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class=\"line\">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class=\"line\">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class=\"line\">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING,Types.STRING,Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;,&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#rate&quot;,</span><br><span class=\"line\">      &quot;1#US Dollar#102&quot;,</span><br><span class=\"line\">      &quot;1#Euro#114&quot;,</span><br><span class=\"line\">      &quot;1#Yen#1&quot;,</span><br><span class=\"line\">      &quot;3#Euro#116&quot;,</span><br><span class=\"line\">      &quot;5#Euro#119&quot;,</span><br><span class=\"line\">      &quot;7#Pounds#108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#amount&quot;,</span><br><span class=\"line\">      &quot;2#Euro#10&quot;,</span><br><span class=\"line\">      &quot;4#Euro#10&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">    * Example:</span><br><span class=\"line\">    * genCsvSink(</span><br><span class=\"line\">    *   Array[String](&quot;word&quot;, &quot;count&quot;),</span><br><span class=\"line\">    *   Array[TypeInformation[_] ](Types.STRING, Types.LONG))</span><br><span class=\"line\">    */</span><br><span class=\"line\">  def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>运行结果如下 ：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A05E658F-C511-45E2-870F-5E3227A94570.png\" alt></p>\n<h2>内部实现原理</h2>\n<p>我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示:</p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png\" alt></p>\n<h1>Temporal Table JOIN vs 双流JOIN vs Lateral JOIN</h1>\n<p>在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？</p>\n<ul>\n<li>双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。</li>\n<li>LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。</li>\n<li>Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table  或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点\n单流驱动，Temporal Table 是被动查询。</li>\n</ul>\n<h2>Temporal Table JOIN vs  LATERAL JOIN</h2>\n<p>从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。</p>\n<h2>Temporal Table JOIN vs 双流 JOIN</h2>\n<p>Temporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。</p>\n<h1>Temporal Table JOIN改进</h1>\n<p>个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的<code>FOR SYSTEM_TIME AS OF</code>标准语法。改进后的处理逻辑示意图:\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png\" alt>\n其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。</p>\n<h1>小结</h1>\n<p>本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>什么是Temporal Table</h1>\n<p>在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。</p>\n<p>在<a href=\"https://sigmodrecord.org/publications/sigmodRecord/1209/pdfs/07.industry.kulkarni.pdf\" target=\"_blank\" rel=\"noopener\">ANSI-SQL 2011</a> 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/D05B8A19-6D97-4809-8723-B4B95FE075A7.png\" alt>\n上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。</p>\n<h1>ANSI-SQL 2011 Temporal Table示例</h1>\n<p>我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：</p>\n<ul>\n<li>\n<p>DDL 示例\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Emp</span><br><span class=\"line\">ENo INTEGER,</span><br><span class=\"line\">Sys_Start TIMESTAMP(12) GENERATED</span><br><span class=\"line\">ALWAYS AS ROW Start,</span><br><span class=\"line\">Sys_end TIMESTAMP(12) GENERATED</span><br><span class=\"line\">ALWAYS AS ROW END,</span><br><span class=\"line\">EName VARCHAR(30),</span><br><span class=\"line\">PERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)</span><br><span class=\"line\">) WITH SYSTEM VERSIONING</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>DML 示例</p>\n</li>\n</ul>\n<ol>\n<li>INSERT\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO Emp (ENo, EName) VALUES (22217, &apos;Joe&apos;)</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png\" alt>\n<strong>说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。</strong></p>\n<ol start=\"2\">\n<li>UPDATE</li>\n</ol>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE Emp SET EName = &apos;Tom&apos; WHERE ENo = 22217</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png\" alt>\n \n<strong>说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值&quot;Joe&quot;的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值&quot;Tom&quot;生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。</strong></p>\n<ol start=\"3\">\n<li>DELETE (假设执行DELETE之前的表内容如下)\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png\" alt>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DELETE FROM Emp WHERE ENo = 22217</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/172A63B1-C30D-43EA-B04E-B9BE8706838B.png\" alt></p>\n<p><strong>说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。</strong></p>\n<ol start=\"4\">\n<li>SELECT\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ENo,EName,Sys_Start,Sys_End FROM Emp </span><br><span class=\"line\">FOR SYSTEM_TIME AS OF TIMESTAMP &apos;2011-01-02 00:00:00&apos;</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p><strong>说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。</strong></p>\n<h1>SQLServer Temporal Table 示例</h1>\n<h2>DDL</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE Department</span><br><span class=\"line\">(</span><br><span class=\"line\">DeptID int NOT NULL PRIMARY KEY CLUSTERED</span><br><span class=\"line\">, DeptName varchar(50) NOT NULL</span><br><span class=\"line\">, ManagerID INT NULL</span><br><span class=\"line\">, ParentDeptID int NULL</span><br><span class=\"line\">, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL</span><br><span class=\"line\">, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL</span><br><span class=\"line\">, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)</span><br><span class=\"line\">)</span><br><span class=\"line\">WITH (SYSTEM_VERSIONING = ON);</span><br></pre></td></tr></table></figure></p>\n<p>执行上面的语句，在数据库会创建当前表和历史表，如下图：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/6EAC4329-C402-4497-BAE2-7E7B778E613B.png\" alt>\nDepartment 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：<code>SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。</code></p>\n<h2>DML</h2>\n<ul>\n<li>INSERT - 插入列不包含SysStartTime和SysEndTime列\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])</span><br><span class=\"line\">VALUES(10, &apos;Marketing&apos;, 101, 1);</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行之后我们分别查询当前表和历史表，如下图：<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png\" alt>\n我们第一条INSERT语句数据值的有效时间是操作那一刻<code>2018-06-06 05:50:20.7913985</code> 到永远 <code>9999-12-31 23:59:59.9999999</code>，但这时刻历史表还没有任何信息。我们接下来进行更新操作。</p>\n<ul>\n<li>UPDATE\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/75AC8012-59C7-4105-AE66-4453384E762B.png\" alt>\n注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png\" alt>\n到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。</p>\n<ul>\n<li>SELECT \n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]</span><br><span class=\"line\">FROM [dbo].[Department]</span><br><span class=\"line\">FOR SYSTEM_TIME AS OF &apos;2018-06-06 05:50:21.0000000&apos; ;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A94E2FAD-F04A-4691-8C34-3C697FF81476.png\" alt>\nSELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 <code>SysStartTime &lt;= '2018-06-06 05:50:21.0000000' and SysEndTime &gt; '2018-06-06 05:50:21.0000000'</code> 。</p>\n<h1>Apache Flink Temporal Table</h1>\n<p>我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用<code>FOR SYSTEM_TIME AS OF</code>的语法，目前Apache Flink中使用 <code>LATERAL TABLE(TemporalTableFunction)</code>的语法。这一点后续需要推动社区进行改进。</p>\n<h2>为啥需要 Temporal Table</h2>\n<p>我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下：</p>\n<table>\n<thead>\n<tr>\n<th>rowtime(ts)</th>\n<th>currency(pk)</th>\n<th>rate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>09:00:00</td>\n<td>US Dollar</td>\n<td>102</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Euro</td>\n<td>114</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Yen</td>\n<td>1</td>\n</tr>\n<tr>\n<td>10:45:00</td>\n<td>Euro</td>\n<td>116</td>\n</tr>\n<tr>\n<td>11:15:00</td>\n<td>Euro</td>\n<td>119</td>\n</tr>\n<tr>\n<td>11:49:00</td>\n<td>Pounds</td>\n<td>108</td>\n</tr>\n</tbody>\n</table>\n<p>RatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。</p>\n<p>假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT *</span><br><span class=\"line\">FROM RatesHistory AS r</span><br><span class=\"line\">WHERE r.rowtime = (</span><br><span class=\"line\">  SELECT MAX(rowtime)</span><br><span class=\"line\">  FROM RatesHistory AS r2</span><br><span class=\"line\">  WHERE r2.currency = r.currency</span><br><span class=\"line\">  AND r2.rowtime &lt;= &apos;10:58&apos;);</span><br></pre></td></tr></table></figure></p>\n<p>相应Flink代码如下：</p>\n<ul>\n<li>\n<p>定义数据源-genRatesHistorySource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class=\"line\">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class=\"line\">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class=\"line\">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class=\"line\">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\"> writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING,Types.STRING,Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;,&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  def writeToTempFile(</span><br><span class=\"line\">    contents: String,</span><br><span class=\"line\">    filePrefix: String,</span><br><span class=\"line\">    fileSuffix: String,</span><br><span class=\"line\">    charset: String = &quot;UTF-8&quot;): String = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(filePrefix, fileSuffix)</span><br><span class=\"line\">    val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset)</span><br><span class=\"line\">    tmpWriter.write(contents)</span><br><span class=\"line\">    tmpWriter.close()</span><br><span class=\"line\">    tempFile.getAbsolutePath</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>主程序代码\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    // Streaming 环境</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">    //方便我们查出输出数据</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\"></span><br><span class=\"line\">    val sourceTableName = &quot;RatesHistory&quot;</span><br><span class=\"line\">    // 创建CSV source数据结构</span><br><span class=\"line\">    val tableSource = CsvTableSourceUtils.genRatesHistorySource</span><br><span class=\"line\">    // 注册source</span><br><span class=\"line\">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 注册retract sink</span><br><span class=\"line\">    val sinkTableName = &quot;retractSink&quot;</span><br><span class=\"line\">    val fieldNames = Array(&quot;rowtime&quot;, &quot;currency&quot;, &quot;rate&quot;)</span><br><span class=\"line\">    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING)</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTableSink(</span><br><span class=\"line\">      sinkTableName,</span><br><span class=\"line\">      fieldNames,</span><br><span class=\"line\">      fieldTypes,</span><br><span class=\"line\">      new MemoryRetractSink)</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQL =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT *</span><br><span class=\"line\">        |FROM RatesHistory AS r</span><br><span class=\"line\">        |WHERE r.rowtime = (</span><br><span class=\"line\">        |  SELECT MAX(rowtime)</span><br><span class=\"line\">        |  FROM RatesHistory AS r2</span><br><span class=\"line\">        |  WHERE r2.currency = r.currency</span><br><span class=\"line\">        |  AND r2.rowtime &lt;= &apos;10:58:00&apos;  )</span><br><span class=\"line\">      &quot;&quot;&quot;.stripMargin</span><br><span class=\"line\"></span><br><span class=\"line\">    // 执行查询</span><br><span class=\"line\">    val result = tEnv.SQLQuery(SQL)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 将结果插入sink</span><br><span class=\"line\">    result.insertInto(sinkTableName)</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>执行结果如下图：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png\" alt>\n结果表格化一下：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>rowtime(ts)</th>\n<th>currency(pk)</th>\n<th>rate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>09:00:00</td>\n<td>US Dollar</td>\n<td>102</td>\n</tr>\n<tr>\n<td>09:00:00</td>\n<td>Yen</td>\n<td>1</td>\n</tr>\n<tr>\n<td>10:45:00</td>\n<td>Euro</td>\n<td>116</td>\n</tr>\n</tbody>\n</table>\n<p>Temporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append  Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。</p>\n<p>在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。</p>\n<h2>如何定义Temporal Table</h2>\n<p>在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。</p>\n<ul>\n<li>\n<p>内部TemporalTableFunction定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class TemporalTableFunction private(</span><br><span class=\"line\">    @transient private val underlyingHistoryTable: Table,</span><br><span class=\"line\">    // 时间属性，相当于版本信息</span><br><span class=\"line\">    private val timeAttribute: Expression,</span><br><span class=\"line\">    // 主键定义</span><br><span class=\"line\">    private val primaryKey: String,</span><br><span class=\"line\">    private val resultType: RowTypeInfo)</span><br><span class=\"line\">  extends TableFunction[Row] &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>用户创建TemporalTableFunction方式\n在<code>Table</code>中添加了<code>createTemporalTableFunction</code>方法，该方法需要传入时间属性和主键，接口定义如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Creates TemporalTableFunction backed up by this table as a history table.</span><br><span class=\"line\"></span><br><span class=\"line\">def createTemporalTableFunction(</span><br><span class=\"line\">      timeAttribute: Expression,</span><br><span class=\"line\">      primaryKey: Expression): TemporalTableFunction = &#123;</span><br><span class=\"line\">   ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val tab = ...</span><br><span class=\"line\">val temporalTableFunction = tab.createTemporalTableFunction(&apos;time, &apos;pk)</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h2>案例代码</h2>\n<ul>\n<li>需求描述\n假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。</li>\n<li>Orders 数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>amount</th>\n<th>currency</th>\n<th>order_time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2</td>\n<td>Euro</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1</td>\n<td>US Dollar</td>\n<td>3</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Yen</td>\n<td>4</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Euro</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Rates 数据</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>currency</th>\n<th>rate</th>\n<th>rate_time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>US Dollar</td>\n<td>102</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>114</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Yen</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>116</td>\n<td>5</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>117</td>\n<td>7</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p>统计需求对应的SQL\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT o.currency, o.amount, r.rate</span><br><span class=\"line\">  o.amount * r.rate AS yen_amount</span><br><span class=\"line\">FROM</span><br><span class=\"line\">  Orders AS o,</span><br><span class=\"line\">  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class=\"line\">WHERE r.currency = o.currency</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>预期结果</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>currency</th>\n<th>amount</th>\n<th>rate</th>\n<th>yen_amount</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>US Dollar</td>\n<td>1</td>\n<td>102</td>\n<td>102</td>\n</tr>\n<tr>\n<td>Yen</td>\n<td>50</td>\n<td>1</td>\n<td>50</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>2</td>\n<td>114</td>\n<td>228</td>\n</tr>\n<tr>\n<td>Euro</td>\n<td>3</td>\n<td>116</td>\n<td>348</td>\n</tr>\n</tbody>\n</table>\n<h3>Without connnector 实现代码</h3>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object TemporalTableJoinTest &#123;</span><br><span class=\"line\">  def main(args: Array[String]): Unit = &#123;</span><br><span class=\"line\">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\">    env.setParallelism(1)</span><br><span class=\"line\">// 设置时间类型是 event-time  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">    // 构造订单数据</span><br><span class=\"line\">    val ordersData = new mutable.MutableList[(Long, String, Timestamp)]</span><br><span class=\"line\">    ordersData.+=((2L, &quot;Euro&quot;, new Timestamp(2L)))</span><br><span class=\"line\">    ordersData.+=((1L, &quot;US Dollar&quot;, new Timestamp(3L)))</span><br><span class=\"line\">    ordersData.+=((50L, &quot;Yen&quot;, new Timestamp(4L)))</span><br><span class=\"line\">    ordersData.+=((3L, &quot;Euro&quot;, new Timestamp(5L)))</span><br><span class=\"line\"></span><br><span class=\"line\">    //构造汇率数据</span><br><span class=\"line\">    val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)]</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;US Dollar&quot;, 102L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 114L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Yen&quot;, 1L, new Timestamp(1L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 116L, new Timestamp(5L)))</span><br><span class=\"line\">    ratesHistoryData.+=((&quot;Euro&quot;, 119L, new Timestamp(7L)))</span><br><span class=\"line\"></span><br><span class=\"line\">// 进行订单表 event-time 的提取</span><br><span class=\"line\">    val orders = env</span><br><span class=\"line\">      .fromCollection(ordersData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;amount, &apos;currency, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">// 进行汇率表 event-time 的提取</span><br><span class=\"line\">    val ratesHistory = env</span><br><span class=\"line\">      .fromCollection(ratesHistoryData)</span><br><span class=\"line\">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]())</span><br><span class=\"line\">      .toTable(tEnv, &apos;currency, &apos;rate, &apos;rowtime.rowtime)</span><br><span class=\"line\"></span><br><span class=\"line\">// 注册订单表和汇率表</span><br><span class=\"line\">    tEnv.registerTable(&quot;Orders&quot;, orders)</span><br><span class=\"line\">    tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory)</span><br><span class=\"line\">    val tab = tEnv.scan(&quot;RatesHistory&quot;);</span><br><span class=\"line\">// 创建TemporalTableFunction</span><br><span class=\"line\">    val temporalTableFunction = tab.createTemporalTableFunction(&apos;rowtime, &apos;currency)</span><br><span class=\"line\">//注册TemporalTableFunction</span><br><span class=\"line\">tEnv.registerFunction(&quot;Rates&quot;,temporalTableFunction)</span><br><span class=\"line\"></span><br><span class=\"line\">    val SQLQuery =</span><br><span class=\"line\">      &quot;&quot;&quot;</span><br><span class=\"line\">        |SELECT o.currency, o.amount, r.rate,</span><br><span class=\"line\">        |  o.amount * r.rate AS yen_amount</span><br><span class=\"line\">        |FROM</span><br><span class=\"line\">        |  Orders AS o,</span><br><span class=\"line\">        |  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class=\"line\">        |WHERE r.currency = o.currency</span><br><span class=\"line\">        |&quot;&quot;&quot;.stripMargin</span><br><span class=\"line\"></span><br><span class=\"line\">    tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.SQLQuery(SQLQuery))</span><br><span class=\"line\"></span><br><span class=\"line\">    val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row]</span><br><span class=\"line\">    // 打印查询结果</span><br><span class=\"line\">    result.print()</span><br><span class=\"line\">    env.execute()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的<code>TimeCharacteristic.EventTime</code> 模式，需要调用<code>assignTimestampsAndWatermarks</code>方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) -  Watermark》。 在本示例中提取EventTime的完整代码如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.SQL.Timestamp</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class=\"line\">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class=\"line\"></span><br><span class=\"line\">class OrderTimestampExtractor[T1, T2]</span><br><span class=\"line\">  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123;</span><br><span class=\"line\">  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123;</span><br><span class=\"line\">    element._3.getTime</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>查看运行结果:\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/44FDFE22-3B21-4514-8E91-E339F9188E4E.png\" alt></p>\n<h3>With CSVConnector 实现代码</h3>\n<p>在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。</p>\n<ul>\n<li>\n<p>genEventRatesHistorySource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#rate&quot;,</span><br><span class=\"line\">      &quot;1#US Dollar#102&quot;,</span><br><span class=\"line\">      &quot;1#Euro#114&quot;,</span><br><span class=\"line\">      &quot;1#Yen#1&quot;,</span><br><span class=\"line\">      &quot;3#Euro#116&quot;,</span><br><span class=\"line\">      &quot;5#Euro#119&quot;,</span><br><span class=\"line\">      &quot;7#Pounds#108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>genRatesOrderSource\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#amount&quot;,</span><br><span class=\"line\">      &quot;2#Euro#10&quot;,</span><br><span class=\"line\">      &quot;4#Euro#10&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>主程序代码\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class=\"line\"> * or more contributor license agreements.  See the NOTICE file</span><br><span class=\"line\"> * distributed with this work for additional information</span><br><span class=\"line\"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class=\"line\"> * to you under the Apache License, Version 2.0 (the</span><br><span class=\"line\"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class=\"line\"> * with the License.  You may obtain a copy of the License at</span><br><span class=\"line\"> *</span><br><span class=\"line\"> *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * Unless required by applicable law or agreed to in writing, software</span><br><span class=\"line\"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class=\"line\"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class=\"line\"> * See the License for the specific language governing permissions and</span><br><span class=\"line\"> * limitations under the License.</span><br><span class=\"line\"> */</span><br><span class=\"line\"></span><br><span class=\"line\">package org.apache.flink.book.connectors</span><br><span class=\"line\"></span><br><span class=\"line\">import java.io.File</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.flink.api.common.typeinfo.&#123;TypeInformation, Types&#125;</span><br><span class=\"line\">import org.apache.flink.book.utils.&#123;CommonUtils, FileUtils&#125;</span><br><span class=\"line\">import org.apache.flink.table.sinks.&#123;CsvTableSink, TableSink&#125;</span><br><span class=\"line\">import org.apache.flink.table.sources.CsvTableSource</span><br><span class=\"line\">import org.apache.flink.types.Row</span><br><span class=\"line\"></span><br><span class=\"line\">object CsvTableSourceUtils &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genWordCountSource: CsvTableSource = &#123;</span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;words&quot;,</span><br><span class=\"line\">      &quot;Hello Flink&quot;,</span><br><span class=\"line\">      &quot;Hi, Apache Flink&quot;,</span><br><span class=\"line\">      &quot;Apache FlinkBook&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;words&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class=\"line\">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class=\"line\">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class=\"line\">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class=\"line\">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class=\"line\">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.STRING,Types.STRING,Types.STRING</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;,&quot;,</span><br><span class=\"line\">      rowDelim = &quot;$&quot;,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#rate&quot;,</span><br><span class=\"line\">      &quot;1#US Dollar#102&quot;,</span><br><span class=\"line\">      &quot;1#Euro#114&quot;,</span><br><span class=\"line\">      &quot;1#Yen#1&quot;,</span><br><span class=\"line\">      &quot;3#Euro#116&quot;,</span><br><span class=\"line\">      &quot;5#Euro#119&quot;,</span><br><span class=\"line\">      &quot;7#Pounds#108&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val csvRecords = Seq(</span><br><span class=\"line\">      &quot;ts#currency#amount&quot;,</span><br><span class=\"line\">      &quot;2#Euro#10&quot;,</span><br><span class=\"line\">      &quot;4#Euro#10&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">    // 测试数据写入临时文件</span><br><span class=\"line\">    val tempFilePath =</span><br><span class=\"line\">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // 创建Source connector</span><br><span class=\"line\">    new CsvTableSource(</span><br><span class=\"line\">      tempFilePath,</span><br><span class=\"line\">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class=\"line\">      Array(</span><br><span class=\"line\">        Types.LONG,Types.STRING,Types.LONG</span><br><span class=\"line\">      ),</span><br><span class=\"line\">      fieldDelim = &quot;#&quot;,</span><br><span class=\"line\">      rowDelim = CommonUtils.line,</span><br><span class=\"line\">      ignoreFirstLine = true,</span><br><span class=\"line\">      ignoreComments = &quot;%&quot;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">    * Example:</span><br><span class=\"line\">    * genCsvSink(</span><br><span class=\"line\">    *   Array[String](&quot;word&quot;, &quot;count&quot;),</span><br><span class=\"line\">    *   Array[TypeInformation[_] ](Types.STRING, Types.LONG))</span><br><span class=\"line\">    */</span><br><span class=\"line\">  def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = &#123;</span><br><span class=\"line\">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class=\"line\">    if (tempFile.exists()) &#123;</span><br><span class=\"line\">      tempFile.delete()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>运行结果如下 ：\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A05E658F-C511-45E2-870F-5E3227A94570.png\" alt></p>\n<h2>内部实现原理</h2>\n<p>我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示:</p>\n<p><img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png\" alt></p>\n<h1>Temporal Table JOIN vs 双流JOIN vs Lateral JOIN</h1>\n<p>在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？</p>\n<ul>\n<li>双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。</li>\n<li>LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。</li>\n<li>Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table  或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点\n单流驱动，Temporal Table 是被动查询。</li>\n</ul>\n<h2>Temporal Table JOIN vs  LATERAL JOIN</h2>\n<p>从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。</p>\n<h2>Temporal Table JOIN vs 双流 JOIN</h2>\n<p>Temporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。</p>\n<h1>Temporal Table JOIN改进</h1>\n<p>个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的<code>FOR SYSTEM_TIME AS OF</code>标准语法。改进后的处理逻辑示意图:\n<img src=\"/2019/03/18/Apache Flink 漫谈系列 - Temporal Table JOIN/A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png\" alt>\n其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。</p>\n<h1>小结</h1>\n<p>本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。</p>\n<h1>关于点赞和评论</h1>\n<p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>\n"},{"title":"Apache Flink 视频系列 - 2019.08.06直播(德国柏林)","date":"2019-08-08T10:18:18.000Z","_content":"# 3W\n - What: 演讲主题《Apache Flink Python API 现状及规划》\n - Where: 德国柏林ververic办公区\n - When: 2019.08.06，德国时间下午14点，北京时间晚上20点\n\n# 视频概要\n通过今天的分享，大家会有如下三方面的收获：\n\n- 了解到Apache Flink Python API的前世今生，以及未来的规划。\n- 了解到Apache Flink Python API 在1.9中的全新架构和清楚的知道如果搭建Python API的开发环境，了解如何开发Python API，并以各种方式提交Python 作业。\n- 也可以掌握Python API的核心的算子的语义和使用方法\n\n你可以在即将发布的Flink 1.9之后，轻松的进行Python API的开发。\n\n# 相关资源\n- 视频\n![](3C082C16-792A-4824-8112-9FD3C87768B5.png)\n[视频回放](http://1t.click/Azb) \n- 代码\n[https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806](https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806)\n- PPT\nhttp://1t.click/A6s\n\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n\n\n\n","source":"_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林).md","raw":"\n---\ntitle: Apache Flink 视频系列 - 2019.08.06直播(德国柏林)\ndate: 2019-08-08 18:18:18\ncategories: Apache Flink 视频\ntags: [Flink, 视频，Python]\n---\n# 3W\n - What: 演讲主题《Apache Flink Python API 现状及规划》\n - Where: 德国柏林ververic办公区\n - When: 2019.08.06，德国时间下午14点，北京时间晚上20点\n\n# 视频概要\n通过今天的分享，大家会有如下三方面的收获：\n\n- 了解到Apache Flink Python API的前世今生，以及未来的规划。\n- 了解到Apache Flink Python API 在1.9中的全新架构和清楚的知道如果搭建Python API的开发环境，了解如何开发Python API，并以各种方式提交Python 作业。\n- 也可以掌握Python API的核心的算子的语义和使用方法\n\n你可以在即将发布的Flink 1.9之后，轻松的进行Python API的开发。\n\n# 相关资源\n- 视频\n![](3C082C16-792A-4824-8112-9FD3C87768B5.png)\n[视频回放](http://1t.click/Azb) \n- 代码\n[https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806](https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806)\n- PPT\nhttp://1t.click/A6s\n\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n\n\n\n","slug":"Apache Flink 视频系列 - 2019.08.06直播(德国柏林)","published":1,"updated":"2019-08-09T06:55:56.739Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fo000qs94hfz1hpgj7","content":"<h1>3W</h1>\n<ul>\n<li>What: 演讲主题《Apache Flink Python API 现状及规划》</li>\n<li>Where: 德国柏林ververic办公区</li>\n<li>When: 2019.08.06，德国时间下午14点，北京时间晚上20点</li>\n</ul>\n<h1>视频概要</h1>\n<p>通过今天的分享，大家会有如下三方面的收获：</p>\n<ul>\n<li>了解到Apache Flink Python API的前世今生，以及未来的规划。</li>\n<li>了解到Apache Flink Python API 在1.9中的全新架构和清楚的知道如果搭建Python API的开发环境，了解如何开发Python API，并以各种方式提交Python 作业。</li>\n<li>也可以掌握Python API的核心的算子的语义和使用方法</li>\n</ul>\n<p>你可以在即将发布的Flink 1.9之后，轻松的进行Python API的开发。</p>\n<h1>相关资源</h1>\n<ul>\n<li>视频\n<img src=\"/2019/08/08/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/3C082C16-792A-4824-8112-9FD3C87768B5.png\" alt>\n<a href=\"http://1t.click/Azb\" target=\"_blank\" rel=\"noopener\">视频回放</a></li>\n<li>代码\n<a href=\"https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806</a></li>\n<li>PPT\nhttp://1t.click/A6s</li>\n</ul>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/08/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>3W</h1>\n<ul>\n<li>What: 演讲主题《Apache Flink Python API 现状及规划》</li>\n<li>Where: 德国柏林ververic办公区</li>\n<li>When: 2019.08.06，德国时间下午14点，北京时间晚上20点</li>\n</ul>\n<h1>视频概要</h1>\n<p>通过今天的分享，大家会有如下三方面的收获：</p>\n<ul>\n<li>了解到Apache Flink Python API的前世今生，以及未来的规划。</li>\n<li>了解到Apache Flink Python API 在1.9中的全新架构和清楚的知道如果搭建Python API的开发环境，了解如何开发Python API，并以各种方式提交Python 作业。</li>\n<li>也可以掌握Python API的核心的算子的语义和使用方法</li>\n</ul>\n<p>你可以在即将发布的Flink 1.9之后，轻松的进行Python API的开发。</p>\n<h1>相关资源</h1>\n<ul>\n<li>视频\n<img src=\"/2019/08/08/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/3C082C16-792A-4824-8112-9FD3C87768B5.png\" alt>\n<a href=\"http://1t.click/Azb\" target=\"_blank\" rel=\"noopener\">视频回放</a></li>\n<li>代码\n<a href=\"https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806</a></li>\n<li>PPT\nhttp://1t.click/A6s</li>\n</ul>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/08/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/comment.png\" alt></p>\n"},{"title":"Apache Flink 漫谈系列 - 流表对偶(duality)性","date":"2019-01-01T10:18:18.000Z","updated":"2019-01-28T10:18:18.000Z","_content":"# 实际问题\n很多大数据计算产品，都对用户提供了SQL API，比如Hive, Spark, Flink等，那么SQL作为传统关系数据库的查询语言，是应用在批查询场景的。Hive和Spark本质上都是Batch的计算模式(在《Apache Flink 漫谈系列 - 概述》我们介绍过Spark是Micro Batching模式)，提供SQL API很容易被人理解，但是Flink是纯流（Native Streaming）的计算模式, 流与批在数据集和计算过程上有很大的区别.\n\n\n如图所示：\n\n![](EF81F039-86CC-4799-8DCF-F3B0144B76EC.png)\n\n* 批查询场景的特点 - 有限数据集，一次查询返回一个计算结果就结束查询\n\n* 流查询场景的特点 - 无限数据集，一次查询不断修正计算结果，查询永远不结束\n\n我们发现批与流的查询场景在数据集合和计算过程上都有很大的不同，那么基于Native Streaming模式的Apache Flink为啥也能为用户提供SQL API呢？\n\n# 流与批的语义关系\n我们知道SQL都是作用于关系表的，在传统数据库中进行查询时候，SQL所要查询的表在触发查询时候数据是不会变化的，也就是说在查询那一刻，表是一张静态表，相当于是一个有限的批数据，这样也说明SQL是源于对批计算的查询的，那么要回答Apache Flink为啥也能为用户提供SQL API，我们首先要理解流与批在语义层面的关系。我们以一个具体示例说明，如下图：\n![](3794F522-3458-4386-A101-9458FD2D8DBF.png)\n上图展现的是一个携带时间戳和用户名的点击事件流，我们先对这些事件流进行流式统计，同时在最后的流事件上触发批计算。流计算中每接收一个数据都会触发一次计算，我们以2018/4/30 22:37:45 Mary到来那一时间切片看，无论是在流还是批上计算结果都是6。也就是说在相同的数据源，相同的查询逻辑下，流和批的计算结果是相同的。相同的SQL在流和批这两种模式下，最终结果是一致的，那么流与批在语义上是完全相同的。\n\n# 流与表的关系\n流与批在语义上是一致的，SQL是作用于表的，那么要回答Apache Flink为啥也能为用户提供SQL API的问题，就变成了流与表是否具有等价性，也就是本篇要重点介绍的为什么流表具有对偶(duality)性？如下图所示，一张表可以看做为流吗？同样流可以看做是一张表吗？如果可以需要怎样的条件和变通？\n![](41690DE4-8705-4305-BF74-AC47F8C870B3.png)\n\n# MySQL主备复制\n在介绍流与表的关系之前我们先聊聊MySQL的主备复制，binlog是MySQL实现主备复制的核心手段，简单来说MySQL主备复制实现分成三个步骤：\n* Master将改变(change logs)以二进制日志事件(binary log events)形式记录到二进制日志(binlog)中；\n* Slave将Master的binary log events拷贝到它的中继日志(relay log)；\n* Slave重做中继日志中的事件，将改变反映到数据；\n\n具体如下图所示:\n   ![](FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png)\n   \n# binlog\n接下来我们从binlog模式，binlog格式以及通过查看binlog的具体内容来详尽介绍binlog与表的关系。\n## binlog模式\n上面介绍的MySQL主备复制的核心手段是利用binlog实现的，那边binlog会记录那些内容呢？binlog记录了数据库所有的增、删、更新等操作。MySQL支持三种方式记录binlog:\n* statement-based logging - Events contain SQL statements that produce data changes (inserts, updates, deletes);\n* row-based logging - Events describe changes to individual rows;\n* mixed-base logging - 该模式默认是statement-based，当遇到如下情况会自动切换到row-based:\n    * NDB存储引擎，DML操作以row格式记录;\n    * 使用UUID()、USER()、CURRENT_USER()、FOUND_ROWS()等不确定函数;\n    * 使用Insert Delay语句;\n    * 使用用户自定义函数(UDF);\n    * 使用临时表;\n    \n## binlog格式\n我们以row-based 模式为例介绍一下[binlog的存储格式](https://dev.MySQL.com/doc/internals/en/event-structure.html) ，所有的 binary log events都是字节序列，由两部分组成：\n* event header\n* event data\n\n关于event header和event data 的格式在数据库的不同版本略有不同，但共同的地方如下：\n```\n+=====================================+\n| event  | timestamp         0 : 4    |\n| header +----------------------------+\n|        | type_code         4 : 1    |\n|        +----------------------------+\n|        | server_id         5 : 4    |\n|        +----------------------------+\n|        | event_length      9 : 4    |\n|        +----------------------------+\n|        |不同版本不一样(省略)          |\n+=====================================+\n| event  | fixed part                 |\n| data   +----------------------------+\n|        | variable part              |\n+=====================================+\n```\n这里有个值得我们注意的地方就是在binlog的header中有一个属性是timestamp，这个属性是标识了change发生的先后顺序，在备库进行复制时候会严格按照时间顺序进行log的重放。\n\n## binlog的生成\n我们以对MySQL进行实际操作的方式，直观的介绍一下binlog的生成，binlog是二进制存储的，下面我们会利用工具查看binlog的文本内容。\n\n* 查看一下binlog是否打开：\n```\nshow variables like 'log_bin'\n    -&gt; ;\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| log_bin       | ON    |\n+---------------+-------+\n1 row in set (0.00 sec)\n```\n* 查看一下binlog的模式(我需要row-base模式)：\n```\nshow variables like 'binlog_format';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set (0.00 sec)\n```\n* 清除现有的binlog\n```\nMySQL&gt; reset master;\nQuery OK, 0 rows affected (0.00 sec)创建一张我们做实验的表MySQL&gt; create table tab(\n    -&gt;    id INT NOT NULL AUTO_INCREMENT,\n    -&gt;    user VARCHAR(100) NOT NULL,\n    -&gt;    clicks INT NOT NULL,\n    -&gt;    PRIMARY KEY (id)\n    -&gt; );\nQuery OK, 0 rows affected (0.10 sec)\n\nMySQL&gt; show tables;\n+-------------------+\n| Tables_in_Apache Flinkdb |\n+-------------------+\n| tab         |\n+-------------------+\n1 row in set (0.00 sec)\n```\n\n* 进行DML操作\n```\nMySQL&gt; insert into tab(user, clicks) values ('Mary', 1);\nQuery OK, 1 row affected (0.03 sec)\n\nMySQL&gt; insert into tab(user, clicks) values ('Bob', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL&gt; update tab set clicks=2 where user='Mary'\n    -&gt; ;\nQuery OK, 1 row affected (0.06 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; insert into tab(user, clicks) values ('Llz', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL&gt; update tab set clicks=2 where user='Bob';\nQuery OK, 1 row affected (0.01 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; update tab set clicks=3 where user='Mary';\nQuery OK, 1 row affected (0.05 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; select * from tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      3 |\n|  2 | Bob  |      2 |\n|  3 | Llz  |      1 |\n+----+------+--------+\n3 rows in set (0.00 sec)\n```\n* 查看正在操作的binlog\n```\nMySQL&gt; show master status\\G\n*************************** 1. row ***************************\n             File: binlog.000001\n         Position: 2547\n     Binlog_Do_DB: \n Binlog_Ignore_DB: \nExecuted_Gtid_Set: \n1 row in set (0.00 sec)\n```\n上面 binlog.000001 文件是我们正在操作的binlog。\n\n* 查看binlog.000001文件的操作记录\n```\nMySQL&gt; show binlog events in 'binlog.000001';\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Log_name      | Pos  | Event_type     | Server_id | End_log_pos | Info                                                                                                                                                                |\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| binlog.000001 |    4 | Format_desc    |         1 |         124 | Server ver: 8.0.11, Binlog ver: 4                                                                                                                                   |\n| binlog.000001 |  124 | Previous_gtids |         1 |         155 |                                                                                                                                                                     |\n| binlog.000001 |  155 | Anonymous_Gtid |         1 |         228 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  228 | Query          |         1 |         368 | use `Apache Flinkdb`; DROP TABLE `tab` /* generated by server */ /* xid=22 */                                                                                        |\n| binlog.000001 |  368 | Anonymous_Gtid |         1 |         443 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  443 | Query          |         1 |         670 | use `Apache Flinkdb`; create table tab(\n   id INT NOT NULL AUTO_INCREMENT,\n   user VARCHAR(100) NOT NULL,\n   clicks INT NOT NULL,\n   PRIMARY KEY (id)\n) /* xid=23 */ |\n| binlog.000001 |  670 | Anonymous_Gtid |         1 |         745 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  745 | Query          |         1 |         823 | BEGIN                                                                                                                                                               |\n| binlog.000001 |  823 | Table_map      |         1 |         890 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 |  890 | Write_rows     |         1 |         940 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 |  940 | Xid            |         1 |         971 | COMMIT /* xid=25 */                                                                                                                                                 |\n| binlog.000001 |  971 | Anonymous_Gtid |         1 |        1046 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1046 | Query          |         1 |        1124 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1124 | Table_map      |         1 |        1191 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1191 | Write_rows     |         1 |        1240 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1240 | Xid            |         1 |        1271 | COMMIT /* xid=26 */                                                                                                                                                 |\n| binlog.000001 | 1271 | Anonymous_Gtid |         1 |        1346 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1346 | Query          |         1 |        1433 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1433 | Table_map      |         1 |        1500 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1500 | Update_rows    |         1 |        1566 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1566 | Xid            |         1 |        1597 | COMMIT /* xid=27 */                                                                                                                                                 |\n| binlog.000001 | 1597 | Anonymous_Gtid |         1 |        1672 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1672 | Query          |         1 |        1750 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1750 | Table_map      |         1 |        1817 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1817 | Write_rows     |         1 |        1866 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1866 | Xid            |         1 |        1897 | COMMIT /* xid=28 */                                                                                                                                                 |\n| binlog.000001 | 1897 | Anonymous_Gtid |         1 |        1972 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1972 | Query          |         1 |        2059 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 2059 | Table_map      |         1 |        2126 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 2126 | Update_rows    |         1 |        2190 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 2190 | Xid            |         1 |        2221 | COMMIT /* xid=29 */                                                                                                                                                 |\n| binlog.000001 | 2221 | Anonymous_Gtid |         1 |        2296 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 2296 | Query          |         1 |        2383 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 2383 | Table_map      |         1 |        2450 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 2450 | Update_rows    |         1 |        2516 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 2516 | Xid            |         1 |        2547 | COMMIT /* xid=30 */                                                                                                                                                 |\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n36 rows in set (0.00 sec)\n```\n上面我们进行了3次insert和3次update，那么在binlog中我们看到了三条Write_rows和三条Update_rows，并且在记录顺序和操作顺序保持一致，接下来我们看看Write_rows和Update_rows的具体timestamp和data明文。\n\n* 导出明文\n```\nsudo MySQLbinlog --start-datetime='2018-04-29 00:00:03' --stop-datetime='2018-05-02 00:30:00' --base64-output=decode-rows -v /usr/local/MySQL/data/binlog.000001 &gt; ~/binlog.txt\n```\n\n打开binlog.txt 内容如下:\n```\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;\n/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;\nDELIMITER /*!*/;\n# at 4\n#180430 22:29:33 server id 1  end_log_pos 124 CRC32 0xff61797c  Start: binlog v 4, server v 8.0.11 created 180430 22:29:33 at startup\n# Warning: this binlog is either in use or was not closed properly.\nROLLBACK/*!*/;\n# at 124\n#180430 22:29:33 server id 1  end_log_pos 155 CRC32 0x629ae755  Previous-GTIDs\n# [empty]\n# at 155\n#180430 22:32:11 server id 1  end_log_pos 228 CRC32 0xbde49fca  Anonymous_GTID  last_committed=0        sequence_number=1       rbr_only=no     original_committed_timestamp=1525098731207902   immediate_commit_timestamp=1525098731207902     transaction_length=213\n# original_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)\n# immediate_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525098731207902*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 228\n#180430 22:32:11 server id 1  end_log_pos 368 CRC32 0xe5f330e7  Query   thread_id=9     exec_time=0     error_code=0    Xid = 22\nuse `Apache Flinkdb`/*!*/;\nSET TIMESTAMP=1525098731/*!*/;\nSET @@session.pseudo_thread_id=9/*!*/;\nSET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;\nSET @@session.sql_mode=1168113696/*!*/;\nSET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;\n/*!\\C utf8mb4 *//*!*/;\nSET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;\nSET @@session.lc_time_names=0/*!*/;\nSET @@session.collation_database=DEFAULT/*!*/;\n/*!80005 SET @@session.default_collation_for_utf8mb4=255*//*!*/;\nDROP TABLE `tab` /* generated by server */\n/*!*/;\n# at 368\n#180430 22:32:21 server id 1  end_log_pos 443 CRC32 0x50e5acb7  Anonymous_GTID  last_committed=1        sequence_number=2       rbr_only=no     original_committed_timestamp=1525098741628960   immediate_commit_timestamp=1525098741628960     transaction_length=302\n# original_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)\n# immediate_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525098741628960*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 443\n#180430 22:32:21 server id 1  end_log_pos 670 CRC32 0xe1353dd6  Query   thread_id=9     exec_time=0     error_code=0    Xid = 23\nSET TIMESTAMP=1525098741/*!*/;\ncreate table tab(\n   id INT NOT NULL AUTO_INCREMENT,\n   user VARCHAR(100) NOT NULL,\n   clicks INT NOT NULL,\n   PRIMARY KEY (id)\n)\n/*!*/;\n# at 670\n#180430 22:36:53 server id 1  end_log_pos 745 CRC32 0xcf436fbb  Anonymous_GTID  last_committed=2        sequence_number=3       rbr_only=yes    original_committed_timestamp=1525099013988373   immediate_commit_timestamp=1525099013988373     transaction_length=301\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)\n# immediate_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099013988373*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 745\n#180430 22:36:53 server id 1  end_log_pos 823 CRC32 0x71c64dd2  Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099013/*!*/;\nBEGIN\n/*!*/;\n# at 823\n#180430 22:36:53 server id 1  end_log_pos 890 CRC32 0x63792f6b  Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 890\n#180430 22:36:53 server id 1  end_log_pos 940 CRC32 0xf2dade22  Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=1\n# at 940\n#180430 22:36:53 server id 1  end_log_pos 971 CRC32 0x7db3e61e  Xid = 25\nCOMMIT/*!*/;\n# at 971\n#180430 22:37:06 server id 1  end_log_pos 1046 CRC32 0xd05dd12c         Anonymous_GTID  last_committed=3        sequence_number=4       rbr_only=yes    original_committed_timestamp=1525099026328547   immediate_commit_timestamp=1525099026328547     transaction_length=300\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)\n# immediate_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099026328547*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1046\n#180430 22:37:06 server id 1  end_log_pos 1124 CRC32 0x80f259e0         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099026/*!*/;\nBEGIN\n/*!*/;\n# at 1124\n#180430 22:37:06 server id 1  end_log_pos 1191 CRC32 0x255903ba         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1191\n#180430 22:37:06 server id 1  end_log_pos 1240 CRC32 0xe76bfc79         Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=2\n###   @2='Bob'\n###   @3=1\n# at 1240\n#180430 22:37:06 server id 1  end_log_pos 1271 CRC32 0x83cddfef         Xid = 26\nCOMMIT/*!*/;\n# at 1271\n#180430 22:37:15 server id 1  end_log_pos 1346 CRC32 0x7095baee         Anonymous_GTID  last_committed=4        sequence_number=5       rbr_only=yes    original_committed_timestamp=1525099035811597   immediate_commit_timestamp=1525099035811597     transaction_length=326\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)\n# immediate_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099035811597*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1346\n#180430 22:37:15 server id 1  end_log_pos 1433 CRC32 0x70ef97e2         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099035/*!*/;\nBEGIN\n/*!*/;\n# at 1433\n#180430 22:37:15 server id 1  end_log_pos 1500 CRC32 0x75f1f399         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1500\n#180430 22:37:15 server id 1  end_log_pos 1566 CRC32 0x256bd4b8         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=1\n###   @2='Mary'\n###   @3=1\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=2\n# at 1566\n#180430 22:37:15 server id 1  end_log_pos 1597 CRC32 0x93c86579         Xid = 27\nCOMMIT/*!*/;\n# at 1597\n#180430 22:37:27 server id 1  end_log_pos 1672 CRC32 0xe8bd63e7         Anonymous_GTID  last_committed=5        sequence_number=6       rbr_only=yes    original_committed_timestamp=1525099047219517   immediate_commit_timestamp=1525099047219517     transaction_length=300\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)\n# immediate_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099047219517*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1672\n#180430 22:37:27 server id 1  end_log_pos 1750 CRC32 0x5356c3c7         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099047/*!*/;\nBEGIN\n/*!*/;\n# at 1750\n#180430 22:37:27 server id 1  end_log_pos 1817 CRC32 0x37e6b1ce         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1817\n#180430 22:37:27 server id 1  end_log_pos 1866 CRC32 0x6ab1bbe6         Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=3\n###   @2='Llz'\n###   @3=1\n# at 1866\n#180430 22:37:27 server id 1  end_log_pos 1897 CRC32 0x3b62b153         Xid = 28\nCOMMIT/*!*/;\n# at 1897\n#180430 22:37:36 server id 1  end_log_pos 1972 CRC32 0x603134c1         Anonymous_GTID  last_committed=6        sequence_number=7       rbr_only=yes    original_committed_timestamp=1525099056866022   immediate_commit_timestamp=1525099056866022     transaction_length=324\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)\n# immediate_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099056866022*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1972\n#180430 22:37:36 server id 1  end_log_pos 2059 CRC32 0xe17df4e4         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099056/*!*/;\nBEGIN\n/*!*/;\n# at 2059\n#180430 22:37:36 server id 1  end_log_pos 2126 CRC32 0x53888b05         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 2126\n#180430 22:37:36 server id 1  end_log_pos 2190 CRC32 0x85f34996         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=2\n###   @2='Bob'\n###   @3=1\n### SET\n###   @1=2\n###   @2='Bob'\n###   @3=2\n# at 2190\n#180430 22:37:36 server id 1  end_log_pos 2221 CRC32 0x877f1e23         Xid = 29\nCOMMIT/*!*/;\n# at 2221\n#180430 22:37:45 server id 1  end_log_pos 2296 CRC32 0xfbc7e868         Anonymous_GTID  last_committed=7        sequence_number=8       rbr_only=yes    original_committed_timestamp=1525099065089940   immediate_commit_timestamp=1525099065089940     transaction_length=326\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)\n# immediate_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099065089940*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 2296\n#180430 22:37:45 server id 1  end_log_pos 2383 CRC32 0x8a514364         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099065/*!*/;\nBEGIN\n/*!*/;\n# at 2383\n#180430 22:37:45 server id 1  end_log_pos 2450 CRC32 0xdf18ca60         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 2450\n#180430 22:37:45 server id 1  end_log_pos 2516 CRC32 0xd50de69f         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=1\n###   @2='Mary'\n###   @3=2\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=3\n# at 2516\n#180430 22:37:45 server id 1  end_log_pos 2547 CRC32 0x94f89393         Xid = 30\nCOMMIT/*!*/;\nSET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by MySQLbinlog */ /*!*/;\nDELIMITER ;\n# End of log file\n/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n```\n* 梳理操作和binlog的记录关系\n<div class=\"table-contianer\"><table><tbody><tr><td style=\"width: 200px;\">DML</td><td style=\"width: 216.667px;\">binlog-header(timestamp)</td><td style=\"width: 201.111px;\">data</td></tr><tr><td style=\"width: 200px;\">insert into blink_tab(user, clicks) values ('Mary', 1);</td><td style=\"width: 216.667px;\"><p>1525099013</p><p>(2018/4/30 22:36:53)</p></td><td style=\"width: 201.111px;\">## INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=1</td></tr><tr><td style=\"width: 200px;\">insert into blink_tab(user, clicks) values ('Bob', 1);</td><td style=\"width: 216.667px;\"><p>1525099026</p><p>(2018/4/30 22:37:06)</p></td><td style=\"width: 201.111px;\">### INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=2<br />### @2='Bob'<br />### @3=1&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=2 where user='Mary';</td><td style=\"width: 216.667px;\"><p>&nbsp;1525099035</p><p>(2018/4/30 22:37:15)</p></td><td style=\"width: 201.111px;\">&nbsp;### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=1<br />### @2='Mary'<br />### @3=1<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=2</td></tr><tr><td style=\"width: 200px;\">&nbsp;insert into blink_tab(user, clicks) values ('Llz', 1);</td><td style=\"width: 216.667px;\"><p>1525099047</p><p>(2018/4/30 22:37:27)&nbsp;</p></td><td style=\"width: 201.111px;\">### INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=3<br />### @2='Llz'<br />### @3=1&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=2 where user='Bob';</td><td style=\"width: 216.667px;\"><p>&nbsp;1525099056</p><p>(2018/4/30 22:37:36)</p></td><td style=\"width: 201.111px;\">### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=2<br />### @2='Bob'<br />### @3=1<br />### SET<br />### @1=2<br />### @2='Bob'<br />### @3=2&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=3 where user='Mary';</td><td style=\"width: 216.667px;\"><p>1525099065</p><p>(2018/4/30 22:37:45)&nbsp;</p></td><td style=\"width: 201.111px;\">### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=1<br />### @2='Mary'<br />### @3=2<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=3&nbsp;</td></tr></tbody></table></div>\n\n* 简化一下binlog\n![](DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png)\n\n* replay binlog会得到如下表数据（按timestamp顺序)\n![](2052C4F1-2EE4-41D1-8A8D-165A40827462.png)\n\n* 表与binlog的关系简单示意如下\n![](F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png)\n# 流表对偶(duality)性\n前面我花费了一些时间介绍了MySQL主备复制机制和binlog的数据格式，binlog中携带时间戳，我们将所有表的操作都按时间进行记录下来形成binlog，而对binlog的event进行重放的过程就是流数据处理的过程，重放的结果恰恰又形成了一张表。也就是表的操作会形成携带时间的事件流，对流的处理又会形成一张不断变化的表，表和流具有等价性，可以互转。随着时间推移，DML操作不断进行，那么表的内容也不断变化，具体如下：\n\n![](299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png)\n\n如上图所示内容，流和表具备相同的特征：\n* 表 - Schema，Data，DML操作时间\n* 流 - Schema，Data, Data处理时间\n\n我们发现，虽然大多数表上面没有明确的显示出DML操作时间，但本质上数据库系统里面是有数据操作时间信息的，这个和流上数据的处理时间（processing time)/产生时间（event-time)相对应。流与表具备相同的特征，可以信息无损的相互转换，我称之为流表对偶(duality)性。\n\n上面我们描述的表，在流上称之为动态表(Dynamic Table)，原因是在流上面任何一个事件的到来都是对表上数据的一次更新(包括插入和删除),表的内容是不断的变化的，任何一个时刻流的状态和表的快照一一对应。流与动态表(Dynamic Table)在时间维度上面具有等价性，这种等价性我们称之为流和动态表(Dynamic Table)的对偶(duality)性。\n\n# 小结\n本篇主要介绍Apache Flink作为一个流计算平台为什么可以为用户提供SQL API。其根本原因是如果将流上的数据看做是结构化的数据，流任务的核心是将一个具有时间属性的结构化数据变成同样具有时间属性的另一个结构化数据，而表的数据变化过程binlog恰恰就是一份具有时间属性的流数据，流与表具有信息无损的相互转换的特性，这种流表对偶性也决定了Apache Flink可以采用SQL作为流任务的开发语言。\n\n","source":"_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性.md","raw":"---\ntitle: Apache Flink 漫谈系列 - 流表对偶(duality)性\ndate: 2019-01-01 18:18:18\nupdated: 2019-01-28 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 实际问题\n很多大数据计算产品，都对用户提供了SQL API，比如Hive, Spark, Flink等，那么SQL作为传统关系数据库的查询语言，是应用在批查询场景的。Hive和Spark本质上都是Batch的计算模式(在《Apache Flink 漫谈系列 - 概述》我们介绍过Spark是Micro Batching模式)，提供SQL API很容易被人理解，但是Flink是纯流（Native Streaming）的计算模式, 流与批在数据集和计算过程上有很大的区别.\n\n\n如图所示：\n\n![](EF81F039-86CC-4799-8DCF-F3B0144B76EC.png)\n\n* 批查询场景的特点 - 有限数据集，一次查询返回一个计算结果就结束查询\n\n* 流查询场景的特点 - 无限数据集，一次查询不断修正计算结果，查询永远不结束\n\n我们发现批与流的查询场景在数据集合和计算过程上都有很大的不同，那么基于Native Streaming模式的Apache Flink为啥也能为用户提供SQL API呢？\n\n# 流与批的语义关系\n我们知道SQL都是作用于关系表的，在传统数据库中进行查询时候，SQL所要查询的表在触发查询时候数据是不会变化的，也就是说在查询那一刻，表是一张静态表，相当于是一个有限的批数据，这样也说明SQL是源于对批计算的查询的，那么要回答Apache Flink为啥也能为用户提供SQL API，我们首先要理解流与批在语义层面的关系。我们以一个具体示例说明，如下图：\n![](3794F522-3458-4386-A101-9458FD2D8DBF.png)\n上图展现的是一个携带时间戳和用户名的点击事件流，我们先对这些事件流进行流式统计，同时在最后的流事件上触发批计算。流计算中每接收一个数据都会触发一次计算，我们以2018/4/30 22:37:45 Mary到来那一时间切片看，无论是在流还是批上计算结果都是6。也就是说在相同的数据源，相同的查询逻辑下，流和批的计算结果是相同的。相同的SQL在流和批这两种模式下，最终结果是一致的，那么流与批在语义上是完全相同的。\n\n# 流与表的关系\n流与批在语义上是一致的，SQL是作用于表的，那么要回答Apache Flink为啥也能为用户提供SQL API的问题，就变成了流与表是否具有等价性，也就是本篇要重点介绍的为什么流表具有对偶(duality)性？如下图所示，一张表可以看做为流吗？同样流可以看做是一张表吗？如果可以需要怎样的条件和变通？\n![](41690DE4-8705-4305-BF74-AC47F8C870B3.png)\n\n# MySQL主备复制\n在介绍流与表的关系之前我们先聊聊MySQL的主备复制，binlog是MySQL实现主备复制的核心手段，简单来说MySQL主备复制实现分成三个步骤：\n* Master将改变(change logs)以二进制日志事件(binary log events)形式记录到二进制日志(binlog)中；\n* Slave将Master的binary log events拷贝到它的中继日志(relay log)；\n* Slave重做中继日志中的事件，将改变反映到数据；\n\n具体如下图所示:\n   ![](FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png)\n   \n# binlog\n接下来我们从binlog模式，binlog格式以及通过查看binlog的具体内容来详尽介绍binlog与表的关系。\n## binlog模式\n上面介绍的MySQL主备复制的核心手段是利用binlog实现的，那边binlog会记录那些内容呢？binlog记录了数据库所有的增、删、更新等操作。MySQL支持三种方式记录binlog:\n* statement-based logging - Events contain SQL statements that produce data changes (inserts, updates, deletes);\n* row-based logging - Events describe changes to individual rows;\n* mixed-base logging - 该模式默认是statement-based，当遇到如下情况会自动切换到row-based:\n    * NDB存储引擎，DML操作以row格式记录;\n    * 使用UUID()、USER()、CURRENT_USER()、FOUND_ROWS()等不确定函数;\n    * 使用Insert Delay语句;\n    * 使用用户自定义函数(UDF);\n    * 使用临时表;\n    \n## binlog格式\n我们以row-based 模式为例介绍一下[binlog的存储格式](https://dev.MySQL.com/doc/internals/en/event-structure.html) ，所有的 binary log events都是字节序列，由两部分组成：\n* event header\n* event data\n\n关于event header和event data 的格式在数据库的不同版本略有不同，但共同的地方如下：\n```\n+=====================================+\n| event  | timestamp         0 : 4    |\n| header +----------------------------+\n|        | type_code         4 : 1    |\n|        +----------------------------+\n|        | server_id         5 : 4    |\n|        +----------------------------+\n|        | event_length      9 : 4    |\n|        +----------------------------+\n|        |不同版本不一样(省略)          |\n+=====================================+\n| event  | fixed part                 |\n| data   +----------------------------+\n|        | variable part              |\n+=====================================+\n```\n这里有个值得我们注意的地方就是在binlog的header中有一个属性是timestamp，这个属性是标识了change发生的先后顺序，在备库进行复制时候会严格按照时间顺序进行log的重放。\n\n## binlog的生成\n我们以对MySQL进行实际操作的方式，直观的介绍一下binlog的生成，binlog是二进制存储的，下面我们会利用工具查看binlog的文本内容。\n\n* 查看一下binlog是否打开：\n```\nshow variables like 'log_bin'\n    -&gt; ;\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| log_bin       | ON    |\n+---------------+-------+\n1 row in set (0.00 sec)\n```\n* 查看一下binlog的模式(我需要row-base模式)：\n```\nshow variables like 'binlog_format';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n1 row in set (0.00 sec)\n```\n* 清除现有的binlog\n```\nMySQL&gt; reset master;\nQuery OK, 0 rows affected (0.00 sec)创建一张我们做实验的表MySQL&gt; create table tab(\n    -&gt;    id INT NOT NULL AUTO_INCREMENT,\n    -&gt;    user VARCHAR(100) NOT NULL,\n    -&gt;    clicks INT NOT NULL,\n    -&gt;    PRIMARY KEY (id)\n    -&gt; );\nQuery OK, 0 rows affected (0.10 sec)\n\nMySQL&gt; show tables;\n+-------------------+\n| Tables_in_Apache Flinkdb |\n+-------------------+\n| tab         |\n+-------------------+\n1 row in set (0.00 sec)\n```\n\n* 进行DML操作\n```\nMySQL&gt; insert into tab(user, clicks) values ('Mary', 1);\nQuery OK, 1 row affected (0.03 sec)\n\nMySQL&gt; insert into tab(user, clicks) values ('Bob', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL&gt; update tab set clicks=2 where user='Mary'\n    -&gt; ;\nQuery OK, 1 row affected (0.06 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; insert into tab(user, clicks) values ('Llz', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL&gt; update tab set clicks=2 where user='Bob';\nQuery OK, 1 row affected (0.01 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; update tab set clicks=3 where user='Mary';\nQuery OK, 1 row affected (0.05 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL&gt; select * from tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      3 |\n|  2 | Bob  |      2 |\n|  3 | Llz  |      1 |\n+----+------+--------+\n3 rows in set (0.00 sec)\n```\n* 查看正在操作的binlog\n```\nMySQL&gt; show master status\\G\n*************************** 1. row ***************************\n             File: binlog.000001\n         Position: 2547\n     Binlog_Do_DB: \n Binlog_Ignore_DB: \nExecuted_Gtid_Set: \n1 row in set (0.00 sec)\n```\n上面 binlog.000001 文件是我们正在操作的binlog。\n\n* 查看binlog.000001文件的操作记录\n```\nMySQL&gt; show binlog events in 'binlog.000001';\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Log_name      | Pos  | Event_type     | Server_id | End_log_pos | Info                                                                                                                                                                |\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| binlog.000001 |    4 | Format_desc    |         1 |         124 | Server ver: 8.0.11, Binlog ver: 4                                                                                                                                   |\n| binlog.000001 |  124 | Previous_gtids |         1 |         155 |                                                                                                                                                                     |\n| binlog.000001 |  155 | Anonymous_Gtid |         1 |         228 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  228 | Query          |         1 |         368 | use `Apache Flinkdb`; DROP TABLE `tab` /* generated by server */ /* xid=22 */                                                                                        |\n| binlog.000001 |  368 | Anonymous_Gtid |         1 |         443 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  443 | Query          |         1 |         670 | use `Apache Flinkdb`; create table tab(\n   id INT NOT NULL AUTO_INCREMENT,\n   user VARCHAR(100) NOT NULL,\n   clicks INT NOT NULL,\n   PRIMARY KEY (id)\n) /* xid=23 */ |\n| binlog.000001 |  670 | Anonymous_Gtid |         1 |         745 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 |  745 | Query          |         1 |         823 | BEGIN                                                                                                                                                               |\n| binlog.000001 |  823 | Table_map      |         1 |         890 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 |  890 | Write_rows     |         1 |         940 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 |  940 | Xid            |         1 |         971 | COMMIT /* xid=25 */                                                                                                                                                 |\n| binlog.000001 |  971 | Anonymous_Gtid |         1 |        1046 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1046 | Query          |         1 |        1124 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1124 | Table_map      |         1 |        1191 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1191 | Write_rows     |         1 |        1240 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1240 | Xid            |         1 |        1271 | COMMIT /* xid=26 */                                                                                                                                                 |\n| binlog.000001 | 1271 | Anonymous_Gtid |         1 |        1346 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1346 | Query          |         1 |        1433 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1433 | Table_map      |         1 |        1500 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1500 | Update_rows    |         1 |        1566 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1566 | Xid            |         1 |        1597 | COMMIT /* xid=27 */                                                                                                                                                 |\n| binlog.000001 | 1597 | Anonymous_Gtid |         1 |        1672 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1672 | Query          |         1 |        1750 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 1750 | Table_map      |         1 |        1817 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 1817 | Write_rows     |         1 |        1866 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 1866 | Xid            |         1 |        1897 | COMMIT /* xid=28 */                                                                                                                                                 |\n| binlog.000001 | 1897 | Anonymous_Gtid |         1 |        1972 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 1972 | Query          |         1 |        2059 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 2059 | Table_map      |         1 |        2126 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 2126 | Update_rows    |         1 |        2190 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 2190 | Xid            |         1 |        2221 | COMMIT /* xid=29 */                                                                                                                                                 |\n| binlog.000001 | 2221 | Anonymous_Gtid |         1 |        2296 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                                                                                                                |\n| binlog.000001 | 2296 | Query          |         1 |        2383 | BEGIN                                                                                                                                                               |\n| binlog.000001 | 2383 | Table_map      |         1 |        2450 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |\n| binlog.000001 | 2450 | Update_rows    |         1 |        2516 | table_id: 96 flags: STMT_END_F                                                                                                                                      |\n| binlog.000001 | 2516 | Xid            |         1 |        2547 | COMMIT /* xid=30 */                                                                                                                                                 |\n+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n36 rows in set (0.00 sec)\n```\n上面我们进行了3次insert和3次update，那么在binlog中我们看到了三条Write_rows和三条Update_rows，并且在记录顺序和操作顺序保持一致，接下来我们看看Write_rows和Update_rows的具体timestamp和data明文。\n\n* 导出明文\n```\nsudo MySQLbinlog --start-datetime='2018-04-29 00:00:03' --stop-datetime='2018-05-02 00:30:00' --base64-output=decode-rows -v /usr/local/MySQL/data/binlog.000001 &gt; ~/binlog.txt\n```\n\n打开binlog.txt 内容如下:\n```\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;\n/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;\nDELIMITER /*!*/;\n# at 4\n#180430 22:29:33 server id 1  end_log_pos 124 CRC32 0xff61797c  Start: binlog v 4, server v 8.0.11 created 180430 22:29:33 at startup\n# Warning: this binlog is either in use or was not closed properly.\nROLLBACK/*!*/;\n# at 124\n#180430 22:29:33 server id 1  end_log_pos 155 CRC32 0x629ae755  Previous-GTIDs\n# [empty]\n# at 155\n#180430 22:32:11 server id 1  end_log_pos 228 CRC32 0xbde49fca  Anonymous_GTID  last_committed=0        sequence_number=1       rbr_only=no     original_committed_timestamp=1525098731207902   immediate_commit_timestamp=1525098731207902     transaction_length=213\n# original_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)\n# immediate_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525098731207902*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 228\n#180430 22:32:11 server id 1  end_log_pos 368 CRC32 0xe5f330e7  Query   thread_id=9     exec_time=0     error_code=0    Xid = 22\nuse `Apache Flinkdb`/*!*/;\nSET TIMESTAMP=1525098731/*!*/;\nSET @@session.pseudo_thread_id=9/*!*/;\nSET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;\nSET @@session.sql_mode=1168113696/*!*/;\nSET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;\n/*!\\C utf8mb4 *//*!*/;\nSET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;\nSET @@session.lc_time_names=0/*!*/;\nSET @@session.collation_database=DEFAULT/*!*/;\n/*!80005 SET @@session.default_collation_for_utf8mb4=255*//*!*/;\nDROP TABLE `tab` /* generated by server */\n/*!*/;\n# at 368\n#180430 22:32:21 server id 1  end_log_pos 443 CRC32 0x50e5acb7  Anonymous_GTID  last_committed=1        sequence_number=2       rbr_only=no     original_committed_timestamp=1525098741628960   immediate_commit_timestamp=1525098741628960     transaction_length=302\n# original_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)\n# immediate_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525098741628960*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 443\n#180430 22:32:21 server id 1  end_log_pos 670 CRC32 0xe1353dd6  Query   thread_id=9     exec_time=0     error_code=0    Xid = 23\nSET TIMESTAMP=1525098741/*!*/;\ncreate table tab(\n   id INT NOT NULL AUTO_INCREMENT,\n   user VARCHAR(100) NOT NULL,\n   clicks INT NOT NULL,\n   PRIMARY KEY (id)\n)\n/*!*/;\n# at 670\n#180430 22:36:53 server id 1  end_log_pos 745 CRC32 0xcf436fbb  Anonymous_GTID  last_committed=2        sequence_number=3       rbr_only=yes    original_committed_timestamp=1525099013988373   immediate_commit_timestamp=1525099013988373     transaction_length=301\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)\n# immediate_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099013988373*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 745\n#180430 22:36:53 server id 1  end_log_pos 823 CRC32 0x71c64dd2  Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099013/*!*/;\nBEGIN\n/*!*/;\n# at 823\n#180430 22:36:53 server id 1  end_log_pos 890 CRC32 0x63792f6b  Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 890\n#180430 22:36:53 server id 1  end_log_pos 940 CRC32 0xf2dade22  Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=1\n# at 940\n#180430 22:36:53 server id 1  end_log_pos 971 CRC32 0x7db3e61e  Xid = 25\nCOMMIT/*!*/;\n# at 971\n#180430 22:37:06 server id 1  end_log_pos 1046 CRC32 0xd05dd12c         Anonymous_GTID  last_committed=3        sequence_number=4       rbr_only=yes    original_committed_timestamp=1525099026328547   immediate_commit_timestamp=1525099026328547     transaction_length=300\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)\n# immediate_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099026328547*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1046\n#180430 22:37:06 server id 1  end_log_pos 1124 CRC32 0x80f259e0         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099026/*!*/;\nBEGIN\n/*!*/;\n# at 1124\n#180430 22:37:06 server id 1  end_log_pos 1191 CRC32 0x255903ba         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1191\n#180430 22:37:06 server id 1  end_log_pos 1240 CRC32 0xe76bfc79         Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=2\n###   @2='Bob'\n###   @3=1\n# at 1240\n#180430 22:37:06 server id 1  end_log_pos 1271 CRC32 0x83cddfef         Xid = 26\nCOMMIT/*!*/;\n# at 1271\n#180430 22:37:15 server id 1  end_log_pos 1346 CRC32 0x7095baee         Anonymous_GTID  last_committed=4        sequence_number=5       rbr_only=yes    original_committed_timestamp=1525099035811597   immediate_commit_timestamp=1525099035811597     transaction_length=326\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)\n# immediate_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099035811597*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1346\n#180430 22:37:15 server id 1  end_log_pos 1433 CRC32 0x70ef97e2         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099035/*!*/;\nBEGIN\n/*!*/;\n# at 1433\n#180430 22:37:15 server id 1  end_log_pos 1500 CRC32 0x75f1f399         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1500\n#180430 22:37:15 server id 1  end_log_pos 1566 CRC32 0x256bd4b8         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=1\n###   @2='Mary'\n###   @3=1\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=2\n# at 1566\n#180430 22:37:15 server id 1  end_log_pos 1597 CRC32 0x93c86579         Xid = 27\nCOMMIT/*!*/;\n# at 1597\n#180430 22:37:27 server id 1  end_log_pos 1672 CRC32 0xe8bd63e7         Anonymous_GTID  last_committed=5        sequence_number=6       rbr_only=yes    original_committed_timestamp=1525099047219517   immediate_commit_timestamp=1525099047219517     transaction_length=300\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)\n# immediate_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099047219517*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1672\n#180430 22:37:27 server id 1  end_log_pos 1750 CRC32 0x5356c3c7         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099047/*!*/;\nBEGIN\n/*!*/;\n# at 1750\n#180430 22:37:27 server id 1  end_log_pos 1817 CRC32 0x37e6b1ce         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 1817\n#180430 22:37:27 server id 1  end_log_pos 1866 CRC32 0x6ab1bbe6         Write_rows: table id 96 flags: STMT_END_F\n### INSERT INTO `Apache Flinkdb`.`tab`\n### SET\n###   @1=3\n###   @2='Llz'\n###   @3=1\n# at 1866\n#180430 22:37:27 server id 1  end_log_pos 1897 CRC32 0x3b62b153         Xid = 28\nCOMMIT/*!*/;\n# at 1897\n#180430 22:37:36 server id 1  end_log_pos 1972 CRC32 0x603134c1         Anonymous_GTID  last_committed=6        sequence_number=7       rbr_only=yes    original_committed_timestamp=1525099056866022   immediate_commit_timestamp=1525099056866022     transaction_length=324\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)\n# immediate_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099056866022*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 1972\n#180430 22:37:36 server id 1  end_log_pos 2059 CRC32 0xe17df4e4         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099056/*!*/;\nBEGIN\n/*!*/;\n# at 2059\n#180430 22:37:36 server id 1  end_log_pos 2126 CRC32 0x53888b05         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 2126\n#180430 22:37:36 server id 1  end_log_pos 2190 CRC32 0x85f34996         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=2\n###   @2='Bob'\n###   @3=1\n### SET\n###   @1=2\n###   @2='Bob'\n###   @3=2\n# at 2190\n#180430 22:37:36 server id 1  end_log_pos 2221 CRC32 0x877f1e23         Xid = 29\nCOMMIT/*!*/;\n# at 2221\n#180430 22:37:45 server id 1  end_log_pos 2296 CRC32 0xfbc7e868         Anonymous_GTID  last_committed=7        sequence_number=8       rbr_only=yes    original_committed_timestamp=1525099065089940   immediate_commit_timestamp=1525099065089940     transaction_length=326\n/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;\n# original_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)\n# immediate_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)\n/*!80001 SET @@session.original_commit_timestamp=1525099065089940*//*!*/;\nSET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/;\n# at 2296\n#180430 22:37:45 server id 1  end_log_pos 2383 CRC32 0x8a514364         Query   thread_id=9     exec_time=0     error_code=0\nSET TIMESTAMP=1525099065/*!*/;\nBEGIN\n/*!*/;\n# at 2383\n#180430 22:37:45 server id 1  end_log_pos 2450 CRC32 0xdf18ca60         Table_map: `Apache Flinkdb`.`tab` mapped to number 96\n# at 2450\n#180430 22:37:45 server id 1  end_log_pos 2516 CRC32 0xd50de69f         Update_rows: table id 96 flags: STMT_END_F\n### UPDATE `Apache Flinkdb`.`tab`\n### WHERE\n###   @1=1\n###   @2='Mary'\n###   @3=2\n### SET\n###   @1=1\n###   @2='Mary'\n###   @3=3\n# at 2516\n#180430 22:37:45 server id 1  end_log_pos 2547 CRC32 0x94f89393         Xid = 30\nCOMMIT/*!*/;\nSET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by MySQLbinlog */ /*!*/;\nDELIMITER ;\n# End of log file\n/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n```\n* 梳理操作和binlog的记录关系\n<div class=\"table-contianer\"><table><tbody><tr><td style=\"width: 200px;\">DML</td><td style=\"width: 216.667px;\">binlog-header(timestamp)</td><td style=\"width: 201.111px;\">data</td></tr><tr><td style=\"width: 200px;\">insert into blink_tab(user, clicks) values ('Mary', 1);</td><td style=\"width: 216.667px;\"><p>1525099013</p><p>(2018/4/30 22:36:53)</p></td><td style=\"width: 201.111px;\">## INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=1</td></tr><tr><td style=\"width: 200px;\">insert into blink_tab(user, clicks) values ('Bob', 1);</td><td style=\"width: 216.667px;\"><p>1525099026</p><p>(2018/4/30 22:37:06)</p></td><td style=\"width: 201.111px;\">### INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=2<br />### @2='Bob'<br />### @3=1&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=2 where user='Mary';</td><td style=\"width: 216.667px;\"><p>&nbsp;1525099035</p><p>(2018/4/30 22:37:15)</p></td><td style=\"width: 201.111px;\">&nbsp;### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=1<br />### @2='Mary'<br />### @3=1<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=2</td></tr><tr><td style=\"width: 200px;\">&nbsp;insert into blink_tab(user, clicks) values ('Llz', 1);</td><td style=\"width: 216.667px;\"><p>1525099047</p><p>(2018/4/30 22:37:27)&nbsp;</p></td><td style=\"width: 201.111px;\">### INSERT INTO `blinkdb`.`blink_tab`<br />### SET<br />### @1=3<br />### @2='Llz'<br />### @3=1&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=2 where user='Bob';</td><td style=\"width: 216.667px;\"><p>&nbsp;1525099056</p><p>(2018/4/30 22:37:36)</p></td><td style=\"width: 201.111px;\">### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=2<br />### @2='Bob'<br />### @3=1<br />### SET<br />### @1=2<br />### @2='Bob'<br />### @3=2&nbsp;</td></tr><tr><td style=\"width: 200px;\">&nbsp;update blink_tab set clicks=3 where user='Mary';</td><td style=\"width: 216.667px;\"><p>1525099065</p><p>(2018/4/30 22:37:45)&nbsp;</p></td><td style=\"width: 201.111px;\">### UPDATE `blinkdb`.`blink_tab`<br />### WHERE<br />### @1=1<br />### @2='Mary'<br />### @3=2<br />### SET<br />### @1=1<br />### @2='Mary'<br />### @3=3&nbsp;</td></tr></tbody></table></div>\n\n* 简化一下binlog\n![](DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png)\n\n* replay binlog会得到如下表数据（按timestamp顺序)\n![](2052C4F1-2EE4-41D1-8A8D-165A40827462.png)\n\n* 表与binlog的关系简单示意如下\n![](F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png)\n# 流表对偶(duality)性\n前面我花费了一些时间介绍了MySQL主备复制机制和binlog的数据格式，binlog中携带时间戳，我们将所有表的操作都按时间进行记录下来形成binlog，而对binlog的event进行重放的过程就是流数据处理的过程，重放的结果恰恰又形成了一张表。也就是表的操作会形成携带时间的事件流，对流的处理又会形成一张不断变化的表，表和流具有等价性，可以互转。随着时间推移，DML操作不断进行，那么表的内容也不断变化，具体如下：\n\n![](299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png)\n\n如上图所示内容，流和表具备相同的特征：\n* 表 - Schema，Data，DML操作时间\n* 流 - Schema，Data, Data处理时间\n\n我们发现，虽然大多数表上面没有明确的显示出DML操作时间，但本质上数据库系统里面是有数据操作时间信息的，这个和流上数据的处理时间（processing time)/产生时间（event-time)相对应。流与表具备相同的特征，可以信息无损的相互转换，我称之为流表对偶(duality)性。\n\n上面我们描述的表，在流上称之为动态表(Dynamic Table)，原因是在流上面任何一个事件的到来都是对表上数据的一次更新(包括插入和删除),表的内容是不断的变化的，任何一个时刻流的状态和表的快照一一对应。流与动态表(Dynamic Table)在时间维度上面具有等价性，这种等价性我们称之为流和动态表(Dynamic Table)的对偶(duality)性。\n\n# 小结\n本篇主要介绍Apache Flink作为一个流计算平台为什么可以为用户提供SQL API。其根本原因是如果将流上的数据看做是结构化的数据，流任务的核心是将一个具有时间属性的结构化数据变成同样具有时间属性的另一个结构化数据，而表的数据变化过程binlog恰恰就是一份具有时间属性的流数据，流与表具有信息无损的相互转换的特性，这种流表对偶性也决定了Apache Flink可以采用SQL作为流任务的开发语言。\n\n","slug":"Apache Flink 漫谈系列 - 流表对偶(duality)性","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fr000ts94h4vqwxoql","content":"<h1>实际问题</h1>\n<p>很多大数据计算产品，都对用户提供了SQL API，比如Hive, Spark, Flink等，那么SQL作为传统关系数据库的查询语言，是应用在批查询场景的。Hive和Spark本质上都是Batch的计算模式(在《Apache Flink 漫谈系列 - 概述》我们介绍过Spark是Micro Batching模式)，提供SQL API很容易被人理解，但是Flink是纯流（Native Streaming）的计算模式, 流与批在数据集和计算过程上有很大的区别.</p>\n<p>如图所示：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/EF81F039-86CC-4799-8DCF-F3B0144B76EC.png\" alt></p>\n<ul>\n<li>\n<p>批查询场景的特点 - 有限数据集，一次查询返回一个计算结果就结束查询</p>\n</li>\n<li>\n<p>流查询场景的特点 - 无限数据集，一次查询不断修正计算结果，查询永远不结束</p>\n</li>\n</ul>\n<p>我们发现批与流的查询场景在数据集合和计算过程上都有很大的不同，那么基于Native Streaming模式的Apache Flink为啥也能为用户提供SQL API呢？</p>\n<h1>流与批的语义关系</h1>\n<p>我们知道SQL都是作用于关系表的，在传统数据库中进行查询时候，SQL所要查询的表在触发查询时候数据是不会变化的，也就是说在查询那一刻，表是一张静态表，相当于是一个有限的批数据，这样也说明SQL是源于对批计算的查询的，那么要回答Apache Flink为啥也能为用户提供SQL API，我们首先要理解流与批在语义层面的关系。我们以一个具体示例说明，如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/3794F522-3458-4386-A101-9458FD2D8DBF.png\" alt>\n上图展现的是一个携带时间戳和用户名的点击事件流，我们先对这些事件流进行流式统计，同时在最后的流事件上触发批计算。流计算中每接收一个数据都会触发一次计算，我们以2018/4/30 22:37:45 Mary到来那一时间切片看，无论是在流还是批上计算结果都是6。也就是说在相同的数据源，相同的查询逻辑下，流和批的计算结果是相同的。相同的SQL在流和批这两种模式下，最终结果是一致的，那么流与批在语义上是完全相同的。</p>\n<h1>流与表的关系</h1>\n<p>流与批在语义上是一致的，SQL是作用于表的，那么要回答Apache Flink为啥也能为用户提供SQL API的问题，就变成了流与表是否具有等价性，也就是本篇要重点介绍的为什么流表具有对偶(duality)性？如下图所示，一张表可以看做为流吗？同样流可以看做是一张表吗？如果可以需要怎样的条件和变通？\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/41690DE4-8705-4305-BF74-AC47F8C870B3.png\" alt></p>\n<h1>MySQL主备复制</h1>\n<p>在介绍流与表的关系之前我们先聊聊MySQL的主备复制，binlog是MySQL实现主备复制的核心手段，简单来说MySQL主备复制实现分成三个步骤：</p>\n<ul>\n<li>Master将改变(change logs)以二进制日志事件(binary log events)形式记录到二进制日志(binlog)中；</li>\n<li>Slave将Master的binary log events拷贝到它的中继日志(relay log)；</li>\n<li>Slave重做中继日志中的事件，将改变反映到数据；</li>\n</ul>\n<p>具体如下图所示:\n   <img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png\" alt></p>\n<h1>binlog</h1>\n<p>接下来我们从binlog模式，binlog格式以及通过查看binlog的具体内容来详尽介绍binlog与表的关系。</p>\n<h2>binlog模式</h2>\n<p>上面介绍的MySQL主备复制的核心手段是利用binlog实现的，那边binlog会记录那些内容呢？binlog记录了数据库所有的增、删、更新等操作。MySQL支持三种方式记录binlog:</p>\n<ul>\n<li>statement-based logging - Events contain SQL statements that produce data changes (inserts, updates, deletes);</li>\n<li>row-based logging - Events describe changes to individual rows;</li>\n<li>mixed-base logging - 该模式默认是statement-based，当遇到如下情况会自动切换到row-based:\n<ul>\n<li>NDB存储引擎，DML操作以row格式记录;</li>\n<li>使用UUID()、USER()、CURRENT_USER()、FOUND_ROWS()等不确定函数;</li>\n<li>使用Insert Delay语句;</li>\n<li>使用用户自定义函数(UDF);</li>\n<li>使用临时表;</li>\n</ul>\n</li>\n</ul>\n<h2>binlog格式</h2>\n<p>我们以row-based 模式为例介绍一下<a href=\"https://dev.MySQL.com/doc/internals/en/event-structure.html\" target=\"_blank\" rel=\"noopener\">binlog的存储格式</a> ，所有的 binary log events都是字节序列，由两部分组成：</p>\n<ul>\n<li>event header</li>\n<li>event data</li>\n</ul>\n<p>关于event header和event data 的格式在数据库的不同版本略有不同，但共同的地方如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">+=====================================+</span><br><span class=\"line\">| event  | timestamp         0 : 4    |</span><br><span class=\"line\">| header +----------------------------+</span><br><span class=\"line\">|        | type_code         4 : 1    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        | server_id         5 : 4    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        | event_length      9 : 4    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        |不同版本不一样(省略)          |</span><br><span class=\"line\">+=====================================+</span><br><span class=\"line\">| event  | fixed part                 |</span><br><span class=\"line\">| data   +----------------------------+</span><br><span class=\"line\">|        | variable part              |</span><br><span class=\"line\">+=====================================+</span><br></pre></td></tr></table></figure></p>\n<p>这里有个值得我们注意的地方就是在binlog的header中有一个属性是timestamp，这个属性是标识了change发生的先后顺序，在备库进行复制时候会严格按照时间顺序进行log的重放。</p>\n<h2>binlog的生成</h2>\n<p>我们以对MySQL进行实际操作的方式，直观的介绍一下binlog的生成，binlog是二进制存储的，下面我们会利用工具查看binlog的文本内容。</p>\n<ul>\n<li>\n<p>查看一下binlog是否打开：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show variables like &apos;log_bin&apos;</span><br><span class=\"line\">    -&amp;gt; ;</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| Variable_name | Value |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| log_bin       | ON    |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>查看一下binlog的模式(我需要row-base模式)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show variables like &apos;binlog_format&apos;;</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| Variable_name | Value |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| binlog_format | ROW   |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>清除现有的binlog\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; reset master;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)创建一张我们做实验的表MySQL&amp;gt; create table tab(</span><br><span class=\"line\">    -&amp;gt;    id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">    -&amp;gt;    user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">    -&amp;gt;    clicks INT NOT NULL,</span><br><span class=\"line\">    -&amp;gt;    PRIMARY KEY (id)</span><br><span class=\"line\">    -&amp;gt; );</span><br><span class=\"line\">Query OK, 0 rows affected (0.10 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; show tables;</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| Tables_in_Apache Flinkdb |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| tab         |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>进行DML操作\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Mary&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.03 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Bob&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=2 where user=&apos;Mary&apos;</span><br><span class=\"line\">    -&amp;gt; ;</span><br><span class=\"line\">Query OK, 1 row affected (0.06 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Llz&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=2 where user=&apos;Bob&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.01 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=3 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.05 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; select * from tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      3 |</span><br><span class=\"line\">|  2 | Bob  |      2 |</span><br><span class=\"line\">|  3 | Llz  |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>查看正在操作的binlog\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; show master status\\G</span><br><span class=\"line\">*************************** 1. row ***************************</span><br><span class=\"line\">             File: binlog.000001</span><br><span class=\"line\">         Position: 2547</span><br><span class=\"line\">     Binlog_Do_DB: </span><br><span class=\"line\"> Binlog_Ignore_DB: </span><br><span class=\"line\">Executed_Gtid_Set: </span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>上面 binlog.000001 文件是我们正在操作的binlog。</p>\n<ul>\n<li>查看binlog.000001文件的操作记录\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; show binlog events in &apos;binlog.000001&apos;;</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">| Log_name      | Pos  | Event_type     | Server_id | End_log_pos | Info                                                                                                                                                                |</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">| binlog.000001 |    4 | Format_desc    |         1 |         124 | Server ver: 8.0.11, Binlog ver: 4                                                                                                                                   |</span><br><span class=\"line\">| binlog.000001 |  124 | Previous_gtids |         1 |         155 |                                                                                                                                                                     |</span><br><span class=\"line\">| binlog.000001 |  155 | Anonymous_Gtid |         1 |         228 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  228 | Query          |         1 |         368 | use `Apache Flinkdb`; DROP TABLE `tab` /* generated by server */ /* xid=22 */                                                                                        |</span><br><span class=\"line\">| binlog.000001 |  368 | Anonymous_Gtid |         1 |         443 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  443 | Query          |         1 |         670 | use `Apache Flinkdb`; create table tab(</span><br><span class=\"line\">   id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">   user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">   clicks INT NOT NULL,</span><br><span class=\"line\">   PRIMARY KEY (id)</span><br><span class=\"line\">) /* xid=23 */ |</span><br><span class=\"line\">| binlog.000001 |  670 | Anonymous_Gtid |         1 |         745 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  745 | Query          |         1 |         823 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 |  823 | Table_map      |         1 |         890 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 |  890 | Write_rows     |         1 |         940 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 |  940 | Xid            |         1 |         971 | COMMIT /* xid=25 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 |  971 | Anonymous_Gtid |         1 |        1046 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1046 | Query          |         1 |        1124 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1124 | Table_map      |         1 |        1191 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1191 | Write_rows     |         1 |        1240 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1240 | Xid            |         1 |        1271 | COMMIT /* xid=26 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1271 | Anonymous_Gtid |         1 |        1346 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1346 | Query          |         1 |        1433 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1433 | Table_map      |         1 |        1500 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1500 | Update_rows    |         1 |        1566 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1566 | Xid            |         1 |        1597 | COMMIT /* xid=27 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1597 | Anonymous_Gtid |         1 |        1672 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1672 | Query          |         1 |        1750 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1750 | Table_map      |         1 |        1817 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1817 | Write_rows     |         1 |        1866 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1866 | Xid            |         1 |        1897 | COMMIT /* xid=28 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1897 | Anonymous_Gtid |         1 |        1972 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1972 | Query          |         1 |        2059 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 2059 | Table_map      |         1 |        2126 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 2126 | Update_rows    |         1 |        2190 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 2190 | Xid            |         1 |        2221 | COMMIT /* xid=29 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 2221 | Anonymous_Gtid |         1 |        2296 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 2296 | Query          |         1 |        2383 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 2383 | Table_map      |         1 |        2450 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 2450 | Update_rows    |         1 |        2516 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 2516 | Xid            |         1 |        2547 | COMMIT /* xid=30 */                                                                                                                                                 |</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">36 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面我们进行了3次insert和3次update，那么在binlog中我们看到了三条Write_rows和三条Update_rows，并且在记录顺序和操作顺序保持一致，接下来我们看看Write_rows和Update_rows的具体timestamp和data明文。</p>\n<ul>\n<li>导出明文\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo MySQLbinlog --start-datetime=&apos;2018-04-29 00:00:03&apos; --stop-datetime=&apos;2018-05-02 00:30:00&apos; --base64-output=decode-rows -v /usr/local/MySQL/data/binlog.000001 &amp;gt; ~/binlog.txt</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>打开binlog.txt 内容如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;</span><br><span class=\"line\">/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;</span><br><span class=\"line\">DELIMITER /*!*/;</span><br><span class=\"line\"># at 4</span><br><span class=\"line\">#180430 22:29:33 server id 1  end_log_pos 124 CRC32 0xff61797c  Start: binlog v 4, server v 8.0.11 created 180430 22:29:33 at startup</span><br><span class=\"line\"># Warning: this binlog is either in use or was not closed properly.</span><br><span class=\"line\">ROLLBACK/*!*/;</span><br><span class=\"line\"># at 124</span><br><span class=\"line\">#180430 22:29:33 server id 1  end_log_pos 155 CRC32 0x629ae755  Previous-GTIDs</span><br><span class=\"line\"># [empty]</span><br><span class=\"line\"># at 155</span><br><span class=\"line\">#180430 22:32:11 server id 1  end_log_pos 228 CRC32 0xbde49fca  Anonymous_GTID  last_committed=0        sequence_number=1       rbr_only=no     original_committed_timestamp=1525098731207902   immediate_commit_timestamp=1525098731207902     transaction_length=213</span><br><span class=\"line\"># original_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525098731207902*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 228</span><br><span class=\"line\">#180430 22:32:11 server id 1  end_log_pos 368 CRC32 0xe5f330e7  Query   thread_id=9     exec_time=0     error_code=0    Xid = 22</span><br><span class=\"line\">use `Apache Flinkdb`/*!*/;</span><br><span class=\"line\">SET TIMESTAMP=1525098731/*!*/;</span><br><span class=\"line\">SET @@session.pseudo_thread_id=9/*!*/;</span><br><span class=\"line\">SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;</span><br><span class=\"line\">SET @@session.sql_mode=1168113696/*!*/;</span><br><span class=\"line\">SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;</span><br><span class=\"line\">/*!\\C utf8mb4 *//*!*/;</span><br><span class=\"line\">SET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;</span><br><span class=\"line\">SET @@session.lc_time_names=0/*!*/;</span><br><span class=\"line\">SET @@session.collation_database=DEFAULT/*!*/;</span><br><span class=\"line\">/*!80005 SET @@session.default_collation_for_utf8mb4=255*//*!*/;</span><br><span class=\"line\">DROP TABLE `tab` /* generated by server */</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 368</span><br><span class=\"line\">#180430 22:32:21 server id 1  end_log_pos 443 CRC32 0x50e5acb7  Anonymous_GTID  last_committed=1        sequence_number=2       rbr_only=no     original_committed_timestamp=1525098741628960   immediate_commit_timestamp=1525098741628960     transaction_length=302</span><br><span class=\"line\"># original_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525098741628960*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 443</span><br><span class=\"line\">#180430 22:32:21 server id 1  end_log_pos 670 CRC32 0xe1353dd6  Query   thread_id=9     exec_time=0     error_code=0    Xid = 23</span><br><span class=\"line\">SET TIMESTAMP=1525098741/*!*/;</span><br><span class=\"line\">create table tab(</span><br><span class=\"line\">   id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">   user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">   clicks INT NOT NULL,</span><br><span class=\"line\">   PRIMARY KEY (id)</span><br><span class=\"line\">)</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 670</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 745 CRC32 0xcf436fbb  Anonymous_GTID  last_committed=2        sequence_number=3       rbr_only=yes    original_committed_timestamp=1525099013988373   immediate_commit_timestamp=1525099013988373     transaction_length=301</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099013988373*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 745</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 823 CRC32 0x71c64dd2  Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099013/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 823</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 890 CRC32 0x63792f6b  Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 890</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 940 CRC32 0xf2dade22  Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 940</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 971 CRC32 0x7db3e61e  Xid = 25</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 971</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1046 CRC32 0xd05dd12c         Anonymous_GTID  last_committed=3        sequence_number=4       rbr_only=yes    original_committed_timestamp=1525099026328547   immediate_commit_timestamp=1525099026328547     transaction_length=300</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099026328547*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1046</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1124 CRC32 0x80f259e0         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099026/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1124</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1191 CRC32 0x255903ba         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1191</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1240 CRC32 0xe76bfc79         Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 1240</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1271 CRC32 0x83cddfef         Xid = 26</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1271</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1346 CRC32 0x7095baee         Anonymous_GTID  last_committed=4        sequence_number=5       rbr_only=yes    original_committed_timestamp=1525099035811597   immediate_commit_timestamp=1525099035811597     transaction_length=326</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099035811597*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1346</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1433 CRC32 0x70ef97e2         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099035/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1433</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1500 CRC32 0x75f1f399         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1500</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1566 CRC32 0x256bd4b8         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\"># at 1566</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1597 CRC32 0x93c86579         Xid = 27</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1597</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1672 CRC32 0xe8bd63e7         Anonymous_GTID  last_committed=5        sequence_number=6       rbr_only=yes    original_committed_timestamp=1525099047219517   immediate_commit_timestamp=1525099047219517     transaction_length=300</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099047219517*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1672</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1750 CRC32 0x5356c3c7         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099047/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1750</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1817 CRC32 0x37e6b1ce         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1817</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1866 CRC32 0x6ab1bbe6         Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=3</span><br><span class=\"line\">###   @2=&apos;Llz&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 1866</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1897 CRC32 0x3b62b153         Xid = 28</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1897</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 1972 CRC32 0x603134c1         Anonymous_GTID  last_committed=6        sequence_number=7       rbr_only=yes    original_committed_timestamp=1525099056866022   immediate_commit_timestamp=1525099056866022     transaction_length=324</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099056866022*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1972</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2059 CRC32 0xe17df4e4         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099056/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 2059</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2126 CRC32 0x53888b05         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 2126</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2190 CRC32 0x85f34996         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\"># at 2190</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2221 CRC32 0x877f1e23         Xid = 29</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 2221</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2296 CRC32 0xfbc7e868         Anonymous_GTID  last_committed=7        sequence_number=8       rbr_only=yes    original_committed_timestamp=1525099065089940   immediate_commit_timestamp=1525099065089940     transaction_length=326</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099065089940*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 2296</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2383 CRC32 0x8a514364         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099065/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 2383</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2450 CRC32 0xdf18ca60         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 2450</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2516 CRC32 0xd50de69f         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=3</span><br><span class=\"line\"># at 2516</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2547 CRC32 0x94f89393         Xid = 30</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by MySQLbinlog */ /*!*/;</span><br><span class=\"line\">DELIMITER ;</span><br><span class=\"line\"># End of log file</span><br><span class=\"line\">/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;</span><br><span class=\"line\">/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>\n<p>梳理操作和binlog的记录关系\n&lt;div class=&quot;table-contianer&quot;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;DML&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;binlog-header(timestamp)&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;data&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;insert into blink_tab(user, clicks) values ('Mary', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099013&lt;/p&gt;&lt;p&gt;(2018/4/30 22:36:53)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;## INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;insert into blink_tab(user, clicks) values ('Bob', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099026&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:06)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=1 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=2 where user='Mary';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt; 1525099035&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:15)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt; ### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=1&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; insert into blink_tab(user, clicks) values ('Llz', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099047&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:27) &lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=3&lt;br /&gt;### @2='Llz'&lt;br /&gt;### @3=1 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=2 where user='Bob';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt; 1525099056&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:36)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=1&lt;br /&gt;### SET&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=3 where user='Mary';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099065&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:45) &lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=2&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=3 &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;</p>\n</li>\n<li>\n<p>简化一下binlog\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png\" alt></p>\n</li>\n<li>\n<p>replay binlog会得到如下表数据（按timestamp顺序)\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/2052C4F1-2EE4-41D1-8A8D-165A40827462.png\" alt></p>\n</li>\n<li>\n<p>表与binlog的关系简单示意如下\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png\" alt></p>\n</li>\n</ul>\n<h1>流表对偶(duality)性</h1>\n<p>前面我花费了一些时间介绍了MySQL主备复制机制和binlog的数据格式，binlog中携带时间戳，我们将所有表的操作都按时间进行记录下来形成binlog，而对binlog的event进行重放的过程就是流数据处理的过程，重放的结果恰恰又形成了一张表。也就是表的操作会形成携带时间的事件流，对流的处理又会形成一张不断变化的表，表和流具有等价性，可以互转。随着时间推移，DML操作不断进行，那么表的内容也不断变化，具体如下：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png\" alt></p>\n<p>如上图所示内容，流和表具备相同的特征：</p>\n<ul>\n<li>表 - Schema，Data，DML操作时间</li>\n<li>流 - Schema，Data, Data处理时间</li>\n</ul>\n<p>我们发现，虽然大多数表上面没有明确的显示出DML操作时间，但本质上数据库系统里面是有数据操作时间信息的，这个和流上数据的处理时间（processing time)/产生时间（event-time)相对应。流与表具备相同的特征，可以信息无损的相互转换，我称之为流表对偶(duality)性。</p>\n<p>上面我们描述的表，在流上称之为动态表(Dynamic Table)，原因是在流上面任何一个事件的到来都是对表上数据的一次更新(包括插入和删除),表的内容是不断的变化的，任何一个时刻流的状态和表的快照一一对应。流与动态表(Dynamic Table)在时间维度上面具有等价性，这种等价性我们称之为流和动态表(Dynamic Table)的对偶(duality)性。</p>\n<h1>小结</h1>\n<p>本篇主要介绍Apache Flink作为一个流计算平台为什么可以为用户提供SQL API。其根本原因是如果将流上的数据看做是结构化的数据，流任务的核心是将一个具有时间属性的结构化数据变成同样具有时间属性的另一个结构化数据，而表的数据变化过程binlog恰恰就是一份具有时间属性的流数据，流与表具有信息无损的相互转换的特性，这种流表对偶性也决定了Apache Flink可以采用SQL作为流任务的开发语言。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>实际问题</h1>\n<p>很多大数据计算产品，都对用户提供了SQL API，比如Hive, Spark, Flink等，那么SQL作为传统关系数据库的查询语言，是应用在批查询场景的。Hive和Spark本质上都是Batch的计算模式(在《Apache Flink 漫谈系列 - 概述》我们介绍过Spark是Micro Batching模式)，提供SQL API很容易被人理解，但是Flink是纯流（Native Streaming）的计算模式, 流与批在数据集和计算过程上有很大的区别.</p>\n<p>如图所示：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/EF81F039-86CC-4799-8DCF-F3B0144B76EC.png\" alt></p>\n<ul>\n<li>\n<p>批查询场景的特点 - 有限数据集，一次查询返回一个计算结果就结束查询</p>\n</li>\n<li>\n<p>流查询场景的特点 - 无限数据集，一次查询不断修正计算结果，查询永远不结束</p>\n</li>\n</ul>\n<p>我们发现批与流的查询场景在数据集合和计算过程上都有很大的不同，那么基于Native Streaming模式的Apache Flink为啥也能为用户提供SQL API呢？</p>\n<h1>流与批的语义关系</h1>\n<p>我们知道SQL都是作用于关系表的，在传统数据库中进行查询时候，SQL所要查询的表在触发查询时候数据是不会变化的，也就是说在查询那一刻，表是一张静态表，相当于是一个有限的批数据，这样也说明SQL是源于对批计算的查询的，那么要回答Apache Flink为啥也能为用户提供SQL API，我们首先要理解流与批在语义层面的关系。我们以一个具体示例说明，如下图：\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/3794F522-3458-4386-A101-9458FD2D8DBF.png\" alt>\n上图展现的是一个携带时间戳和用户名的点击事件流，我们先对这些事件流进行流式统计，同时在最后的流事件上触发批计算。流计算中每接收一个数据都会触发一次计算，我们以2018/4/30 22:37:45 Mary到来那一时间切片看，无论是在流还是批上计算结果都是6。也就是说在相同的数据源，相同的查询逻辑下，流和批的计算结果是相同的。相同的SQL在流和批这两种模式下，最终结果是一致的，那么流与批在语义上是完全相同的。</p>\n<h1>流与表的关系</h1>\n<p>流与批在语义上是一致的，SQL是作用于表的，那么要回答Apache Flink为啥也能为用户提供SQL API的问题，就变成了流与表是否具有等价性，也就是本篇要重点介绍的为什么流表具有对偶(duality)性？如下图所示，一张表可以看做为流吗？同样流可以看做是一张表吗？如果可以需要怎样的条件和变通？\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/41690DE4-8705-4305-BF74-AC47F8C870B3.png\" alt></p>\n<h1>MySQL主备复制</h1>\n<p>在介绍流与表的关系之前我们先聊聊MySQL的主备复制，binlog是MySQL实现主备复制的核心手段，简单来说MySQL主备复制实现分成三个步骤：</p>\n<ul>\n<li>Master将改变(change logs)以二进制日志事件(binary log events)形式记录到二进制日志(binlog)中；</li>\n<li>Slave将Master的binary log events拷贝到它的中继日志(relay log)；</li>\n<li>Slave重做中继日志中的事件，将改变反映到数据；</li>\n</ul>\n<p>具体如下图所示:\n   <img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png\" alt></p>\n<h1>binlog</h1>\n<p>接下来我们从binlog模式，binlog格式以及通过查看binlog的具体内容来详尽介绍binlog与表的关系。</p>\n<h2>binlog模式</h2>\n<p>上面介绍的MySQL主备复制的核心手段是利用binlog实现的，那边binlog会记录那些内容呢？binlog记录了数据库所有的增、删、更新等操作。MySQL支持三种方式记录binlog:</p>\n<ul>\n<li>statement-based logging - Events contain SQL statements that produce data changes (inserts, updates, deletes);</li>\n<li>row-based logging - Events describe changes to individual rows;</li>\n<li>mixed-base logging - 该模式默认是statement-based，当遇到如下情况会自动切换到row-based:\n<ul>\n<li>NDB存储引擎，DML操作以row格式记录;</li>\n<li>使用UUID()、USER()、CURRENT_USER()、FOUND_ROWS()等不确定函数;</li>\n<li>使用Insert Delay语句;</li>\n<li>使用用户自定义函数(UDF);</li>\n<li>使用临时表;</li>\n</ul>\n</li>\n</ul>\n<h2>binlog格式</h2>\n<p>我们以row-based 模式为例介绍一下<a href=\"https://dev.MySQL.com/doc/internals/en/event-structure.html\" target=\"_blank\" rel=\"noopener\">binlog的存储格式</a> ，所有的 binary log events都是字节序列，由两部分组成：</p>\n<ul>\n<li>event header</li>\n<li>event data</li>\n</ul>\n<p>关于event header和event data 的格式在数据库的不同版本略有不同，但共同的地方如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">+=====================================+</span><br><span class=\"line\">| event  | timestamp         0 : 4    |</span><br><span class=\"line\">| header +----------------------------+</span><br><span class=\"line\">|        | type_code         4 : 1    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        | server_id         5 : 4    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        | event_length      9 : 4    |</span><br><span class=\"line\">|        +----------------------------+</span><br><span class=\"line\">|        |不同版本不一样(省略)          |</span><br><span class=\"line\">+=====================================+</span><br><span class=\"line\">| event  | fixed part                 |</span><br><span class=\"line\">| data   +----------------------------+</span><br><span class=\"line\">|        | variable part              |</span><br><span class=\"line\">+=====================================+</span><br></pre></td></tr></table></figure></p>\n<p>这里有个值得我们注意的地方就是在binlog的header中有一个属性是timestamp，这个属性是标识了change发生的先后顺序，在备库进行复制时候会严格按照时间顺序进行log的重放。</p>\n<h2>binlog的生成</h2>\n<p>我们以对MySQL进行实际操作的方式，直观的介绍一下binlog的生成，binlog是二进制存储的，下面我们会利用工具查看binlog的文本内容。</p>\n<ul>\n<li>\n<p>查看一下binlog是否打开：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show variables like &apos;log_bin&apos;</span><br><span class=\"line\">    -&amp;gt; ;</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| Variable_name | Value |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| log_bin       | ON    |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>查看一下binlog的模式(我需要row-base模式)：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show variables like &apos;binlog_format&apos;;</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| Variable_name | Value |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">| binlog_format | ROW   |</span><br><span class=\"line\">+---------------+-------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>清除现有的binlog\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; reset master;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)创建一张我们做实验的表MySQL&amp;gt; create table tab(</span><br><span class=\"line\">    -&amp;gt;    id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">    -&amp;gt;    user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">    -&amp;gt;    clicks INT NOT NULL,</span><br><span class=\"line\">    -&amp;gt;    PRIMARY KEY (id)</span><br><span class=\"line\">    -&amp;gt; );</span><br><span class=\"line\">Query OK, 0 rows affected (0.10 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; show tables;</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| Tables_in_Apache Flinkdb |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| tab         |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>进行DML操作\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Mary&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.03 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Bob&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=2 where user=&apos;Mary&apos;</span><br><span class=\"line\">    -&amp;gt; ;</span><br><span class=\"line\">Query OK, 1 row affected (0.06 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Llz&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=2 where user=&apos;Bob&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.01 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; update tab set clicks=3 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.05 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&amp;gt; select * from tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      3 |</span><br><span class=\"line\">|  2 | Bob  |      2 |</span><br><span class=\"line\">|  3 | Llz  |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>查看正在操作的binlog\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; show master status\\G</span><br><span class=\"line\">*************************** 1. row ***************************</span><br><span class=\"line\">             File: binlog.000001</span><br><span class=\"line\">         Position: 2547</span><br><span class=\"line\">     Binlog_Do_DB: </span><br><span class=\"line\"> Binlog_Ignore_DB: </span><br><span class=\"line\">Executed_Gtid_Set: </span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>上面 binlog.000001 文件是我们正在操作的binlog。</p>\n<ul>\n<li>查看binlog.000001文件的操作记录\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&amp;gt; show binlog events in &apos;binlog.000001&apos;;</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">| Log_name      | Pos  | Event_type     | Server_id | End_log_pos | Info                                                                                                                                                                |</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">| binlog.000001 |    4 | Format_desc    |         1 |         124 | Server ver: 8.0.11, Binlog ver: 4                                                                                                                                   |</span><br><span class=\"line\">| binlog.000001 |  124 | Previous_gtids |         1 |         155 |                                                                                                                                                                     |</span><br><span class=\"line\">| binlog.000001 |  155 | Anonymous_Gtid |         1 |         228 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  228 | Query          |         1 |         368 | use `Apache Flinkdb`; DROP TABLE `tab` /* generated by server */ /* xid=22 */                                                                                        |</span><br><span class=\"line\">| binlog.000001 |  368 | Anonymous_Gtid |         1 |         443 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  443 | Query          |         1 |         670 | use `Apache Flinkdb`; create table tab(</span><br><span class=\"line\">   id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">   user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">   clicks INT NOT NULL,</span><br><span class=\"line\">   PRIMARY KEY (id)</span><br><span class=\"line\">) /* xid=23 */ |</span><br><span class=\"line\">| binlog.000001 |  670 | Anonymous_Gtid |         1 |         745 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 |  745 | Query          |         1 |         823 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 |  823 | Table_map      |         1 |         890 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 |  890 | Write_rows     |         1 |         940 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 |  940 | Xid            |         1 |         971 | COMMIT /* xid=25 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 |  971 | Anonymous_Gtid |         1 |        1046 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1046 | Query          |         1 |        1124 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1124 | Table_map      |         1 |        1191 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1191 | Write_rows     |         1 |        1240 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1240 | Xid            |         1 |        1271 | COMMIT /* xid=26 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1271 | Anonymous_Gtid |         1 |        1346 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1346 | Query          |         1 |        1433 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1433 | Table_map      |         1 |        1500 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1500 | Update_rows    |         1 |        1566 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1566 | Xid            |         1 |        1597 | COMMIT /* xid=27 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1597 | Anonymous_Gtid |         1 |        1672 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1672 | Query          |         1 |        1750 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 1750 | Table_map      |         1 |        1817 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 1817 | Write_rows     |         1 |        1866 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 1866 | Xid            |         1 |        1897 | COMMIT /* xid=28 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 1897 | Anonymous_Gtid |         1 |        1972 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 1972 | Query          |         1 |        2059 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 2059 | Table_map      |         1 |        2126 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 2126 | Update_rows    |         1 |        2190 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 2190 | Xid            |         1 |        2221 | COMMIT /* xid=29 */                                                                                                                                                 |</span><br><span class=\"line\">| binlog.000001 | 2221 | Anonymous_Gtid |         1 |        2296 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;                                                                                                                                |</span><br><span class=\"line\">| binlog.000001 | 2296 | Query          |         1 |        2383 | BEGIN                                                                                                                                                               |</span><br><span class=\"line\">| binlog.000001 | 2383 | Table_map      |         1 |        2450 | table_id: 96 (Apache Flinkdb.tab)                                                                                                                                    |</span><br><span class=\"line\">| binlog.000001 | 2450 | Update_rows    |         1 |        2516 | table_id: 96 flags: STMT_END_F                                                                                                                                      |</span><br><span class=\"line\">| binlog.000001 | 2516 | Xid            |         1 |        2547 | COMMIT /* xid=30 */                                                                                                                                                 |</span><br><span class=\"line\">+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class=\"line\">36 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面我们进行了3次insert和3次update，那么在binlog中我们看到了三条Write_rows和三条Update_rows，并且在记录顺序和操作顺序保持一致，接下来我们看看Write_rows和Update_rows的具体timestamp和data明文。</p>\n<ul>\n<li>导出明文\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo MySQLbinlog --start-datetime=&apos;2018-04-29 00:00:03&apos; --stop-datetime=&apos;2018-05-02 00:30:00&apos; --base64-output=decode-rows -v /usr/local/MySQL/data/binlog.000001 &amp;gt; ~/binlog.txt</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>打开binlog.txt 内容如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;</span><br><span class=\"line\">/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;</span><br><span class=\"line\">DELIMITER /*!*/;</span><br><span class=\"line\"># at 4</span><br><span class=\"line\">#180430 22:29:33 server id 1  end_log_pos 124 CRC32 0xff61797c  Start: binlog v 4, server v 8.0.11 created 180430 22:29:33 at startup</span><br><span class=\"line\"># Warning: this binlog is either in use or was not closed properly.</span><br><span class=\"line\">ROLLBACK/*!*/;</span><br><span class=\"line\"># at 124</span><br><span class=\"line\">#180430 22:29:33 server id 1  end_log_pos 155 CRC32 0x629ae755  Previous-GTIDs</span><br><span class=\"line\"># [empty]</span><br><span class=\"line\"># at 155</span><br><span class=\"line\">#180430 22:32:11 server id 1  end_log_pos 228 CRC32 0xbde49fca  Anonymous_GTID  last_committed=0        sequence_number=1       rbr_only=no     original_committed_timestamp=1525098731207902   immediate_commit_timestamp=1525098731207902     transaction_length=213</span><br><span class=\"line\"># original_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525098731207902*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 228</span><br><span class=\"line\">#180430 22:32:11 server id 1  end_log_pos 368 CRC32 0xe5f330e7  Query   thread_id=9     exec_time=0     error_code=0    Xid = 22</span><br><span class=\"line\">use `Apache Flinkdb`/*!*/;</span><br><span class=\"line\">SET TIMESTAMP=1525098731/*!*/;</span><br><span class=\"line\">SET @@session.pseudo_thread_id=9/*!*/;</span><br><span class=\"line\">SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;</span><br><span class=\"line\">SET @@session.sql_mode=1168113696/*!*/;</span><br><span class=\"line\">SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;</span><br><span class=\"line\">/*!\\C utf8mb4 *//*!*/;</span><br><span class=\"line\">SET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;</span><br><span class=\"line\">SET @@session.lc_time_names=0/*!*/;</span><br><span class=\"line\">SET @@session.collation_database=DEFAULT/*!*/;</span><br><span class=\"line\">/*!80005 SET @@session.default_collation_for_utf8mb4=255*//*!*/;</span><br><span class=\"line\">DROP TABLE `tab` /* generated by server */</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 368</span><br><span class=\"line\">#180430 22:32:21 server id 1  end_log_pos 443 CRC32 0x50e5acb7  Anonymous_GTID  last_committed=1        sequence_number=2       rbr_only=no     original_committed_timestamp=1525098741628960   immediate_commit_timestamp=1525098741628960     transaction_length=302</span><br><span class=\"line\"># original_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525098741628960*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 443</span><br><span class=\"line\">#180430 22:32:21 server id 1  end_log_pos 670 CRC32 0xe1353dd6  Query   thread_id=9     exec_time=0     error_code=0    Xid = 23</span><br><span class=\"line\">SET TIMESTAMP=1525098741/*!*/;</span><br><span class=\"line\">create table tab(</span><br><span class=\"line\">   id INT NOT NULL AUTO_INCREMENT,</span><br><span class=\"line\">   user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">   clicks INT NOT NULL,</span><br><span class=\"line\">   PRIMARY KEY (id)</span><br><span class=\"line\">)</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 670</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 745 CRC32 0xcf436fbb  Anonymous_GTID  last_committed=2        sequence_number=3       rbr_only=yes    original_committed_timestamp=1525099013988373   immediate_commit_timestamp=1525099013988373     transaction_length=301</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099013988373*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 745</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 823 CRC32 0x71c64dd2  Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099013/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 823</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 890 CRC32 0x63792f6b  Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 890</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 940 CRC32 0xf2dade22  Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 940</span><br><span class=\"line\">#180430 22:36:53 server id 1  end_log_pos 971 CRC32 0x7db3e61e  Xid = 25</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 971</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1046 CRC32 0xd05dd12c         Anonymous_GTID  last_committed=3        sequence_number=4       rbr_only=yes    original_committed_timestamp=1525099026328547   immediate_commit_timestamp=1525099026328547     transaction_length=300</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099026328547*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1046</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1124 CRC32 0x80f259e0         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099026/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1124</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1191 CRC32 0x255903ba         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1191</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1240 CRC32 0xe76bfc79         Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 1240</span><br><span class=\"line\">#180430 22:37:06 server id 1  end_log_pos 1271 CRC32 0x83cddfef         Xid = 26</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1271</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1346 CRC32 0x7095baee         Anonymous_GTID  last_committed=4        sequence_number=5       rbr_only=yes    original_committed_timestamp=1525099035811597   immediate_commit_timestamp=1525099035811597     transaction_length=326</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099035811597*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1346</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1433 CRC32 0x70ef97e2         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099035/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1433</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1500 CRC32 0x75f1f399         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1500</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1566 CRC32 0x256bd4b8         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\"># at 1566</span><br><span class=\"line\">#180430 22:37:15 server id 1  end_log_pos 1597 CRC32 0x93c86579         Xid = 27</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1597</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1672 CRC32 0xe8bd63e7         Anonymous_GTID  last_committed=5        sequence_number=6       rbr_only=yes    original_committed_timestamp=1525099047219517   immediate_commit_timestamp=1525099047219517     transaction_length=300</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099047219517*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1672</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1750 CRC32 0x5356c3c7         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099047/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 1750</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1817 CRC32 0x37e6b1ce         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 1817</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1866 CRC32 0x6ab1bbe6         Write_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### INSERT INTO `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=3</span><br><span class=\"line\">###   @2=&apos;Llz&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\"># at 1866</span><br><span class=\"line\">#180430 22:37:27 server id 1  end_log_pos 1897 CRC32 0x3b62b153         Xid = 28</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 1897</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 1972 CRC32 0x603134c1         Anonymous_GTID  last_committed=6        sequence_number=7       rbr_only=yes    original_committed_timestamp=1525099056866022   immediate_commit_timestamp=1525099056866022     transaction_length=324</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099056866022*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 1972</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2059 CRC32 0xe17df4e4         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099056/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 2059</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2126 CRC32 0x53888b05         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 2126</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2190 CRC32 0x85f34996         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=1</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=2</span><br><span class=\"line\">###   @2=&apos;Bob&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\"># at 2190</span><br><span class=\"line\">#180430 22:37:36 server id 1  end_log_pos 2221 CRC32 0x877f1e23         Xid = 29</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\"># at 2221</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2296 CRC32 0xfbc7e868         Anonymous_GTID  last_committed=7        sequence_number=8       rbr_only=yes    original_committed_timestamp=1525099065089940   immediate_commit_timestamp=1525099065089940     transaction_length=326</span><br><span class=\"line\">/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;</span><br><span class=\"line\"># original_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)</span><br><span class=\"line\"># immediate_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)</span><br><span class=\"line\">/*!80001 SET @@session.original_commit_timestamp=1525099065089940*//*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;</span><br><span class=\"line\"># at 2296</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2383 CRC32 0x8a514364         Query   thread_id=9     exec_time=0     error_code=0</span><br><span class=\"line\">SET TIMESTAMP=1525099065/*!*/;</span><br><span class=\"line\">BEGIN</span><br><span class=\"line\">/*!*/;</span><br><span class=\"line\"># at 2383</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2450 CRC32 0xdf18ca60         Table_map: `Apache Flinkdb`.`tab` mapped to number 96</span><br><span class=\"line\"># at 2450</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2516 CRC32 0xd50de69f         Update_rows: table id 96 flags: STMT_END_F</span><br><span class=\"line\">### UPDATE `Apache Flinkdb`.`tab`</span><br><span class=\"line\">### WHERE</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=2</span><br><span class=\"line\">### SET</span><br><span class=\"line\">###   @1=1</span><br><span class=\"line\">###   @2=&apos;Mary&apos;</span><br><span class=\"line\">###   @3=3</span><br><span class=\"line\"># at 2516</span><br><span class=\"line\">#180430 22:37:45 server id 1  end_log_pos 2547 CRC32 0x94f89393         Xid = 30</span><br><span class=\"line\">COMMIT/*!*/;</span><br><span class=\"line\">SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by MySQLbinlog */ /*!*/;</span><br><span class=\"line\">DELIMITER ;</span><br><span class=\"line\"># End of log file</span><br><span class=\"line\">/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;</span><br><span class=\"line\">/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>\n<p>梳理操作和binlog的记录关系\n&lt;div class=&quot;table-contianer&quot;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;DML&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;binlog-header(timestamp)&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;data&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;insert into blink_tab(user, clicks) values ('Mary', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099013&lt;/p&gt;&lt;p&gt;(2018/4/30 22:36:53)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;## INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt;insert into blink_tab(user, clicks) values ('Bob', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099026&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:06)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=1 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=2 where user='Mary';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt; 1525099035&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:15)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt; ### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=1&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; insert into blink_tab(user, clicks) values ('Llz', 1);&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099047&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:27) &lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### INSERT INTO <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### SET&lt;br /&gt;### @1=3&lt;br /&gt;### @2='Llz'&lt;br /&gt;### @3=1 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=2 where user='Bob';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt; 1525099056&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:36)&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=1&lt;br /&gt;### SET&lt;br /&gt;### @1=2&lt;br /&gt;### @2='Bob'&lt;br /&gt;### @3=2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;width: 200px;&quot;&gt; update blink_tab set clicks=3 where user='Mary';&lt;/td&gt;&lt;td style=&quot;width: 216.667px;&quot;&gt;&lt;p&gt;1525099065&lt;/p&gt;&lt;p&gt;(2018/4/30 22:37:45) &lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;width: 201.111px;&quot;&gt;### UPDATE <code>blinkdb</code>.<code>blink_tab</code>&lt;br /&gt;### WHERE&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=2&lt;br /&gt;### SET&lt;br /&gt;### @1=1&lt;br /&gt;### @2='Mary'&lt;br /&gt;### @3=3 &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;</p>\n</li>\n<li>\n<p>简化一下binlog\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png\" alt></p>\n</li>\n<li>\n<p>replay binlog会得到如下表数据（按timestamp顺序)\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/2052C4F1-2EE4-41D1-8A8D-165A40827462.png\" alt></p>\n</li>\n<li>\n<p>表与binlog的关系简单示意如下\n<img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png\" alt></p>\n</li>\n</ul>\n<h1>流表对偶(duality)性</h1>\n<p>前面我花费了一些时间介绍了MySQL主备复制机制和binlog的数据格式，binlog中携带时间戳，我们将所有表的操作都按时间进行记录下来形成binlog，而对binlog的event进行重放的过程就是流数据处理的过程，重放的结果恰恰又形成了一张表。也就是表的操作会形成携带时间的事件流，对流的处理又会形成一张不断变化的表，表和流具有等价性，可以互转。随着时间推移，DML操作不断进行，那么表的内容也不断变化，具体如下：</p>\n<p><img src=\"/2019/01/01/Apache Flink 漫谈系列 - 流表对偶(duality)性/299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png\" alt></p>\n<p>如上图所示内容，流和表具备相同的特征：</p>\n<ul>\n<li>表 - Schema，Data，DML操作时间</li>\n<li>流 - Schema，Data, Data处理时间</li>\n</ul>\n<p>我们发现，虽然大多数表上面没有明确的显示出DML操作时间，但本质上数据库系统里面是有数据操作时间信息的，这个和流上数据的处理时间（processing time)/产生时间（event-time)相对应。流与表具备相同的特征，可以信息无损的相互转换，我称之为流表对偶(duality)性。</p>\n<p>上面我们描述的表，在流上称之为动态表(Dynamic Table)，原因是在流上面任何一个事件的到来都是对表上数据的一次更新(包括插入和删除),表的内容是不断的变化的，任何一个时刻流的状态和表的快照一一对应。流与动态表(Dynamic Table)在时间维度上面具有等价性，这种等价性我们称之为流和动态表(Dynamic Table)的对偶(duality)性。</p>\n<h1>小结</h1>\n<p>本篇主要介绍Apache Flink作为一个流计算平台为什么可以为用户提供SQL API。其根本原因是如果将流上的数据看做是结构化的数据，流任务的核心是将一个具有时间属性的结构化数据变成同样具有时间属性的另一个结构化数据，而表的数据变化过程binlog恰恰就是一份具有时间属性的流数据，流与表具有信息无损的相互转换的特性，这种流表对偶性也决定了Apache Flink可以采用SQL作为流任务的开发语言。</p>\n"},{"title":"Apache Flink 漫谈系列 - 持续查询(Continuous Queries)","date":"2019-03-01T10:18:18.000Z","_content":"# 实际问题\n我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。\n\n# 数据管理\n在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图：\n![](266A687D-8E31-4266-B52F-200B75244DDB.png)\n\n如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。\n\n那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？\n\n# 静态查询\n传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下：\n```\nselect * from flink_tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      1 |\n+----+------+--------+\n1 row in set (0.00 sec)\n```\n当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。\n\n# 持续查询\n什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着...\n\n# 静态/持续查询特点\n\n静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：\n| 查询类型 | 计算次数 |计算结果  |\n| --- | --- | --- |\n| 静态查询 | 1 | 最终结果 |\n| 持续查询 | 无限 | 不断更新 |\n\n        \n# 静态/持续查询关系\n接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下：\n```\nMySQL> insert into flink_tab(user, clicks) values ('Bob', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL> update flink_tab set clicks=2 where user='Mary';\nQuery OK, 1 row affected (0.06 sec)\n\n```\n\n这时候我们再进行查询 `select * from flink_tab` ，结果如下：\n```\nMySQL> select * from flink_tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      2 |\n|  2 | Bob  |      1 |\n+----+------+--------+\n2 rows in set (0.00 sec)\n```\n那么我们看见，相同的查询SQL(`select * from flink_tab`)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样：\n![](1401BEB3-37C3-4203-B48E-AABF6B08DB65.png)\n\n假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下：\n```\nMySQL> insert into flink_tab(user, clicks) values ('Llz', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL> update flink_tab set clicks=2 where user='Bob';\nQuery OK, 1 row affected (0.01 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL> update flink_tab set clicks=3 where user='Mary';\nQuery OK, 1 row affected (0.05 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n```\n到现在我们不难想象，上面图内容的核心要点如下：\n\n* 时间 \n* 表数据变化\n* 触发计算 \n* 计算结果更新\n\n接下来我们利用传统数据库现有的机制模拟一下持续查询...\n\n## 无PK的 Append only 场景\n接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下：\n\n\n|timestamp\t|user\t|clicks\n| --- | --- | --- |\n|1525099013\t|Mary\t|1|\n|1525099026\t|Bob\t|1|\n|1525099035\t|Mary\t|2|\n|1525099047\t|Llz\t|1|\n|1525099056\t|Bob\t|2|\n|1525099065\t|Mary\t|3|\n\n```\n// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中\n DELIMITER ;;\ncreate trigger flink_tab_ts_trigger_insert after insert\non flink_tab_ts for each row\n  begin\n       select ts, user, clicks from flink_tab_ts into OUTFILE '/Users/jincheng.sunjc/testdir/atas/trigger.sql';\n  end ;;\nDELIMITER ;\n```\n上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下：\n```\nMySQL> show variables like '%secure%';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| require_secure_transport | OFF   |\n| secure_file_priv         | NULL  |\n+--------------------------+-------+\n2 rows in set (0.00 sec)\n```\n上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=''打开写文件限制；\n```\nMySQL> show variables like '%secure%';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| require_secure_transport | OFF   |\n| secure_file_priv         |       |\n+--------------------------+-------+\n2 rows in set (0.00 sec)\n```\n下面我们对flink_tab_ts进行INSERT操作：\n\n![](166F2FE8-93FB-4966-87B7-F6C98DFF4915.png)\n\n\n我们再来看看6次trigger 查询计算的结果：\n\n![](146851FF-D705-4B2D-8C3B-77528BB8DAB5.png)\n\n大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。\n\n## 有PK的Update场景\n我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger：\n```\ndrop table flink_tab;\ncreate table flink_tab(\n    user VARCHAR(100) NOT NULL,\n    clicks INT NOT NULL,\n    PRIMARY KEY (user)\n );\n\n DELIMITER ;;\ncreate trigger flink_tab_trigger_insert after insert\non flink_tab for each row\n  begin\n       select user, clicks from flink_tab into OUTFILE '/tmp/trigger.sql';\n  end ;;\nDELIMITER ;\n\nDELIMITER ;;\ncreate trigger flink_tab_trigger_ after update\non flink_tab for each row\n  begin\n        select ts, user, clicks from flink_tab into OUTFILE '/tmp/trigger.sql';\n  end ;;\nDELIMITER ;\n```\n 同样我做如下6次DML操作，Trigger 6次查询计算：\n\n![](46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png)\n\n在来看看这次的结果与append only 有什么不同？\n\n![](E095C802-8CCF-4922-837C-6AAE54A335D8.png)\n\n我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)!\n\n## 关系定义 \n上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(`select * from flink_tab_ts`) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。\n\n那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点),  在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。\n\n**特别说明：** 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录!\n\n# Apache Flink 如何做到持续查询\n## 动态表上面持续查询\n在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图：\n![](43A59ED3-C317-4B5D-80F3-CC332933B93B.png)\n\n## 增量计算\n我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下：\n\n![](3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png)\n\n一个简单的计数和求和查询SQL：\n```\n// 求订单总数和所有订单的总金额\nselect count(id) as cnt，sum(amount)as sumAmount from order_tab;\n```\n这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图：\n![](BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png)\n\n如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即:\n```\nresult(n) = calculation(result(n-1), n)。\n```\n\n### 无PK的Append Only 场景 \n在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下：\n\n![](78D31D91-3693-44C8-A7E4-71D6798A78AE.png)\n\n\n### 有PK的Update 场景\n现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下：\n\n![](B2EEEC59-21F9-41E1-A306-8338A7A72A64.png)\n\n查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下：\n```\n CREATE TABLE order_tab(\n   id BIGINT,\n   region VARCHAR\n ) \n\nCREATE TABLE region_count_sink(\n   order_cnt BIGINT, \n   region_cnt BIGINT,\n   PRIMARY KEY(order_cnt) -- 主键\n) \n\n-- 按地区分组计算每个地区的订单数量\nCREATE VIEW order_count_view AS\n    SELECT\n        region, count(id) AS order_cnt\n    FROM  order_tab \n    GROUP BY region;\n\n-- 按订单数量分组统计具有相同订单数量的地区数量\nINSERT INTO region_count_sink \n    SELECT \n        order_cnt,\n        count(region) as region_cnt\n    FROM order_count_view \n    GROUP BY order_cnt;\n\n```\n上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，[了解更多](https://data.aliyun.com/product/sc))：\n![](A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png)\n\n上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。\n\n### 错误处理\n* 第一层分析：`SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region; `\n![](17FB1F6A-C813-43D4-AD6D-31FC03B84533.png)\n\n* 第二层分析：`SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;`\n![](A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png)\n\n按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？\n\n其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？\n\n### 正确处理\n* 第一层分析：`SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;`\n![](0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png)\n\n* 第二层分析：`SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;`\n![](5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png)\n\n上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下：\n```\ndef accumulate(acc: CountAccumulator): Unit = {\n    acc.f0 += 1L // acc.f0 存储记数\n}\n\ndef retract(acc: CountAccumulator, value: Any): Unit = {\n    if (value != null) {\n      acc.f0 -= 1L //acc.f0 存储记数\n    }\n}\n```\nApache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。\n\n# Apache Flink Connector 类型\n本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？\n\n## Apache Flink Source\n现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明：\n```\nSQL：\n\nCREATE TABLE inventory_tab(\n   product_id VARCHAR,\n   product_count BIGINT\n); \n\nCREATE TABLE sales_tab(\n   product_id VARCHAR,\n   sales_count BIGINT\n  ) ;\n\nCREATE TABLE join_sink(\n   product_id VARCHAR, \n   product_count BIGINT,\n   sales_count BIGINT,\n   PRIMARY KEY(product_id)\n);\n\nCREATE VIEW join_view AS\n    SELECT\n        l.product_id, \n        l.product_count,\n        r.sales_count\n    FROM inventory_tab l \n        JOIN  sales_tab r \n        ON l.product_id = r.product_id;\n\nINSERT INTO join_sink \n  SELECT \n      product_id, \n      product_count,\n      sales_count\n  FROM join_view ;\n  \n```\n  \n代码结构图：\n\n![](BE66E18F-DE01-42EC-9336-5F6172B22636.png)\n\n实现示意图：\n\n![](B5F9DD64-3574-480A-A409-BEE96561091B.png)\n\n上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。\n\n### 问题\n上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。\n\n那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？\n\n### 解决方案\n上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下：\n```\nSQL:\n\nCREATE TABLE inventory_tab(\n   product_id VARCHAR,\n   product_count BIGINT\n  ) \n\n CREATE TABLE sales_tab(\n   product_id VARCHAR,\n   sales_count BIGINT\n  )\nCREATE VIEW inventory_view AS\n    SELECT \n    product_id,\n    LAST_VALUE(product_count) AS product_count\n    FROM inventory_tab\n    GROUP BY product_id;\n\nCREATE VIEW sales_view AS\n    SELECT \n    product_id,\n    LAST_VALUE(sales_count) AS sales_count\n    FROM sales_tab\n    GROUP BY product_id;\n\nCREATE TABLE join_sink(\n   product_id VARCHAR, \n   product_count BIGINT,\n   sales_count BIGINT,\n   PRIMARY KEY(product_id)\n)WITH (\n    type = 'print'\n) ;\n\nCREATE VIEW join_view AS\n    SELECT\n        l.product_id, \n        l.product_count,\n        r.sales_count\n    FROM inventory_view l \n        JOIN  sales_view r \n        ON l.product_id = r.product_id;\n\n INSERT INTO join_sink \n  SELECT \n      product_id, \n      product_count,\n      sales_count\n  FROM join_view ;\n```\n代码结构：\n\n![](4134FE05-67E4-4625-824D-BBD7B444BFE6.png)\n\n\n示意图:\n![](D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png)\n\n如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。\n\n**说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。**\n\n## Apache Flink Sink\n在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型:\n\n* Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句；\n* Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新；\n* Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令；\n\n# 小结\n本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。\n","source":"_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries).md","raw":"---\ntitle: Apache Flink 漫谈系列 - 持续查询(Continuous Queries)\ndate: 2019-03-01 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# 实际问题\n我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。\n\n# 数据管理\n在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图：\n![](266A687D-8E31-4266-B52F-200B75244DDB.png)\n\n如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。\n\n那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？\n\n# 静态查询\n传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下：\n```\nselect * from flink_tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      1 |\n+----+------+--------+\n1 row in set (0.00 sec)\n```\n当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。\n\n# 持续查询\n什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着...\n\n# 静态/持续查询特点\n\n静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：\n| 查询类型 | 计算次数 |计算结果  |\n| --- | --- | --- |\n| 静态查询 | 1 | 最终结果 |\n| 持续查询 | 无限 | 不断更新 |\n\n        \n# 静态/持续查询关系\n接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下：\n```\nMySQL> insert into flink_tab(user, clicks) values ('Bob', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL> update flink_tab set clicks=2 where user='Mary';\nQuery OK, 1 row affected (0.06 sec)\n\n```\n\n这时候我们再进行查询 `select * from flink_tab` ，结果如下：\n```\nMySQL> select * from flink_tab;\n+----+------+--------+\n| id | user | clicks |\n+----+------+--------+\n|  1 | Mary |      2 |\n|  2 | Bob  |      1 |\n+----+------+--------+\n2 rows in set (0.00 sec)\n```\n那么我们看见，相同的查询SQL(`select * from flink_tab`)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样：\n![](1401BEB3-37C3-4203-B48E-AABF6B08DB65.png)\n\n假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下：\n```\nMySQL> insert into flink_tab(user, clicks) values ('Llz', 1);\nQuery OK, 1 row affected (0.08 sec)\n\nMySQL> update flink_tab set clicks=2 where user='Bob';\nQuery OK, 1 row affected (0.01 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nMySQL> update flink_tab set clicks=3 where user='Mary';\nQuery OK, 1 row affected (0.05 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n```\n到现在我们不难想象，上面图内容的核心要点如下：\n\n* 时间 \n* 表数据变化\n* 触发计算 \n* 计算结果更新\n\n接下来我们利用传统数据库现有的机制模拟一下持续查询...\n\n## 无PK的 Append only 场景\n接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下：\n\n\n|timestamp\t|user\t|clicks\n| --- | --- | --- |\n|1525099013\t|Mary\t|1|\n|1525099026\t|Bob\t|1|\n|1525099035\t|Mary\t|2|\n|1525099047\t|Llz\t|1|\n|1525099056\t|Bob\t|2|\n|1525099065\t|Mary\t|3|\n\n```\n// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中\n DELIMITER ;;\ncreate trigger flink_tab_ts_trigger_insert after insert\non flink_tab_ts for each row\n  begin\n       select ts, user, clicks from flink_tab_ts into OUTFILE '/Users/jincheng.sunjc/testdir/atas/trigger.sql';\n  end ;;\nDELIMITER ;\n```\n上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下：\n```\nMySQL> show variables like '%secure%';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| require_secure_transport | OFF   |\n| secure_file_priv         | NULL  |\n+--------------------------+-------+\n2 rows in set (0.00 sec)\n```\n上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=''打开写文件限制；\n```\nMySQL> show variables like '%secure%';\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| require_secure_transport | OFF   |\n| secure_file_priv         |       |\n+--------------------------+-------+\n2 rows in set (0.00 sec)\n```\n下面我们对flink_tab_ts进行INSERT操作：\n\n![](166F2FE8-93FB-4966-87B7-F6C98DFF4915.png)\n\n\n我们再来看看6次trigger 查询计算的结果：\n\n![](146851FF-D705-4B2D-8C3B-77528BB8DAB5.png)\n\n大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。\n\n## 有PK的Update场景\n我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger：\n```\ndrop table flink_tab;\ncreate table flink_tab(\n    user VARCHAR(100) NOT NULL,\n    clicks INT NOT NULL,\n    PRIMARY KEY (user)\n );\n\n DELIMITER ;;\ncreate trigger flink_tab_trigger_insert after insert\non flink_tab for each row\n  begin\n       select user, clicks from flink_tab into OUTFILE '/tmp/trigger.sql';\n  end ;;\nDELIMITER ;\n\nDELIMITER ;;\ncreate trigger flink_tab_trigger_ after update\non flink_tab for each row\n  begin\n        select ts, user, clicks from flink_tab into OUTFILE '/tmp/trigger.sql';\n  end ;;\nDELIMITER ;\n```\n 同样我做如下6次DML操作，Trigger 6次查询计算：\n\n![](46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png)\n\n在来看看这次的结果与append only 有什么不同？\n\n![](E095C802-8CCF-4922-837C-6AAE54A335D8.png)\n\n我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)!\n\n## 关系定义 \n上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(`select * from flink_tab_ts`) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。\n\n那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点),  在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。\n\n**特别说明：** 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录!\n\n# Apache Flink 如何做到持续查询\n## 动态表上面持续查询\n在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图：\n![](43A59ED3-C317-4B5D-80F3-CC332933B93B.png)\n\n## 增量计算\n我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下：\n\n![](3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png)\n\n一个简单的计数和求和查询SQL：\n```\n// 求订单总数和所有订单的总金额\nselect count(id) as cnt，sum(amount)as sumAmount from order_tab;\n```\n这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图：\n![](BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png)\n\n如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即:\n```\nresult(n) = calculation(result(n-1), n)。\n```\n\n### 无PK的Append Only 场景 \n在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下：\n\n![](78D31D91-3693-44C8-A7E4-71D6798A78AE.png)\n\n\n### 有PK的Update 场景\n现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下：\n\n![](B2EEEC59-21F9-41E1-A306-8338A7A72A64.png)\n\n查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下：\n```\n CREATE TABLE order_tab(\n   id BIGINT,\n   region VARCHAR\n ) \n\nCREATE TABLE region_count_sink(\n   order_cnt BIGINT, \n   region_cnt BIGINT,\n   PRIMARY KEY(order_cnt) -- 主键\n) \n\n-- 按地区分组计算每个地区的订单数量\nCREATE VIEW order_count_view AS\n    SELECT\n        region, count(id) AS order_cnt\n    FROM  order_tab \n    GROUP BY region;\n\n-- 按订单数量分组统计具有相同订单数量的地区数量\nINSERT INTO region_count_sink \n    SELECT \n        order_cnt,\n        count(region) as region_cnt\n    FROM order_count_view \n    GROUP BY order_cnt;\n\n```\n上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，[了解更多](https://data.aliyun.com/product/sc))：\n![](A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png)\n\n上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。\n\n### 错误处理\n* 第一层分析：`SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region; `\n![](17FB1F6A-C813-43D4-AD6D-31FC03B84533.png)\n\n* 第二层分析：`SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;`\n![](A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png)\n\n按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？\n\n其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？\n\n### 正确处理\n* 第一层分析：`SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;`\n![](0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png)\n\n* 第二层分析：`SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;`\n![](5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png)\n\n上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下：\n```\ndef accumulate(acc: CountAccumulator): Unit = {\n    acc.f0 += 1L // acc.f0 存储记数\n}\n\ndef retract(acc: CountAccumulator, value: Any): Unit = {\n    if (value != null) {\n      acc.f0 -= 1L //acc.f0 存储记数\n    }\n}\n```\nApache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。\n\n# Apache Flink Connector 类型\n本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？\n\n## Apache Flink Source\n现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明：\n```\nSQL：\n\nCREATE TABLE inventory_tab(\n   product_id VARCHAR,\n   product_count BIGINT\n); \n\nCREATE TABLE sales_tab(\n   product_id VARCHAR,\n   sales_count BIGINT\n  ) ;\n\nCREATE TABLE join_sink(\n   product_id VARCHAR, \n   product_count BIGINT,\n   sales_count BIGINT,\n   PRIMARY KEY(product_id)\n);\n\nCREATE VIEW join_view AS\n    SELECT\n        l.product_id, \n        l.product_count,\n        r.sales_count\n    FROM inventory_tab l \n        JOIN  sales_tab r \n        ON l.product_id = r.product_id;\n\nINSERT INTO join_sink \n  SELECT \n      product_id, \n      product_count,\n      sales_count\n  FROM join_view ;\n  \n```\n  \n代码结构图：\n\n![](BE66E18F-DE01-42EC-9336-5F6172B22636.png)\n\n实现示意图：\n\n![](B5F9DD64-3574-480A-A409-BEE96561091B.png)\n\n上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。\n\n### 问题\n上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。\n\n那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？\n\n### 解决方案\n上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下：\n```\nSQL:\n\nCREATE TABLE inventory_tab(\n   product_id VARCHAR,\n   product_count BIGINT\n  ) \n\n CREATE TABLE sales_tab(\n   product_id VARCHAR,\n   sales_count BIGINT\n  )\nCREATE VIEW inventory_view AS\n    SELECT \n    product_id,\n    LAST_VALUE(product_count) AS product_count\n    FROM inventory_tab\n    GROUP BY product_id;\n\nCREATE VIEW sales_view AS\n    SELECT \n    product_id,\n    LAST_VALUE(sales_count) AS sales_count\n    FROM sales_tab\n    GROUP BY product_id;\n\nCREATE TABLE join_sink(\n   product_id VARCHAR, \n   product_count BIGINT,\n   sales_count BIGINT,\n   PRIMARY KEY(product_id)\n)WITH (\n    type = 'print'\n) ;\n\nCREATE VIEW join_view AS\n    SELECT\n        l.product_id, \n        l.product_count,\n        r.sales_count\n    FROM inventory_view l \n        JOIN  sales_view r \n        ON l.product_id = r.product_id;\n\n INSERT INTO join_sink \n  SELECT \n      product_id, \n      product_count,\n      sales_count\n  FROM join_view ;\n```\n代码结构：\n\n![](4134FE05-67E4-4625-824D-BBD7B444BFE6.png)\n\n\n示意图:\n![](D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png)\n\n如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。\n\n**说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。**\n\n## Apache Flink Sink\n在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型:\n\n* Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句；\n* Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新；\n* Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令；\n\n# 小结\n本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。\n","slug":"Apache Flink 漫谈系列 - 持续查询(Continuous Queries)","published":1,"updated":"2019-07-13T13:10:01.207Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fu000ys94hhr6r8oxe","content":"<h1>实际问题</h1>\n<p>我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。</p>\n<h1>数据管理</h1>\n<p>在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/266A687D-8E31-4266-B52F-200B75244DDB.png\" alt></p>\n<p>如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。</p>\n<p>那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？</p>\n<h1>静态查询</h1>\n<p>传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from flink_tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。</p>\n<h1>持续查询</h1>\n<p>什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着...</p>\n<h1>静态/持续查询特点</h1>\n<p>静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：</p>\n<table>\n<thead>\n<tr>\n<th>查询类型</th>\n<th>计算次数</th>\n<th>计算结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>静态查询</td>\n<td>1</td>\n<td>最终结果</td>\n</tr>\n<tr>\n<td>持续查询</td>\n<td>无限</td>\n<td>不断更新</td>\n</tr>\n</tbody>\n</table>\n<h1>静态/持续查询关系</h1>\n<p>接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Bob&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.06 sec)</span><br></pre></td></tr></table></figure></p>\n<p>这时候我们再进行查询 <code>select * from flink_tab</code> ，结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; select * from flink_tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      2 |</span><br><span class=\"line\">|  2 | Bob  |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>那么我们看见，相同的查询SQL(<code>select * from flink_tab</code>)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/1401BEB3-37C3-4203-B48E-AABF6B08DB65.png\" alt></p>\n<p>假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Llz&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Bob&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.01 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=3 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.05 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br></pre></td></tr></table></figure></p>\n<p>到现在我们不难想象，上面图内容的核心要点如下：</p>\n<ul>\n<li>时间</li>\n<li>表数据变化</li>\n<li>触发计算</li>\n<li>计算结果更新</li>\n</ul>\n<p>接下来我们利用传统数据库现有的机制模拟一下持续查询...</p>\n<h2>无PK的 Append only 场景</h2>\n<p>接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下：</p>\n<table>\n<thead>\n<tr>\n<th>timestamp</th>\n<th>user</th>\n<th>clicks</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1525099013</td>\n<td>Mary</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099026</td>\n<td>Bob</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099035</td>\n<td>Mary</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1525099047</td>\n<td>Llz</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099056</td>\n<td>Bob</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1525099065</td>\n<td>Mary</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中</span><br><span class=\"line\"> DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_ts_trigger_insert after insert</span><br><span class=\"line\">on flink_tab_ts for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">       select ts, user, clicks from flink_tab_ts into OUTFILE &apos;/Users/jincheng.sunjc/testdir/atas/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br></pre></td></tr></table></figure></p>\n<p>上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| Variable_name            | Value |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| require_secure_transport | OFF   |</span><br><span class=\"line\">| secure_file_priv         | NULL  |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=''打开写文件限制；\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| Variable_name            | Value |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| require_secure_transport | OFF   |</span><br><span class=\"line\">| secure_file_priv         |       |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>下面我们对flink_tab_ts进行INSERT操作：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/166F2FE8-93FB-4966-87B7-F6C98DFF4915.png\" alt></p>\n<p>我们再来看看6次trigger 查询计算的结果：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/146851FF-D705-4B2D-8C3B-77528BB8DAB5.png\" alt></p>\n<p>大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。</p>\n<h2>有PK的Update场景</h2>\n<p>我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drop table flink_tab;</span><br><span class=\"line\">create table flink_tab(</span><br><span class=\"line\">    user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">    clicks INT NOT NULL,</span><br><span class=\"line\">    PRIMARY KEY (user)</span><br><span class=\"line\"> );</span><br><span class=\"line\"></span><br><span class=\"line\"> DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_trigger_insert after insert</span><br><span class=\"line\">on flink_tab for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">       select user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br><span class=\"line\"></span><br><span class=\"line\">DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_trigger_ after update</span><br><span class=\"line\">on flink_tab for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">        select ts, user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br></pre></td></tr></table></figure></p>\n<p>同样我做如下6次DML操作，Trigger 6次查询计算：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png\" alt></p>\n<p>在来看看这次的结果与append only 有什么不同？</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/E095C802-8CCF-4922-837C-6AAE54A335D8.png\" alt></p>\n<p>我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)!</p>\n<h2>关系定义</h2>\n<p>上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(<code>select * from flink_tab_ts</code>) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。</p>\n<p>那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点),  在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。</p>\n<p><strong>特别说明：</strong> 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录!</p>\n<h1>Apache Flink 如何做到持续查询</h1>\n<h2>动态表上面持续查询</h2>\n<p>在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/43A59ED3-C317-4B5D-80F3-CC332933B93B.png\" alt></p>\n<h2>增量计算</h2>\n<p>我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png\" alt></p>\n<p>一个简单的计数和求和查询SQL：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 求订单总数和所有订单的总金额</span><br><span class=\"line\">select count(id) as cnt，sum(amount)as sumAmount from order_tab;</span><br></pre></td></tr></table></figure></p>\n<p>这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png\" alt></p>\n<p>如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">result(n) = calculation(result(n-1), n)。</span><br></pre></td></tr></table></figure></p>\n<h3>无PK的Append Only 场景</h3>\n<p>在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/78D31D91-3693-44C8-A7E4-71D6798A78AE.png\" alt></p>\n<h3>有PK的Update 场景</h3>\n<p>现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B2EEEC59-21F9-41E1-A306-8338A7A72A64.png\" alt></p>\n<p>查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CREATE TABLE order_tab(</span><br><span class=\"line\">   id BIGINT,</span><br><span class=\"line\">   region VARCHAR</span><br><span class=\"line\"> ) </span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE region_count_sink(</span><br><span class=\"line\">   order_cnt BIGINT, </span><br><span class=\"line\">   region_cnt BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(order_cnt) -- 主键</span><br><span class=\"line\">) </span><br><span class=\"line\"></span><br><span class=\"line\">-- 按地区分组计算每个地区的订单数量</span><br><span class=\"line\">CREATE VIEW order_count_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        region, count(id) AS order_cnt</span><br><span class=\"line\">    FROM  order_tab </span><br><span class=\"line\">    GROUP BY region;</span><br><span class=\"line\"></span><br><span class=\"line\">-- 按订单数量分组统计具有相同订单数量的地区数量</span><br><span class=\"line\">INSERT INTO region_count_sink </span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">        order_cnt,</span><br><span class=\"line\">        count(region) as region_cnt</span><br><span class=\"line\">    FROM order_count_view </span><br><span class=\"line\">    GROUP BY order_cnt;</span><br></pre></td></tr></table></figure></p>\n<p>上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，<a href=\"https://data.aliyun.com/product/sc\" target=\"_blank\" rel=\"noopener\">了解更多</a>)：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png\" alt></p>\n<p>上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。</p>\n<h3>错误处理</h3>\n<ul>\n<li>\n<p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/17FB1F6A-C813-43D4-AD6D-31FC03B84533.png\" alt></p>\n</li>\n<li>\n<p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png\" alt></p>\n</li>\n</ul>\n<p>按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？</p>\n<p>其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？</p>\n<h3>正确处理</h3>\n<ul>\n<li>\n<p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png\" alt></p>\n</li>\n<li>\n<p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png\" alt></p>\n</li>\n</ul>\n<p>上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def accumulate(acc: CountAccumulator): Unit = &#123;</span><br><span class=\"line\">    acc.f0 += 1L // acc.f0 存储记数</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">def retract(acc: CountAccumulator, value: Any): Unit = &#123;</span><br><span class=\"line\">    if (value != null) &#123;</span><br><span class=\"line\">      acc.f0 -= 1L //acc.f0 存储记数</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Apache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。</p>\n<h1>Apache Flink Connector 类型</h1>\n<p>本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？</p>\n<h2>Apache Flink Source</h2>\n<p>现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SQL：</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE inventory_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   product_count BIGINT</span><br><span class=\"line\">); </span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE sales_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   sales_count BIGINT</span><br><span class=\"line\">  ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE join_sink(</span><br><span class=\"line\">   product_id VARCHAR, </span><br><span class=\"line\">   product_count BIGINT,</span><br><span class=\"line\">   sales_count BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(product_id)</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW join_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        l.product_id, </span><br><span class=\"line\">        l.product_count,</span><br><span class=\"line\">        r.sales_count</span><br><span class=\"line\">    FROM inventory_tab l </span><br><span class=\"line\">        JOIN  sales_tab r </span><br><span class=\"line\">        ON l.product_id = r.product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">INSERT INTO join_sink </span><br><span class=\"line\">  SELECT </span><br><span class=\"line\">      product_id, </span><br><span class=\"line\">      product_count,</span><br><span class=\"line\">      sales_count</span><br><span class=\"line\">  FROM join_view ;</span><br></pre></td></tr></table></figure></p>\n<p>代码结构图：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BE66E18F-DE01-42EC-9336-5F6172B22636.png\" alt></p>\n<p>实现示意图：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B5F9DD64-3574-480A-A409-BEE96561091B.png\" alt></p>\n<p>上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。</p>\n<h3>问题</h3>\n<p>上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。</p>\n<p>那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？</p>\n<h3>解决方案</h3>\n<p>上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SQL:</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE inventory_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   product_count BIGINT</span><br><span class=\"line\">  ) </span><br><span class=\"line\"></span><br><span class=\"line\"> CREATE TABLE sales_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   sales_count BIGINT</span><br><span class=\"line\">  )</span><br><span class=\"line\">CREATE VIEW inventory_view AS</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    product_id,</span><br><span class=\"line\">    LAST_VALUE(product_count) AS product_count</span><br><span class=\"line\">    FROM inventory_tab</span><br><span class=\"line\">    GROUP BY product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW sales_view AS</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    product_id,</span><br><span class=\"line\">    LAST_VALUE(sales_count) AS sales_count</span><br><span class=\"line\">    FROM sales_tab</span><br><span class=\"line\">    GROUP BY product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE join_sink(</span><br><span class=\"line\">   product_id VARCHAR, </span><br><span class=\"line\">   product_count BIGINT,</span><br><span class=\"line\">   sales_count BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(product_id)</span><br><span class=\"line\">)WITH (</span><br><span class=\"line\">    type = &apos;print&apos;</span><br><span class=\"line\">) ;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW join_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        l.product_id, </span><br><span class=\"line\">        l.product_count,</span><br><span class=\"line\">        r.sales_count</span><br><span class=\"line\">    FROM inventory_view l </span><br><span class=\"line\">        JOIN  sales_view r </span><br><span class=\"line\">        ON l.product_id = r.product_id;</span><br><span class=\"line\"></span><br><span class=\"line\"> INSERT INTO join_sink </span><br><span class=\"line\">  SELECT </span><br><span class=\"line\">      product_id, </span><br><span class=\"line\">      product_count,</span><br><span class=\"line\">      sales_count</span><br><span class=\"line\">  FROM join_view ;</span><br></pre></td></tr></table></figure></p>\n<p>代码结构：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/4134FE05-67E4-4625-824D-BBD7B444BFE6.png\" alt></p>\n<p>示意图:\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png\" alt></p>\n<p>如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。</p>\n<p><strong>说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。</strong></p>\n<h2>Apache Flink Sink</h2>\n<p>在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型:</p>\n<ul>\n<li>Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句；</li>\n<li>Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新；</li>\n<li>Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令；</li>\n</ul>\n<h1>小结</h1>\n<p>本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>实际问题</h1>\n<p>我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。</p>\n<h1>数据管理</h1>\n<p>在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/266A687D-8E31-4266-B52F-200B75244DDB.png\" alt></p>\n<p>如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。</p>\n<p>那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？</p>\n<h1>静态查询</h1>\n<p>传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from flink_tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。</p>\n<h1>持续查询</h1>\n<p>什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着...</p>\n<h1>静态/持续查询特点</h1>\n<p>静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：</p>\n<table>\n<thead>\n<tr>\n<th>查询类型</th>\n<th>计算次数</th>\n<th>计算结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>静态查询</td>\n<td>1</td>\n<td>最终结果</td>\n</tr>\n<tr>\n<td>持续查询</td>\n<td>无限</td>\n<td>不断更新</td>\n</tr>\n</tbody>\n</table>\n<h1>静态/持续查询关系</h1>\n<p>接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Bob&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.06 sec)</span><br></pre></td></tr></table></figure></p>\n<p>这时候我们再进行查询 <code>select * from flink_tab</code> ，结果如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; select * from flink_tab;</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">| id | user | clicks |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">|  1 | Mary |      2 |</span><br><span class=\"line\">|  2 | Bob  |      1 |</span><br><span class=\"line\">+----+------+--------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>那么我们看见，相同的查询SQL(<code>select * from flink_tab</code>)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/1401BEB3-37C3-4203-B48E-AABF6B08DB65.png\" alt></p>\n<p>假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Llz&apos;, 1);</span><br><span class=\"line\">Query OK, 1 row affected (0.08 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Bob&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.01 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class=\"line\"></span><br><span class=\"line\">MySQL&gt; update flink_tab set clicks=3 where user=&apos;Mary&apos;;</span><br><span class=\"line\">Query OK, 1 row affected (0.05 sec)</span><br><span class=\"line\">Rows matched: 1  Changed: 1  Warnings: 0</span><br></pre></td></tr></table></figure></p>\n<p>到现在我们不难想象，上面图内容的核心要点如下：</p>\n<ul>\n<li>时间</li>\n<li>表数据变化</li>\n<li>触发计算</li>\n<li>计算结果更新</li>\n</ul>\n<p>接下来我们利用传统数据库现有的机制模拟一下持续查询...</p>\n<h2>无PK的 Append only 场景</h2>\n<p>接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下：</p>\n<table>\n<thead>\n<tr>\n<th>timestamp</th>\n<th>user</th>\n<th>clicks</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1525099013</td>\n<td>Mary</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099026</td>\n<td>Bob</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099035</td>\n<td>Mary</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1525099047</td>\n<td>Llz</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1525099056</td>\n<td>Bob</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1525099065</td>\n<td>Mary</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中</span><br><span class=\"line\"> DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_ts_trigger_insert after insert</span><br><span class=\"line\">on flink_tab_ts for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">       select ts, user, clicks from flink_tab_ts into OUTFILE &apos;/Users/jincheng.sunjc/testdir/atas/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br></pre></td></tr></table></figure></p>\n<p>上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| Variable_name            | Value |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| require_secure_transport | OFF   |</span><br><span class=\"line\">| secure_file_priv         | NULL  |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=''打开写文件限制；\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| Variable_name            | Value |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">| require_secure_transport | OFF   |</span><br><span class=\"line\">| secure_file_priv         |       |</span><br><span class=\"line\">+--------------------------+-------+</span><br><span class=\"line\">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>下面我们对flink_tab_ts进行INSERT操作：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/166F2FE8-93FB-4966-87B7-F6C98DFF4915.png\" alt></p>\n<p>我们再来看看6次trigger 查询计算的结果：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/146851FF-D705-4B2D-8C3B-77528BB8DAB5.png\" alt></p>\n<p>大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。</p>\n<h2>有PK的Update场景</h2>\n<p>我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drop table flink_tab;</span><br><span class=\"line\">create table flink_tab(</span><br><span class=\"line\">    user VARCHAR(100) NOT NULL,</span><br><span class=\"line\">    clicks INT NOT NULL,</span><br><span class=\"line\">    PRIMARY KEY (user)</span><br><span class=\"line\"> );</span><br><span class=\"line\"></span><br><span class=\"line\"> DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_trigger_insert after insert</span><br><span class=\"line\">on flink_tab for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">       select user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br><span class=\"line\"></span><br><span class=\"line\">DELIMITER ;;</span><br><span class=\"line\">create trigger flink_tab_trigger_ after update</span><br><span class=\"line\">on flink_tab for each row</span><br><span class=\"line\">  begin</span><br><span class=\"line\">        select ts, user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class=\"line\">  end ;;</span><br><span class=\"line\">DELIMITER ;</span><br></pre></td></tr></table></figure></p>\n<p>同样我做如下6次DML操作，Trigger 6次查询计算：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png\" alt></p>\n<p>在来看看这次的结果与append only 有什么不同？</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/E095C802-8CCF-4922-837C-6AAE54A335D8.png\" alt></p>\n<p>我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)!</p>\n<h2>关系定义</h2>\n<p>上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(<code>select * from flink_tab_ts</code>) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。</p>\n<p>那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点),  在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。</p>\n<p><strong>特别说明：</strong> 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录!</p>\n<h1>Apache Flink 如何做到持续查询</h1>\n<h2>动态表上面持续查询</h2>\n<p>在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/43A59ED3-C317-4B5D-80F3-CC332933B93B.png\" alt></p>\n<h2>增量计算</h2>\n<p>我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png\" alt></p>\n<p>一个简单的计数和求和查询SQL：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 求订单总数和所有订单的总金额</span><br><span class=\"line\">select count(id) as cnt，sum(amount)as sumAmount from order_tab;</span><br></pre></td></tr></table></figure></p>\n<p>这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png\" alt></p>\n<p>如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">result(n) = calculation(result(n-1), n)。</span><br></pre></td></tr></table></figure></p>\n<h3>无PK的Append Only 场景</h3>\n<p>在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/78D31D91-3693-44C8-A7E4-71D6798A78AE.png\" alt></p>\n<h3>有PK的Update 场景</h3>\n<p>现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B2EEEC59-21F9-41E1-A306-8338A7A72A64.png\" alt></p>\n<p>查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CREATE TABLE order_tab(</span><br><span class=\"line\">   id BIGINT,</span><br><span class=\"line\">   region VARCHAR</span><br><span class=\"line\"> ) </span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE region_count_sink(</span><br><span class=\"line\">   order_cnt BIGINT, </span><br><span class=\"line\">   region_cnt BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(order_cnt) -- 主键</span><br><span class=\"line\">) </span><br><span class=\"line\"></span><br><span class=\"line\">-- 按地区分组计算每个地区的订单数量</span><br><span class=\"line\">CREATE VIEW order_count_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        region, count(id) AS order_cnt</span><br><span class=\"line\">    FROM  order_tab </span><br><span class=\"line\">    GROUP BY region;</span><br><span class=\"line\"></span><br><span class=\"line\">-- 按订单数量分组统计具有相同订单数量的地区数量</span><br><span class=\"line\">INSERT INTO region_count_sink </span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">        order_cnt,</span><br><span class=\"line\">        count(region) as region_cnt</span><br><span class=\"line\">    FROM order_count_view </span><br><span class=\"line\">    GROUP BY order_cnt;</span><br></pre></td></tr></table></figure></p>\n<p>上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，<a href=\"https://data.aliyun.com/product/sc\" target=\"_blank\" rel=\"noopener\">了解更多</a>)：\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png\" alt></p>\n<p>上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。</p>\n<h3>错误处理</h3>\n<ul>\n<li>\n<p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/17FB1F6A-C813-43D4-AD6D-31FC03B84533.png\" alt></p>\n</li>\n<li>\n<p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png\" alt></p>\n</li>\n</ul>\n<p>按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？</p>\n<p>其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？</p>\n<h3>正确处理</h3>\n<ul>\n<li>\n<p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png\" alt></p>\n</li>\n<li>\n<p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code>\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png\" alt></p>\n</li>\n</ul>\n<p>上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def accumulate(acc: CountAccumulator): Unit = &#123;</span><br><span class=\"line\">    acc.f0 += 1L // acc.f0 存储记数</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">def retract(acc: CountAccumulator, value: Any): Unit = &#123;</span><br><span class=\"line\">    if (value != null) &#123;</span><br><span class=\"line\">      acc.f0 -= 1L //acc.f0 存储记数</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Apache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。</p>\n<h1>Apache Flink Connector 类型</h1>\n<p>本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？</p>\n<h2>Apache Flink Source</h2>\n<p>现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SQL：</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE inventory_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   product_count BIGINT</span><br><span class=\"line\">); </span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE sales_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   sales_count BIGINT</span><br><span class=\"line\">  ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE join_sink(</span><br><span class=\"line\">   product_id VARCHAR, </span><br><span class=\"line\">   product_count BIGINT,</span><br><span class=\"line\">   sales_count BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(product_id)</span><br><span class=\"line\">);</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW join_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        l.product_id, </span><br><span class=\"line\">        l.product_count,</span><br><span class=\"line\">        r.sales_count</span><br><span class=\"line\">    FROM inventory_tab l </span><br><span class=\"line\">        JOIN  sales_tab r </span><br><span class=\"line\">        ON l.product_id = r.product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">INSERT INTO join_sink </span><br><span class=\"line\">  SELECT </span><br><span class=\"line\">      product_id, </span><br><span class=\"line\">      product_count,</span><br><span class=\"line\">      sales_count</span><br><span class=\"line\">  FROM join_view ;</span><br></pre></td></tr></table></figure></p>\n<p>代码结构图：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BE66E18F-DE01-42EC-9336-5F6172B22636.png\" alt></p>\n<p>实现示意图：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B5F9DD64-3574-480A-A409-BEE96561091B.png\" alt></p>\n<p>上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。</p>\n<h3>问题</h3>\n<p>上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。</p>\n<p>那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？</p>\n<h3>解决方案</h3>\n<p>上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SQL:</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE inventory_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   product_count BIGINT</span><br><span class=\"line\">  ) </span><br><span class=\"line\"></span><br><span class=\"line\"> CREATE TABLE sales_tab(</span><br><span class=\"line\">   product_id VARCHAR,</span><br><span class=\"line\">   sales_count BIGINT</span><br><span class=\"line\">  )</span><br><span class=\"line\">CREATE VIEW inventory_view AS</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    product_id,</span><br><span class=\"line\">    LAST_VALUE(product_count) AS product_count</span><br><span class=\"line\">    FROM inventory_tab</span><br><span class=\"line\">    GROUP BY product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW sales_view AS</span><br><span class=\"line\">    SELECT </span><br><span class=\"line\">    product_id,</span><br><span class=\"line\">    LAST_VALUE(sales_count) AS sales_count</span><br><span class=\"line\">    FROM sales_tab</span><br><span class=\"line\">    GROUP BY product_id;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE join_sink(</span><br><span class=\"line\">   product_id VARCHAR, </span><br><span class=\"line\">   product_count BIGINT,</span><br><span class=\"line\">   sales_count BIGINT,</span><br><span class=\"line\">   PRIMARY KEY(product_id)</span><br><span class=\"line\">)WITH (</span><br><span class=\"line\">    type = &apos;print&apos;</span><br><span class=\"line\">) ;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE VIEW join_view AS</span><br><span class=\"line\">    SELECT</span><br><span class=\"line\">        l.product_id, </span><br><span class=\"line\">        l.product_count,</span><br><span class=\"line\">        r.sales_count</span><br><span class=\"line\">    FROM inventory_view l </span><br><span class=\"line\">        JOIN  sales_view r </span><br><span class=\"line\">        ON l.product_id = r.product_id;</span><br><span class=\"line\"></span><br><span class=\"line\"> INSERT INTO join_sink </span><br><span class=\"line\">  SELECT </span><br><span class=\"line\">      product_id, </span><br><span class=\"line\">      product_count,</span><br><span class=\"line\">      sales_count</span><br><span class=\"line\">  FROM join_view ;</span><br></pre></td></tr></table></figure></p>\n<p>代码结构：</p>\n<p><img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/4134FE05-67E4-4625-824D-BBD7B444BFE6.png\" alt></p>\n<p>示意图:\n<img src=\"/2019/03/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png\" alt></p>\n<p>如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。</p>\n<p><strong>说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。</strong></p>\n<h2>Apache Flink Sink</h2>\n<p>在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型:</p>\n<ul>\n<li>Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句；</li>\n<li>Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新；</li>\n<li>Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令；</li>\n</ul>\n<h1>小结</h1>\n<p>本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。</p>\n"},{"title":"Apache Flink 说道系列- Python API 中如何使用 Kafka","_content":"\n# 开篇说道\n老子说\"圣人为腹不为目\"，核心意思是说看问题要看本质，不要着眼于外表。交人处事也是一样，不要看其外表和语言要看内心和本质。这里和大家分享一个故事，\"九方皋(Gao)相马\"。\n\n春秋时期的伯乐是识别千里马的大师，在他上了年纪之后 秦穆公 对他说:\"在你的子孙里面给我推荐一个能够识别千里马的人才吧？\"，伯乐说:\"在我的子孙中没有这样的人才，不过我有个朋友，名叫 九方皋 是个相马的高手，我推荐给您吧\"。\n\n于是 秦穆公 召见 九方皋，让他去出巡千里马，过了数月九方皋回来禀告说:\"我已经找到千里马了，就在沙丘那个地方\"。于是 秦穆公 兴奋的问:\"那是一匹什么样子的马啊\"？九方皋 回答:\"是一匹黄色的母马\"。\n不过 秦穆公 派人把那匹马找来发现是一匹黑色的公马。秦穆公很生气，就派人把伯乐叫来说:\"你推荐的相马的人实在糟透了，他连马的颜色和公母都分不清，怎么能认识千里马呢\"？ 伯乐听了内心无比佩服的对 秦穆公 说:\"这就是他比我还高明的地方，他重视的是马的精神，忽略了马的外表，重视马的内在品质，而忽略了马的颜色和雌雄，九方皋只关注他所关注的重点，而忽略不重要的特征，这样才真的能找到难寻的好马啊\"。后来经过检验这的确是一匹天下少有的千里马。\n\n\n所以\"圣人为腹不为目\"，注重本质而忽略外在。祝愿大家在日常生活和工作中能看清事情的内在和深层次的所在，而后做出你最正确的决定。\n\n# 如何创建Source\nApache Flink 1.9 有两种方式创建Source：\n* Table Descriptor\n* Table Source\n\n## Table Descriptor\n利用Descriptor方式创建Source是比较推荐的做法，以简单的CSV示例如下：\n```\nt_env.connect(FileSystem()\n    .path(source_path)\n    .with_format(OldCsv()\n        .field(\"word\",  DataTypes.STRING()))\n    .with_schema(Schema()\n        .field(\"word\", DataTypes.STRING()))      \n    .register_table_source(\"source\")\n\ntable = t_env.scan(\"source\")\n```\n\n## Table Source\n创建TableSource的方式也可以完成Source的创建，如下:\n```\ncsv_source = CsvTableSource(\n    source_path, \n    [\"word\"],  \n    [DataTypes.STRING()])\n    \nt_env.register_table_source(\"source\", csv_source)\n\ntable = t_env.scan(\"source\")\n```\n\n# 如何创建Sink\n和创建Source一样，Apache Flink 1.9 有两种方式创建Sink：\n* Table Descriptor\n* Table Sink\n\n## Table Descriptor\n利用Descriptor方式创建Sink是比较推荐的做法，以简单的CSV示例如下：\n```\nt_env.connect(FileSystem()\n    .path(result_path)\n    .with_format(OldCsv()\n        .field(\"word\",  DataTypes.STRING()))\n    .with_schema(Schema()\n        .field(\"word\", DataTypes.STRING()))      \n    .register_table_sink(\"results\")\n\nresult_table.insert_into(\"results\")\n```\n\n## Table Sink\n创建TableSink的方式也可以完成Sink的创建，如下:\n```\ncsv_sink = CsvTableSink(\n    [\"word\"],  \n    [DataTypes.STRING()])\n    \nt_env.register_table_sink(\"results\", csv_sink)\nresult_table.insert_into(\"results\")\n```\n\n## 关于Format和Schema\n不论是Source和Sink在利用Table Descriptor进行实例化的时候都是需要设置Format和Schema的，那么什么是Format和Schema呢？\n* Format - 是描述数据在外部存储系统里面的数据格式。\n* Schema - 是外部数据加载到Flink系统之后，形成Table的数据结构描述。\n\n# Kafka connector JAR加载\n如《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》所描述的问题一样，我们需要显示的将Kafka connector JARs添加到classpath下面。所以在源码构建PyFlink发布包时候，需要在完成源码编译之后，将Kafka相关的JARs复制到 `build-target/lib`下面。并且需要对应的Format的JARs。以`kafka-0.11`和`Json` Format为例：\n\nCopy Kafka相关JARs\n```\ncp flink-connectors/flink-sql-connector-kafka-0.11/target/flink-sql-connector-kafka-0.11_*-SNAPSHOT.jar build-target/lib\n```\n\nCopy Json Format相关JARs\n```\ncp flink-formats/flink-json/target/flink-json-*-SNAPSHOT-sql-jar.jar build-target/lib\n```\n构建PyFlink发布包并安装：\n```\ncd flink-python; python setup.py sdist \npip install dist/*.tar.gz --user\n```\n环境安装的详细内容可以参考《Apache Flink 说道系列- Python Table API 开发环境搭建》。\n\n# 安装Kafka环境\n如果你已经有Kafka的集群环境，可以忽略本步骤，如果没有，为了完成本篇的测试，你可以部署一个简单的Kafka环境。\n* 下载Kafka安装包\n```\nwget https://archive.apache.org/dist/kafka/0.11.0.3/kafka_2.11-0.11.0.3.tgz\n```\n* 解压\n```\ntar zxvf kafka_2.11-0.11.0.3.tgz\n```\n* 启动Zookeeper\n```\ncd kafka_2.11-0.11.0.3; bin/zookeeper-server-start.sh config/zookeeper.properties\n\n最终输出：\n[2019-08-28 08:47:16,437] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n[2019-08-28 08:47:16,478] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n```\n\n* 支持删除Topic\n  要支持删除Topic，我们需要修改server.properties配置，将`delete.topic.enable=true` 打开。\n  \n* 启动Kafka\n```\nbin/kafka-server-start.sh config/server.properties\n\n最终输出：\n[2019-08-28 08:49:20,280] INFO Kafka commitId : 26ddb9e3197be39a (org.apache.kafka.common.utils.AppInfoParser)\n[2019-08-28 08:49:20,281] INFO [Kafka Server 0], started (kafka.server.KafkaServer)\n```\n如上过程我们完成了简单Kafka的环境搭建。更多细节参考Kafka[官方文档](https://kafka.apache.org/quickstart)\n# 编写一个简单示例\n这个示例我们从Kafka读取数据，然后创建一个简单的tumble window，最终将计算结果写到CSV文件系统里面。\n\n## 数据结构\n我们在Kafka中以Json的数据格式进行存储，假设有如下数据结构：\n```\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"a\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"b\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"c\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"time\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"format\": \"date-time\"\n\t\t}\n\t}\n}\n```\n如上数据结构中time是为了演示event-time window，其他字段没有特殊含义。\n\n## 数据准备\n我们按如上数据结构来向Kafka中写入测试数据，我用python操作Kafka，需要安装Kafka的PythonAPI，如下：\n```\npip install kafka-python\n```\n将测试数据写入Kafka：\n```\n# -*- coding: UTF-8 -*-\n\nfrom kafka import KafkaProducer\nfrom kafka import KafkaAdminClient\nfrom kafka import KafkaConsumer\nfrom kafka.admin import NewTopic\nimport json\n\n# 写入消息\ndef send_msg(topic='test', msg=None):\n    producer = KafkaProducer(bootstrap_servers='localhost:9092',\n                             value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n    if msg is not None:\n        future = producer.send(topic, msg)\n        future.get()\n\n# 读取消息\ndef get_msg(topic='test'):\n    consumer = KafkaConsumer(topic, auto_offset_reset='earliest')\n    for message in consumer:\n        print(message)\n\n# 查询所有Topic\ndef list_topics():\n    global_consumer = KafkaConsumer(bootstrap_servers='localhost:9092')\n    topics = global_consumer.topics()\n    return topics\n# 创建Topic\ndef create_topic(topic='test'):\n    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n    topics = list_topics()\n    if topic not in topics:\n        topic_obj = NewTopic(topic, 1, 1)\n        admin.create_topics(new_topics=[topic_obj])\n\n# 删除Topic\ndef delete_topics(topic='test'):\n    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n    topics = list_topics()\n    if topic in topics:\n        admin.delete_topics(topics=[topic])\n\n\nif __name__ == '__main__':\n    topic = 'mytopic'\n    topics = list_topics()\n    if topic in topics:\n        delete_topics(topic)\n\n    create_topic(topic)\n    msgs = [{'a': 'a', 'b': 1, 'c': 1, 'time': '2013-01-01T00:14:13Z'},\n            {'a': 'b', 'b': 2, 'c': 2, 'time': '2013-01-01T00:24:13Z'},\n            {'a': 'a', 'b': 3, 'c': 3, 'time': '2013-01-01T00:34:13Z'},\n            {'a': 'a', 'b': 4, 'c': 4, 'time': '2013-01-01T01:14:13Z'},\n            {'a': 'b', 'b': 4, 'c': 5, 'time': '2013-01-01T01:24:13Z'},\n            {'a': 'a', 'b': 5, 'c': 2, 'time': '2013-01-01T01:34:13Z'}]\n    for msg in msgs:\n        send_msg(topic, msg)\n    # print test data\n    get_msg(topic)\n    \n```\n源码:[prepare_data.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py)\n\n运行上面的代码，控制台会输出如下：\n![](7A1AA068-AA3C-4439-898D-5DE507CC24C6.png)\n如上信息证明我们已经完成了数据准备工作。\n\n# 开发示例\n## 创建Kafka的数据源表\n我们以Table Descriptor的方式进行创建：\n```\nst_env.connect(Kafka()\n     .version(\"0.11\")\n     .topic(\"user\")\n     .start_from_earliest()\n     .property(\"zookeeper.connect\", \"localhost:2181\")\n            .property(\n            \"bootstrap.servers\", \"localhost:9092\"))\n     .with_format(Json()\n            .fail_on_missing_field(True)\n            .json_schema(\n                \"{\"\n                \"  type: 'object',\"\n                \"  properties: {\"\n                \"    a: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    b: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    c: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    time: {\"\n                \"      type: 'string',\"\n                \"      format: 'date-time'\"\n                \"    }\"\n                \"  }\"\n                \"}\"\n             )\n         )\n    .with_schema(Schema()\n        .field(\"rowtime\", DataTypes.TIMESTAMP())\n        .rowtime(Rowtime()\n            .timestamps_from_field(\"time\")\n            .watermarks_periodic_bounded(60000))\n            .field(\"a\", DataTypes.STRING())\n            .field(\"b\", DataTypes.STRING())\n            .field(\"c\", DataTypes.STRING())\n         )\n     .in_append_mode()\n     .register_table_source(\"source\")\n```\n\n## 创建CSV的结果表\n我们以创TableSink实例的方式创建Sink表：\n```\nst_env.register_table_sink(\n\"result\", \nCsvTableSink(\n    [\"a\", \"b\"],\n    [DataTypes.STRING(),\n    DataTypes.STRING()],\n    result_file)\n)\n```\n\n## 创1小时的Tumble窗口聚合\n```\nst_env.scan(\"source\")\n    .window(Tumble\n        .over(\"1.hours\").on(\"rowtime\").alias(\"w\"))\n    .group_by(\"w, a\")\n    .select(\"a, max(b)\")\n    .insert_into(\"result\")\n```\n\n完整代码 [tumble_window.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/tumble_window.py)\n运行源码，由于我们执行的是Stream作业，作业不会自动停止，我们启动之后，执行如下命令查看运行结果：\n```\ncat /tmp/tumble_time_window_streaming.csv\n```\n![](02C1D4A6-7290-408E-832A-1FBDCDED75E5.png)\n\n# 小结\n本篇核心是向大家介绍在Flink Python Table API如何读取Kafka数据，并以一个Event-Time的TumbleWindow示例结束本篇介绍。开篇说道部分想建议大家要做到 \"圣人为腹不为目\"，任何事情都要追求其本质。愿你在本篇中有所收获！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n","source":"_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka.md","raw":"---\ntitle: Apache Flink 说道系列- Python API 中如何使用 Kafka\ncategories: Apache Flink 说道\ntags: [Flink, 道德经, 圣人为腹不为目]\n---\n\n# 开篇说道\n老子说\"圣人为腹不为目\"，核心意思是说看问题要看本质，不要着眼于外表。交人处事也是一样，不要看其外表和语言要看内心和本质。这里和大家分享一个故事，\"九方皋(Gao)相马\"。\n\n春秋时期的伯乐是识别千里马的大师，在他上了年纪之后 秦穆公 对他说:\"在你的子孙里面给我推荐一个能够识别千里马的人才吧？\"，伯乐说:\"在我的子孙中没有这样的人才，不过我有个朋友，名叫 九方皋 是个相马的高手，我推荐给您吧\"。\n\n于是 秦穆公 召见 九方皋，让他去出巡千里马，过了数月九方皋回来禀告说:\"我已经找到千里马了，就在沙丘那个地方\"。于是 秦穆公 兴奋的问:\"那是一匹什么样子的马啊\"？九方皋 回答:\"是一匹黄色的母马\"。\n不过 秦穆公 派人把那匹马找来发现是一匹黑色的公马。秦穆公很生气，就派人把伯乐叫来说:\"你推荐的相马的人实在糟透了，他连马的颜色和公母都分不清，怎么能认识千里马呢\"？ 伯乐听了内心无比佩服的对 秦穆公 说:\"这就是他比我还高明的地方，他重视的是马的精神，忽略了马的外表，重视马的内在品质，而忽略了马的颜色和雌雄，九方皋只关注他所关注的重点，而忽略不重要的特征，这样才真的能找到难寻的好马啊\"。后来经过检验这的确是一匹天下少有的千里马。\n\n\n所以\"圣人为腹不为目\"，注重本质而忽略外在。祝愿大家在日常生活和工作中能看清事情的内在和深层次的所在，而后做出你最正确的决定。\n\n# 如何创建Source\nApache Flink 1.9 有两种方式创建Source：\n* Table Descriptor\n* Table Source\n\n## Table Descriptor\n利用Descriptor方式创建Source是比较推荐的做法，以简单的CSV示例如下：\n```\nt_env.connect(FileSystem()\n    .path(source_path)\n    .with_format(OldCsv()\n        .field(\"word\",  DataTypes.STRING()))\n    .with_schema(Schema()\n        .field(\"word\", DataTypes.STRING()))      \n    .register_table_source(\"source\")\n\ntable = t_env.scan(\"source\")\n```\n\n## Table Source\n创建TableSource的方式也可以完成Source的创建，如下:\n```\ncsv_source = CsvTableSource(\n    source_path, \n    [\"word\"],  \n    [DataTypes.STRING()])\n    \nt_env.register_table_source(\"source\", csv_source)\n\ntable = t_env.scan(\"source\")\n```\n\n# 如何创建Sink\n和创建Source一样，Apache Flink 1.9 有两种方式创建Sink：\n* Table Descriptor\n* Table Sink\n\n## Table Descriptor\n利用Descriptor方式创建Sink是比较推荐的做法，以简单的CSV示例如下：\n```\nt_env.connect(FileSystem()\n    .path(result_path)\n    .with_format(OldCsv()\n        .field(\"word\",  DataTypes.STRING()))\n    .with_schema(Schema()\n        .field(\"word\", DataTypes.STRING()))      \n    .register_table_sink(\"results\")\n\nresult_table.insert_into(\"results\")\n```\n\n## Table Sink\n创建TableSink的方式也可以完成Sink的创建，如下:\n```\ncsv_sink = CsvTableSink(\n    [\"word\"],  \n    [DataTypes.STRING()])\n    \nt_env.register_table_sink(\"results\", csv_sink)\nresult_table.insert_into(\"results\")\n```\n\n## 关于Format和Schema\n不论是Source和Sink在利用Table Descriptor进行实例化的时候都是需要设置Format和Schema的，那么什么是Format和Schema呢？\n* Format - 是描述数据在外部存储系统里面的数据格式。\n* Schema - 是外部数据加载到Flink系统之后，形成Table的数据结构描述。\n\n# Kafka connector JAR加载\n如《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》所描述的问题一样，我们需要显示的将Kafka connector JARs添加到classpath下面。所以在源码构建PyFlink发布包时候，需要在完成源码编译之后，将Kafka相关的JARs复制到 `build-target/lib`下面。并且需要对应的Format的JARs。以`kafka-0.11`和`Json` Format为例：\n\nCopy Kafka相关JARs\n```\ncp flink-connectors/flink-sql-connector-kafka-0.11/target/flink-sql-connector-kafka-0.11_*-SNAPSHOT.jar build-target/lib\n```\n\nCopy Json Format相关JARs\n```\ncp flink-formats/flink-json/target/flink-json-*-SNAPSHOT-sql-jar.jar build-target/lib\n```\n构建PyFlink发布包并安装：\n```\ncd flink-python; python setup.py sdist \npip install dist/*.tar.gz --user\n```\n环境安装的详细内容可以参考《Apache Flink 说道系列- Python Table API 开发环境搭建》。\n\n# 安装Kafka环境\n如果你已经有Kafka的集群环境，可以忽略本步骤，如果没有，为了完成本篇的测试，你可以部署一个简单的Kafka环境。\n* 下载Kafka安装包\n```\nwget https://archive.apache.org/dist/kafka/0.11.0.3/kafka_2.11-0.11.0.3.tgz\n```\n* 解压\n```\ntar zxvf kafka_2.11-0.11.0.3.tgz\n```\n* 启动Zookeeper\n```\ncd kafka_2.11-0.11.0.3; bin/zookeeper-server-start.sh config/zookeeper.properties\n\n最终输出：\n[2019-08-28 08:47:16,437] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n[2019-08-28 08:47:16,478] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n```\n\n* 支持删除Topic\n  要支持删除Topic，我们需要修改server.properties配置，将`delete.topic.enable=true` 打开。\n  \n* 启动Kafka\n```\nbin/kafka-server-start.sh config/server.properties\n\n最终输出：\n[2019-08-28 08:49:20,280] INFO Kafka commitId : 26ddb9e3197be39a (org.apache.kafka.common.utils.AppInfoParser)\n[2019-08-28 08:49:20,281] INFO [Kafka Server 0], started (kafka.server.KafkaServer)\n```\n如上过程我们完成了简单Kafka的环境搭建。更多细节参考Kafka[官方文档](https://kafka.apache.org/quickstart)\n# 编写一个简单示例\n这个示例我们从Kafka读取数据，然后创建一个简单的tumble window，最终将计算结果写到CSV文件系统里面。\n\n## 数据结构\n我们在Kafka中以Json的数据格式进行存储，假设有如下数据结构：\n```\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"a\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"b\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"c\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"time\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"format\": \"date-time\"\n\t\t}\n\t}\n}\n```\n如上数据结构中time是为了演示event-time window，其他字段没有特殊含义。\n\n## 数据准备\n我们按如上数据结构来向Kafka中写入测试数据，我用python操作Kafka，需要安装Kafka的PythonAPI，如下：\n```\npip install kafka-python\n```\n将测试数据写入Kafka：\n```\n# -*- coding: UTF-8 -*-\n\nfrom kafka import KafkaProducer\nfrom kafka import KafkaAdminClient\nfrom kafka import KafkaConsumer\nfrom kafka.admin import NewTopic\nimport json\n\n# 写入消息\ndef send_msg(topic='test', msg=None):\n    producer = KafkaProducer(bootstrap_servers='localhost:9092',\n                             value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n    if msg is not None:\n        future = producer.send(topic, msg)\n        future.get()\n\n# 读取消息\ndef get_msg(topic='test'):\n    consumer = KafkaConsumer(topic, auto_offset_reset='earliest')\n    for message in consumer:\n        print(message)\n\n# 查询所有Topic\ndef list_topics():\n    global_consumer = KafkaConsumer(bootstrap_servers='localhost:9092')\n    topics = global_consumer.topics()\n    return topics\n# 创建Topic\ndef create_topic(topic='test'):\n    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n    topics = list_topics()\n    if topic not in topics:\n        topic_obj = NewTopic(topic, 1, 1)\n        admin.create_topics(new_topics=[topic_obj])\n\n# 删除Topic\ndef delete_topics(topic='test'):\n    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n    topics = list_topics()\n    if topic in topics:\n        admin.delete_topics(topics=[topic])\n\n\nif __name__ == '__main__':\n    topic = 'mytopic'\n    topics = list_topics()\n    if topic in topics:\n        delete_topics(topic)\n\n    create_topic(topic)\n    msgs = [{'a': 'a', 'b': 1, 'c': 1, 'time': '2013-01-01T00:14:13Z'},\n            {'a': 'b', 'b': 2, 'c': 2, 'time': '2013-01-01T00:24:13Z'},\n            {'a': 'a', 'b': 3, 'c': 3, 'time': '2013-01-01T00:34:13Z'},\n            {'a': 'a', 'b': 4, 'c': 4, 'time': '2013-01-01T01:14:13Z'},\n            {'a': 'b', 'b': 4, 'c': 5, 'time': '2013-01-01T01:24:13Z'},\n            {'a': 'a', 'b': 5, 'c': 2, 'time': '2013-01-01T01:34:13Z'}]\n    for msg in msgs:\n        send_msg(topic, msg)\n    # print test data\n    get_msg(topic)\n    \n```\n源码:[prepare_data.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py)\n\n运行上面的代码，控制台会输出如下：\n![](7A1AA068-AA3C-4439-898D-5DE507CC24C6.png)\n如上信息证明我们已经完成了数据准备工作。\n\n# 开发示例\n## 创建Kafka的数据源表\n我们以Table Descriptor的方式进行创建：\n```\nst_env.connect(Kafka()\n     .version(\"0.11\")\n     .topic(\"user\")\n     .start_from_earliest()\n     .property(\"zookeeper.connect\", \"localhost:2181\")\n            .property(\n            \"bootstrap.servers\", \"localhost:9092\"))\n     .with_format(Json()\n            .fail_on_missing_field(True)\n            .json_schema(\n                \"{\"\n                \"  type: 'object',\"\n                \"  properties: {\"\n                \"    a: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    b: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    c: {\"\n                \"      type: 'string'\"\n                \"    },\"\n                \"    time: {\"\n                \"      type: 'string',\"\n                \"      format: 'date-time'\"\n                \"    }\"\n                \"  }\"\n                \"}\"\n             )\n         )\n    .with_schema(Schema()\n        .field(\"rowtime\", DataTypes.TIMESTAMP())\n        .rowtime(Rowtime()\n            .timestamps_from_field(\"time\")\n            .watermarks_periodic_bounded(60000))\n            .field(\"a\", DataTypes.STRING())\n            .field(\"b\", DataTypes.STRING())\n            .field(\"c\", DataTypes.STRING())\n         )\n     .in_append_mode()\n     .register_table_source(\"source\")\n```\n\n## 创建CSV的结果表\n我们以创TableSink实例的方式创建Sink表：\n```\nst_env.register_table_sink(\n\"result\", \nCsvTableSink(\n    [\"a\", \"b\"],\n    [DataTypes.STRING(),\n    DataTypes.STRING()],\n    result_file)\n)\n```\n\n## 创1小时的Tumble窗口聚合\n```\nst_env.scan(\"source\")\n    .window(Tumble\n        .over(\"1.hours\").on(\"rowtime\").alias(\"w\"))\n    .group_by(\"w, a\")\n    .select(\"a, max(b)\")\n    .insert_into(\"result\")\n```\n\n完整代码 [tumble_window.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/tumble_window.py)\n运行源码，由于我们执行的是Stream作业，作业不会自动停止，我们启动之后，执行如下命令查看运行结果：\n```\ncat /tmp/tumble_time_window_streaming.csv\n```\n![](02C1D4A6-7290-408E-832A-1FBDCDED75E5.png)\n\n# 小结\n本篇核心是向大家介绍在Flink Python Table API如何读取Kafka数据，并以一个Event-Time的TumbleWindow示例结束本篇介绍。开篇说道部分想建议大家要做到 \"圣人为腹不为目\"，任何事情都要追求其本质。愿你在本篇中有所收获！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n","slug":"Apache Flink 说道系列- Python API 中如何使用 Kafka","published":1,"date":"2019-08-28T03:05:01.162Z","updated":"2019-08-28T04:35:25.875Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5fy0011s94hjuxah9ps","content":"<h1>开篇说道</h1>\n<p>老子说&quot;圣人为腹不为目&quot;，核心意思是说看问题要看本质，不要着眼于外表。交人处事也是一样，不要看其外表和语言要看内心和本质。这里和大家分享一个故事，&quot;九方皋(Gao)相马&quot;。</p>\n<p>春秋时期的伯乐是识别千里马的大师，在他上了年纪之后 秦穆公 对他说:&quot;在你的子孙里面给我推荐一个能够识别千里马的人才吧？&quot;，伯乐说:&quot;在我的子孙中没有这样的人才，不过我有个朋友，名叫 九方皋 是个相马的高手，我推荐给您吧&quot;。</p>\n<p>于是 秦穆公 召见 九方皋，让他去出巡千里马，过了数月九方皋回来禀告说:&quot;我已经找到千里马了，就在沙丘那个地方&quot;。于是 秦穆公 兴奋的问:&quot;那是一匹什么样子的马啊&quot;？九方皋 回答:&quot;是一匹黄色的母马&quot;。\n不过 秦穆公 派人把那匹马找来发现是一匹黑色的公马。秦穆公很生气，就派人把伯乐叫来说:&quot;你推荐的相马的人实在糟透了，他连马的颜色和公母都分不清，怎么能认识千里马呢&quot;？ 伯乐听了内心无比佩服的对 秦穆公 说:&quot;这就是他比我还高明的地方，他重视的是马的精神，忽略了马的外表，重视马的内在品质，而忽略了马的颜色和雌雄，九方皋只关注他所关注的重点，而忽略不重要的特征，这样才真的能找到难寻的好马啊&quot;。后来经过检验这的确是一匹天下少有的千里马。</p>\n<p>所以&quot;圣人为腹不为目&quot;，注重本质而忽略外在。祝愿大家在日常生活和工作中能看清事情的内在和深层次的所在，而后做出你最正确的决定。</p>\n<h1>如何创建Source</h1>\n<p>Apache Flink 1.9 有两种方式创建Source：</p>\n<ul>\n<li>Table Descriptor</li>\n<li>Table Source</li>\n</ul>\n<h2>Table Descriptor</h2>\n<p>利用Descriptor方式创建Source是比较推荐的做法，以简单的CSV示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.connect(FileSystem()</span><br><span class=\"line\">    .path(source_path)</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">        .field(&quot;word&quot;,  DataTypes.STRING()))</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;word&quot;, DataTypes.STRING()))      </span><br><span class=\"line\">    .register_table_source(&quot;source&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">table = t_env.scan(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>Table Source</h2>\n<p>创建TableSource的方式也可以完成Source的创建，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">csv_source = CsvTableSource(</span><br><span class=\"line\">    source_path, </span><br><span class=\"line\">    [&quot;word&quot;],  </span><br><span class=\"line\">    [DataTypes.STRING()])</span><br><span class=\"line\">    </span><br><span class=\"line\">t_env.register_table_source(&quot;source&quot;, csv_source)</span><br><span class=\"line\"></span><br><span class=\"line\">table = t_env.scan(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h1>如何创建Sink</h1>\n<p>和创建Source一样，Apache Flink 1.9 有两种方式创建Sink：</p>\n<ul>\n<li>Table Descriptor</li>\n<li>Table Sink</li>\n</ul>\n<h2>Table Descriptor</h2>\n<p>利用Descriptor方式创建Sink是比较推荐的做法，以简单的CSV示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.connect(FileSystem()</span><br><span class=\"line\">    .path(result_path)</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">        .field(&quot;word&quot;,  DataTypes.STRING()))</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;word&quot;, DataTypes.STRING()))      </span><br><span class=\"line\">    .register_table_sink(&quot;results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">result_table.insert_into(&quot;results&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>Table Sink</h2>\n<p>创建TableSink的方式也可以完成Sink的创建，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">csv_sink = CsvTableSink(</span><br><span class=\"line\">    [&quot;word&quot;],  </span><br><span class=\"line\">    [DataTypes.STRING()])</span><br><span class=\"line\">    </span><br><span class=\"line\">t_env.register_table_sink(&quot;results&quot;, csv_sink)</span><br><span class=\"line\">result_table.insert_into(&quot;results&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>关于Format和Schema</h2>\n<p>不论是Source和Sink在利用Table Descriptor进行实例化的时候都是需要设置Format和Schema的，那么什么是Format和Schema呢？</p>\n<ul>\n<li>Format - 是描述数据在外部存储系统里面的数据格式。</li>\n<li>Schema - 是外部数据加载到Flink系统之后，形成Table的数据结构描述。</li>\n</ul>\n<h1>Kafka connector JAR加载</h1>\n<p>如《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》所描述的问题一样，我们需要显示的将Kafka connector JARs添加到classpath下面。所以在源码构建PyFlink发布包时候，需要在完成源码编译之后，将Kafka相关的JARs复制到 <code>build-target/lib</code>下面。并且需要对应的Format的JARs。以<code>kafka-0.11</code>和<code>Json</code> Format为例：</p>\n<p>Copy Kafka相关JARs\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp flink-connectors/flink-sql-connector-kafka-0.11/target/flink-sql-connector-kafka-0.11_*-SNAPSHOT.jar build-target/lib</span><br></pre></td></tr></table></figure></p>\n<p>Copy Json Format相关JARs\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp flink-formats/flink-json/target/flink-json-*-SNAPSHOT-sql-jar.jar build-target/lib</span><br></pre></td></tr></table></figure></p>\n<p>构建PyFlink发布包并安装：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-python; python setup.py sdist </span><br><span class=\"line\">pip install dist/*.tar.gz --user</span><br></pre></td></tr></table></figure></p>\n<p>环境安装的详细内容可以参考《Apache Flink 说道系列- Python Table API 开发环境搭建》。</p>\n<h1>安装Kafka环境</h1>\n<p>如果你已经有Kafka的集群环境，可以忽略本步骤，如果没有，为了完成本篇的测试，你可以部署一个简单的Kafka环境。</p>\n<ul>\n<li>\n<p>下载Kafka安装包\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://archive.apache.org/dist/kafka/0.11.0.3/kafka_2.11-0.11.0.3.tgz</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>解压\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar zxvf kafka_2.11-0.11.0.3.tgz</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>启动Zookeeper\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd kafka_2.11-0.11.0.3; bin/zookeeper-server-start.sh config/zookeeper.properties</span><br><span class=\"line\"></span><br><span class=\"line\">最终输出：</span><br><span class=\"line\">[2019-08-28 08:47:16,437] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)</span><br><span class=\"line\">[2019-08-28 08:47:16,478] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>支持删除Topic\n要支持删除Topic，我们需要修改server.properties配置，将<code>delete.topic.enable=true</code> 打开。</p>\n</li>\n<li>\n<p>启动Kafka\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-server-start.sh config/server.properties</span><br><span class=\"line\"></span><br><span class=\"line\">最终输出：</span><br><span class=\"line\">[2019-08-28 08:49:20,280] INFO Kafka commitId : 26ddb9e3197be39a (org.apache.kafka.common.utils.AppInfoParser)</span><br><span class=\"line\">[2019-08-28 08:49:20,281] INFO [Kafka Server 0], started (kafka.server.KafkaServer)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>如上过程我们完成了简单Kafka的环境搭建。更多细节参考Kafka<a href=\"https://kafka.apache.org/quickstart\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1>编写一个简单示例</h1>\n<p>这个示例我们从Kafka读取数据，然后创建一个简单的tumble window，最终将计算结果写到CSV文件系统里面。</p>\n<h2>数据结构</h2>\n<p>我们在Kafka中以Json的数据格式进行存储，假设有如下数据结构：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t&quot;type&quot;: &quot;object&quot;,</span><br><span class=\"line\">\t&quot;properties&quot;: &#123;</span><br><span class=\"line\">\t\t&quot;a&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;b&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;c&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;time&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;,</span><br><span class=\"line\">\t\t\t&quot;format&quot;: &quot;date-time&quot;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如上数据结构中time是为了演示event-time window，其他字段没有特殊含义。</p>\n<h2>数据准备</h2>\n<p>我们按如上数据结构来向Kafka中写入测试数据，我用python操作Kafka，需要安装Kafka的PythonAPI，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install kafka-python</span><br></pre></td></tr></table></figure></p>\n<p>将测试数据写入Kafka：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding: UTF-8 -*-</span><br><span class=\"line\"></span><br><span class=\"line\">from kafka import KafkaProducer</span><br><span class=\"line\">from kafka import KafkaAdminClient</span><br><span class=\"line\">from kafka import KafkaConsumer</span><br><span class=\"line\">from kafka.admin import NewTopic</span><br><span class=\"line\">import json</span><br><span class=\"line\"></span><br><span class=\"line\"># 写入消息</span><br><span class=\"line\">def send_msg(topic=&apos;test&apos;, msg=None):</span><br><span class=\"line\">    producer = KafkaProducer(bootstrap_servers=&apos;localhost:9092&apos;,</span><br><span class=\"line\">                             value_serializer=lambda v: json.dumps(v).encode(&apos;utf-8&apos;))</span><br><span class=\"line\">    if msg is not None:</span><br><span class=\"line\">        future = producer.send(topic, msg)</span><br><span class=\"line\">        future.get()</span><br><span class=\"line\"></span><br><span class=\"line\"># 读取消息</span><br><span class=\"line\">def get_msg(topic=&apos;test&apos;):</span><br><span class=\"line\">    consumer = KafkaConsumer(topic, auto_offset_reset=&apos;earliest&apos;)</span><br><span class=\"line\">    for message in consumer:</span><br><span class=\"line\">        print(message)</span><br><span class=\"line\"></span><br><span class=\"line\"># 查询所有Topic</span><br><span class=\"line\">def list_topics():</span><br><span class=\"line\">    global_consumer = KafkaConsumer(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = global_consumer.topics()</span><br><span class=\"line\">    return topics</span><br><span class=\"line\"># 创建Topic</span><br><span class=\"line\">def create_topic(topic=&apos;test&apos;):</span><br><span class=\"line\">    admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic not in topics:</span><br><span class=\"line\">        topic_obj = NewTopic(topic, 1, 1)</span><br><span class=\"line\">        admin.create_topics(new_topics=[topic_obj])</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除Topic</span><br><span class=\"line\">def delete_topics(topic=&apos;test&apos;):</span><br><span class=\"line\">    admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        admin.delete_topics(topics=[topic])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    topic = &apos;mytopic&apos;</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        delete_topics(topic)</span><br><span class=\"line\"></span><br><span class=\"line\">    create_topic(topic)</span><br><span class=\"line\">    msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;]</span><br><span class=\"line\">    for msg in msgs:</span><br><span class=\"line\">        send_msg(topic, msg)</span><br><span class=\"line\">    # print test data</span><br><span class=\"line\">    get_msg(topic)</span><br></pre></td></tr></table></figure></p>\n<p>源码:<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py\" target=\"_blank\" rel=\"noopener\">prepare_data.py</a></p>\n<p>运行上面的代码，控制台会输出如下：\n<img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/7A1AA068-AA3C-4439-898D-5DE507CC24C6.png\" alt>\n如上信息证明我们已经完成了数据准备工作。</p>\n<h1>开发示例</h1>\n<h2>创建Kafka的数据源表</h2>\n<p>我们以Table Descriptor的方式进行创建：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.connect(Kafka()</span><br><span class=\"line\">     .version(&quot;0.11&quot;)</span><br><span class=\"line\">     .topic(&quot;user&quot;)</span><br><span class=\"line\">     .start_from_earliest()</span><br><span class=\"line\">     .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;)</span><br><span class=\"line\">            .property(</span><br><span class=\"line\">            &quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;))</span><br><span class=\"line\">     .with_format(Json()</span><br><span class=\"line\">            .fail_on_missing_field(True)</span><br><span class=\"line\">            .json_schema(</span><br><span class=\"line\">                &quot;&#123;&quot;</span><br><span class=\"line\">                &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                &quot;    a: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    b: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    c: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    time: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;&quot;</span><br><span class=\"line\">                &quot;  &#125;&quot;</span><br><span class=\"line\">                &quot;&#125;&quot;</span><br><span class=\"line\">             )</span><br><span class=\"line\">         )</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;rowtime&quot;, DataTypes.TIMESTAMP())</span><br><span class=\"line\">        .rowtime(Rowtime()</span><br><span class=\"line\">            .timestamps_from_field(&quot;time&quot;)</span><br><span class=\"line\">            .watermarks_periodic_bounded(60000))</span><br><span class=\"line\">            .field(&quot;a&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;b&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;c&quot;, DataTypes.STRING())</span><br><span class=\"line\">         )</span><br><span class=\"line\">     .in_append_mode()</span><br><span class=\"line\">     .register_table_source(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>创建CSV的结果表</h2>\n<p>我们以创TableSink实例的方式创建Sink表：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.register_table_sink(</span><br><span class=\"line\">&quot;result&quot;, </span><br><span class=\"line\">CsvTableSink(</span><br><span class=\"line\">    [&quot;a&quot;, &quot;b&quot;],</span><br><span class=\"line\">    [DataTypes.STRING(),</span><br><span class=\"line\">    DataTypes.STRING()],</span><br><span class=\"line\">    result_file)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<h2>创1小时的Tumble窗口聚合</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.scan(&quot;source&quot;)</span><br><span class=\"line\">    .window(Tumble</span><br><span class=\"line\">        .over(&quot;1.hours&quot;).on(&quot;rowtime&quot;).alias(&quot;w&quot;))</span><br><span class=\"line\">    .group_by(&quot;w, a&quot;)</span><br><span class=\"line\">    .select(&quot;a, max(b)&quot;)</span><br><span class=\"line\">    .insert_into(&quot;result&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>完整代码 <a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/tumble_window.py\" target=\"_blank\" rel=\"noopener\">tumble_window.py</a>\n运行源码，由于我们执行的是Stream作业，作业不会自动停止，我们启动之后，执行如下命令查看运行结果：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /tmp/tumble_time_window_streaming.csv</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/02C1D4A6-7290-408E-832A-1FBDCDED75E5.png\" alt></p>\n<h1>小结</h1>\n<p>本篇核心是向大家介绍在Flink Python Table API如何读取Kafka数据，并以一个Event-Time的TumbleWindow示例结束本篇介绍。开篇说道部分想建议大家要做到 &quot;圣人为腹不为目&quot;，任何事情都要追求其本质。愿你在本篇中有所收获！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>开篇说道</h1>\n<p>老子说&quot;圣人为腹不为目&quot;，核心意思是说看问题要看本质，不要着眼于外表。交人处事也是一样，不要看其外表和语言要看内心和本质。这里和大家分享一个故事，&quot;九方皋(Gao)相马&quot;。</p>\n<p>春秋时期的伯乐是识别千里马的大师，在他上了年纪之后 秦穆公 对他说:&quot;在你的子孙里面给我推荐一个能够识别千里马的人才吧？&quot;，伯乐说:&quot;在我的子孙中没有这样的人才，不过我有个朋友，名叫 九方皋 是个相马的高手，我推荐给您吧&quot;。</p>\n<p>于是 秦穆公 召见 九方皋，让他去出巡千里马，过了数月九方皋回来禀告说:&quot;我已经找到千里马了，就在沙丘那个地方&quot;。于是 秦穆公 兴奋的问:&quot;那是一匹什么样子的马啊&quot;？九方皋 回答:&quot;是一匹黄色的母马&quot;。\n不过 秦穆公 派人把那匹马找来发现是一匹黑色的公马。秦穆公很生气，就派人把伯乐叫来说:&quot;你推荐的相马的人实在糟透了，他连马的颜色和公母都分不清，怎么能认识千里马呢&quot;？ 伯乐听了内心无比佩服的对 秦穆公 说:&quot;这就是他比我还高明的地方，他重视的是马的精神，忽略了马的外表，重视马的内在品质，而忽略了马的颜色和雌雄，九方皋只关注他所关注的重点，而忽略不重要的特征，这样才真的能找到难寻的好马啊&quot;。后来经过检验这的确是一匹天下少有的千里马。</p>\n<p>所以&quot;圣人为腹不为目&quot;，注重本质而忽略外在。祝愿大家在日常生活和工作中能看清事情的内在和深层次的所在，而后做出你最正确的决定。</p>\n<h1>如何创建Source</h1>\n<p>Apache Flink 1.9 有两种方式创建Source：</p>\n<ul>\n<li>Table Descriptor</li>\n<li>Table Source</li>\n</ul>\n<h2>Table Descriptor</h2>\n<p>利用Descriptor方式创建Source是比较推荐的做法，以简单的CSV示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.connect(FileSystem()</span><br><span class=\"line\">    .path(source_path)</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">        .field(&quot;word&quot;,  DataTypes.STRING()))</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;word&quot;, DataTypes.STRING()))      </span><br><span class=\"line\">    .register_table_source(&quot;source&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">table = t_env.scan(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>Table Source</h2>\n<p>创建TableSource的方式也可以完成Source的创建，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">csv_source = CsvTableSource(</span><br><span class=\"line\">    source_path, </span><br><span class=\"line\">    [&quot;word&quot;],  </span><br><span class=\"line\">    [DataTypes.STRING()])</span><br><span class=\"line\">    </span><br><span class=\"line\">t_env.register_table_source(&quot;source&quot;, csv_source)</span><br><span class=\"line\"></span><br><span class=\"line\">table = t_env.scan(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h1>如何创建Sink</h1>\n<p>和创建Source一样，Apache Flink 1.9 有两种方式创建Sink：</p>\n<ul>\n<li>Table Descriptor</li>\n<li>Table Sink</li>\n</ul>\n<h2>Table Descriptor</h2>\n<p>利用Descriptor方式创建Sink是比较推荐的做法，以简单的CSV示例如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.connect(FileSystem()</span><br><span class=\"line\">    .path(result_path)</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">        .field(&quot;word&quot;,  DataTypes.STRING()))</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;word&quot;, DataTypes.STRING()))      </span><br><span class=\"line\">    .register_table_sink(&quot;results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">result_table.insert_into(&quot;results&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>Table Sink</h2>\n<p>创建TableSink的方式也可以完成Sink的创建，如下:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">csv_sink = CsvTableSink(</span><br><span class=\"line\">    [&quot;word&quot;],  </span><br><span class=\"line\">    [DataTypes.STRING()])</span><br><span class=\"line\">    </span><br><span class=\"line\">t_env.register_table_sink(&quot;results&quot;, csv_sink)</span><br><span class=\"line\">result_table.insert_into(&quot;results&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>关于Format和Schema</h2>\n<p>不论是Source和Sink在利用Table Descriptor进行实例化的时候都是需要设置Format和Schema的，那么什么是Format和Schema呢？</p>\n<ul>\n<li>Format - 是描述数据在外部存储系统里面的数据格式。</li>\n<li>Schema - 是外部数据加载到Flink系统之后，形成Table的数据结构描述。</li>\n</ul>\n<h1>Kafka connector JAR加载</h1>\n<p>如《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》所描述的问题一样，我们需要显示的将Kafka connector JARs添加到classpath下面。所以在源码构建PyFlink发布包时候，需要在完成源码编译之后，将Kafka相关的JARs复制到 <code>build-target/lib</code>下面。并且需要对应的Format的JARs。以<code>kafka-0.11</code>和<code>Json</code> Format为例：</p>\n<p>Copy Kafka相关JARs\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp flink-connectors/flink-sql-connector-kafka-0.11/target/flink-sql-connector-kafka-0.11_*-SNAPSHOT.jar build-target/lib</span><br></pre></td></tr></table></figure></p>\n<p>Copy Json Format相关JARs\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp flink-formats/flink-json/target/flink-json-*-SNAPSHOT-sql-jar.jar build-target/lib</span><br></pre></td></tr></table></figure></p>\n<p>构建PyFlink发布包并安装：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-python; python setup.py sdist </span><br><span class=\"line\">pip install dist/*.tar.gz --user</span><br></pre></td></tr></table></figure></p>\n<p>环境安装的详细内容可以参考《Apache Flink 说道系列- Python Table API 开发环境搭建》。</p>\n<h1>安装Kafka环境</h1>\n<p>如果你已经有Kafka的集群环境，可以忽略本步骤，如果没有，为了完成本篇的测试，你可以部署一个简单的Kafka环境。</p>\n<ul>\n<li>\n<p>下载Kafka安装包\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://archive.apache.org/dist/kafka/0.11.0.3/kafka_2.11-0.11.0.3.tgz</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>解压\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar zxvf kafka_2.11-0.11.0.3.tgz</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>启动Zookeeper\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd kafka_2.11-0.11.0.3; bin/zookeeper-server-start.sh config/zookeeper.properties</span><br><span class=\"line\"></span><br><span class=\"line\">最终输出：</span><br><span class=\"line\">[2019-08-28 08:47:16,437] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)</span><br><span class=\"line\">[2019-08-28 08:47:16,478] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>支持删除Topic\n要支持删除Topic，我们需要修改server.properties配置，将<code>delete.topic.enable=true</code> 打开。</p>\n</li>\n<li>\n<p>启动Kafka\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-server-start.sh config/server.properties</span><br><span class=\"line\"></span><br><span class=\"line\">最终输出：</span><br><span class=\"line\">[2019-08-28 08:49:20,280] INFO Kafka commitId : 26ddb9e3197be39a (org.apache.kafka.common.utils.AppInfoParser)</span><br><span class=\"line\">[2019-08-28 08:49:20,281] INFO [Kafka Server 0], started (kafka.server.KafkaServer)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>如上过程我们完成了简单Kafka的环境搭建。更多细节参考Kafka<a href=\"https://kafka.apache.org/quickstart\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1>编写一个简单示例</h1>\n<p>这个示例我们从Kafka读取数据，然后创建一个简单的tumble window，最终将计算结果写到CSV文件系统里面。</p>\n<h2>数据结构</h2>\n<p>我们在Kafka中以Json的数据格式进行存储，假设有如下数据结构：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t&quot;type&quot;: &quot;object&quot;,</span><br><span class=\"line\">\t&quot;properties&quot;: &#123;</span><br><span class=\"line\">\t\t&quot;a&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;b&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;c&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\t&quot;time&quot;: &#123;</span><br><span class=\"line\">\t\t\t&quot;type&quot;: &quot;string&quot;,</span><br><span class=\"line\">\t\t\t&quot;format&quot;: &quot;date-time&quot;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如上数据结构中time是为了演示event-time window，其他字段没有特殊含义。</p>\n<h2>数据准备</h2>\n<p>我们按如上数据结构来向Kafka中写入测试数据，我用python操作Kafka，需要安装Kafka的PythonAPI，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install kafka-python</span><br></pre></td></tr></table></figure></p>\n<p>将测试数据写入Kafka：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding: UTF-8 -*-</span><br><span class=\"line\"></span><br><span class=\"line\">from kafka import KafkaProducer</span><br><span class=\"line\">from kafka import KafkaAdminClient</span><br><span class=\"line\">from kafka import KafkaConsumer</span><br><span class=\"line\">from kafka.admin import NewTopic</span><br><span class=\"line\">import json</span><br><span class=\"line\"></span><br><span class=\"line\"># 写入消息</span><br><span class=\"line\">def send_msg(topic=&apos;test&apos;, msg=None):</span><br><span class=\"line\">    producer = KafkaProducer(bootstrap_servers=&apos;localhost:9092&apos;,</span><br><span class=\"line\">                             value_serializer=lambda v: json.dumps(v).encode(&apos;utf-8&apos;))</span><br><span class=\"line\">    if msg is not None:</span><br><span class=\"line\">        future = producer.send(topic, msg)</span><br><span class=\"line\">        future.get()</span><br><span class=\"line\"></span><br><span class=\"line\"># 读取消息</span><br><span class=\"line\">def get_msg(topic=&apos;test&apos;):</span><br><span class=\"line\">    consumer = KafkaConsumer(topic, auto_offset_reset=&apos;earliest&apos;)</span><br><span class=\"line\">    for message in consumer:</span><br><span class=\"line\">        print(message)</span><br><span class=\"line\"></span><br><span class=\"line\"># 查询所有Topic</span><br><span class=\"line\">def list_topics():</span><br><span class=\"line\">    global_consumer = KafkaConsumer(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = global_consumer.topics()</span><br><span class=\"line\">    return topics</span><br><span class=\"line\"># 创建Topic</span><br><span class=\"line\">def create_topic(topic=&apos;test&apos;):</span><br><span class=\"line\">    admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic not in topics:</span><br><span class=\"line\">        topic_obj = NewTopic(topic, 1, 1)</span><br><span class=\"line\">        admin.create_topics(new_topics=[topic_obj])</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除Topic</span><br><span class=\"line\">def delete_topics(topic=&apos;test&apos;):</span><br><span class=\"line\">    admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;)</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        admin.delete_topics(topics=[topic])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    topic = &apos;mytopic&apos;</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        delete_topics(topic)</span><br><span class=\"line\"></span><br><span class=\"line\">    create_topic(topic)</span><br><span class=\"line\">    msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;]</span><br><span class=\"line\">    for msg in msgs:</span><br><span class=\"line\">        send_msg(topic, msg)</span><br><span class=\"line\">    # print test data</span><br><span class=\"line\">    get_msg(topic)</span><br></pre></td></tr></table></figure></p>\n<p>源码:<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py\" target=\"_blank\" rel=\"noopener\">prepare_data.py</a></p>\n<p>运行上面的代码，控制台会输出如下：\n<img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/7A1AA068-AA3C-4439-898D-5DE507CC24C6.png\" alt>\n如上信息证明我们已经完成了数据准备工作。</p>\n<h1>开发示例</h1>\n<h2>创建Kafka的数据源表</h2>\n<p>我们以Table Descriptor的方式进行创建：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.connect(Kafka()</span><br><span class=\"line\">     .version(&quot;0.11&quot;)</span><br><span class=\"line\">     .topic(&quot;user&quot;)</span><br><span class=\"line\">     .start_from_earliest()</span><br><span class=\"line\">     .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;)</span><br><span class=\"line\">            .property(</span><br><span class=\"line\">            &quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;))</span><br><span class=\"line\">     .with_format(Json()</span><br><span class=\"line\">            .fail_on_missing_field(True)</span><br><span class=\"line\">            .json_schema(</span><br><span class=\"line\">                &quot;&#123;&quot;</span><br><span class=\"line\">                &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                &quot;    a: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    b: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    c: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;,&quot;</span><br><span class=\"line\">                &quot;    time: &#123;&quot;</span><br><span class=\"line\">                &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                &quot;    &#125;&quot;</span><br><span class=\"line\">                &quot;  &#125;&quot;</span><br><span class=\"line\">                &quot;&#125;&quot;</span><br><span class=\"line\">             )</span><br><span class=\"line\">         )</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">        .field(&quot;rowtime&quot;, DataTypes.TIMESTAMP())</span><br><span class=\"line\">        .rowtime(Rowtime()</span><br><span class=\"line\">            .timestamps_from_field(&quot;time&quot;)</span><br><span class=\"line\">            .watermarks_periodic_bounded(60000))</span><br><span class=\"line\">            .field(&quot;a&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;b&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;c&quot;, DataTypes.STRING())</span><br><span class=\"line\">         )</span><br><span class=\"line\">     .in_append_mode()</span><br><span class=\"line\">     .register_table_source(&quot;source&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>创建CSV的结果表</h2>\n<p>我们以创TableSink实例的方式创建Sink表：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.register_table_sink(</span><br><span class=\"line\">&quot;result&quot;, </span><br><span class=\"line\">CsvTableSink(</span><br><span class=\"line\">    [&quot;a&quot;, &quot;b&quot;],</span><br><span class=\"line\">    [DataTypes.STRING(),</span><br><span class=\"line\">    DataTypes.STRING()],</span><br><span class=\"line\">    result_file)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<h2>创1小时的Tumble窗口聚合</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.scan(&quot;source&quot;)</span><br><span class=\"line\">    .window(Tumble</span><br><span class=\"line\">        .over(&quot;1.hours&quot;).on(&quot;rowtime&quot;).alias(&quot;w&quot;))</span><br><span class=\"line\">    .group_by(&quot;w, a&quot;)</span><br><span class=\"line\">    .select(&quot;a, max(b)&quot;)</span><br><span class=\"line\">    .insert_into(&quot;result&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>完整代码 <a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/tumble_window.py\" target=\"_blank\" rel=\"noopener\">tumble_window.py</a>\n运行源码，由于我们执行的是Stream作业，作业不会自动停止，我们启动之后，执行如下命令查看运行结果：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /tmp/tumble_time_window_streaming.csv</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/02C1D4A6-7290-408E-832A-1FBDCDED75E5.png\" alt></p>\n<h1>小结</h1>\n<p>本篇核心是向大家介绍在Flink Python Table API如何读取Kafka数据，并以一个Event-Time的TumbleWindow示例结束本篇介绍。开篇说道部分想建议大家要做到 &quot;圣人为腹不为目&quot;，任何事情都要追求其本质。愿你在本篇中有所收获！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/28/Apache Flink 说道系列- Python API 中如何使用 Kafka/comment.png\" alt></p>\n"},{"title":"Apache Flink 说道系列 - 序","date":"2019-07-13T13:25:14.000Z","_content":"# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。\n\n# What\nApache Flink 说\"道\"系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对\"道\"的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。\n\n# Why\n我自己在阅读《Apache Flink 漫谈系列》博文时候，有一个感觉，就是“枯燥”和“累”，该系列的博文篇幅比较长，需要有很强学习欲望的同学才能完整的阅读全篇，虽然每篇的内容应该算是很完整和有一定的知识含量，但是没有为读者提供轻松愉悦的学习环境。就如 马爸爸 所说的要为阿里的同学提供 “认真工作，开心生活” 的工作环境，那么我也挑战一下自己，争取让我的博文能让读者有“开心学习，轻松工作”的感受！因此，我努力开启Apache Flink 说\"道\"系列的分享！\n\n# When\nApache Flink 说\"道\"系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。\n\n","source":"_posts/Apache-Flink-说道系列-序.md","raw":"---\ntitle: Apache Flink 说道系列 - 序\ndate: 2019-07-13 21:25:14\ncategories: Apache Flink 说道\ntags: [Flink, 道德经]\n---\n# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。\n\n# What\nApache Flink 说\"道\"系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对\"道\"的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。\n\n# Why\n我自己在阅读《Apache Flink 漫谈系列》博文时候，有一个感觉，就是“枯燥”和“累”，该系列的博文篇幅比较长，需要有很强学习欲望的同学才能完整的阅读全篇，虽然每篇的内容应该算是很完整和有一定的知识含量，但是没有为读者提供轻松愉悦的学习环境。就如 马爸爸 所说的要为阿里的同学提供 “认真工作，开心生活” 的工作环境，那么我也挑战一下自己，争取让我的博文能让读者有“开心学习，轻松工作”的感受！因此，我努力开启Apache Flink 说\"道\"系列的分享！\n\n# When\nApache Flink 说\"道\"系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。\n\n","slug":"Apache-Flink-说道系列-序","published":1,"updated":"2019-07-13T14:56:30.861Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5g20014s94heloekcqm","content":"<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>Apache Flink 说&quot;道&quot;系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对&quot;道&quot;的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。</p>\n<h1>Why</h1>\n<p>我自己在阅读《Apache Flink 漫谈系列》博文时候，有一个感觉，就是“枯燥”和“累”，该系列的博文篇幅比较长，需要有很强学习欲望的同学才能完整的阅读全篇，虽然每篇的内容应该算是很完整和有一定的知识含量，但是没有为读者提供轻松愉悦的学习环境。就如 马爸爸 所说的要为阿里的同学提供 “认真工作，开心生活” 的工作环境，那么我也挑战一下自己，争取让我的博文能让读者有“开心学习，轻松工作”的感受！因此，我努力开启Apache Flink 说&quot;道&quot;系列的分享！</p>\n<h1>When</h1>\n<p>Apache Flink 说&quot;道&quot;系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>Apache Flink 说&quot;道&quot;系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对&quot;道&quot;的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。</p>\n<h1>Why</h1>\n<p>我自己在阅读《Apache Flink 漫谈系列》博文时候，有一个感觉，就是“枯燥”和“累”，该系列的博文篇幅比较长，需要有很强学习欲望的同学才能完整的阅读全篇，虽然每篇的内容应该算是很完整和有一定的知识含量，但是没有为读者提供轻松愉悦的学习环境。就如 马爸爸 所说的要为阿里的同学提供 “认真工作，开心生活” 的工作环境，那么我也挑战一下自己，争取让我的博文能让读者有“开心学习，轻松工作”的感受！因此，我努力开启Apache Flink 说&quot;道&quot;系列的分享！</p>\n<h1>When</h1>\n<p>Apache Flink 说&quot;道&quot;系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。</p>\n"},{"title":"Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)","_content":"\n# 开篇说道\n老子说\"孰能浊以静之徐清? 孰能安以动之徐生?\",大概是说谁能像浑浊的水流一样停止流动，安静下来慢慢变得澄清？谁可以像草木那样保持长时的静寂，却萌动生机而不息？对于与世为人来说，这句话是指谁能在浊世中保持内心的清净？只有智者可以静看花开花落，就像是莲花，出淤泥而不染。\"孰能安以动之徐生\" 一句则说在保持安静的同时，还要激发着无穷的生机，顺应自然的同时，默默改变这人世界。老子并不提倡举世皆浊我独清的态度，而是提倡“混兮其若浊”、“和其光，同其尘”，也就是说不要认为自己了不起，每个人的行为都有其道理，我们应该理解他人和接纳他人，和他人一样，表面看起来有点浑浑噩噩，但内心要\"静\",要\"清\"，就像一杯含有沙子的黄河水，只有在安静的状态下，才能慢慢变清，我们内心要从纷繁复杂的\"浊\"中理清头绪，变得清醒。但静静地弄清楚了还不够，还需要动起来，给外界反馈，发挥自己积极的作用。但切记要安静，即心态要平稳，如果急于求成，慌里慌张地做事情，那就会变成真的\"浊\"，变得稀里糊涂了。老子建议做一切事都要不急不躁，不“乱”不“浊”，一切要悠然\"徐生\"，水到渠成。\n![](hehua.png)\n\n# 问题\n为什么要聊在Python API 中如何使用 Java 自定义 Source/Sink？目前在Apache Flink中集成了一些常用的connector，比如Kafka,elasticsearch6,filesystem等常用的外部系统。但这仍然无法满足需求各异的用户。比如用户用Kafka但是可能用户使用的Kafka版本Flink并没有支持（当然社区也可以添加支持，我们只是说可能情况），还比如用户想使用自建的内部产品，那一定无法集成用户的内部产品到Flink社区，这时候就要用户根据Flink自定义Connector的规范开发自己的Connector了。那么本篇我们就要介绍如果你已经完成了自定义connector，如何在Python Table API中进行使用。\n\n# 如何自定义Java Connector\nApache Flink Table API/SQL中采用了Service Provider Interfaces (SPI)方式查找系统内置的和用户自定义的Connector。简单说任何Connector都需一个对应的[TableFactory](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableFactory.java)的实现，其核心方法是`requiredContext`,该方法定义对应Connector的唯一特征属性，connector.type，connector.version等。Connector的类型和版本就唯一决定了一个具体的Connector实现。当然任何Connector都要实现具体的[TableSource](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java)或[TableSink](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sinks/TableSink.java)。关于自定义Java Connector我们会在一篇完整的Blog进行详细叙述，本篇重点集中在如果已经完成了自定义的Connector，我们如何在Python Table API中使用。\n\n# Python Table API中注册Connector\n我们知道Flink Python Table API中采用Py4j的方式将Python Table API转换为Java Table API，而Java Table API中采用SPI的方式发现和查找Connector，也就是说Pytohn API需要定义一种方式能够告诉Java Table API 用户自定义的Connector信息。对于一个自定义Connector有两个非常重要的信息：\n \n - CustomConnectorDescriptor - Connector自有的属性定义；\n - CustomFormatDescriptor - Connector的数据格式定义；\n\n所以在Python Table API中我们利用 `CustomConnectorDescriptor` 和`CustomFormatDescriptor`来描述用户自定义Connector。\n\n## 自定义Connector类加载到Classpath\n\n用户要使用自定义的Connector，那么第一步是将用户自定义的类要加载到Classpath中，操作方式与Python API中使用Java UDF一致，详见：《[Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业](http://1t.click/BSa)》\n\n## Python Table API 描述自定义Connector\n我们这里以自定义Kafka Source为例，用户编写Python Table API如下：\n* 定义Connector属性\n我们需要利用`CustomConnectorDescriptor`来描述Connector属性，如下：\n```\ncustom_connector = CustomConnectorDescriptor('kafka', 1, True) \\\n        .property('connector.topic', 'user') \\\n        .property('connector.properties.0.key', 'zookeeper.connect') \\\n        .property('connector.properties.0.value', 'localhost:2181') \\\n        .property('connector.properties.1.key', 'bootstrap.servers') \\\n        .property('connector.properties.1.value', 'localhost:9092') \\\n        .properties({'connector.version': '0.11', 'connector.startup-mode': 'earliest-offset'})\n```\n上面属性与内置的Kafka 0.11版本一致。这里是示例如何利用`CustomConnectorDescriptor`描述自定义Connector属性。\n\n* 定义Connector的数据格式\n我们需要利用`CustomFormatDescriptor`来描述Connector数据格式，如下：\n```\n # the key is 'format.json-schema'\n    custom_format = CustomFormatDescriptor('json', 1) \\\n        .property('format.json-schema',\n                  \"{\"\n                  \"  type: 'object',\"\n                  \"  properties: {\"\n                  \"    a: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    b: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    c: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    time: {\"\n                  \"      type: 'string',\"\n                  \"      format: 'date-time'\"\n                  \"    }\"\n                  \"  }\"\n                  \"}\") \\\n        .properties({'format.fail-on-missing-field': 'true'})\n```\n这个数据格式是沿用了在 《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇中的示例。\n\n* 定义Table Source\n完成Connector属性的描述和Connector数据格式描述之后，我们就可以同使用内置Connector一样定义Table Source了。如下：\n```\n    st_env \\\n        .connect(custom_connector) \\\n        .with_format(custom_format) \\\n        .with_schema(...) \\\n        .register_table_source(\"source\")\n```\n上面`with_schema(...)`是定义数据流入Flink内部后的Table 数据结构。\n\n* 定义查询逻辑\n```\n st_env.scan(\"source\")\n    .window(Tumble.over(\"2.rows\")\n        .on(\"proctime\").alias(\"w\")) \\\n    .group_by(\"w, a\") \\\n    .select(\"a, max(b)\")\n    .insert_into(\"result\")\n```\n\n查询逻辑是一个简单的每2行作为一个Tumble Window。计算按 `a` 分组统计 `b` 的最大值。\n\n\n# 完整示例\n完整的自定义Kafka Source示例:\n\n```\nif __name__ == '__main__':\n    custom_connector = CustomConnectorDescriptor('kafka', 1, True) \\\n        .property('connector.topic', 'user') \\\n        .property('connector.properties.0.key', 'zookeeper.connect') \\\n        .property('connector.properties.0.value', 'localhost:2181') \\\n        .property('connector.properties.1.key', 'bootstrap.servers') \\\n        .property('connector.properties.1.value', 'localhost:9092') \\\n        .properties({'connector.version': '0.11', 'connector.startup-mode': 'earliest-offset'})\n\n    # the key is 'format.json-schema'\n    custom_format = CustomFormatDescriptor('json', 1) \\\n        .property('format.json-schema',\n                  \"{\"\n                  \"  type: 'object',\"\n                  \"  properties: {\"\n                  \"    a: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    b: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    c: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    time: {\"\n                  \"      type: 'string',\"\n                  \"      format: 'date-time'\"\n                  \"    }\"\n                  \"  }\"\n                  \"}\") \\\n        .properties({'format.fail-on-missing-field': 'true'})\n\n    s_env = StreamExecutionEnvironment.get_execution_environment()\n    s_env.set_parallelism(1)\n    s_env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)\n    st_env = StreamTableEnvironment.create(s_env)\n    result_file = \"/tmp/custom_kafka_source_demo.csv\"\n\n    if os.path.exists(result_file):\n        os.remove(result_file)\n\n    st_env \\\n        .connect(custom_connector) \\\n        .with_format(\n        custom_format\n    ) \\\n        .with_schema(  # declare the schema of the table\n        Schema()\n            .field(\"proctime\", DataTypes.TIMESTAMP())\n            .proctime()\n            .field(\"a\", DataTypes.STRING())\n            .field(\"b\", DataTypes.STRING())\n            .field(\"c\", DataTypes.STRING())\n    ) \\\n        .in_append_mode() \\\n        .register_table_source(\"source\")\n\n    st_env.register_table_sink(\"result\",\n                               CsvTableSink([\"a\", \"b\"],\n                                            [DataTypes.STRING(),\n                                             DataTypes.STRING()],\n                                            result_file))\n\n    st_env.scan(\"source\").window(Tumble.over(\"2.rows\").on(\"proctime\").alias(\"w\")) \\\n        .group_by(\"w, a\") \\\n        .select(\"a, max(b)\").insert_into(\"result\")\n\n    st_env.execute(\"custom kafka source demo\")\n\n```\n\n详细代码见:\n[https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py)\n\n* 运行CustomKafkaSourceDemo\n上面示例的运行需要一个Kafka的集群环境，集群信息需要和`CustomConnectorDescriptor`中描述的一致，比如`connector.topic`是`user`， `zookeeper.connect`, `localhost:2181`, `bootstrap.servers`是 `localhost:9092`。 这些信息与《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇中的Kafka环境一致，大家可以参考初始化自己的Kafka环境。\n\n* 准备测试数据\n运行之前大家要向 `user` Topic中写入一些测试数据，\n```\nif __name__ == '__main__':\n    topic = 'user'\n    topics = list_topics()\n    if topic in topics:\n        delete_topics(topic)\n\n    create_topic(topic)\n    msgs = [{'a': 'a', 'b': 1, 'c': 1, 'time': '2013-01-01T00:14:13Z'},\n            {'a': 'b', 'b': 2, 'c': 2, 'time': '2013-01-01T00:24:13Z'},\n            {'a': 'a', 'b': 3, 'c': 3, 'time': '2013-01-01T00:34:13Z'},\n            {'a': 'a', 'b': 4, 'c': 4, 'time': '2013-01-01T01:14:13Z'},\n            {'a': 'b', 'b': 4, 'c': 5, 'time': '2013-01-01T01:24:13Z'},\n            {'a': 'a', 'b': 5, 'c': 2, 'time': '2013-01-01T01:34:13Z'}]\n    for msg in msgs:\n        send_msg(topic, msg)\n    # print test data\n    get_msg(topic)\n```\n\n完整代码可以参考：[https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py)\n\n* 运行代码，并查看最终计算结果\n![338566533211a283d7a715ba32009fd2](Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).resources/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png)\n\n# 待续\n上面示例是在假设用户已经完成了 Java 自定义Connector的基础之上进行介绍如何利用Python Table API使用自定义Connector，但并没有详细介绍用户如何自定义Java Connector。所以后续我会为大家单独在一篇Blog中介绍Apache Flink 如何自定义Java Connector。[官方文档](https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sourceSinks.html)\n\n# 小结\n本篇内容篇幅比较短小，具体介绍了Python Table API如何使用自定义Java Connector。其中Kafka环境相关内容需要结合《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇一起阅读。同时在开篇说道中简单分享了老子关于\"孰能浊以静之徐清? 孰能安以动之徐生?\"的人生智慧，希望对大家有所启迪！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n","source":"_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).md","raw":"---\ntitle: Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)\ncategories: Apache Flink 说道\ntags: [Flink, Python, Connector, 道德经, 孰能浊以静之徐清, 孰能安以动之徐生]\n---\n\n# 开篇说道\n老子说\"孰能浊以静之徐清? 孰能安以动之徐生?\",大概是说谁能像浑浊的水流一样停止流动，安静下来慢慢变得澄清？谁可以像草木那样保持长时的静寂，却萌动生机而不息？对于与世为人来说，这句话是指谁能在浊世中保持内心的清净？只有智者可以静看花开花落，就像是莲花，出淤泥而不染。\"孰能安以动之徐生\" 一句则说在保持安静的同时，还要激发着无穷的生机，顺应自然的同时，默默改变这人世界。老子并不提倡举世皆浊我独清的态度，而是提倡“混兮其若浊”、“和其光，同其尘”，也就是说不要认为自己了不起，每个人的行为都有其道理，我们应该理解他人和接纳他人，和他人一样，表面看起来有点浑浑噩噩，但内心要\"静\",要\"清\"，就像一杯含有沙子的黄河水，只有在安静的状态下，才能慢慢变清，我们内心要从纷繁复杂的\"浊\"中理清头绪，变得清醒。但静静地弄清楚了还不够，还需要动起来，给外界反馈，发挥自己积极的作用。但切记要安静，即心态要平稳，如果急于求成，慌里慌张地做事情，那就会变成真的\"浊\"，变得稀里糊涂了。老子建议做一切事都要不急不躁，不“乱”不“浊”，一切要悠然\"徐生\"，水到渠成。\n![](hehua.png)\n\n# 问题\n为什么要聊在Python API 中如何使用 Java 自定义 Source/Sink？目前在Apache Flink中集成了一些常用的connector，比如Kafka,elasticsearch6,filesystem等常用的外部系统。但这仍然无法满足需求各异的用户。比如用户用Kafka但是可能用户使用的Kafka版本Flink并没有支持（当然社区也可以添加支持，我们只是说可能情况），还比如用户想使用自建的内部产品，那一定无法集成用户的内部产品到Flink社区，这时候就要用户根据Flink自定义Connector的规范开发自己的Connector了。那么本篇我们就要介绍如果你已经完成了自定义connector，如何在Python Table API中进行使用。\n\n# 如何自定义Java Connector\nApache Flink Table API/SQL中采用了Service Provider Interfaces (SPI)方式查找系统内置的和用户自定义的Connector。简单说任何Connector都需一个对应的[TableFactory](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableFactory.java)的实现，其核心方法是`requiredContext`,该方法定义对应Connector的唯一特征属性，connector.type，connector.version等。Connector的类型和版本就唯一决定了一个具体的Connector实现。当然任何Connector都要实现具体的[TableSource](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java)或[TableSink](https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sinks/TableSink.java)。关于自定义Java Connector我们会在一篇完整的Blog进行详细叙述，本篇重点集中在如果已经完成了自定义的Connector，我们如何在Python Table API中使用。\n\n# Python Table API中注册Connector\n我们知道Flink Python Table API中采用Py4j的方式将Python Table API转换为Java Table API，而Java Table API中采用SPI的方式发现和查找Connector，也就是说Pytohn API需要定义一种方式能够告诉Java Table API 用户自定义的Connector信息。对于一个自定义Connector有两个非常重要的信息：\n \n - CustomConnectorDescriptor - Connector自有的属性定义；\n - CustomFormatDescriptor - Connector的数据格式定义；\n\n所以在Python Table API中我们利用 `CustomConnectorDescriptor` 和`CustomFormatDescriptor`来描述用户自定义Connector。\n\n## 自定义Connector类加载到Classpath\n\n用户要使用自定义的Connector，那么第一步是将用户自定义的类要加载到Classpath中，操作方式与Python API中使用Java UDF一致，详见：《[Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业](http://1t.click/BSa)》\n\n## Python Table API 描述自定义Connector\n我们这里以自定义Kafka Source为例，用户编写Python Table API如下：\n* 定义Connector属性\n我们需要利用`CustomConnectorDescriptor`来描述Connector属性，如下：\n```\ncustom_connector = CustomConnectorDescriptor('kafka', 1, True) \\\n        .property('connector.topic', 'user') \\\n        .property('connector.properties.0.key', 'zookeeper.connect') \\\n        .property('connector.properties.0.value', 'localhost:2181') \\\n        .property('connector.properties.1.key', 'bootstrap.servers') \\\n        .property('connector.properties.1.value', 'localhost:9092') \\\n        .properties({'connector.version': '0.11', 'connector.startup-mode': 'earliest-offset'})\n```\n上面属性与内置的Kafka 0.11版本一致。这里是示例如何利用`CustomConnectorDescriptor`描述自定义Connector属性。\n\n* 定义Connector的数据格式\n我们需要利用`CustomFormatDescriptor`来描述Connector数据格式，如下：\n```\n # the key is 'format.json-schema'\n    custom_format = CustomFormatDescriptor('json', 1) \\\n        .property('format.json-schema',\n                  \"{\"\n                  \"  type: 'object',\"\n                  \"  properties: {\"\n                  \"    a: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    b: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    c: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    time: {\"\n                  \"      type: 'string',\"\n                  \"      format: 'date-time'\"\n                  \"    }\"\n                  \"  }\"\n                  \"}\") \\\n        .properties({'format.fail-on-missing-field': 'true'})\n```\n这个数据格式是沿用了在 《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇中的示例。\n\n* 定义Table Source\n完成Connector属性的描述和Connector数据格式描述之后，我们就可以同使用内置Connector一样定义Table Source了。如下：\n```\n    st_env \\\n        .connect(custom_connector) \\\n        .with_format(custom_format) \\\n        .with_schema(...) \\\n        .register_table_source(\"source\")\n```\n上面`with_schema(...)`是定义数据流入Flink内部后的Table 数据结构。\n\n* 定义查询逻辑\n```\n st_env.scan(\"source\")\n    .window(Tumble.over(\"2.rows\")\n        .on(\"proctime\").alias(\"w\")) \\\n    .group_by(\"w, a\") \\\n    .select(\"a, max(b)\")\n    .insert_into(\"result\")\n```\n\n查询逻辑是一个简单的每2行作为一个Tumble Window。计算按 `a` 分组统计 `b` 的最大值。\n\n\n# 完整示例\n完整的自定义Kafka Source示例:\n\n```\nif __name__ == '__main__':\n    custom_connector = CustomConnectorDescriptor('kafka', 1, True) \\\n        .property('connector.topic', 'user') \\\n        .property('connector.properties.0.key', 'zookeeper.connect') \\\n        .property('connector.properties.0.value', 'localhost:2181') \\\n        .property('connector.properties.1.key', 'bootstrap.servers') \\\n        .property('connector.properties.1.value', 'localhost:9092') \\\n        .properties({'connector.version': '0.11', 'connector.startup-mode': 'earliest-offset'})\n\n    # the key is 'format.json-schema'\n    custom_format = CustomFormatDescriptor('json', 1) \\\n        .property('format.json-schema',\n                  \"{\"\n                  \"  type: 'object',\"\n                  \"  properties: {\"\n                  \"    a: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    b: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    c: {\"\n                  \"      type: 'string'\"\n                  \"    },\"\n                  \"    time: {\"\n                  \"      type: 'string',\"\n                  \"      format: 'date-time'\"\n                  \"    }\"\n                  \"  }\"\n                  \"}\") \\\n        .properties({'format.fail-on-missing-field': 'true'})\n\n    s_env = StreamExecutionEnvironment.get_execution_environment()\n    s_env.set_parallelism(1)\n    s_env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)\n    st_env = StreamTableEnvironment.create(s_env)\n    result_file = \"/tmp/custom_kafka_source_demo.csv\"\n\n    if os.path.exists(result_file):\n        os.remove(result_file)\n\n    st_env \\\n        .connect(custom_connector) \\\n        .with_format(\n        custom_format\n    ) \\\n        .with_schema(  # declare the schema of the table\n        Schema()\n            .field(\"proctime\", DataTypes.TIMESTAMP())\n            .proctime()\n            .field(\"a\", DataTypes.STRING())\n            .field(\"b\", DataTypes.STRING())\n            .field(\"c\", DataTypes.STRING())\n    ) \\\n        .in_append_mode() \\\n        .register_table_source(\"source\")\n\n    st_env.register_table_sink(\"result\",\n                               CsvTableSink([\"a\", \"b\"],\n                                            [DataTypes.STRING(),\n                                             DataTypes.STRING()],\n                                            result_file))\n\n    st_env.scan(\"source\").window(Tumble.over(\"2.rows\").on(\"proctime\").alias(\"w\")) \\\n        .group_by(\"w, a\") \\\n        .select(\"a, max(b)\").insert_into(\"result\")\n\n    st_env.execute(\"custom kafka source demo\")\n\n```\n\n详细代码见:\n[https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py)\n\n* 运行CustomKafkaSourceDemo\n上面示例的运行需要一个Kafka的集群环境，集群信息需要和`CustomConnectorDescriptor`中描述的一致，比如`connector.topic`是`user`， `zookeeper.connect`, `localhost:2181`, `bootstrap.servers`是 `localhost:9092`。 这些信息与《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇中的Kafka环境一致，大家可以参考初始化自己的Kafka环境。\n\n* 准备测试数据\n运行之前大家要向 `user` Topic中写入一些测试数据，\n```\nif __name__ == '__main__':\n    topic = 'user'\n    topics = list_topics()\n    if topic in topics:\n        delete_topics(topic)\n\n    create_topic(topic)\n    msgs = [{'a': 'a', 'b': 1, 'c': 1, 'time': '2013-01-01T00:14:13Z'},\n            {'a': 'b', 'b': 2, 'c': 2, 'time': '2013-01-01T00:24:13Z'},\n            {'a': 'a', 'b': 3, 'c': 3, 'time': '2013-01-01T00:34:13Z'},\n            {'a': 'a', 'b': 4, 'c': 4, 'time': '2013-01-01T01:14:13Z'},\n            {'a': 'b', 'b': 4, 'c': 5, 'time': '2013-01-01T01:24:13Z'},\n            {'a': 'a', 'b': 5, 'c': 2, 'time': '2013-01-01T01:34:13Z'}]\n    for msg in msgs:\n        send_msg(topic, msg)\n    # print test data\n    get_msg(topic)\n```\n\n完整代码可以参考：[https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py](https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py)\n\n* 运行代码，并查看最终计算结果\n![338566533211a283d7a715ba32009fd2](Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).resources/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png)\n\n# 待续\n上面示例是在假设用户已经完成了 Java 自定义Connector的基础之上进行介绍如何利用Python Table API使用自定义Connector，但并没有详细介绍用户如何自定义Java Connector。所以后续我会为大家单独在一篇Blog中介绍Apache Flink 如何自定义Java Connector。[官方文档](https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sourceSinks.html)\n\n# 小结\n本篇内容篇幅比较短小，具体介绍了Python Table API如何使用自定义Java Connector。其中Kafka环境相关内容需要结合《[Apache Flink 说道系列- Python API 中如何使用 Kafka](http://1t.click/aej7)》一篇一起阅读。同时在开篇说道中简单分享了老子关于\"孰能浊以静之徐清? 孰能安以动之徐生?\"的人生智慧，希望对大家有所启迪！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n","slug":"Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)","published":1,"date":"2019-09-09T01:53:57.524Z","updated":"2019-09-09T06:35:00.819Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5g50019s94hk012tecz","content":"<h1>开篇说道</h1>\n<p>老子说&quot;孰能浊以静之徐清? 孰能安以动之徐生?&quot;,大概是说谁能像浑浊的水流一样停止流动，安静下来慢慢变得澄清？谁可以像草木那样保持长时的静寂，却萌动生机而不息？对于与世为人来说，这句话是指谁能在浊世中保持内心的清净？只有智者可以静看花开花落，就像是莲花，出淤泥而不染。&quot;孰能安以动之徐生&quot; 一句则说在保持安静的同时，还要激发着无穷的生机，顺应自然的同时，默默改变这人世界。老子并不提倡举世皆浊我独清的态度，而是提倡“混兮其若浊”、“和其光，同其尘”，也就是说不要认为自己了不起，每个人的行为都有其道理，我们应该理解他人和接纳他人，和他人一样，表面看起来有点浑浑噩噩，但内心要&quot;静&quot;,要&quot;清&quot;，就像一杯含有沙子的黄河水，只有在安静的状态下，才能慢慢变清，我们内心要从纷繁复杂的&quot;浊&quot;中理清头绪，变得清醒。但静静地弄清楚了还不够，还需要动起来，给外界反馈，发挥自己积极的作用。但切记要安静，即心态要平稳，如果急于求成，慌里慌张地做事情，那就会变成真的&quot;浊&quot;，变得稀里糊涂了。老子建议做一切事都要不急不躁，不“乱”不“浊”，一切要悠然&quot;徐生&quot;，水到渠成。\n<img src=\"/2019/09/09/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/hehua.png\" alt></p>\n<h1>问题</h1>\n<p>为什么要聊在Python API 中如何使用 Java 自定义 Source/Sink？目前在Apache Flink中集成了一些常用的connector，比如Kafka,elasticsearch6,filesystem等常用的外部系统。但这仍然无法满足需求各异的用户。比如用户用Kafka但是可能用户使用的Kafka版本Flink并没有支持（当然社区也可以添加支持，我们只是说可能情况），还比如用户想使用自建的内部产品，那一定无法集成用户的内部产品到Flink社区，这时候就要用户根据Flink自定义Connector的规范开发自己的Connector了。那么本篇我们就要介绍如果你已经完成了自定义connector，如何在Python Table API中进行使用。</p>\n<h1>如何自定义Java Connector</h1>\n<p>Apache Flink Table API/SQL中采用了Service Provider Interfaces (SPI)方式查找系统内置的和用户自定义的Connector。简单说任何Connector都需一个对应的<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableFactory.java\" target=\"_blank\" rel=\"noopener\">TableFactory</a>的实现，其核心方法是<code>requiredContext</code>,该方法定义对应Connector的唯一特征属性，connector.type，connector.version等。Connector的类型和版本就唯一决定了一个具体的Connector实现。当然任何Connector都要实现具体的<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java\" target=\"_blank\" rel=\"noopener\">TableSource</a>或<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sinks/TableSink.java\" target=\"_blank\" rel=\"noopener\">TableSink</a>。关于自定义Java Connector我们会在一篇完整的Blog进行详细叙述，本篇重点集中在如果已经完成了自定义的Connector，我们如何在Python Table API中使用。</p>\n<h1>Python Table API中注册Connector</h1>\n<p>我们知道Flink Python Table API中采用Py4j的方式将Python Table API转换为Java Table API，而Java Table API中采用SPI的方式发现和查找Connector，也就是说Pytohn API需要定义一种方式能够告诉Java Table API 用户自定义的Connector信息。对于一个自定义Connector有两个非常重要的信息：</p>\n<ul>\n<li>CustomConnectorDescriptor - Connector自有的属性定义；</li>\n<li>CustomFormatDescriptor - Connector的数据格式定义；</li>\n</ul>\n<p>所以在Python Table API中我们利用 <code>CustomConnectorDescriptor</code> 和<code>CustomFormatDescriptor</code>来描述用户自定义Connector。</p>\n<h2>自定义Connector类加载到Classpath</h2>\n<p>用户要使用自定义的Connector，那么第一步是将用户自定义的类要加载到Classpath中，操作方式与Python API中使用Java UDF一致，详见：《<a href=\"http://1t.click/BSa\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业</a>》</p>\n<h2>Python Table API 描述自定义Connector</h2>\n<p>我们这里以自定义Kafka Source为例，用户编写Python Table API如下：</p>\n<ul>\n<li>定义Connector属性\n我们需要利用<code>CustomConnectorDescriptor</code>来描述Connector属性，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \\</span><br><span class=\"line\">        .property(&apos;connector.topic&apos;, &apos;user&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面属性与内置的Kafka 0.11版本一致。这里是示例如何利用<code>CustomConnectorDescriptor</code>描述自定义Connector属性。</p>\n<ul>\n<li>定义Connector的数据格式\n我们需要利用<code>CustomFormatDescriptor</code>来描述Connector数据格式，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># the key is &apos;format.json-schema&apos;</span><br><span class=\"line\">   custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \\</span><br><span class=\"line\">       .property(&apos;format.json-schema&apos;,</span><br><span class=\"line\">                 &quot;&#123;&quot;</span><br><span class=\"line\">                 &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                 &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                 &quot;    a: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    b: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    c: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    time: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                 &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;&quot;</span><br><span class=\"line\">                 &quot;  &#125;&quot;</span><br><span class=\"line\">                 &quot;&#125;&quot;) \\</span><br><span class=\"line\">       .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>这个数据格式是沿用了在 《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇中的示例。</p>\n<ul>\n<li>定义Table Source\n完成Connector属性的描述和Connector数据格式描述之后，我们就可以同使用内置Connector一样定义Table Source了。如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env \\</span><br><span class=\"line\">    .connect(custom_connector) \\</span><br><span class=\"line\">    .with_format(custom_format) \\</span><br><span class=\"line\">    .with_schema(...) \\</span><br><span class=\"line\">    .register_table_source(&quot;source&quot;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面<code>with_schema(...)</code>是定义数据流入Flink内部后的Table 数据结构。</p>\n<ul>\n<li>定义查询逻辑\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.scan(&quot;source&quot;)</span><br><span class=\"line\">   .window(Tumble.over(&quot;2.rows&quot;)</span><br><span class=\"line\">       .on(&quot;proctime&quot;).alias(&quot;w&quot;)) \\</span><br><span class=\"line\">   .group_by(&quot;w, a&quot;) \\</span><br><span class=\"line\">   .select(&quot;a, max(b)&quot;)</span><br><span class=\"line\">   .insert_into(&quot;result&quot;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询逻辑是一个简单的每2行作为一个Tumble Window。计算按 <code>a</code> 分组统计 <code>b</code> 的最大值。</p>\n<h1>完整示例</h1>\n<p>完整的自定义Kafka Source示例:</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \\</span><br><span class=\"line\">        .property(&apos;connector.topic&apos;, &apos;user&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    # the key is &apos;format.json-schema&apos;</span><br><span class=\"line\">    custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \\</span><br><span class=\"line\">        .property(&apos;format.json-schema&apos;,</span><br><span class=\"line\">                  &quot;&#123;&quot;</span><br><span class=\"line\">                  &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                  &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                  &quot;    a: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    b: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    c: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    time: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                  &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;&quot;</span><br><span class=\"line\">                  &quot;  &#125;&quot;</span><br><span class=\"line\">                  &quot;&#125;&quot;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    s_env = StreamExecutionEnvironment.get_execution_environment()</span><br><span class=\"line\">    s_env.set_parallelism(1)</span><br><span class=\"line\">    s_env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)</span><br><span class=\"line\">    st_env = StreamTableEnvironment.create(s_env)</span><br><span class=\"line\">    result_file = &quot;/tmp/custom_kafka_source_demo.csv&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    if os.path.exists(result_file):</span><br><span class=\"line\">        os.remove(result_file)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env \\</span><br><span class=\"line\">        .connect(custom_connector) \\</span><br><span class=\"line\">        .with_format(</span><br><span class=\"line\">        custom_format</span><br><span class=\"line\">    ) \\</span><br><span class=\"line\">        .with_schema(  # declare the schema of the table</span><br><span class=\"line\">        Schema()</span><br><span class=\"line\">            .field(&quot;proctime&quot;, DataTypes.TIMESTAMP())</span><br><span class=\"line\">            .proctime()</span><br><span class=\"line\">            .field(&quot;a&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;b&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;c&quot;, DataTypes.STRING())</span><br><span class=\"line\">    ) \\</span><br><span class=\"line\">        .in_append_mode() \\</span><br><span class=\"line\">        .register_table_source(&quot;source&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.register_table_sink(&quot;result&quot;,</span><br><span class=\"line\">                               CsvTableSink([&quot;a&quot;, &quot;b&quot;],</span><br><span class=\"line\">                                            [DataTypes.STRING(),</span><br><span class=\"line\">                                             DataTypes.STRING()],</span><br><span class=\"line\">                                            result_file))</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.scan(&quot;source&quot;).window(Tumble.over(&quot;2.rows&quot;).on(&quot;proctime&quot;).alias(&quot;w&quot;)) \\</span><br><span class=\"line\">        .group_by(&quot;w, a&quot;) \\</span><br><span class=\"line\">        .select(&quot;a, max(b)&quot;).insert_into(&quot;result&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.execute(&quot;custom kafka source demo&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>详细代码见:\n<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py</a></p>\n<ul>\n<li>\n<p>运行CustomKafkaSourceDemo\n上面示例的运行需要一个Kafka的集群环境，集群信息需要和<code>CustomConnectorDescriptor</code>中描述的一致，比如<code>connector.topic</code>是<code>user</code>， <code>zookeeper.connect</code>, <code>localhost:2181</code>, <code>bootstrap.servers</code>是 <code>localhost:9092</code>。 这些信息与《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇中的Kafka环境一致，大家可以参考初始化自己的Kafka环境。</p>\n</li>\n<li>\n<p>准备测试数据\n运行之前大家要向 <code>user</code> Topic中写入一些测试数据，\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    topic = &apos;user&apos;</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        delete_topics(topic)</span><br><span class=\"line\"></span><br><span class=\"line\">    create_topic(topic)</span><br><span class=\"line\">    msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;]</span><br><span class=\"line\">    for msg in msgs:</span><br><span class=\"line\">        send_msg(topic, msg)</span><br><span class=\"line\">    # print test data</span><br><span class=\"line\">    get_msg(topic)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>完整代码可以参考：<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py</a></p>\n<ul>\n<li>运行代码，并查看最终计算结果\n![338566533211a283d7a715ba32009fd2](Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).resources/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png)</li>\n</ul>\n<h1>待续</h1>\n<p>上面示例是在假设用户已经完成了 Java 自定义Connector的基础之上进行介绍如何利用Python Table API使用自定义Connector，但并没有详细介绍用户如何自定义Java Connector。所以后续我会为大家单独在一篇Blog中介绍Apache Flink 如何自定义Java Connector。<a href=\"https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sourceSinks.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1>小结</h1>\n<p>本篇内容篇幅比较短小，具体介绍了Python Table API如何使用自定义Java Connector。其中Kafka环境相关内容需要结合《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇一起阅读。同时在开篇说道中简单分享了老子关于&quot;孰能浊以静之徐清? 孰能安以动之徐生?&quot;的人生智慧，希望对大家有所启迪！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/09/09/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>开篇说道</h1>\n<p>老子说&quot;孰能浊以静之徐清? 孰能安以动之徐生?&quot;,大概是说谁能像浑浊的水流一样停止流动，安静下来慢慢变得澄清？谁可以像草木那样保持长时的静寂，却萌动生机而不息？对于与世为人来说，这句话是指谁能在浊世中保持内心的清净？只有智者可以静看花开花落，就像是莲花，出淤泥而不染。&quot;孰能安以动之徐生&quot; 一句则说在保持安静的同时，还要激发着无穷的生机，顺应自然的同时，默默改变这人世界。老子并不提倡举世皆浊我独清的态度，而是提倡“混兮其若浊”、“和其光，同其尘”，也就是说不要认为自己了不起，每个人的行为都有其道理，我们应该理解他人和接纳他人，和他人一样，表面看起来有点浑浑噩噩，但内心要&quot;静&quot;,要&quot;清&quot;，就像一杯含有沙子的黄河水，只有在安静的状态下，才能慢慢变清，我们内心要从纷繁复杂的&quot;浊&quot;中理清头绪，变得清醒。但静静地弄清楚了还不够，还需要动起来，给外界反馈，发挥自己积极的作用。但切记要安静，即心态要平稳，如果急于求成，慌里慌张地做事情，那就会变成真的&quot;浊&quot;，变得稀里糊涂了。老子建议做一切事都要不急不躁，不“乱”不“浊”，一切要悠然&quot;徐生&quot;，水到渠成。\n<img src=\"/2019/09/09/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/hehua.png\" alt></p>\n<h1>问题</h1>\n<p>为什么要聊在Python API 中如何使用 Java 自定义 Source/Sink？目前在Apache Flink中集成了一些常用的connector，比如Kafka,elasticsearch6,filesystem等常用的外部系统。但这仍然无法满足需求各异的用户。比如用户用Kafka但是可能用户使用的Kafka版本Flink并没有支持（当然社区也可以添加支持，我们只是说可能情况），还比如用户想使用自建的内部产品，那一定无法集成用户的内部产品到Flink社区，这时候就要用户根据Flink自定义Connector的规范开发自己的Connector了。那么本篇我们就要介绍如果你已经完成了自定义connector，如何在Python Table API中进行使用。</p>\n<h1>如何自定义Java Connector</h1>\n<p>Apache Flink Table API/SQL中采用了Service Provider Interfaces (SPI)方式查找系统内置的和用户自定义的Connector。简单说任何Connector都需一个对应的<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableFactory.java\" target=\"_blank\" rel=\"noopener\">TableFactory</a>的实现，其核心方法是<code>requiredContext</code>,该方法定义对应Connector的唯一特征属性，connector.type，connector.version等。Connector的类型和版本就唯一决定了一个具体的Connector实现。当然任何Connector都要实现具体的<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sources/TableSource.java\" target=\"_blank\" rel=\"noopener\">TableSource</a>或<a href=\"https://github.com/apache/flink/blob/release-1.9.0/flink-table/flink-table-common/src/main/java/org/apache/flink/table/sinks/TableSink.java\" target=\"_blank\" rel=\"noopener\">TableSink</a>。关于自定义Java Connector我们会在一篇完整的Blog进行详细叙述，本篇重点集中在如果已经完成了自定义的Connector，我们如何在Python Table API中使用。</p>\n<h1>Python Table API中注册Connector</h1>\n<p>我们知道Flink Python Table API中采用Py4j的方式将Python Table API转换为Java Table API，而Java Table API中采用SPI的方式发现和查找Connector，也就是说Pytohn API需要定义一种方式能够告诉Java Table API 用户自定义的Connector信息。对于一个自定义Connector有两个非常重要的信息：</p>\n<ul>\n<li>CustomConnectorDescriptor - Connector自有的属性定义；</li>\n<li>CustomFormatDescriptor - Connector的数据格式定义；</li>\n</ul>\n<p>所以在Python Table API中我们利用 <code>CustomConnectorDescriptor</code> 和<code>CustomFormatDescriptor</code>来描述用户自定义Connector。</p>\n<h2>自定义Connector类加载到Classpath</h2>\n<p>用户要使用自定义的Connector，那么第一步是将用户自定义的类要加载到Classpath中，操作方式与Python API中使用Java UDF一致，详见：《<a href=\"http://1t.click/BSa\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业</a>》</p>\n<h2>Python Table API 描述自定义Connector</h2>\n<p>我们这里以自定义Kafka Source为例，用户编写Python Table API如下：</p>\n<ul>\n<li>定义Connector属性\n我们需要利用<code>CustomConnectorDescriptor</code>来描述Connector属性，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \\</span><br><span class=\"line\">        .property(&apos;connector.topic&apos;, &apos;user&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面属性与内置的Kafka 0.11版本一致。这里是示例如何利用<code>CustomConnectorDescriptor</code>描述自定义Connector属性。</p>\n<ul>\n<li>定义Connector的数据格式\n我们需要利用<code>CustomFormatDescriptor</code>来描述Connector数据格式，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># the key is &apos;format.json-schema&apos;</span><br><span class=\"line\">   custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \\</span><br><span class=\"line\">       .property(&apos;format.json-schema&apos;,</span><br><span class=\"line\">                 &quot;&#123;&quot;</span><br><span class=\"line\">                 &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                 &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                 &quot;    a: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    b: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    c: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;,&quot;</span><br><span class=\"line\">                 &quot;    time: &#123;&quot;</span><br><span class=\"line\">                 &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                 &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                 &quot;    &#125;&quot;</span><br><span class=\"line\">                 &quot;  &#125;&quot;</span><br><span class=\"line\">                 &quot;&#125;&quot;) \\</span><br><span class=\"line\">       .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>这个数据格式是沿用了在 《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇中的示例。</p>\n<ul>\n<li>定义Table Source\n完成Connector属性的描述和Connector数据格式描述之后，我们就可以同使用内置Connector一样定义Table Source了。如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env \\</span><br><span class=\"line\">    .connect(custom_connector) \\</span><br><span class=\"line\">    .with_format(custom_format) \\</span><br><span class=\"line\">    .with_schema(...) \\</span><br><span class=\"line\">    .register_table_source(&quot;source&quot;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>上面<code>with_schema(...)</code>是定义数据流入Flink内部后的Table 数据结构。</p>\n<ul>\n<li>定义查询逻辑\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">st_env.scan(&quot;source&quot;)</span><br><span class=\"line\">   .window(Tumble.over(&quot;2.rows&quot;)</span><br><span class=\"line\">       .on(&quot;proctime&quot;).alias(&quot;w&quot;)) \\</span><br><span class=\"line\">   .group_by(&quot;w, a&quot;) \\</span><br><span class=\"line\">   .select(&quot;a, max(b)&quot;)</span><br><span class=\"line\">   .insert_into(&quot;result&quot;)</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>查询逻辑是一个简单的每2行作为一个Tumble Window。计算按 <code>a</code> 分组统计 <code>b</code> 的最大值。</p>\n<h1>完整示例</h1>\n<p>完整的自定义Kafka Source示例:</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \\</span><br><span class=\"line\">        .property(&apos;connector.topic&apos;, &apos;user&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \\</span><br><span class=\"line\">        .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    # the key is &apos;format.json-schema&apos;</span><br><span class=\"line\">    custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \\</span><br><span class=\"line\">        .property(&apos;format.json-schema&apos;,</span><br><span class=\"line\">                  &quot;&#123;&quot;</span><br><span class=\"line\">                  &quot;  type: &apos;object&apos;,&quot;</span><br><span class=\"line\">                  &quot;  properties: &#123;&quot;</span><br><span class=\"line\">                  &quot;    a: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    b: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    c: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;,&quot;</span><br><span class=\"line\">                  &quot;    time: &#123;&quot;</span><br><span class=\"line\">                  &quot;      type: &apos;string&apos;,&quot;</span><br><span class=\"line\">                  &quot;      format: &apos;date-time&apos;&quot;</span><br><span class=\"line\">                  &quot;    &#125;&quot;</span><br><span class=\"line\">                  &quot;  &#125;&quot;</span><br><span class=\"line\">                  &quot;&#125;&quot;) \\</span><br><span class=\"line\">        .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    s_env = StreamExecutionEnvironment.get_execution_environment()</span><br><span class=\"line\">    s_env.set_parallelism(1)</span><br><span class=\"line\">    s_env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)</span><br><span class=\"line\">    st_env = StreamTableEnvironment.create(s_env)</span><br><span class=\"line\">    result_file = &quot;/tmp/custom_kafka_source_demo.csv&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    if os.path.exists(result_file):</span><br><span class=\"line\">        os.remove(result_file)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env \\</span><br><span class=\"line\">        .connect(custom_connector) \\</span><br><span class=\"line\">        .with_format(</span><br><span class=\"line\">        custom_format</span><br><span class=\"line\">    ) \\</span><br><span class=\"line\">        .with_schema(  # declare the schema of the table</span><br><span class=\"line\">        Schema()</span><br><span class=\"line\">            .field(&quot;proctime&quot;, DataTypes.TIMESTAMP())</span><br><span class=\"line\">            .proctime()</span><br><span class=\"line\">            .field(&quot;a&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;b&quot;, DataTypes.STRING())</span><br><span class=\"line\">            .field(&quot;c&quot;, DataTypes.STRING())</span><br><span class=\"line\">    ) \\</span><br><span class=\"line\">        .in_append_mode() \\</span><br><span class=\"line\">        .register_table_source(&quot;source&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.register_table_sink(&quot;result&quot;,</span><br><span class=\"line\">                               CsvTableSink([&quot;a&quot;, &quot;b&quot;],</span><br><span class=\"line\">                                            [DataTypes.STRING(),</span><br><span class=\"line\">                                             DataTypes.STRING()],</span><br><span class=\"line\">                                            result_file))</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.scan(&quot;source&quot;).window(Tumble.over(&quot;2.rows&quot;).on(&quot;proctime&quot;).alias(&quot;w&quot;)) \\</span><br><span class=\"line\">        .group_by(&quot;w, a&quot;) \\</span><br><span class=\"line\">        .select(&quot;a, max(b)&quot;).insert_into(&quot;result&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    st_env.execute(&quot;custom kafka source demo&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>详细代码见:\n<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py</a></p>\n<ul>\n<li>\n<p>运行CustomKafkaSourceDemo\n上面示例的运行需要一个Kafka的集群环境，集群信息需要和<code>CustomConnectorDescriptor</code>中描述的一致，比如<code>connector.topic</code>是<code>user</code>， <code>zookeeper.connect</code>, <code>localhost:2181</code>, <code>bootstrap.servers</code>是 <code>localhost:9092</code>。 这些信息与《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇中的Kafka环境一致，大家可以参考初始化自己的Kafka环境。</p>\n</li>\n<li>\n<p>准备测试数据\n运行之前大家要向 <code>user</code> Topic中写入一些测试数据，\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    topic = &apos;user&apos;</span><br><span class=\"line\">    topics = list_topics()</span><br><span class=\"line\">    if topic in topics:</span><br><span class=\"line\">        delete_topics(topic)</span><br><span class=\"line\"></span><br><span class=\"line\">    create_topic(topic)</span><br><span class=\"line\">    msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;,</span><br><span class=\"line\">            &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;]</span><br><span class=\"line\">    for msg in msgs:</span><br><span class=\"line\">        send_msg(topic, msg)</span><br><span class=\"line\">    # print test data</span><br><span class=\"line\">    get_msg(topic)</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n<p>完整代码可以参考：<a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py</a></p>\n<ul>\n<li>运行代码，并查看最终计算结果\n![338566533211a283d7a715ba32009fd2](Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).resources/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png)</li>\n</ul>\n<h1>待续</h1>\n<p>上面示例是在假设用户已经完成了 Java 自定义Connector的基础之上进行介绍如何利用Python Table API使用自定义Connector，但并没有详细介绍用户如何自定义Java Connector。所以后续我会为大家单独在一篇Blog中介绍Apache Flink 如何自定义Java Connector。<a href=\"https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sourceSinks.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1>小结</h1>\n<p>本篇内容篇幅比较短小，具体介绍了Python Table API如何使用自定义Java Connector。其中Kafka环境相关内容需要结合《<a href=\"http://1t.click/aej7\" target=\"_blank\" rel=\"noopener\">Apache Flink 说道系列- Python API 中如何使用 Kafka</a>》一篇一起阅读。同时在开篇说道中简单分享了老子关于&quot;孰能浊以静之徐清? 孰能安以动之徐生?&quot;的人生智慧，希望对大家有所启迪！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/09/09/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/comment.png\" alt></p>\n"},{"title":"Apache Flink 漫谈系列 - 概述","date":"2019-01-05T10:18:18.000Z","_content":"# Apache Flink 的命脉\n\"**命脉**\" 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以\"**批是流的特例**\"的认知进行系统设计的。\n\n\n# 唯快不破\n我们经常听说 \"天下武功，唯快不破\"，大概意思是说 \"任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破\"。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是\"**快**\",也就是 \"**低延时**\"。\n\n就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 \"**低延时**\" 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 \"**快**\"呢？根本原因是Apache Flink 设计之初就认为 \"**批是流的特例**\"，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。\n\n那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是\"**流是批的特例**\" 和 \"**批是流的特例**\" 两种不同认知产物。\n\n## Micro Batching 模式\nMicro-Batching 计算模式认为 \"**流是批的特例**\"， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图：\n\n![](0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png)\n\n很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。\n\n## Native Streaming 模式\nNative Streaming 计算模式认为 \"\"**批是流的特**\", 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图：\n\n![](Snip20180921_7.png)\n\n很明显Native Streaming模式占据了流计算领域 \"**低延时**\" 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现\nNative Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。\n\n# 丰富的部署模式\nApache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。\n\n## Local 模式\n该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。[参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html)\n\n## Cluster模式\n该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/cluster_setup.html) YARN Cluster [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html)\n这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下：\n\n![](Snip20180921_4.png)\n    \n其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N>=1)个JM候选,进而解决SPOF问题，示意如下：\n    ![](Snip20180921_6.png)\n    \n在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。\n\n## Cloud 模式\n该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/gce_setup.html)，Amazon的EC2 [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html)，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。\n\n# 完善的容错机制\n## 什么是容错\n容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。\n\n## 容错的处理模式\n在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够\"运行\"外，还要求能\"正确运行\",也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制：\n\n* **At Most Once**：最多消费一次，这种处理机制会存在数据丢失的可能。\n* **At Least Once**：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。\n* **Exactly Once**：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。\n\n## Apache Flink的容错机制\nApache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图:\n\n![](Snip20180922_17.png)\n\n目前Apache Flink 支持两种数据容错机制：\n* **At Least Once**\n* **Exactly Once**\n\n其中 **Exactly Once** 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 **End-to-End** 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景：\n\n* **场景一**：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？\n![](Snip20180922_18.png)\n\n* **场景二**：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？\n![](BF1A3338-5B92-4898-BC81-A4A6626D4208.png)\n\n* **场景三**：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？\n![](7DD374B0-1901-424C-8683-2AE890B8A5FA.png)\n\n### 系统内部容错\nApache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/pdf/1506.08603.pdf) 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 [Determining-Global-States-of-a-Distributed-System Paper](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf)。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了**At Least Once** 和 **Exactly Once** 两种容错处理模式。\n\nApache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图：\n![](Snip20180922_22.png)\n\n这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。\n\n\n### 外部Source容错\nApache Flink 要做到 **End-to-End** 的 **Exactly Once** 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。\n\n### 外部Sink容错\nApache Flink 要做到 **End-to-End** 的 **Exactly Once** 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 **exactly once**的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了**End-to-End** 的 **Exactly Once** 语义(重复写入就变成了**At Least Once**了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。\n\n# 流批统一的计算引擎\n批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？\n\n## 统一的数据传输层\n开篇我们就介绍Apache Flink 的 \"**命脉**\"是以\"**批是流的特例**\"为导向来进行引擎的设计的，系统设计成为 \"**Native Streaming**\"的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。\n\nApache Flink 在网络传输层面有两种数据传输模式：\n* PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。\n* BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。\n\n对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。\n\n## 统一任务调度层\nApache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。\n\n## 统一的用户API层\nApache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。\n\n## 求同存异\nApache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。\n\n# Apache Flink 架构\n## 组件栈\n我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：\n![](B995477B-C4CC-4A04-BE20-09C9100725E0.png)\nTableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？\n## TableAPI&SQL到DataStrem&DataSet的架构\nTableAPI&SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下：\n\n![](B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png)\n\n对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。\n\n## ANSI-SQL的支持\nApache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下：\n\n![](800B7916-C9BF-4422-ADC9-2C5D90E30339.png)\n\n* Declarative - 用户只需要表达我想要什么，不用关心如何计算。\n* Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。\n* Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。\n* Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。\n* Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。\n\n## 无限扩展的优化机制\nApache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner：\n* HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。\n* VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。\n\nFlink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。\n\n# 丰富的类库和算子\nApache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。\n## 类库\n\n* CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。\n* ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。\n* GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。\n\n## 算子\nApache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。\n### 多流操作 \n* UNION - 将多个**字段类型一致**数据流合并为一个数据流，如下示意：\n![](F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png)\n\n* JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意：\n![](013D77B3-0D57-433F-B8D7-476D8CD3384E.png)\n\n如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。\n\n### 单流操作\n将多流变成单流之后，我们按数据输入输出的不同归类如下：\n| 类型 | 输入 |  输出|Table/SQL算子|DataStream/DataSet算子|\n| --- | --- | --- | --- |--- |\n| Scalar Function | 1 | 1 | Built-in & UDF, |Map|\n| Table Function| 1 | N(N>=0) | Built-in & UDTF |FlatMap|\n| Aggregate Function| N(N>=0) |1  |  Built-in & UDAF|Reduce|\n\n如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。\n\n## 存在的问题\nApache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下：\n![](D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png)\n\n这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。\n\n\n# 小结\n本篇概要的介绍了\"**批是流的特例**\"这一设计观点是Apache Flink的\"**命脉**\"，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的\"低延迟\"需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。\n\n本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！\n\n最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！","source":"_posts/Apache Flink 漫谈系列 - 概述.md","raw":"---\ntitle: Apache Flink 漫谈系列 - 概述\ndate: 2019-01-05 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n# Apache Flink 的命脉\n\"**命脉**\" 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以\"**批是流的特例**\"的认知进行系统设计的。\n\n\n# 唯快不破\n我们经常听说 \"天下武功，唯快不破\"，大概意思是说 \"任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破\"。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是\"**快**\",也就是 \"**低延时**\"。\n\n就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 \"**低延时**\" 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 \"**快**\"呢？根本原因是Apache Flink 设计之初就认为 \"**批是流的特例**\"，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。\n\n那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是\"**流是批的特例**\" 和 \"**批是流的特例**\" 两种不同认知产物。\n\n## Micro Batching 模式\nMicro-Batching 计算模式认为 \"**流是批的特例**\"， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图：\n\n![](0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png)\n\n很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。\n\n## Native Streaming 模式\nNative Streaming 计算模式认为 \"\"**批是流的特**\", 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图：\n\n![](Snip20180921_7.png)\n\n很明显Native Streaming模式占据了流计算领域 \"**低延时**\" 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现\nNative Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。\n\n# 丰富的部署模式\nApache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。\n\n## Local 模式\n该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。[参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html)\n\n## Cluster模式\n该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/cluster_setup.html) YARN Cluster [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html)\n这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下：\n\n![](Snip20180921_4.png)\n    \n其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N>=1)个JM候选,进而解决SPOF问题，示意如下：\n    ![](Snip20180921_6.png)\n    \n在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。\n\n## Cloud 模式\n该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/gce_setup.html)，Amazon的EC2 [参考](https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html)，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。\n\n# 完善的容错机制\n## 什么是容错\n容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。\n\n## 容错的处理模式\n在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够\"运行\"外，还要求能\"正确运行\",也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制：\n\n* **At Most Once**：最多消费一次，这种处理机制会存在数据丢失的可能。\n* **At Least Once**：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。\n* **Exactly Once**：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。\n\n## Apache Flink的容错机制\nApache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图:\n\n![](Snip20180922_17.png)\n\n目前Apache Flink 支持两种数据容错机制：\n* **At Least Once**\n* **Exactly Once**\n\n其中 **Exactly Once** 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 **End-to-End** 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景：\n\n* **场景一**：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？\n![](Snip20180922_18.png)\n\n* **场景二**：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？\n![](BF1A3338-5B92-4898-BC81-A4A6626D4208.png)\n\n* **场景三**：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？\n![](7DD374B0-1901-424C-8683-2AE890B8A5FA.png)\n\n### 系统内部容错\nApache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/pdf/1506.08603.pdf) 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 [Determining-Global-States-of-a-Distributed-System Paper](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf)。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了**At Least Once** 和 **Exactly Once** 两种容错处理模式。\n\nApache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图：\n![](Snip20180922_22.png)\n\n这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。\n\n\n### 外部Source容错\nApache Flink 要做到 **End-to-End** 的 **Exactly Once** 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。\n\n### 外部Sink容错\nApache Flink 要做到 **End-to-End** 的 **Exactly Once** 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 **exactly once**的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了**End-to-End** 的 **Exactly Once** 语义(重复写入就变成了**At Least Once**了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。\n\n# 流批统一的计算引擎\n批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？\n\n## 统一的数据传输层\n开篇我们就介绍Apache Flink 的 \"**命脉**\"是以\"**批是流的特例**\"为导向来进行引擎的设计的，系统设计成为 \"**Native Streaming**\"的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。\n\nApache Flink 在网络传输层面有两种数据传输模式：\n* PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。\n* BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。\n\n对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。\n\n## 统一任务调度层\nApache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。\n\n## 统一的用户API层\nApache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。\n\n## 求同存异\nApache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。\n\n# Apache Flink 架构\n## 组件栈\n我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：\n![](B995477B-C4CC-4A04-BE20-09C9100725E0.png)\nTableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？\n## TableAPI&SQL到DataStrem&DataSet的架构\nTableAPI&SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下：\n\n![](B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png)\n\n对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。\n\n## ANSI-SQL的支持\nApache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下：\n\n![](800B7916-C9BF-4422-ADC9-2C5D90E30339.png)\n\n* Declarative - 用户只需要表达我想要什么，不用关心如何计算。\n* Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。\n* Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。\n* Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。\n* Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。\n\n## 无限扩展的优化机制\nApache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner：\n* HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。\n* VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。\n\nFlink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。\n\n# 丰富的类库和算子\nApache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。\n## 类库\n\n* CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。\n* ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。\n* GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。\n\n## 算子\nApache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。\n### 多流操作 \n* UNION - 将多个**字段类型一致**数据流合并为一个数据流，如下示意：\n![](F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png)\n\n* JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意：\n![](013D77B3-0D57-433F-B8D7-476D8CD3384E.png)\n\n如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。\n\n### 单流操作\n将多流变成单流之后，我们按数据输入输出的不同归类如下：\n| 类型 | 输入 |  输出|Table/SQL算子|DataStream/DataSet算子|\n| --- | --- | --- | --- |--- |\n| Scalar Function | 1 | 1 | Built-in & UDF, |Map|\n| Table Function| 1 | N(N>=0) | Built-in & UDTF |FlatMap|\n| Aggregate Function| N(N>=0) |1  |  Built-in & UDAF|Reduce|\n\n如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。\n\n## 存在的问题\nApache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下：\n![](D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png)\n\n这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。\n\n\n# 小结\n本篇概要的介绍了\"**批是流的特例**\"这一设计观点是Apache Flink的\"**命脉**\"，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的\"低延迟\"需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。\n\n本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！\n\n最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！","slug":"Apache Flink 漫谈系列 - 概述","published":1,"updated":"2019-07-13T13:10:10.275Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5g9001cs94hltig9zj6","content":"<h1>Apache Flink 的命脉</h1>\n<p>&quot;<strong>命脉</strong>&quot; 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以&quot;<strong>批是流的特例</strong>&quot;的认知进行系统设计的。</p>\n<h1>唯快不破</h1>\n<p>我们经常听说 &quot;天下武功，唯快不破&quot;，大概意思是说 &quot;任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破&quot;。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是&quot;<strong>快</strong>&quot;,也就是 &quot;<strong>低延时</strong>&quot;。</p>\n<p>就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 &quot;<strong>低延时</strong>&quot; 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 &quot;<strong>快</strong>&quot;呢？根本原因是Apache Flink 设计之初就认为 &quot;<strong>批是流的特例</strong>&quot;，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。</p>\n<p>那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是&quot;<strong>流是批的特例</strong>&quot; 和 &quot;<strong>批是流的特例</strong>&quot; 两种不同认知产物。</p>\n<h2>Micro Batching 模式</h2>\n<p>Micro-Batching 计算模式认为 &quot;<strong>流是批的特例</strong>&quot;， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png\" alt></p>\n<p>很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。</p>\n<h2>Native Streaming 模式</h2>\n<p>Native Streaming 计算模式认为 &quot;&quot;<strong>批是流的特</strong>&quot;, 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_7.png\" alt></p>\n<p>很明显Native Streaming模式占据了流计算领域 &quot;<strong>低延时</strong>&quot; 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现\nNative Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。</p>\n<h1>丰富的部署模式</h1>\n<p>Apache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。</p>\n<h2>Local 模式</h2>\n<p>该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<h2>Cluster模式</h2>\n<p>该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/cluster_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a> YARN Cluster <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a>\n这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_4.png\" alt></p>\n<p>其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N&gt;=1)个JM候选,进而解决SPOF问题，示意如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_6.png\" alt></p>\n<p>在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。</p>\n<h2>Cloud 模式</h2>\n<p>该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/gce_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a>，Amazon的EC2 <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html\" target=\"_blank\" rel=\"noopener\">参考</a>，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。</p>\n<h1>完善的容错机制</h1>\n<h2>什么是容错</h2>\n<p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。</p>\n<h2>容错的处理模式</h2>\n<p>在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够&quot;运行&quot;外，还要求能&quot;正确运行&quot;,也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制：</p>\n<ul>\n<li><strong>At Most Once</strong>：最多消费一次，这种处理机制会存在数据丢失的可能。</li>\n<li><strong>At Least Once</strong>：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。</li>\n<li><strong>Exactly Once</strong>：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。</li>\n</ul>\n<h2>Apache Flink的容错机制</h2>\n<p>Apache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图:</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_17.png\" alt></p>\n<p>目前Apache Flink 支持两种数据容错机制：</p>\n<ul>\n<li><strong>At Least Once</strong></li>\n<li><strong>Exactly Once</strong></li>\n</ul>\n<p>其中 <strong>Exactly Once</strong> 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 <strong>End-to-End</strong> 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景：</p>\n<ul>\n<li>\n<p><strong>场景一</strong>：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_18.png\" alt></p>\n</li>\n<li>\n<p><strong>场景二</strong>：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/BF1A3338-5B92-4898-BC81-A4A6626D4208.png\" alt></p>\n</li>\n<li>\n<p><strong>场景三</strong>：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/7DD374B0-1901-424C-8683-2AE890B8A5FA.png\" alt></p>\n</li>\n</ul>\n<h3>系统内部容错</h3>\n<p>Apache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 <a href=\"https://arxiv.org/pdf/1506.08603.pdf\" target=\"_blank\" rel=\"noopener\">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf\" target=\"_blank\" rel=\"noopener\">Determining-Global-States-of-a-Distributed-System Paper</a>。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了<strong>At Least Once</strong> 和 <strong>Exactly Once</strong> 两种容错处理模式。</p>\n<p>Apache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_22.png\" alt></p>\n<p>这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。</p>\n<h3>外部Source容错</h3>\n<p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。</p>\n<h3>外部Sink容错</h3>\n<p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 <strong>exactly once</strong>的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了<strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 语义(重复写入就变成了<strong>At Least Once</strong>了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>\n<h1>流批统一的计算引擎</h1>\n<p>批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？</p>\n<h2>统一的数据传输层</h2>\n<p>开篇我们就介绍Apache Flink 的 &quot;<strong>命脉</strong>&quot;是以&quot;<strong>批是流的特例</strong>&quot;为导向来进行引擎的设计的，系统设计成为 &quot;<strong>Native Streaming</strong>&quot;的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。</p>\n<p>Apache Flink 在网络传输层面有两种数据传输模式：</p>\n<ul>\n<li>PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。</li>\n<li>BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。</li>\n</ul>\n<p>对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。</p>\n<h2>统一任务调度层</h2>\n<p>Apache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。</p>\n<h2>统一的用户API层</h2>\n<p>Apache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。</p>\n<h2>求同存异</h2>\n<p>Apache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。</p>\n<h1>Apache Flink 架构</h1>\n<h2>组件栈</h2>\n<p>我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/B995477B-C4CC-4A04-BE20-09C9100725E0.png\" alt>\nTableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？</p>\n<h2>TableAPI&amp;SQL到DataStrem&amp;DataSet的架构</h2>\n<p>TableAPI&amp;SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png\" alt></p>\n<p>对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。</p>\n<h2>ANSI-SQL的支持</h2>\n<p>Apache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/800B7916-C9BF-4422-ADC9-2C5D90E30339.png\" alt></p>\n<ul>\n<li>Declarative - 用户只需要表达我想要什么，不用关心如何计算。</li>\n<li>Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。</li>\n<li>Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。</li>\n<li>Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。</li>\n<li>Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。</li>\n</ul>\n<h2>无限扩展的优化机制</h2>\n<p>Apache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner：</p>\n<ul>\n<li>HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。</li>\n<li>VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。</li>\n</ul>\n<p>Flink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。</p>\n<h1>丰富的类库和算子</h1>\n<p>Apache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。</p>\n<h2>类库</h2>\n<ul>\n<li>CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。</li>\n<li>ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。</li>\n<li>GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。</li>\n</ul>\n<h2>算子</h2>\n<p>Apache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。</p>\n<h3>多流操作</h3>\n<ul>\n<li>\n<p>UNION - 将多个<strong>字段类型一致</strong>数据流合并为一个数据流，如下示意：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png\" alt></p>\n</li>\n<li>\n<p>JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/013D77B3-0D57-433F-B8D7-476D8CD3384E.png\" alt></p>\n</li>\n</ul>\n<p>如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。</p>\n<h3>单流操作</h3>\n<p>将多流变成单流之后，我们按数据输入输出的不同归类如下：</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>输入</th>\n<th>输出</th>\n<th>Table/SQL算子</th>\n<th>DataStream/DataSet算子</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Scalar Function</td>\n<td>1</td>\n<td>1</td>\n<td>Built-in &amp; UDF,</td>\n<td>Map</td>\n</tr>\n<tr>\n<td>Table Function</td>\n<td>1</td>\n<td>N(N&gt;=0)</td>\n<td>Built-in &amp; UDTF</td>\n<td>FlatMap</td>\n</tr>\n<tr>\n<td>Aggregate Function</td>\n<td>N(N&gt;=0)</td>\n<td>1</td>\n<td>Built-in &amp; UDAF</td>\n<td>Reduce</td>\n</tr>\n</tbody>\n</table>\n<p>如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。</p>\n<h2>存在的问题</h2>\n<p>Apache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png\" alt></p>\n<p>这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。</p>\n<h1>小结</h1>\n<p>本篇概要的介绍了&quot;<strong>批是流的特例</strong>&quot;这一设计观点是Apache Flink的&quot;<strong>命脉</strong>&quot;，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的&quot;低延迟&quot;需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。</p>\n<p>本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！</p>\n<p>最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Apache Flink 的命脉</h1>\n<p>&quot;<strong>命脉</strong>&quot; 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以&quot;<strong>批是流的特例</strong>&quot;的认知进行系统设计的。</p>\n<h1>唯快不破</h1>\n<p>我们经常听说 &quot;天下武功，唯快不破&quot;，大概意思是说 &quot;任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破&quot;。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是&quot;<strong>快</strong>&quot;,也就是 &quot;<strong>低延时</strong>&quot;。</p>\n<p>就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 &quot;<strong>低延时</strong>&quot; 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 &quot;<strong>快</strong>&quot;呢？根本原因是Apache Flink 设计之初就认为 &quot;<strong>批是流的特例</strong>&quot;，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。</p>\n<p>那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是&quot;<strong>流是批的特例</strong>&quot; 和 &quot;<strong>批是流的特例</strong>&quot; 两种不同认知产物。</p>\n<h2>Micro Batching 模式</h2>\n<p>Micro-Batching 计算模式认为 &quot;<strong>流是批的特例</strong>&quot;， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png\" alt></p>\n<p>很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。</p>\n<h2>Native Streaming 模式</h2>\n<p>Native Streaming 计算模式认为 &quot;&quot;<strong>批是流的特</strong>&quot;, 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_7.png\" alt></p>\n<p>很明显Native Streaming模式占据了流计算领域 &quot;<strong>低延时</strong>&quot; 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现\nNative Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。</p>\n<h1>丰富的部署模式</h1>\n<p>Apache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。</p>\n<h2>Local 模式</h2>\n<p>该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<h2>Cluster模式</h2>\n<p>该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/cluster_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a> YARN Cluster <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a>\n这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_4.png\" alt></p>\n<p>其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N&gt;=1)个JM候选,进而解决SPOF问题，示意如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180921_6.png\" alt></p>\n<p>在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。</p>\n<h2>Cloud 模式</h2>\n<p>该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/gce_setup.html\" target=\"_blank\" rel=\"noopener\">参考</a>，Amazon的EC2 <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html\" target=\"_blank\" rel=\"noopener\">参考</a>，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。</p>\n<h1>完善的容错机制</h1>\n<h2>什么是容错</h2>\n<p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。</p>\n<h2>容错的处理模式</h2>\n<p>在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够&quot;运行&quot;外，还要求能&quot;正确运行&quot;,也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制：</p>\n<ul>\n<li><strong>At Most Once</strong>：最多消费一次，这种处理机制会存在数据丢失的可能。</li>\n<li><strong>At Least Once</strong>：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。</li>\n<li><strong>Exactly Once</strong>：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。</li>\n</ul>\n<h2>Apache Flink的容错机制</h2>\n<p>Apache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图:</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_17.png\" alt></p>\n<p>目前Apache Flink 支持两种数据容错机制：</p>\n<ul>\n<li><strong>At Least Once</strong></li>\n<li><strong>Exactly Once</strong></li>\n</ul>\n<p>其中 <strong>Exactly Once</strong> 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 <strong>End-to-End</strong> 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景：</p>\n<ul>\n<li>\n<p><strong>场景一</strong>：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_18.png\" alt></p>\n</li>\n<li>\n<p><strong>场景二</strong>：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/BF1A3338-5B92-4898-BC81-A4A6626D4208.png\" alt></p>\n</li>\n<li>\n<p><strong>场景三</strong>：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/7DD374B0-1901-424C-8683-2AE890B8A5FA.png\" alt></p>\n</li>\n</ul>\n<h3>系统内部容错</h3>\n<p>Apache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 <a href=\"https://arxiv.org/pdf/1506.08603.pdf\" target=\"_blank\" rel=\"noopener\">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf\" target=\"_blank\" rel=\"noopener\">Determining-Global-States-of-a-Distributed-System Paper</a>。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了<strong>At Least Once</strong> 和 <strong>Exactly Once</strong> 两种容错处理模式。</p>\n<p>Apache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/Snip20180922_22.png\" alt></p>\n<p>这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。</p>\n<h3>外部Source容错</h3>\n<p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。</p>\n<h3>外部Sink容错</h3>\n<p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 <strong>exactly once</strong>的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了<strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 语义(重复写入就变成了<strong>At Least Once</strong>了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>\n<h1>流批统一的计算引擎</h1>\n<p>批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？</p>\n<h2>统一的数据传输层</h2>\n<p>开篇我们就介绍Apache Flink 的 &quot;<strong>命脉</strong>&quot;是以&quot;<strong>批是流的特例</strong>&quot;为导向来进行引擎的设计的，系统设计成为 &quot;<strong>Native Streaming</strong>&quot;的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。</p>\n<p>Apache Flink 在网络传输层面有两种数据传输模式：</p>\n<ul>\n<li>PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。</li>\n<li>BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。</li>\n</ul>\n<p>对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。</p>\n<h2>统一任务调度层</h2>\n<p>Apache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。</p>\n<h2>统一的用户API层</h2>\n<p>Apache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。</p>\n<h2>求同存异</h2>\n<p>Apache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。</p>\n<h1>Apache Flink 架构</h1>\n<h2>组件栈</h2>\n<p>我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/B995477B-C4CC-4A04-BE20-09C9100725E0.png\" alt>\nTableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？</p>\n<h2>TableAPI&amp;SQL到DataStrem&amp;DataSet的架构</h2>\n<p>TableAPI&amp;SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png\" alt></p>\n<p>对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。</p>\n<h2>ANSI-SQL的支持</h2>\n<p>Apache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下：</p>\n<p><img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/800B7916-C9BF-4422-ADC9-2C5D90E30339.png\" alt></p>\n<ul>\n<li>Declarative - 用户只需要表达我想要什么，不用关心如何计算。</li>\n<li>Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。</li>\n<li>Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。</li>\n<li>Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。</li>\n<li>Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。</li>\n</ul>\n<h2>无限扩展的优化机制</h2>\n<p>Apache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner：</p>\n<ul>\n<li>HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。</li>\n<li>VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。</li>\n</ul>\n<p>Flink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。</p>\n<h1>丰富的类库和算子</h1>\n<p>Apache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。</p>\n<h2>类库</h2>\n<ul>\n<li>CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。</li>\n<li>ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。</li>\n<li>GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。</li>\n</ul>\n<h2>算子</h2>\n<p>Apache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。</p>\n<h3>多流操作</h3>\n<ul>\n<li>\n<p>UNION - 将多个<strong>字段类型一致</strong>数据流合并为一个数据流，如下示意：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png\" alt></p>\n</li>\n<li>\n<p>JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/013D77B3-0D57-433F-B8D7-476D8CD3384E.png\" alt></p>\n</li>\n</ul>\n<p>如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。</p>\n<h3>单流操作</h3>\n<p>将多流变成单流之后，我们按数据输入输出的不同归类如下：</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>输入</th>\n<th>输出</th>\n<th>Table/SQL算子</th>\n<th>DataStream/DataSet算子</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Scalar Function</td>\n<td>1</td>\n<td>1</td>\n<td>Built-in &amp; UDF,</td>\n<td>Map</td>\n</tr>\n<tr>\n<td>Table Function</td>\n<td>1</td>\n<td>N(N&gt;=0)</td>\n<td>Built-in &amp; UDTF</td>\n<td>FlatMap</td>\n</tr>\n<tr>\n<td>Aggregate Function</td>\n<td>N(N&gt;=0)</td>\n<td>1</td>\n<td>Built-in &amp; UDAF</td>\n<td>Reduce</td>\n</tr>\n</tbody>\n</table>\n<p>如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。</p>\n<h2>存在的问题</h2>\n<p>Apache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下：\n<img src=\"/2019/01/05/Apache Flink 漫谈系列 - 概述/D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png\" alt></p>\n<p>这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。</p>\n<h1>小结</h1>\n<p>本篇概要的介绍了&quot;<strong>批是流的特例</strong>&quot;这一设计观点是Apache Flink的&quot;<strong>命脉</strong>&quot;，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的&quot;低延迟&quot;需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。</p>\n<p>本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！</p>\n<p>最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！</p>\n"},{"title":"Apache Flink 说道系列- Python API 时代的产物","date":"2019-07-21T15:05:15.000Z","_content":"\n# 开篇说道-上善若水\n\n我们常说\"知识就是力量\", \"力量\"也意味着\"能力\"，\"能力\"在没有增加任何限定词语的时候往往是指一个人的综合素质。比如，当阅读本文时候你是想学习Flink Python API如果你很快就能普获道重点，这是学习能力！再如，在我写这篇博文的角度说，如果我写的内容晦涩难懂，那就体现了文字表达能力问题。所以\"能力\"是包含方方面面的综合素质的体现！那么本篇我们说道的内容就是当你具备了很高的\"能力\"之后，具体一点就是如果由于你的能力突出做了领导，你将如何处理和你能力相当或者能力不如你的人的关系呢？就此你思考10秒看看你脑子里的答案是什么？\n\n我想能阅读本篇文章的人都是一心向善，与人友善的好同学，但是在内心很清楚\"友善\"的原则并且在与人发生矛盾时候妥善解决(大多数谦让他人)的同时自己的内心也能做到愉悦和平静的同学请在本篇评论区留言:) \n\n回答刚才的问题: \"如何处理和你能力相当或者能力不如你的人的关系呢？\" 我的建议是：\"上善若水\"，出自老子的《道德经第八章》。老子用水来表达与人为事的自然之道。水至柔，她可以根据与之接触的各种物体的形状改变自己，她以各种形态存在，可以化作甘露滋润万物，可以凝聚江河载船运物！水至刚，她可以滴水穿石、无坚不摧！\"刚\"是一种能力，\"柔\"是一种态度，水有能力，但却始终 以一种谦和的低姿态，从高处流往低处！在《道德经》中老子更强调的是自然，任何事情符合自然规律！所谓\"无为\"并不是\"无作为\"而是\"不违背自然规律\"。所以遇到与人发生矛盾的时候，要心怀包容之心，水至柔可以容纳万物之形，人在有能力的时候也要容纳各色之人，容纳\"蛮不讲理\"之人，因为我们不能\"以蛮治蛮\",容纳\"污言秽语\"之人，因为\"身正影自直\"，容纳\"处处与你为敌\"之人，因为\"孤掌难鸣\"，你不理它，它自消去。容纳\"能弱气盛\"之人，因为他因\"能力不足而容易暴躁\"，只要你诚心助之，让他靠自己能力而得到认可，气自然消失，最终也会变得言和语悦！所以任何事情的发生都有气必然的道理，尊重事实，顺其自然，永远修行自身，位高而卑谦，**上善若水**，与世无争, 无争于世！\n\n![](B83130D4-9E1F-4BBC-88FA-2286951D18E5.png)\n\n# 为啥需要 Python API\n开篇\"上善若水\"的与人之道是我们正式介绍Python API之前的开胃菜。希望是真的开胃，你还有胃口继续读完全篇。那么Apache Flink 的runtime是用Java编写的，为啥还需要增加对Python的支持呢？也许大家都很清楚，目前很多著名的开源项目都支持Python，比如Beam，spark，kafka等，Flink自然也需要增加对Python的支持，这个角度分析也许很好，但我们想想为啥这些非常火热的项目都纷纷支持Python呢？Python语言有怎样的\"魔力\"让这些著名的项目都青睐于他？我们看一统计数据：\n\n## 最流行的编程语言\n我们看看行业分析公司[RedMonk](https://redmonk.com/sogrady/2019/07/18/language-rankings-6-19/)最新的最受欢迎的语言排名数据如下：\n![](DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png)\n上图从github和Stack Overflow两个维度进行分析的前十名如下：\n* JavaScript\n* Java\n* Python\n* PHP\n* C++\n* C#\n* CSS\n* Ruby\n* C\n* TypeScript\n\n我们发现Python排在了第3名，目前也非常流行的R和Go语言排在了15和16名。这个很客观的分享足以证明Python的受众是多么的庞大，任何项目对Python的支持就是在无形的扩大项目的受众用户！\n\n## 互联网最火热的领域\n就目前而言互联网最热的领域应该是大数据计算，以前单机的计算时代已经过去了，为啥单机时代过去了？因为单机处理能力的提高速度远远落后于数据的与日俱增的速度，以几个方面看看为啥目前处于大数据时代，而最炽热的互联网领域是大数据计算？\n\n### 为啥是大数据时代 - 数据量与日俱增\n\n随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下：\n\n![](9B952F03-53BF-43DF-8051-A2FAC84A600C.png)\n\n\n上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是：\n\n* 1 Byte =8 bit\n* 1 KB = 1,024 Bytes \n* 1 MB = 1,024 KB \n* 1 GB = 1,024 MB \n* 1 TB = 1,024 GB \n* 1 PB = 1,024 TB \n* 1 EB = 1,024 PB \n* 1 ZB = 1,024 EB \n* 1 YB = 1,024 ZB \n* 1 BB = 1,024 YB \n* 1 NB = 1,024 BB \n* 1 DB = 1,024 NB \n\n看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下：\n\n![](1AF4C656-6ABA-4F69-A771-E6EC623B382D.png)\n\n### 数据价值的产生 - 数据分析\n\n我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文：\n\n* GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。\n* MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示：\n\n![](B31690BA-A8E2-4C22-A948-8735728542B0.png)\n\n* BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。\n\n受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下：\n\n![](1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png)\n\n但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及在业界家喻户晓！\n\n### 数据价值最大化 - 时效性\n每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目于2010年开始于德国柏林。\n\n## AlphaGo - 人工智能\n在2016谷歌AlphaGo以4:1战胜围棋世界冠军、职业九段棋手李世石之后，人们逐渐用新的眼光审视让深度学习，而且掀起了人工智能的“狂热”。百度百科对人工智能(Artificial Intelligence)，英文缩写为AI的定义是: 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新技术科学。\n\n![](6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png)\n\n而机器学习是进行人工智能的一种方法或者工具。机器学习在以Spark，Flink为首的大数据技术平台中具有较高的地位，尤其Spark近些年在ML方面做了巨大的努力，同时PySpark集成了很多优秀的机器学习类库，如典型的Pandas，在这方面远远超过了Flink，所以Flink正面面对自己的不足，在Flink1.9中开启了新的ML接口和新的flink-python模块！\n\n那么机器学习的重要和Python又有什么关系呢？我们一下统计数据，看什么语言是最流行的机器学习语言？\nIBM 的数据科学家 Jean-Francois Puget 曾经做过一个有趣的分析。他爬取了著名的求职网站 indeed 上雇主的岗位要求变动趋势，来评估当下市场上最受欢迎的岗位语言。其中当他单独搜索\"machine learning\"时候，也可以得到一个近似的结果:\n![](070B3285-C790-4D6F-BB9B-6981756255CA.jpg)\n其结构发现Python是与热的\"machine learning\"，虽然这是2016年的调查，但是也足以证明Python在\"machine learning\"方面的地位，同时上面我们提到的redmonk的统计数据也足以证明这一点！\n\n不仅仅是各种调查，我们也可以从Python的特点和现有的Python生态来说说为什么Python是机器学习的最好语言。\n\nPython 是一种面向对象的解释型程序语言，由荷兰人(Guido van Rossum)于1989年发明，并于1991年发布了第一个版。Python 作为解释型语言，虽然跑得比谁都慢，但Python设计者的哲学是\"用一种方法并且只有一种方法来做一件事\"。在开发新的Python语法时，如果面临多种选择，Python开发者一般会选择明确的没有或者很少有歧义的语法。简单易学的特点促使了Python有庞大的用户群体，进而很多机器学习的类库也是由Python开发的，比如：NumPy、SciPy和结构化数据操作可以通过Pandas等等。所以Python这种丰富的生态系统为机器学习提供了一定程度的便利性，也必然成为了最受欢迎的机器学习语言！\n\n# 小结\n本篇重点描述了Apache Flink 为啥需要支持Python API。以实际数字说明目前我们处于一个大数据时代，数据的价值要依靠大数据分析，而由于数据的时效性的重要性而催生了著名的Apache Flink流式计算平台。目前在大数据计算时代，AI是赤手可热的发展方向，机器学习是AI的重要手段之一，而恰恰由于语言的特点和生态的优势，Python成为了机器学习最重要的语言，进而表明了Apache Flink 着手发力 Flink API的动机！所谓时势造英雄，Apache Flink Python API是时代的产物，是水顺自然而得之的必然！最后再次强调本篇所提的 \"上善如水\"的为人之道值得我们以实际生活来慢慢体会！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n","source":"_posts/Apache Flink 说道系列- Python API 时代的产物.md","raw":"---\ntitle: Apache Flink 说道系列- Python API 时代的产物\ndate: 2019-07-21 23:05:15\ncategories: Apache Flink 说道\ntags: [Flink, 道德经,上善若水]\n---\n\n# 开篇说道-上善若水\n\n我们常说\"知识就是力量\", \"力量\"也意味着\"能力\"，\"能力\"在没有增加任何限定词语的时候往往是指一个人的综合素质。比如，当阅读本文时候你是想学习Flink Python API如果你很快就能普获道重点，这是学习能力！再如，在我写这篇博文的角度说，如果我写的内容晦涩难懂，那就体现了文字表达能力问题。所以\"能力\"是包含方方面面的综合素质的体现！那么本篇我们说道的内容就是当你具备了很高的\"能力\"之后，具体一点就是如果由于你的能力突出做了领导，你将如何处理和你能力相当或者能力不如你的人的关系呢？就此你思考10秒看看你脑子里的答案是什么？\n\n我想能阅读本篇文章的人都是一心向善，与人友善的好同学，但是在内心很清楚\"友善\"的原则并且在与人发生矛盾时候妥善解决(大多数谦让他人)的同时自己的内心也能做到愉悦和平静的同学请在本篇评论区留言:) \n\n回答刚才的问题: \"如何处理和你能力相当或者能力不如你的人的关系呢？\" 我的建议是：\"上善若水\"，出自老子的《道德经第八章》。老子用水来表达与人为事的自然之道。水至柔，她可以根据与之接触的各种物体的形状改变自己，她以各种形态存在，可以化作甘露滋润万物，可以凝聚江河载船运物！水至刚，她可以滴水穿石、无坚不摧！\"刚\"是一种能力，\"柔\"是一种态度，水有能力，但却始终 以一种谦和的低姿态，从高处流往低处！在《道德经》中老子更强调的是自然，任何事情符合自然规律！所谓\"无为\"并不是\"无作为\"而是\"不违背自然规律\"。所以遇到与人发生矛盾的时候，要心怀包容之心，水至柔可以容纳万物之形，人在有能力的时候也要容纳各色之人，容纳\"蛮不讲理\"之人，因为我们不能\"以蛮治蛮\",容纳\"污言秽语\"之人，因为\"身正影自直\"，容纳\"处处与你为敌\"之人，因为\"孤掌难鸣\"，你不理它，它自消去。容纳\"能弱气盛\"之人，因为他因\"能力不足而容易暴躁\"，只要你诚心助之，让他靠自己能力而得到认可，气自然消失，最终也会变得言和语悦！所以任何事情的发生都有气必然的道理，尊重事实，顺其自然，永远修行自身，位高而卑谦，**上善若水**，与世无争, 无争于世！\n\n![](B83130D4-9E1F-4BBC-88FA-2286951D18E5.png)\n\n# 为啥需要 Python API\n开篇\"上善若水\"的与人之道是我们正式介绍Python API之前的开胃菜。希望是真的开胃，你还有胃口继续读完全篇。那么Apache Flink 的runtime是用Java编写的，为啥还需要增加对Python的支持呢？也许大家都很清楚，目前很多著名的开源项目都支持Python，比如Beam，spark，kafka等，Flink自然也需要增加对Python的支持，这个角度分析也许很好，但我们想想为啥这些非常火热的项目都纷纷支持Python呢？Python语言有怎样的\"魔力\"让这些著名的项目都青睐于他？我们看一统计数据：\n\n## 最流行的编程语言\n我们看看行业分析公司[RedMonk](https://redmonk.com/sogrady/2019/07/18/language-rankings-6-19/)最新的最受欢迎的语言排名数据如下：\n![](DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png)\n上图从github和Stack Overflow两个维度进行分析的前十名如下：\n* JavaScript\n* Java\n* Python\n* PHP\n* C++\n* C#\n* CSS\n* Ruby\n* C\n* TypeScript\n\n我们发现Python排在了第3名，目前也非常流行的R和Go语言排在了15和16名。这个很客观的分享足以证明Python的受众是多么的庞大，任何项目对Python的支持就是在无形的扩大项目的受众用户！\n\n## 互联网最火热的领域\n就目前而言互联网最热的领域应该是大数据计算，以前单机的计算时代已经过去了，为啥单机时代过去了？因为单机处理能力的提高速度远远落后于数据的与日俱增的速度，以几个方面看看为啥目前处于大数据时代，而最炽热的互联网领域是大数据计算？\n\n### 为啥是大数据时代 - 数据量与日俱增\n\n随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下：\n\n![](9B952F03-53BF-43DF-8051-A2FAC84A600C.png)\n\n\n上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是：\n\n* 1 Byte =8 bit\n* 1 KB = 1,024 Bytes \n* 1 MB = 1,024 KB \n* 1 GB = 1,024 MB \n* 1 TB = 1,024 GB \n* 1 PB = 1,024 TB \n* 1 EB = 1,024 PB \n* 1 ZB = 1,024 EB \n* 1 YB = 1,024 ZB \n* 1 BB = 1,024 YB \n* 1 NB = 1,024 BB \n* 1 DB = 1,024 NB \n\n看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下：\n\n![](1AF4C656-6ABA-4F69-A771-E6EC623B382D.png)\n\n### 数据价值的产生 - 数据分析\n\n我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文：\n\n* GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。\n* MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示：\n\n![](B31690BA-A8E2-4C22-A948-8735728542B0.png)\n\n* BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。\n\n受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下：\n\n![](1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png)\n\n但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及在业界家喻户晓！\n\n### 数据价值最大化 - 时效性\n每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目于2010年开始于德国柏林。\n\n## AlphaGo - 人工智能\n在2016谷歌AlphaGo以4:1战胜围棋世界冠军、职业九段棋手李世石之后，人们逐渐用新的眼光审视让深度学习，而且掀起了人工智能的“狂热”。百度百科对人工智能(Artificial Intelligence)，英文缩写为AI的定义是: 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新技术科学。\n\n![](6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png)\n\n而机器学习是进行人工智能的一种方法或者工具。机器学习在以Spark，Flink为首的大数据技术平台中具有较高的地位，尤其Spark近些年在ML方面做了巨大的努力，同时PySpark集成了很多优秀的机器学习类库，如典型的Pandas，在这方面远远超过了Flink，所以Flink正面面对自己的不足，在Flink1.9中开启了新的ML接口和新的flink-python模块！\n\n那么机器学习的重要和Python又有什么关系呢？我们一下统计数据，看什么语言是最流行的机器学习语言？\nIBM 的数据科学家 Jean-Francois Puget 曾经做过一个有趣的分析。他爬取了著名的求职网站 indeed 上雇主的岗位要求变动趋势，来评估当下市场上最受欢迎的岗位语言。其中当他单独搜索\"machine learning\"时候，也可以得到一个近似的结果:\n![](070B3285-C790-4D6F-BB9B-6981756255CA.jpg)\n其结构发现Python是与热的\"machine learning\"，虽然这是2016年的调查，但是也足以证明Python在\"machine learning\"方面的地位，同时上面我们提到的redmonk的统计数据也足以证明这一点！\n\n不仅仅是各种调查，我们也可以从Python的特点和现有的Python生态来说说为什么Python是机器学习的最好语言。\n\nPython 是一种面向对象的解释型程序语言，由荷兰人(Guido van Rossum)于1989年发明，并于1991年发布了第一个版。Python 作为解释型语言，虽然跑得比谁都慢，但Python设计者的哲学是\"用一种方法并且只有一种方法来做一件事\"。在开发新的Python语法时，如果面临多种选择，Python开发者一般会选择明确的没有或者很少有歧义的语法。简单易学的特点促使了Python有庞大的用户群体，进而很多机器学习的类库也是由Python开发的，比如：NumPy、SciPy和结构化数据操作可以通过Pandas等等。所以Python这种丰富的生态系统为机器学习提供了一定程度的便利性，也必然成为了最受欢迎的机器学习语言！\n\n# 小结\n本篇重点描述了Apache Flink 为啥需要支持Python API。以实际数字说明目前我们处于一个大数据时代，数据的价值要依靠大数据分析，而由于数据的时效性的重要性而催生了著名的Apache Flink流式计算平台。目前在大数据计算时代，AI是赤手可热的发展方向，机器学习是AI的重要手段之一，而恰恰由于语言的特点和生态的优势，Python成为了机器学习最重要的语言，进而表明了Apache Flink 着手发力 Flink API的动机！所谓时势造英雄，Apache Flink Python API是时代的产物，是水顺自然而得之的必然！最后再次强调本篇所提的 \"上善如水\"的为人之道值得我们以实际生活来慢慢体会！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n","slug":"Apache Flink 说道系列- Python API 时代的产物","published":1,"updated":"2019-08-02T09:48:02.465Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5ge001fs94hv6z984rl","content":"<h1>开篇说道-上善若水</h1>\n<p>我们常说&quot;知识就是力量&quot;, &quot;力量&quot;也意味着&quot;能力&quot;，&quot;能力&quot;在没有增加任何限定词语的时候往往是指一个人的综合素质。比如，当阅读本文时候你是想学习Flink Python API如果你很快就能普获道重点，这是学习能力！再如，在我写这篇博文的角度说，如果我写的内容晦涩难懂，那就体现了文字表达能力问题。所以&quot;能力&quot;是包含方方面面的综合素质的体现！那么本篇我们说道的内容就是当你具备了很高的&quot;能力&quot;之后，具体一点就是如果由于你的能力突出做了领导，你将如何处理和你能力相当或者能力不如你的人的关系呢？就此你思考10秒看看你脑子里的答案是什么？</p>\n<p>我想能阅读本篇文章的人都是一心向善，与人友善的好同学，但是在内心很清楚&quot;友善&quot;的原则并且在与人发生矛盾时候妥善解决(大多数谦让他人)的同时自己的内心也能做到愉悦和平静的同学请在本篇评论区留言:)</p>\n<p>回答刚才的问题: &quot;如何处理和你能力相当或者能力不如你的人的关系呢？&quot; 我的建议是：&quot;上善若水&quot;，出自老子的《道德经第八章》。老子用水来表达与人为事的自然之道。水至柔，她可以根据与之接触的各种物体的形状改变自己，她以各种形态存在，可以化作甘露滋润万物，可以凝聚江河载船运物！水至刚，她可以滴水穿石、无坚不摧！&quot;刚&quot;是一种能力，&quot;柔&quot;是一种态度，水有能力，但却始终 以一种谦和的低姿态，从高处流往低处！在《道德经》中老子更强调的是自然，任何事情符合自然规律！所谓&quot;无为&quot;并不是&quot;无作为&quot;而是&quot;不违背自然规律&quot;。所以遇到与人发生矛盾的时候，要心怀包容之心，水至柔可以容纳万物之形，人在有能力的时候也要容纳各色之人，容纳&quot;蛮不讲理&quot;之人，因为我们不能&quot;以蛮治蛮&quot;,容纳&quot;污言秽语&quot;之人，因为&quot;身正影自直&quot;，容纳&quot;处处与你为敌&quot;之人，因为&quot;孤掌难鸣&quot;，你不理它，它自消去。容纳&quot;能弱气盛&quot;之人，因为他因&quot;能力不足而容易暴躁&quot;，只要你诚心助之，让他靠自己能力而得到认可，气自然消失，最终也会变得言和语悦！所以任何事情的发生都有气必然的道理，尊重事实，顺其自然，永远修行自身，位高而卑谦，<strong>上善若水</strong>，与世无争, 无争于世！</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/B83130D4-9E1F-4BBC-88FA-2286951D18E5.png\" alt></p>\n<h1>为啥需要 Python API</h1>\n<p>开篇&quot;上善若水&quot;的与人之道是我们正式介绍Python API之前的开胃菜。希望是真的开胃，你还有胃口继续读完全篇。那么Apache Flink 的runtime是用Java编写的，为啥还需要增加对Python的支持呢？也许大家都很清楚，目前很多著名的开源项目都支持Python，比如Beam，spark，kafka等，Flink自然也需要增加对Python的支持，这个角度分析也许很好，但我们想想为啥这些非常火热的项目都纷纷支持Python呢？Python语言有怎样的&quot;魔力&quot;让这些著名的项目都青睐于他？我们看一统计数据：</p>\n<h2>最流行的编程语言</h2>\n<p>我们看看行业分析公司<a href=\"https://redmonk.com/sogrady/2019/07/18/language-rankings-6-19/\" target=\"_blank\" rel=\"noopener\">RedMonk</a>最新的最受欢迎的语言排名数据如下：\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png\" alt>\n上图从github和Stack Overflow两个维度进行分析的前十名如下：</p>\n<ul>\n<li>JavaScript</li>\n<li>Java</li>\n<li>Python</li>\n<li>PHP</li>\n<li>C++</li>\n<li>C#</li>\n<li>CSS</li>\n<li>Ruby</li>\n<li>C</li>\n<li>TypeScript</li>\n</ul>\n<p>我们发现Python排在了第3名，目前也非常流行的R和Go语言排在了15和16名。这个很客观的分享足以证明Python的受众是多么的庞大，任何项目对Python的支持就是在无形的扩大项目的受众用户！</p>\n<h2>互联网最火热的领域</h2>\n<p>就目前而言互联网最热的领域应该是大数据计算，以前单机的计算时代已经过去了，为啥单机时代过去了？因为单机处理能力的提高速度远远落后于数据的与日俱增的速度，以几个方面看看为啥目前处于大数据时代，而最炽热的互联网领域是大数据计算？</p>\n<h3>为啥是大数据时代 - 数据量与日俱增</h3>\n<p>随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/9B952F03-53BF-43DF-8051-A2FAC84A600C.png\" alt></p>\n<p>上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是：</p>\n<ul>\n<li>1 Byte =8 bit</li>\n<li>1 KB = 1,024 Bytes</li>\n<li>1 MB = 1,024 KB</li>\n<li>1 GB = 1,024 MB</li>\n<li>1 TB = 1,024 GB</li>\n<li>1 PB = 1,024 TB</li>\n<li>1 EB = 1,024 PB</li>\n<li>1 ZB = 1,024 EB</li>\n<li>1 YB = 1,024 ZB</li>\n<li>1 BB = 1,024 YB</li>\n<li>1 NB = 1,024 BB</li>\n<li>1 DB = 1,024 NB</li>\n</ul>\n<p>看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/1AF4C656-6ABA-4F69-A771-E6EC623B382D.png\" alt></p>\n<h3>数据价值的产生 - 数据分析</h3>\n<p>我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文：</p>\n<ul>\n<li>GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。</li>\n<li>MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示：</li>\n</ul>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/B31690BA-A8E2-4C22-A948-8735728542B0.png\" alt></p>\n<ul>\n<li>BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。</li>\n</ul>\n<p>受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png\" alt></p>\n<p>但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及在业界家喻户晓！</p>\n<h3>数据价值最大化 - 时效性</h3>\n<p>每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目于2010年开始于德国柏林。</p>\n<h2>AlphaGo - 人工智能</h2>\n<p>在2016谷歌AlphaGo以4:1战胜围棋世界冠军、职业九段棋手李世石之后，人们逐渐用新的眼光审视让深度学习，而且掀起了人工智能的“狂热”。百度百科对人工智能(Artificial Intelligence)，英文缩写为AI的定义是: 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新技术科学。</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png\" alt></p>\n<p>而机器学习是进行人工智能的一种方法或者工具。机器学习在以Spark，Flink为首的大数据技术平台中具有较高的地位，尤其Spark近些年在ML方面做了巨大的努力，同时PySpark集成了很多优秀的机器学习类库，如典型的Pandas，在这方面远远超过了Flink，所以Flink正面面对自己的不足，在Flink1.9中开启了新的ML接口和新的flink-python模块！</p>\n<p>那么机器学习的重要和Python又有什么关系呢？我们一下统计数据，看什么语言是最流行的机器学习语言？\nIBM 的数据科学家 Jean-Francois Puget 曾经做过一个有趣的分析。他爬取了著名的求职网站 indeed 上雇主的岗位要求变动趋势，来评估当下市场上最受欢迎的岗位语言。其中当他单独搜索&quot;machine learning&quot;时候，也可以得到一个近似的结果:\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/070B3285-C790-4D6F-BB9B-6981756255CA.jpg\" alt>\n其结构发现Python是与热的&quot;machine learning&quot;，虽然这是2016年的调查，但是也足以证明Python在&quot;machine learning&quot;方面的地位，同时上面我们提到的redmonk的统计数据也足以证明这一点！</p>\n<p>不仅仅是各种调查，我们也可以从Python的特点和现有的Python生态来说说为什么Python是机器学习的最好语言。</p>\n<p>Python 是一种面向对象的解释型程序语言，由荷兰人(Guido van Rossum)于1989年发明，并于1991年发布了第一个版。Python 作为解释型语言，虽然跑得比谁都慢，但Python设计者的哲学是&quot;用一种方法并且只有一种方法来做一件事&quot;。在开发新的Python语法时，如果面临多种选择，Python开发者一般会选择明确的没有或者很少有歧义的语法。简单易学的特点促使了Python有庞大的用户群体，进而很多机器学习的类库也是由Python开发的，比如：NumPy、SciPy和结构化数据操作可以通过Pandas等等。所以Python这种丰富的生态系统为机器学习提供了一定程度的便利性，也必然成为了最受欢迎的机器学习语言！</p>\n<h1>小结</h1>\n<p>本篇重点描述了Apache Flink 为啥需要支持Python API。以实际数字说明目前我们处于一个大数据时代，数据的价值要依靠大数据分析，而由于数据的时效性的重要性而催生了著名的Apache Flink流式计算平台。目前在大数据计算时代，AI是赤手可热的发展方向，机器学习是AI的重要手段之一，而恰恰由于语言的特点和生态的优势，Python成为了机器学习最重要的语言，进而表明了Apache Flink 着手发力 Flink API的动机！所谓时势造英雄，Apache Flink Python API是时代的产物，是水顺自然而得之的必然！最后再次强调本篇所提的 &quot;上善如水&quot;的为人之道值得我们以实际生活来慢慢体会！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>开篇说道-上善若水</h1>\n<p>我们常说&quot;知识就是力量&quot;, &quot;力量&quot;也意味着&quot;能力&quot;，&quot;能力&quot;在没有增加任何限定词语的时候往往是指一个人的综合素质。比如，当阅读本文时候你是想学习Flink Python API如果你很快就能普获道重点，这是学习能力！再如，在我写这篇博文的角度说，如果我写的内容晦涩难懂，那就体现了文字表达能力问题。所以&quot;能力&quot;是包含方方面面的综合素质的体现！那么本篇我们说道的内容就是当你具备了很高的&quot;能力&quot;之后，具体一点就是如果由于你的能力突出做了领导，你将如何处理和你能力相当或者能力不如你的人的关系呢？就此你思考10秒看看你脑子里的答案是什么？</p>\n<p>我想能阅读本篇文章的人都是一心向善，与人友善的好同学，但是在内心很清楚&quot;友善&quot;的原则并且在与人发生矛盾时候妥善解决(大多数谦让他人)的同时自己的内心也能做到愉悦和平静的同学请在本篇评论区留言:)</p>\n<p>回答刚才的问题: &quot;如何处理和你能力相当或者能力不如你的人的关系呢？&quot; 我的建议是：&quot;上善若水&quot;，出自老子的《道德经第八章》。老子用水来表达与人为事的自然之道。水至柔，她可以根据与之接触的各种物体的形状改变自己，她以各种形态存在，可以化作甘露滋润万物，可以凝聚江河载船运物！水至刚，她可以滴水穿石、无坚不摧！&quot;刚&quot;是一种能力，&quot;柔&quot;是一种态度，水有能力，但却始终 以一种谦和的低姿态，从高处流往低处！在《道德经》中老子更强调的是自然，任何事情符合自然规律！所谓&quot;无为&quot;并不是&quot;无作为&quot;而是&quot;不违背自然规律&quot;。所以遇到与人发生矛盾的时候，要心怀包容之心，水至柔可以容纳万物之形，人在有能力的时候也要容纳各色之人，容纳&quot;蛮不讲理&quot;之人，因为我们不能&quot;以蛮治蛮&quot;,容纳&quot;污言秽语&quot;之人，因为&quot;身正影自直&quot;，容纳&quot;处处与你为敌&quot;之人，因为&quot;孤掌难鸣&quot;，你不理它，它自消去。容纳&quot;能弱气盛&quot;之人，因为他因&quot;能力不足而容易暴躁&quot;，只要你诚心助之，让他靠自己能力而得到认可，气自然消失，最终也会变得言和语悦！所以任何事情的发生都有气必然的道理，尊重事实，顺其自然，永远修行自身，位高而卑谦，<strong>上善若水</strong>，与世无争, 无争于世！</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/B83130D4-9E1F-4BBC-88FA-2286951D18E5.png\" alt></p>\n<h1>为啥需要 Python API</h1>\n<p>开篇&quot;上善若水&quot;的与人之道是我们正式介绍Python API之前的开胃菜。希望是真的开胃，你还有胃口继续读完全篇。那么Apache Flink 的runtime是用Java编写的，为啥还需要增加对Python的支持呢？也许大家都很清楚，目前很多著名的开源项目都支持Python，比如Beam，spark，kafka等，Flink自然也需要增加对Python的支持，这个角度分析也许很好，但我们想想为啥这些非常火热的项目都纷纷支持Python呢？Python语言有怎样的&quot;魔力&quot;让这些著名的项目都青睐于他？我们看一统计数据：</p>\n<h2>最流行的编程语言</h2>\n<p>我们看看行业分析公司<a href=\"https://redmonk.com/sogrady/2019/07/18/language-rankings-6-19/\" target=\"_blank\" rel=\"noopener\">RedMonk</a>最新的最受欢迎的语言排名数据如下：\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png\" alt>\n上图从github和Stack Overflow两个维度进行分析的前十名如下：</p>\n<ul>\n<li>JavaScript</li>\n<li>Java</li>\n<li>Python</li>\n<li>PHP</li>\n<li>C++</li>\n<li>C#</li>\n<li>CSS</li>\n<li>Ruby</li>\n<li>C</li>\n<li>TypeScript</li>\n</ul>\n<p>我们发现Python排在了第3名，目前也非常流行的R和Go语言排在了15和16名。这个很客观的分享足以证明Python的受众是多么的庞大，任何项目对Python的支持就是在无形的扩大项目的受众用户！</p>\n<h2>互联网最火热的领域</h2>\n<p>就目前而言互联网最热的领域应该是大数据计算，以前单机的计算时代已经过去了，为啥单机时代过去了？因为单机处理能力的提高速度远远落后于数据的与日俱增的速度，以几个方面看看为啥目前处于大数据时代，而最炽热的互联网领域是大数据计算？</p>\n<h3>为啥是大数据时代 - 数据量与日俱增</h3>\n<p>随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/9B952F03-53BF-43DF-8051-A2FAC84A600C.png\" alt></p>\n<p>上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是：</p>\n<ul>\n<li>1 Byte =8 bit</li>\n<li>1 KB = 1,024 Bytes</li>\n<li>1 MB = 1,024 KB</li>\n<li>1 GB = 1,024 MB</li>\n<li>1 TB = 1,024 GB</li>\n<li>1 PB = 1,024 TB</li>\n<li>1 EB = 1,024 PB</li>\n<li>1 ZB = 1,024 EB</li>\n<li>1 YB = 1,024 ZB</li>\n<li>1 BB = 1,024 YB</li>\n<li>1 NB = 1,024 BB</li>\n<li>1 DB = 1,024 NB</li>\n</ul>\n<p>看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/1AF4C656-6ABA-4F69-A771-E6EC623B382D.png\" alt></p>\n<h3>数据价值的产生 - 数据分析</h3>\n<p>我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文：</p>\n<ul>\n<li>GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。</li>\n<li>MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示：</li>\n</ul>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/B31690BA-A8E2-4C22-A948-8735728542B0.png\" alt></p>\n<ul>\n<li>BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。</li>\n</ul>\n<p>受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下：</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png\" alt></p>\n<p>但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及在业界家喻户晓！</p>\n<h3>数据价值最大化 - 时效性</h3>\n<p>每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目于2010年开始于德国柏林。</p>\n<h2>AlphaGo - 人工智能</h2>\n<p>在2016谷歌AlphaGo以4:1战胜围棋世界冠军、职业九段棋手李世石之后，人们逐渐用新的眼光审视让深度学习，而且掀起了人工智能的“狂热”。百度百科对人工智能(Artificial Intelligence)，英文缩写为AI的定义是: 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新技术科学。</p>\n<p><img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png\" alt></p>\n<p>而机器学习是进行人工智能的一种方法或者工具。机器学习在以Spark，Flink为首的大数据技术平台中具有较高的地位，尤其Spark近些年在ML方面做了巨大的努力，同时PySpark集成了很多优秀的机器学习类库，如典型的Pandas，在这方面远远超过了Flink，所以Flink正面面对自己的不足，在Flink1.9中开启了新的ML接口和新的flink-python模块！</p>\n<p>那么机器学习的重要和Python又有什么关系呢？我们一下统计数据，看什么语言是最流行的机器学习语言？\nIBM 的数据科学家 Jean-Francois Puget 曾经做过一个有趣的分析。他爬取了著名的求职网站 indeed 上雇主的岗位要求变动趋势，来评估当下市场上最受欢迎的岗位语言。其中当他单独搜索&quot;machine learning&quot;时候，也可以得到一个近似的结果:\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/070B3285-C790-4D6F-BB9B-6981756255CA.jpg\" alt>\n其结构发现Python是与热的&quot;machine learning&quot;，虽然这是2016年的调查，但是也足以证明Python在&quot;machine learning&quot;方面的地位，同时上面我们提到的redmonk的统计数据也足以证明这一点！</p>\n<p>不仅仅是各种调查，我们也可以从Python的特点和现有的Python生态来说说为什么Python是机器学习的最好语言。</p>\n<p>Python 是一种面向对象的解释型程序语言，由荷兰人(Guido van Rossum)于1989年发明，并于1991年发布了第一个版。Python 作为解释型语言，虽然跑得比谁都慢，但Python设计者的哲学是&quot;用一种方法并且只有一种方法来做一件事&quot;。在开发新的Python语法时，如果面临多种选择，Python开发者一般会选择明确的没有或者很少有歧义的语法。简单易学的特点促使了Python有庞大的用户群体，进而很多机器学习的类库也是由Python开发的，比如：NumPy、SciPy和结构化数据操作可以通过Pandas等等。所以Python这种丰富的生态系统为机器学习提供了一定程度的便利性，也必然成为了最受欢迎的机器学习语言！</p>\n<h1>小结</h1>\n<p>本篇重点描述了Apache Flink 为啥需要支持Python API。以实际数字说明目前我们处于一个大数据时代，数据的价值要依靠大数据分析，而由于数据的时效性的重要性而催生了著名的Apache Flink流式计算平台。目前在大数据计算时代，AI是赤手可热的发展方向，机器学习是AI的重要手段之一，而恰恰由于语言的特点和生态的优势，Python成为了机器学习最重要的语言，进而表明了Apache Flink 着手发力 Flink API的动机！所谓时势造英雄，Apache Flink Python API是时代的产物，是水顺自然而得之的必然！最后再次强调本篇所提的 &quot;上善如水&quot;的为人之道值得我们以实际生活来慢慢体会！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/07/21/Apache Flink 说道系列- Python API 时代的产物/comment.png\" alt></p>\n"},{"title":"Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业","_content":"\n\n# 开篇说道\n《老子·第九章》：\"持而盈之，不如其已；揣而锐之，不可长保。金玉满堂，莫之能守；富贵而骄，自遗其咎。功成身退，天之道也\" 在我看来老子说的这些内容，是告诉人们做事不能过于激进，不能贪得无厌，不能因为自己的得势就蛮横无理，任何事情要懂得知足常乐，懂得低调谦让，所谓谦受益满招损，过犹不及，物极必反！\n\n如果你读了本篇文章能够有意识的今后不与人争长论短，那么将是极大的收获。如果你想听争长论短的坏处请评论区留言！\n\n# 问题描述\nApache Flink 1.9版本版本中增加了Python Table API的支持，同时大家可以在Python API中使用Java的UDFs，但是这里有个问题在我的[直播](http://1t.click/BQx)中提到：目前官方没有提供如何在IDE中运行使用Java UDFs的 Python 作业的方式，原因是没有很好的机制将 Java UDFs的JAR集成到Python的执行环境中。\n\n# 问题现象\n如果大家在IDE中执行使用Java UDFs的Python作业会有怎样的现象呢？我还是以我直播中的代码为例:\n```\nt_env.register_java_function(\"len\", \"org.apache.flink.udf.UDFLength\")\n    elements = [(word, 1) for word in content.split(\" \")]\n    t_env.from_elements(elements, [\"word\", \"count\"]) \\\n         .group_by(\"word\") \\\n         .select(\"word, len(word), count(1) as count\") \\\n         .insert_into(\"Results\")\n```\n上面我注册了一个Java UDF `org.apache.flink.udf.UDFLength`。然后在select中使用`len(word)`,如果我们直接执行上面代码，会有如下错误：\n\n![](52A99F20-9648-4AB7-BA20-AA890797A9F0.png)\n上面提示信息很明显，是找不到`org.apache.flink.udf.UDFLength`.\n\n# 问题分析\n上面的问题大家想必都清楚，就是`org.apache.flink.udf.UDFLength`没有在Classpath下面，所以只要我们想办法将`org.apache.flink.udf.UDFLength`所在的JAR添加到classpath就行可以。听了我直播分享的同学应该知道Apache Flink 1.9 Python API的架构，在执行Python的时候本质上是调用Java的API，Apache Flink Python API是如何在运行的时候将Flink Java API的JAR添加到Classpath下面的呢？\n我们运行`python setup.py sdist` 之后，在生产的发布文件中，我们可以运行`tar -tvf dist/apache-flink-1.9.dev0.tar.gz |grep jar   `来查看Flink的Java JARs存放位置，如下图：\n\n![](6F22FA6A-2892-4137-BA10-6D643C60DBEE.png)\n\n我们发现只要在`deps/lib`目录下面的JARs在运行Python时候都是在Classpath下面的。所以我们只要想办法将我们自己定义的UDF放到deps/lib目录下就可以了。那么怎样才能将我们的JAR放到Classpath下面呢？\n## 解决方案\n如果你在安装pyflink之前可以将你需要的JAR放到 `build-target/lib`下面，然后执行`python setup.py sdist`进行打包和进行安装`pip install dist/*.tar.gz`这样你的的UDF的JAR也会在Classpath下面。\n\n如果你已经安装好了，你也可以直接将你的JAR包拷贝到pip安装的目录，比如：\n\n![](9C73FBBB-80AD-45DA-91E5-21436A7811C2.png)\n\n我们可以直接将需要的JARs拷贝到你所使用的phython环境的`$PY_HOME/site-packages/pyflink/lib/`目录下。\n\n# 未来规划\n在Apache Flink 1.10版本我们会将这个过程自动化，使用一种比较方便的方式优化用户的使用体验！\n\n# 小结\n本篇有针对性的为大家讲解了如何在IDE中运行使用Java UDF的 Python 作业。并且 开篇说道 部分为大家分享了老子\"持而盈之，不如其已\"经典语句，并建议大家今后有意识的不要与人争长论短，谨记 谦受益满招损，努力做到谦逊低调，营造欢乐的交际氛围，最后祝福大家在学习Apache Flink Python API的同时，生活工作开心快乐！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n","source":"_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业.md","raw":"---\ntitle: Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业\ncategories: Apache Flink 说道\ntags: [Flink, 道德经, 持而盈之-不如其已]\n---\n\n\n# 开篇说道\n《老子·第九章》：\"持而盈之，不如其已；揣而锐之，不可长保。金玉满堂，莫之能守；富贵而骄，自遗其咎。功成身退，天之道也\" 在我看来老子说的这些内容，是告诉人们做事不能过于激进，不能贪得无厌，不能因为自己的得势就蛮横无理，任何事情要懂得知足常乐，懂得低调谦让，所谓谦受益满招损，过犹不及，物极必反！\n\n如果你读了本篇文章能够有意识的今后不与人争长论短，那么将是极大的收获。如果你想听争长论短的坏处请评论区留言！\n\n# 问题描述\nApache Flink 1.9版本版本中增加了Python Table API的支持，同时大家可以在Python API中使用Java的UDFs，但是这里有个问题在我的[直播](http://1t.click/BQx)中提到：目前官方没有提供如何在IDE中运行使用Java UDFs的 Python 作业的方式，原因是没有很好的机制将 Java UDFs的JAR集成到Python的执行环境中。\n\n# 问题现象\n如果大家在IDE中执行使用Java UDFs的Python作业会有怎样的现象呢？我还是以我直播中的代码为例:\n```\nt_env.register_java_function(\"len\", \"org.apache.flink.udf.UDFLength\")\n    elements = [(word, 1) for word in content.split(\" \")]\n    t_env.from_elements(elements, [\"word\", \"count\"]) \\\n         .group_by(\"word\") \\\n         .select(\"word, len(word), count(1) as count\") \\\n         .insert_into(\"Results\")\n```\n上面我注册了一个Java UDF `org.apache.flink.udf.UDFLength`。然后在select中使用`len(word)`,如果我们直接执行上面代码，会有如下错误：\n\n![](52A99F20-9648-4AB7-BA20-AA890797A9F0.png)\n上面提示信息很明显，是找不到`org.apache.flink.udf.UDFLength`.\n\n# 问题分析\n上面的问题大家想必都清楚，就是`org.apache.flink.udf.UDFLength`没有在Classpath下面，所以只要我们想办法将`org.apache.flink.udf.UDFLength`所在的JAR添加到classpath就行可以。听了我直播分享的同学应该知道Apache Flink 1.9 Python API的架构，在执行Python的时候本质上是调用Java的API，Apache Flink Python API是如何在运行的时候将Flink Java API的JAR添加到Classpath下面的呢？\n我们运行`python setup.py sdist` 之后，在生产的发布文件中，我们可以运行`tar -tvf dist/apache-flink-1.9.dev0.tar.gz |grep jar   `来查看Flink的Java JARs存放位置，如下图：\n\n![](6F22FA6A-2892-4137-BA10-6D643C60DBEE.png)\n\n我们发现只要在`deps/lib`目录下面的JARs在运行Python时候都是在Classpath下面的。所以我们只要想办法将我们自己定义的UDF放到deps/lib目录下就可以了。那么怎样才能将我们的JAR放到Classpath下面呢？\n## 解决方案\n如果你在安装pyflink之前可以将你需要的JAR放到 `build-target/lib`下面，然后执行`python setup.py sdist`进行打包和进行安装`pip install dist/*.tar.gz`这样你的的UDF的JAR也会在Classpath下面。\n\n如果你已经安装好了，你也可以直接将你的JAR包拷贝到pip安装的目录，比如：\n\n![](9C73FBBB-80AD-45DA-91E5-21436A7811C2.png)\n\n我们可以直接将需要的JARs拷贝到你所使用的phython环境的`$PY_HOME/site-packages/pyflink/lib/`目录下。\n\n# 未来规划\n在Apache Flink 1.10版本我们会将这个过程自动化，使用一种比较方便的方式优化用户的使用体验！\n\n# 小结\n本篇有针对性的为大家讲解了如何在IDE中运行使用Java UDF的 Python 作业。并且 开篇说道 部分为大家分享了老子\"持而盈之，不如其已\"经典语句，并建议大家今后有意识的不要与人争长论短，谨记 谦受益满招损，努力做到谦逊低调，营造欢乐的交际氛围，最后祝福大家在学习Apache Flink Python API的同时，生活工作开心快乐！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n","slug":"Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业","published":1,"date":"2019-08-11T12:32:25.148Z","updated":"2019-08-11T12:49:31.733Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5gj001ks94h7ulryqho","content":"<h1>开篇说道</h1>\n<p>《老子·第九章》：&quot;持而盈之，不如其已；揣而锐之，不可长保。金玉满堂，莫之能守；富贵而骄，自遗其咎。功成身退，天之道也&quot; 在我看来老子说的这些内容，是告诉人们做事不能过于激进，不能贪得无厌，不能因为自己的得势就蛮横无理，任何事情要懂得知足常乐，懂得低调谦让，所谓谦受益满招损，过犹不及，物极必反！</p>\n<p>如果你读了本篇文章能够有意识的今后不与人争长论短，那么将是极大的收获。如果你想听争长论短的坏处请评论区留言！</p>\n<h1>问题描述</h1>\n<p>Apache Flink 1.9版本版本中增加了Python Table API的支持，同时大家可以在Python API中使用Java的UDFs，但是这里有个问题在我的<a href=\"http://1t.click/BQx\" target=\"_blank\" rel=\"noopener\">直播</a>中提到：目前官方没有提供如何在IDE中运行使用Java UDFs的 Python 作业的方式，原因是没有很好的机制将 Java UDFs的JAR集成到Python的执行环境中。</p>\n<h1>问题现象</h1>\n<p>如果大家在IDE中执行使用Java UDFs的Python作业会有怎样的现象呢？我还是以我直播中的代码为例:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.register_java_function(&quot;len&quot;, &quot;org.apache.flink.udf.UDFLength&quot;)</span><br><span class=\"line\">    elements = [(word, 1) for word in content.split(&quot; &quot;)]</span><br><span class=\"line\">    t_env.from_elements(elements, [&quot;word&quot;, &quot;count&quot;]) \\</span><br><span class=\"line\">         .group_by(&quot;word&quot;) \\</span><br><span class=\"line\">         .select(&quot;word, len(word), count(1) as count&quot;) \\</span><br><span class=\"line\">         .insert_into(&quot;Results&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>上面我注册了一个Java UDF <code>org.apache.flink.udf.UDFLength</code>。然后在select中使用<code>len(word)</code>,如果我们直接执行上面代码，会有如下错误：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/52A99F20-9648-4AB7-BA20-AA890797A9F0.png\" alt>\n上面提示信息很明显，是找不到<code>org.apache.flink.udf.UDFLength</code>.</p>\n<h1>问题分析</h1>\n<p>上面的问题大家想必都清楚，就是<code>org.apache.flink.udf.UDFLength</code>没有在Classpath下面，所以只要我们想办法将<code>org.apache.flink.udf.UDFLength</code>所在的JAR添加到classpath就行可以。听了我直播分享的同学应该知道Apache Flink 1.9 Python API的架构，在执行Python的时候本质上是调用Java的API，Apache Flink Python API是如何在运行的时候将Flink Java API的JAR添加到Classpath下面的呢？\n我们运行<code>python setup.py sdist</code> 之后，在生产的发布文件中，我们可以运行<code>tar -tvf dist/apache-flink-1.9.dev0.tar.gz |grep jar</code>来查看Flink的Java JARs存放位置，如下图：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/6F22FA6A-2892-4137-BA10-6D643C60DBEE.png\" alt></p>\n<p>我们发现只要在<code>deps/lib</code>目录下面的JARs在运行Python时候都是在Classpath下面的。所以我们只要想办法将我们自己定义的UDF放到deps/lib目录下就可以了。那么怎样才能将我们的JAR放到Classpath下面呢？</p>\n<h2>解决方案</h2>\n<p>如果你在安装pyflink之前可以将你需要的JAR放到 <code>build-target/lib</code>下面，然后执行<code>python setup.py sdist</code>进行打包和进行安装<code>pip install dist/*.tar.gz</code>这样你的的UDF的JAR也会在Classpath下面。</p>\n<p>如果你已经安装好了，你也可以直接将你的JAR包拷贝到pip安装的目录，比如：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/9C73FBBB-80AD-45DA-91E5-21436A7811C2.png\" alt></p>\n<p>我们可以直接将需要的JARs拷贝到你所使用的phython环境的<code>$PY_HOME/site-packages/pyflink/lib/</code>目录下。</p>\n<h1>未来规划</h1>\n<p>在Apache Flink 1.10版本我们会将这个过程自动化，使用一种比较方便的方式优化用户的使用体验！</p>\n<h1>小结</h1>\n<p>本篇有针对性的为大家讲解了如何在IDE中运行使用Java UDF的 Python 作业。并且 开篇说道 部分为大家分享了老子&quot;持而盈之，不如其已&quot;经典语句，并建议大家今后有意识的不要与人争长论短，谨记 谦受益满招损，努力做到谦逊低调，营造欢乐的交际氛围，最后祝福大家在学习Apache Flink Python API的同时，生活工作开心快乐！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>开篇说道</h1>\n<p>《老子·第九章》：&quot;持而盈之，不如其已；揣而锐之，不可长保。金玉满堂，莫之能守；富贵而骄，自遗其咎。功成身退，天之道也&quot; 在我看来老子说的这些内容，是告诉人们做事不能过于激进，不能贪得无厌，不能因为自己的得势就蛮横无理，任何事情要懂得知足常乐，懂得低调谦让，所谓谦受益满招损，过犹不及，物极必反！</p>\n<p>如果你读了本篇文章能够有意识的今后不与人争长论短，那么将是极大的收获。如果你想听争长论短的坏处请评论区留言！</p>\n<h1>问题描述</h1>\n<p>Apache Flink 1.9版本版本中增加了Python Table API的支持，同时大家可以在Python API中使用Java的UDFs，但是这里有个问题在我的<a href=\"http://1t.click/BQx\" target=\"_blank\" rel=\"noopener\">直播</a>中提到：目前官方没有提供如何在IDE中运行使用Java UDFs的 Python 作业的方式，原因是没有很好的机制将 Java UDFs的JAR集成到Python的执行环境中。</p>\n<h1>问题现象</h1>\n<p>如果大家在IDE中执行使用Java UDFs的Python作业会有怎样的现象呢？我还是以我直播中的代码为例:\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t_env.register_java_function(&quot;len&quot;, &quot;org.apache.flink.udf.UDFLength&quot;)</span><br><span class=\"line\">    elements = [(word, 1) for word in content.split(&quot; &quot;)]</span><br><span class=\"line\">    t_env.from_elements(elements, [&quot;word&quot;, &quot;count&quot;]) \\</span><br><span class=\"line\">         .group_by(&quot;word&quot;) \\</span><br><span class=\"line\">         .select(&quot;word, len(word), count(1) as count&quot;) \\</span><br><span class=\"line\">         .insert_into(&quot;Results&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>上面我注册了一个Java UDF <code>org.apache.flink.udf.UDFLength</code>。然后在select中使用<code>len(word)</code>,如果我们直接执行上面代码，会有如下错误：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/52A99F20-9648-4AB7-BA20-AA890797A9F0.png\" alt>\n上面提示信息很明显，是找不到<code>org.apache.flink.udf.UDFLength</code>.</p>\n<h1>问题分析</h1>\n<p>上面的问题大家想必都清楚，就是<code>org.apache.flink.udf.UDFLength</code>没有在Classpath下面，所以只要我们想办法将<code>org.apache.flink.udf.UDFLength</code>所在的JAR添加到classpath就行可以。听了我直播分享的同学应该知道Apache Flink 1.9 Python API的架构，在执行Python的时候本质上是调用Java的API，Apache Flink Python API是如何在运行的时候将Flink Java API的JAR添加到Classpath下面的呢？\n我们运行<code>python setup.py sdist</code> 之后，在生产的发布文件中，我们可以运行<code>tar -tvf dist/apache-flink-1.9.dev0.tar.gz |grep jar</code>来查看Flink的Java JARs存放位置，如下图：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/6F22FA6A-2892-4137-BA10-6D643C60DBEE.png\" alt></p>\n<p>我们发现只要在<code>deps/lib</code>目录下面的JARs在运行Python时候都是在Classpath下面的。所以我们只要想办法将我们自己定义的UDF放到deps/lib目录下就可以了。那么怎样才能将我们的JAR放到Classpath下面呢？</p>\n<h2>解决方案</h2>\n<p>如果你在安装pyflink之前可以将你需要的JAR放到 <code>build-target/lib</code>下面，然后执行<code>python setup.py sdist</code>进行打包和进行安装<code>pip install dist/*.tar.gz</code>这样你的的UDF的JAR也会在Classpath下面。</p>\n<p>如果你已经安装好了，你也可以直接将你的JAR包拷贝到pip安装的目录，比如：</p>\n<p><img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/9C73FBBB-80AD-45DA-91E5-21436A7811C2.png\" alt></p>\n<p>我们可以直接将需要的JARs拷贝到你所使用的phython环境的<code>$PY_HOME/site-packages/pyflink/lib/</code>目录下。</p>\n<h1>未来规划</h1>\n<p>在Apache Flink 1.10版本我们会将这个过程自动化，使用一种比较方便的方式优化用户的使用体验！</p>\n<h1>小结</h1>\n<p>本篇有针对性的为大家讲解了如何在IDE中运行使用Java UDF的 Python 作业。并且 开篇说道 部分为大家分享了老子&quot;持而盈之，不如其已&quot;经典语句，并建议大家今后有意识的不要与人争长论短，谨记 谦受益满招损，努力做到谦逊低调，营造欢乐的交际氛围，最后祝福大家在学习Apache Flink Python API的同时，生活工作开心快乐！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/08/11/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/comment.png\" alt></p>\n"},{"title":"Apache Flink 漫谈系列 - 序","date":"2019-01-01T10:18:18.000Z","_content":"\n# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。\n\n# What\nApache Flink 漫谈系列会分享什么呢？本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。\n\n# Why\n## 闪速成为Apache顶级项目\nApache Flink是时代的产物，是当前纯流式计算引擎的领头羊。最初Apache Flink的名字叫Stratosphere，是位于德国柏林的一所大学的几个博士和研究生发明的，很短的时间便于2014年3月份成为Apache Incubator project。\n\n## 互联网巨头的认可\nApache Flink 于2015年在德国Berlin举行了第一次Flink forward。也是在2015年阿里巴巴的 蒋晓伟 也在带领团队研发基于Apache Flink的新一代计算引擎Blink。并于2016年的Flink forward上面对Blink在Alibaba生态的应用进行了分享。此后Apache Flink在流计算领域风靡至今，Blink也在2015，2016，2017的阿里巴巴双十一狂欢节中创造了很多奇迹，其中 2017年双11创下了每秒处理4.72亿实时日志，每秒32.5万笔支付交易的佳绩。\n\n## 迅速增长的用户群\n在阿里巴巴集团内部基于Apache Flink的新一代计算平台Blink被广泛应用，同时在阿里云官网对外开放了Blink，目前业界大量的用户对Apache Flink 抱有浓厚的学习兴趣。所以本系列专题应需而生，目的在于向广大用户和Apache Flink爱好者深入全面的分析Apache Flink的实现原理，让更多的人更好的了解Apache Flink，并能在Apache Flink中得到更大的利好。\n\n# When\nApache Flink 漫谈系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。\n","source":"_posts/Apache-Flink-漫谈系列-序.md","raw":"---\ntitle: Apache Flink 漫谈系列 - 序\ndate: 2019-01-01 18:18:18\ncategories: Apache Flink 漫谈\ntags: Flink\n---\n\n# Who\n本人 孙金城，淘宝花名\"金竹\"，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。\n\n# What\nApache Flink 漫谈系列会分享什么呢？本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。\n\n# Why\n## 闪速成为Apache顶级项目\nApache Flink是时代的产物，是当前纯流式计算引擎的领头羊。最初Apache Flink的名字叫Stratosphere，是位于德国柏林的一所大学的几个博士和研究生发明的，很短的时间便于2014年3月份成为Apache Incubator project。\n\n## 互联网巨头的认可\nApache Flink 于2015年在德国Berlin举行了第一次Flink forward。也是在2015年阿里巴巴的 蒋晓伟 也在带领团队研发基于Apache Flink的新一代计算引擎Blink。并于2016年的Flink forward上面对Blink在Alibaba生态的应用进行了分享。此后Apache Flink在流计算领域风靡至今，Blink也在2015，2016，2017的阿里巴巴双十一狂欢节中创造了很多奇迹，其中 2017年双11创下了每秒处理4.72亿实时日志，每秒32.5万笔支付交易的佳绩。\n\n## 迅速增长的用户群\n在阿里巴巴集团内部基于Apache Flink的新一代计算平台Blink被广泛应用，同时在阿里云官网对外开放了Blink，目前业界大量的用户对Apache Flink 抱有浓厚的学习兴趣。所以本系列专题应需而生，目的在于向广大用户和Apache Flink爱好者深入全面的分析Apache Flink的实现原理，让更多的人更好的了解Apache Flink，并能在Apache Flink中得到更大的利好。\n\n# When\nApache Flink 漫谈系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。\n","slug":"Apache-Flink-漫谈系列-序","published":1,"updated":"2019-07-13T13:44:46.216Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5gn001ns94hb7dg7xpp","content":"<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>Apache Flink 漫谈系列会分享什么呢？本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。</p>\n<h1>Why</h1>\n<h2>闪速成为Apache顶级项目</h2>\n<p>Apache Flink是时代的产物，是当前纯流式计算引擎的领头羊。最初Apache Flink的名字叫Stratosphere，是位于德国柏林的一所大学的几个博士和研究生发明的，很短的时间便于2014年3月份成为Apache Incubator project。</p>\n<h2>互联网巨头的认可</h2>\n<p>Apache Flink 于2015年在德国Berlin举行了第一次Flink forward。也是在2015年阿里巴巴的 蒋晓伟 也在带领团队研发基于Apache Flink的新一代计算引擎Blink。并于2016年的Flink forward上面对Blink在Alibaba生态的应用进行了分享。此后Apache Flink在流计算领域风靡至今，Blink也在2015，2016，2017的阿里巴巴双十一狂欢节中创造了很多奇迹，其中 2017年双11创下了每秒处理4.72亿实时日志，每秒32.5万笔支付交易的佳绩。</p>\n<h2>迅速增长的用户群</h2>\n<p>在阿里巴巴集团内部基于Apache Flink的新一代计算平台Blink被广泛应用，同时在阿里云官网对外开放了Blink，目前业界大量的用户对Apache Flink 抱有浓厚的学习兴趣。所以本系列专题应需而生，目的在于向广大用户和Apache Flink爱好者深入全面的分析Apache Flink的实现原理，让更多的人更好的了解Apache Flink，并能在Apache Flink中得到更大的利好。</p>\n<h1>When</h1>\n<p>Apache Flink 漫谈系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1>Who</h1>\n<p>本人 孙金城，淘宝花名&quot;金竹&quot;，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。</p>\n<h1>What</h1>\n<p>Apache Flink 漫谈系列会分享什么呢？本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。</p>\n<h1>Why</h1>\n<h2>闪速成为Apache顶级项目</h2>\n<p>Apache Flink是时代的产物，是当前纯流式计算引擎的领头羊。最初Apache Flink的名字叫Stratosphere，是位于德国柏林的一所大学的几个博士和研究生发明的，很短的时间便于2014年3月份成为Apache Incubator project。</p>\n<h2>互联网巨头的认可</h2>\n<p>Apache Flink 于2015年在德国Berlin举行了第一次Flink forward。也是在2015年阿里巴巴的 蒋晓伟 也在带领团队研发基于Apache Flink的新一代计算引擎Blink。并于2016年的Flink forward上面对Blink在Alibaba生态的应用进行了分享。此后Apache Flink在流计算领域风靡至今，Blink也在2015，2016，2017的阿里巴巴双十一狂欢节中创造了很多奇迹，其中 2017年双11创下了每秒处理4.72亿实时日志，每秒32.5万笔支付交易的佳绩。</p>\n<h2>迅速增长的用户群</h2>\n<p>在阿里巴巴集团内部基于Apache Flink的新一代计算平台Blink被广泛应用，同时在阿里云官网对外开放了Blink，目前业界大量的用户对Apache Flink 抱有浓厚的学习兴趣。所以本系列专题应需而生，目的在于向广大用户和Apache Flink爱好者深入全面的分析Apache Flink的实现原理，让更多的人更好的了解Apache Flink，并能在Apache Flink中得到更大的利好。</p>\n<h1>When</h1>\n<p>Apache Flink 漫谈系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。</p>\n"},{"title":"Apache Flink 说道系列- Python Table API 开发环境搭建","date":"2019-07-25T13:05:15.000Z","_content":"# 开篇说道 - 曲则全\n\"曲则全，枉则直，洼则盈，敝则新，少则得，多则惑\" 语出《道德经》第二十二章，整句充分反映了老子的辩证思想，老子用曲则全，枉则直，洼则盈，敝则新，少则得和多则惑来讲述了\"道\"顺其自然的理论。\n\n所谓的顺其自然，有很多方面，比如前面一篇讲的\"上善若水\"，水自高而下的流淌也是顺其自然的体现，如果不顺其自然,而是阻止水的自上而下的流淌会发生什么？那就是 灾难，水灾。大家知道在 大禹治水 之前人类是如何处理洪水的？\"拦截\" 无目的的随处拦截，不是拦截不好，而是只拦截不疏通就违背自然之道了。再想想大禹治水，则是在顺应自然的同时，在疏通水道的同时在恰当的地方进行拦截，使得在不违背水自上而下的自然本性的同时合理影响水流的方向，进而造福人类。\n\n那么本篇的\"曲则全，枉则直，洼则盈，敝则新，少则得，多则惑\"如何理解呢？直译一下就是：\"弯曲才可保全，委屈才可伸展，低洼将可充满，敝旧将可更新，少取反而获得，多取反而迷惑\"。其实不难理解，比如:\"洼则盈\", \"洼\"也就是低洼，\"盈\"就是满，只有低洼的的地势才能赢得水流，也就是只有空的杯子才能装的进水。满的杯子很难再容下其他物品。其实老子无时无刻不在论证为人之道，谦卑将自己放空，才能容纳他人，聚贤聚德。下面我们再以一个简短的故事解释\"曲则全\"来结束本篇的论道：\n\n春秋时期，齐国君主齐景公非常喜欢捕鸟，当时有个名叫 烛雏 的大臣专门为其捕鸟。但有一次由于 烛雏 的疏忽将捕来的鸟弄飞了。这使得齐景公勃然大怒，要杀了 烛雏。当时 晏子 也附和齐景王对 烛雏 说：你有三大罪状：1. 你的疏忽把鸟放跑了，这是第一罪状，2. 因为鸟飞了使得大王要去杀人，使大王背上了喜欢杀人的坏名声，这是你第二大罪状，3. 这件事情传出去天下人会认为大王把鸟看的比人命还重要，破坏了大王的威望，这是你第三大罪状。所以，大王请将 烛雏 处死吧？可想而知 齐景王并没有完全失去理智，虽然 晏子 没有直接劝阻他不要杀死 烛雏，但他听出了 晏子 的用意，所以自然赦免了 烛雏。\n\n所以真正的智者会顺势而为，见机行事，无法直言的时候就要学会迂回，这也是对老子\"曲则全\"理论的典型运用！\n\n# 概要\n欲善其事必利其器，要想利用Apache Flink Python Table API进行业务开发，首先要先搭建一个开发环境。本文将细致的叙述如何搭建开发环境，最后在编写一个简单的WordCount示例程序。\n\n# 依赖环境\n\n* JDK 1.8+ (1.8.0_211)\n* Maven 3.x (3.2.5)\n* Scala 2.11+ (2.12.0)\n* Python 2.7+ (2.7.16)\n* Git 2.20+ (2.20.1)\n\n大家如上依赖配置如果有问题，可以在评论区留言。\n\n# 安装PyFlink\n我们要想利用Apache Flink Python API 进行业务开发，需要将PyFlink发布包进行安装。目前PyFlink并没有发布到Python仓库，比如著名的PyPI(正在社区讨论，[详见](http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Publish-the-PyFlink-into-PyPI-td30095.html))，所以我们需要从源码构建。\n## 源码构建发布包\n### 下载flink源代码\n```\ngit clone https://github.com/apache/flink.git\n```\n签出release-1.9分支(1.9版本是Apache Flink Python Table API的第一个版本）\n```\ngit fetch origin release-1.9\ngit checkout -b release-1.9 origin/release-1.9\n```\n输出如下证明已经签出了release-1.9代码：\n```\nBranch 'release-1.9' set up to track remote branch 'release-1.9' from 'origin'.\nSwitched to a new branch 'release-1.9'\n```\n### 构建Java二进制包\n查看我们在relese-1.9分支并进行源码构建发布包（JAVA）\n```\nmvn clean install -DskipTests -Dfast\n```\n上面命令会执行一段时间(时间长短取决于你机器配置),最终输出如下信息，证明已经成功的完成发布包的构建：\n```\n...\n...\n[INFO] flink-ml-lib ....................................... SUCCESS [  0.190 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14:47 min\n[INFO] Finished at: 2019-07-25T17:38:06+08:00\n[INFO] Final Memory: 537M/2085M\n[INFO] ------------------------------------------------------------------------\n```\n在flink-dist/target/下的lib目录和opt目录中flink-python*.jar 和 flink-table*.jar 都是PyFlink必须依赖的JARs, 在下面构建PyFlink包的时候，会将这些依赖JRAs也包含在里面。\n\n### 构建PyFlink发布包\n一般情况我们期望以pip install的方式安装python的类库，我们要想安装PyFlink的类库，也需要构建可用于pip install的发布包。执行如下命令：\n```\ncd flink-python; python setup.py sdist \n```\n最终输出如下信息，证明是成功的：\n```\n...\n...\ncopying pyflink/table/types.py -> apache-flink-1.9.dev0/pyflink/table\ncopying pyflink/table/window.py -> apache-flink-1.9.dev0/pyflink/table\ncopying pyflink/util/__init__.py -> apache-flink-1.9.dev0/pyflink/util\ncopying pyflink/util/exceptions.py -> apache-flink-1.9.dev0/pyflink/util\ncopying pyflink/util/utils.py -> apache-flink-1.9.dev0/pyflink/util\nWriting apache-flink-1.9.dev0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'apache-flink-1.9.dev0' (and everything under it)\n```\n在dist目录的apache-flink-1.9.dev0.tar.gz就是我们可以用于pip install的PyFlink包.\n\n## 安装PyFlink\n上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，如下：\n```\npip install dist/*.tar.gz\n```\n输出如下信息，证明已经安装成功：\n```\nBuilding wheels for collected packages: apache-flink\n  Building wheel for apache-flink (setup.py) ... done\n  Stored in directory: /Users/jincheng.sunjc/Library/Caches/pip/wheels/34/b1/23/fca6e31a6de419c9c9d75a6d11df197d538c2ef67e017d79ea\nSuccessfully built apache-flink\nInstalling collected packages: apache-flink\nSuccessfully installed apache-flink-1.9.dev0\n```\n用pip命令检查是否安装成功：\n```\nIT-C02YL16BJHD2:flink-python jincheng.sunjc$ pip list\nPackage         Version    \n--------------- -----------\napache-flink    1.9.dev0   \nconfigparser    3.7.4      \nentrypoints     0.3        \nenum34          1.1.6      \nflake8          3.7.8      \nfunctools32     3.2.3.post2\nkafka-python    1.4.6      \nmccabe          0.6.1      \npip             19.1.1     \npy4j            0.10.8.1   \npycodestyle     2.5.0      \npyflakes        2.1.1      \npython-dateutil 2.8.0      \nsetuptools      41.0.1     \nsix             1.12.0     \ntld             0.9.3      \ntyping          3.7.4      \nvirtualenv      16.6.1     \nwheel           0.33.4     \n\n```\n其中 `apache-flink    1.9.dev0`就是我们刚才安装的版本。  \n\n# 配置IDE开发环境\n开发Python最好用的集成开发环境应该是PyCharm了，我们接下来介绍如果配置PyCharm环境。\n## 下载安装包\n从 [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/) 选择你需要的版本，如果你需要mac版本，直接打开如下链接下载：\n[https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg](https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg)\n## 安装配置并创建项目\n上面下载的应该都是可以直接执行的，一路next之后，就可以对PyCharm进行配置了。我们在启动界面选择创建一个项目，如下：\n![](5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png)\n点击 \"Create New Project\"之后，出现如下界面：\n![](4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png)\n1. 我们要项目取一个名字，比如\"myPyFlink\"。\n2. 选择\"Existing interpreter\"。\n3. 配置\"interpreter\" **非常重要**,这个配置必须和你刚才安装PyFlink的时候(pip install)的python完全一致。\n4. 如果没有你需要的python版本，可以点击 4 所示位置进行添加。\n5. 如何知道你pip install对应的python，请使用5所示的命令\"which is python\"进行查看。\n\n如果需要进行4所需要的操作，点击之后界面如下：\n![](8308BB64-2AB2-480C-9B2E-4AADD515F183.png)\n选择配置\"System Interpreter\",并配置\"which is python\"说输出的路径。\n点击 \"OK\"。\n在顺利完成上面配置之后，我们点击 \"Create\"，完成项目的创建，如下图：\n![](77394C3C-D401-4DE6-A005-D4ADFFBFB892.png)\n我们可以核实一下，我们依赖的External Libraries是\n\"which is python\"说输出的Python。\n\n# 开发WordCount\n如果上面的过程一切顺利，那么我们的器已经很锋利了，开始善其事了，新建一个package,名为enjoyment,然后新建一个word_count.py。如下：\n![](6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png)\n然后，在弹出的界面写入word_count之后，点击 \"OK\"。\n![](E371FA28-BAAE-448F-A930-E795CBF09B0E.png)\n\n## 完整代码\n```\n# -*- coding: utf-8 -*-\n\nfrom pyflink.dataset import ExecutionEnvironment\nfrom pyflink.table import TableConfig, DataTypes, BatchTableEnvironment\nfrom pyflink.table.descriptors import Schema, OldCsv, FileSystem\n\nimport os\n\n# 数据源文件\nsource_file = 'source.csv'\n#计算结果文件\nsink_file = 'sink.csv'\n\n# 创建执行环境\nexec_env = ExecutionEnvironment.get_execution_environment()\n# 设置并发为1 方便调试\nexec_env.set_parallelism(1)\nt_config = TableConfig()\nt_env = BatchTableEnvironment.create(exec_env, t_config)\n\n# 如果结果文件已经存在,就先删除\nif os.path.exists(sink_file):\n    os.remove(sink_file)\n\n# 创建数据源表\nt_env.connect(FileSystem().path(source_file)) \\\n    .with_format(OldCsv()\n                 .line_delimiter(',')\n                 .field('word', DataTypes.STRING())) \\\n    .with_schema(Schema()\n                 .field('word', DataTypes.STRING())) \\\n    .register_table_source('mySource')\n\n# 创建结果表\nt_env.connect(FileSystem().path(sink_file)) \\\n    .with_format(OldCsv()\n                 .field_delimiter(',')\n                 .field('word', DataTypes.STRING())\n                 .field('count', DataTypes.BIGINT())) \\\n    .with_schema(Schema()\n                 .field('word', DataTypes.STRING())\n                 .field('count', DataTypes.BIGINT())) \\\n    .register_table_sink('mySink')\n\n# 非常简单的word_count计算逻辑\nt_env.scan('mySource') \\\n    .group_by('word') \\\n    .select('word, count(1)') \\\n    .insert_into('mySink')\n\n# 执行Job\nt_env.execute(\"wordcount\")\n```\n\n## 执行并查看运行结果\n在执行job之前我们需要先创建一个数据源文件,在enjoyment包目录下，创建 source.csv文件，内容如下：\n```\nenjoyment,Apache Flink,cool,Apache Flink\n```\n现在我们可以执行，job了：\n\n![](270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png)\n1. 右键word_count.py, 2. 点击运行'word_count'。\n运行之后，当控制台出现：\"Process finished with exit code 0\"，并且在enjoyment目录下出现sink.csv文件，说明我们第一个word_count就成功运行了:)\n![](34546866-F865-4660-8177-A64C5A14C002.png)\n我们双击 \"sink.csv\",查看运行结果，如下：\n![](0B58FCBF-4D98-41F9-98A3-13B31F939916.png)\n\n到此我们Python Table API的开发环境搭建完成。\n\n# 项目源代码\n为了方便大家快速体验本篇内容，可以下载本篇涉及到的python项目，git地址如下：[https://github.com/sunjincheng121/enjoyment.code](https://github.com/sunjincheng121/enjoyment.code)\n\n# 小结\n本篇是为后续Flink Python Table API 的算子介绍做铺垫，介绍如何搭建开发环境。最后以一个最简单，也是最经典的WordCount具体示例来验证开发环境。同时在开篇说道部分简单为大家分享了老子所宣扬的\"曲则全\"的人生智慧，希望大家在学习技术的同时也能学到为人的智慧！谢谢大家的对本篇博文的查阅！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n\n\n","source":"_posts/Apache Flink 说道系列- Python Table API 开发环境搭建.md","raw":"---\ntitle: Apache Flink 说道系列- Python Table API 开发环境搭建\ndate: 2019-07-25 21:05:15\ncategories: Apache Flink 说道\ntags: [Flink, 道德经, 曲则全]\n---\n# 开篇说道 - 曲则全\n\"曲则全，枉则直，洼则盈，敝则新，少则得，多则惑\" 语出《道德经》第二十二章，整句充分反映了老子的辩证思想，老子用曲则全，枉则直，洼则盈，敝则新，少则得和多则惑来讲述了\"道\"顺其自然的理论。\n\n所谓的顺其自然，有很多方面，比如前面一篇讲的\"上善若水\"，水自高而下的流淌也是顺其自然的体现，如果不顺其自然,而是阻止水的自上而下的流淌会发生什么？那就是 灾难，水灾。大家知道在 大禹治水 之前人类是如何处理洪水的？\"拦截\" 无目的的随处拦截，不是拦截不好，而是只拦截不疏通就违背自然之道了。再想想大禹治水，则是在顺应自然的同时，在疏通水道的同时在恰当的地方进行拦截，使得在不违背水自上而下的自然本性的同时合理影响水流的方向，进而造福人类。\n\n那么本篇的\"曲则全，枉则直，洼则盈，敝则新，少则得，多则惑\"如何理解呢？直译一下就是：\"弯曲才可保全，委屈才可伸展，低洼将可充满，敝旧将可更新，少取反而获得，多取反而迷惑\"。其实不难理解，比如:\"洼则盈\", \"洼\"也就是低洼，\"盈\"就是满，只有低洼的的地势才能赢得水流，也就是只有空的杯子才能装的进水。满的杯子很难再容下其他物品。其实老子无时无刻不在论证为人之道，谦卑将自己放空，才能容纳他人，聚贤聚德。下面我们再以一个简短的故事解释\"曲则全\"来结束本篇的论道：\n\n春秋时期，齐国君主齐景公非常喜欢捕鸟，当时有个名叫 烛雏 的大臣专门为其捕鸟。但有一次由于 烛雏 的疏忽将捕来的鸟弄飞了。这使得齐景公勃然大怒，要杀了 烛雏。当时 晏子 也附和齐景王对 烛雏 说：你有三大罪状：1. 你的疏忽把鸟放跑了，这是第一罪状，2. 因为鸟飞了使得大王要去杀人，使大王背上了喜欢杀人的坏名声，这是你第二大罪状，3. 这件事情传出去天下人会认为大王把鸟看的比人命还重要，破坏了大王的威望，这是你第三大罪状。所以，大王请将 烛雏 处死吧？可想而知 齐景王并没有完全失去理智，虽然 晏子 没有直接劝阻他不要杀死 烛雏，但他听出了 晏子 的用意，所以自然赦免了 烛雏。\n\n所以真正的智者会顺势而为，见机行事，无法直言的时候就要学会迂回，这也是对老子\"曲则全\"理论的典型运用！\n\n# 概要\n欲善其事必利其器，要想利用Apache Flink Python Table API进行业务开发，首先要先搭建一个开发环境。本文将细致的叙述如何搭建开发环境，最后在编写一个简单的WordCount示例程序。\n\n# 依赖环境\n\n* JDK 1.8+ (1.8.0_211)\n* Maven 3.x (3.2.5)\n* Scala 2.11+ (2.12.0)\n* Python 2.7+ (2.7.16)\n* Git 2.20+ (2.20.1)\n\n大家如上依赖配置如果有问题，可以在评论区留言。\n\n# 安装PyFlink\n我们要想利用Apache Flink Python API 进行业务开发，需要将PyFlink发布包进行安装。目前PyFlink并没有发布到Python仓库，比如著名的PyPI(正在社区讨论，[详见](http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Publish-the-PyFlink-into-PyPI-td30095.html))，所以我们需要从源码构建。\n## 源码构建发布包\n### 下载flink源代码\n```\ngit clone https://github.com/apache/flink.git\n```\n签出release-1.9分支(1.9版本是Apache Flink Python Table API的第一个版本）\n```\ngit fetch origin release-1.9\ngit checkout -b release-1.9 origin/release-1.9\n```\n输出如下证明已经签出了release-1.9代码：\n```\nBranch 'release-1.9' set up to track remote branch 'release-1.9' from 'origin'.\nSwitched to a new branch 'release-1.9'\n```\n### 构建Java二进制包\n查看我们在relese-1.9分支并进行源码构建发布包（JAVA）\n```\nmvn clean install -DskipTests -Dfast\n```\n上面命令会执行一段时间(时间长短取决于你机器配置),最终输出如下信息，证明已经成功的完成发布包的构建：\n```\n...\n...\n[INFO] flink-ml-lib ....................................... SUCCESS [  0.190 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14:47 min\n[INFO] Finished at: 2019-07-25T17:38:06+08:00\n[INFO] Final Memory: 537M/2085M\n[INFO] ------------------------------------------------------------------------\n```\n在flink-dist/target/下的lib目录和opt目录中flink-python*.jar 和 flink-table*.jar 都是PyFlink必须依赖的JARs, 在下面构建PyFlink包的时候，会将这些依赖JRAs也包含在里面。\n\n### 构建PyFlink发布包\n一般情况我们期望以pip install的方式安装python的类库，我们要想安装PyFlink的类库，也需要构建可用于pip install的发布包。执行如下命令：\n```\ncd flink-python; python setup.py sdist \n```\n最终输出如下信息，证明是成功的：\n```\n...\n...\ncopying pyflink/table/types.py -> apache-flink-1.9.dev0/pyflink/table\ncopying pyflink/table/window.py -> apache-flink-1.9.dev0/pyflink/table\ncopying pyflink/util/__init__.py -> apache-flink-1.9.dev0/pyflink/util\ncopying pyflink/util/exceptions.py -> apache-flink-1.9.dev0/pyflink/util\ncopying pyflink/util/utils.py -> apache-flink-1.9.dev0/pyflink/util\nWriting apache-flink-1.9.dev0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'apache-flink-1.9.dev0' (and everything under it)\n```\n在dist目录的apache-flink-1.9.dev0.tar.gz就是我们可以用于pip install的PyFlink包.\n\n## 安装PyFlink\n上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，如下：\n```\npip install dist/*.tar.gz\n```\n输出如下信息，证明已经安装成功：\n```\nBuilding wheels for collected packages: apache-flink\n  Building wheel for apache-flink (setup.py) ... done\n  Stored in directory: /Users/jincheng.sunjc/Library/Caches/pip/wheels/34/b1/23/fca6e31a6de419c9c9d75a6d11df197d538c2ef67e017d79ea\nSuccessfully built apache-flink\nInstalling collected packages: apache-flink\nSuccessfully installed apache-flink-1.9.dev0\n```\n用pip命令检查是否安装成功：\n```\nIT-C02YL16BJHD2:flink-python jincheng.sunjc$ pip list\nPackage         Version    \n--------------- -----------\napache-flink    1.9.dev0   \nconfigparser    3.7.4      \nentrypoints     0.3        \nenum34          1.1.6      \nflake8          3.7.8      \nfunctools32     3.2.3.post2\nkafka-python    1.4.6      \nmccabe          0.6.1      \npip             19.1.1     \npy4j            0.10.8.1   \npycodestyle     2.5.0      \npyflakes        2.1.1      \npython-dateutil 2.8.0      \nsetuptools      41.0.1     \nsix             1.12.0     \ntld             0.9.3      \ntyping          3.7.4      \nvirtualenv      16.6.1     \nwheel           0.33.4     \n\n```\n其中 `apache-flink    1.9.dev0`就是我们刚才安装的版本。  \n\n# 配置IDE开发环境\n开发Python最好用的集成开发环境应该是PyCharm了，我们接下来介绍如果配置PyCharm环境。\n## 下载安装包\n从 [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/) 选择你需要的版本，如果你需要mac版本，直接打开如下链接下载：\n[https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg](https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg)\n## 安装配置并创建项目\n上面下载的应该都是可以直接执行的，一路next之后，就可以对PyCharm进行配置了。我们在启动界面选择创建一个项目，如下：\n![](5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png)\n点击 \"Create New Project\"之后，出现如下界面：\n![](4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png)\n1. 我们要项目取一个名字，比如\"myPyFlink\"。\n2. 选择\"Existing interpreter\"。\n3. 配置\"interpreter\" **非常重要**,这个配置必须和你刚才安装PyFlink的时候(pip install)的python完全一致。\n4. 如果没有你需要的python版本，可以点击 4 所示位置进行添加。\n5. 如何知道你pip install对应的python，请使用5所示的命令\"which is python\"进行查看。\n\n如果需要进行4所需要的操作，点击之后界面如下：\n![](8308BB64-2AB2-480C-9B2E-4AADD515F183.png)\n选择配置\"System Interpreter\",并配置\"which is python\"说输出的路径。\n点击 \"OK\"。\n在顺利完成上面配置之后，我们点击 \"Create\"，完成项目的创建，如下图：\n![](77394C3C-D401-4DE6-A005-D4ADFFBFB892.png)\n我们可以核实一下，我们依赖的External Libraries是\n\"which is python\"说输出的Python。\n\n# 开发WordCount\n如果上面的过程一切顺利，那么我们的器已经很锋利了，开始善其事了，新建一个package,名为enjoyment,然后新建一个word_count.py。如下：\n![](6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png)\n然后，在弹出的界面写入word_count之后，点击 \"OK\"。\n![](E371FA28-BAAE-448F-A930-E795CBF09B0E.png)\n\n## 完整代码\n```\n# -*- coding: utf-8 -*-\n\nfrom pyflink.dataset import ExecutionEnvironment\nfrom pyflink.table import TableConfig, DataTypes, BatchTableEnvironment\nfrom pyflink.table.descriptors import Schema, OldCsv, FileSystem\n\nimport os\n\n# 数据源文件\nsource_file = 'source.csv'\n#计算结果文件\nsink_file = 'sink.csv'\n\n# 创建执行环境\nexec_env = ExecutionEnvironment.get_execution_environment()\n# 设置并发为1 方便调试\nexec_env.set_parallelism(1)\nt_config = TableConfig()\nt_env = BatchTableEnvironment.create(exec_env, t_config)\n\n# 如果结果文件已经存在,就先删除\nif os.path.exists(sink_file):\n    os.remove(sink_file)\n\n# 创建数据源表\nt_env.connect(FileSystem().path(source_file)) \\\n    .with_format(OldCsv()\n                 .line_delimiter(',')\n                 .field('word', DataTypes.STRING())) \\\n    .with_schema(Schema()\n                 .field('word', DataTypes.STRING())) \\\n    .register_table_source('mySource')\n\n# 创建结果表\nt_env.connect(FileSystem().path(sink_file)) \\\n    .with_format(OldCsv()\n                 .field_delimiter(',')\n                 .field('word', DataTypes.STRING())\n                 .field('count', DataTypes.BIGINT())) \\\n    .with_schema(Schema()\n                 .field('word', DataTypes.STRING())\n                 .field('count', DataTypes.BIGINT())) \\\n    .register_table_sink('mySink')\n\n# 非常简单的word_count计算逻辑\nt_env.scan('mySource') \\\n    .group_by('word') \\\n    .select('word, count(1)') \\\n    .insert_into('mySink')\n\n# 执行Job\nt_env.execute(\"wordcount\")\n```\n\n## 执行并查看运行结果\n在执行job之前我们需要先创建一个数据源文件,在enjoyment包目录下，创建 source.csv文件，内容如下：\n```\nenjoyment,Apache Flink,cool,Apache Flink\n```\n现在我们可以执行，job了：\n\n![](270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png)\n1. 右键word_count.py, 2. 点击运行'word_count'。\n运行之后，当控制台出现：\"Process finished with exit code 0\"，并且在enjoyment目录下出现sink.csv文件，说明我们第一个word_count就成功运行了:)\n![](34546866-F865-4660-8177-A64C5A14C002.png)\n我们双击 \"sink.csv\",查看运行结果，如下：\n![](0B58FCBF-4D98-41F9-98A3-13B31F939916.png)\n\n到此我们Python Table API的开发环境搭建完成。\n\n# 项目源代码\n为了方便大家快速体验本篇内容，可以下载本篇涉及到的python项目，git地址如下：[https://github.com/sunjincheng121/enjoyment.code](https://github.com/sunjincheng121/enjoyment.code)\n\n# 小结\n本篇是为后续Flink Python Table API 的算子介绍做铺垫，介绍如何搭建开发环境。最后以一个最简单，也是最经典的WordCount具体示例来验证开发环境。同时在开篇说道部分简单为大家分享了老子所宣扬的\"曲则全\"的人生智慧，希望大家在学习技术的同时也能学到为人的智慧！谢谢大家的对本篇博文的查阅！\n\n# 关于评论\n如果你没有看到下面的评论区域，请进行翻墙！\n![](comment.png)\n\n\n\n\n","slug":"Apache Flink 说道系列- Python Table API 开发环境搭建","published":1,"updated":"2019-07-25T14:36:30.629Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck0emm5gr001rs94hlfdma729","content":"<h1>开篇说道 - 曲则全</h1>\n<p>&quot;曲则全，枉则直，洼则盈，敝则新，少则得，多则惑&quot; 语出《道德经》第二十二章，整句充分反映了老子的辩证思想，老子用曲则全，枉则直，洼则盈，敝则新，少则得和多则惑来讲述了&quot;道&quot;顺其自然的理论。</p>\n<p>所谓的顺其自然，有很多方面，比如前面一篇讲的&quot;上善若水&quot;，水自高而下的流淌也是顺其自然的体现，如果不顺其自然,而是阻止水的自上而下的流淌会发生什么？那就是 灾难，水灾。大家知道在 大禹治水 之前人类是如何处理洪水的？&quot;拦截&quot; 无目的的随处拦截，不是拦截不好，而是只拦截不疏通就违背自然之道了。再想想大禹治水，则是在顺应自然的同时，在疏通水道的同时在恰当的地方进行拦截，使得在不违背水自上而下的自然本性的同时合理影响水流的方向，进而造福人类。</p>\n<p>那么本篇的&quot;曲则全，枉则直，洼则盈，敝则新，少则得，多则惑&quot;如何理解呢？直译一下就是：&quot;弯曲才可保全，委屈才可伸展，低洼将可充满，敝旧将可更新，少取反而获得，多取反而迷惑&quot;。其实不难理解，比如:&quot;洼则盈&quot;, &quot;洼&quot;也就是低洼，&quot;盈&quot;就是满，只有低洼的的地势才能赢得水流，也就是只有空的杯子才能装的进水。满的杯子很难再容下其他物品。其实老子无时无刻不在论证为人之道，谦卑将自己放空，才能容纳他人，聚贤聚德。下面我们再以一个简短的故事解释&quot;曲则全&quot;来结束本篇的论道：</p>\n<p>春秋时期，齐国君主齐景公非常喜欢捕鸟，当时有个名叫 烛雏 的大臣专门为其捕鸟。但有一次由于 烛雏 的疏忽将捕来的鸟弄飞了。这使得齐景公勃然大怒，要杀了 烛雏。当时 晏子 也附和齐景王对 烛雏 说：你有三大罪状：1. 你的疏忽把鸟放跑了，这是第一罪状，2. 因为鸟飞了使得大王要去杀人，使大王背上了喜欢杀人的坏名声，这是你第二大罪状，3. 这件事情传出去天下人会认为大王把鸟看的比人命还重要，破坏了大王的威望，这是你第三大罪状。所以，大王请将 烛雏 处死吧？可想而知 齐景王并没有完全失去理智，虽然 晏子 没有直接劝阻他不要杀死 烛雏，但他听出了 晏子 的用意，所以自然赦免了 烛雏。</p>\n<p>所以真正的智者会顺势而为，见机行事，无法直言的时候就要学会迂回，这也是对老子&quot;曲则全&quot;理论的典型运用！</p>\n<h1>概要</h1>\n<p>欲善其事必利其器，要想利用Apache Flink Python Table API进行业务开发，首先要先搭建一个开发环境。本文将细致的叙述如何搭建开发环境，最后在编写一个简单的WordCount示例程序。</p>\n<h1>依赖环境</h1>\n<ul>\n<li>JDK 1.8+ (1.8.0_211)</li>\n<li>Maven 3.x (3.2.5)</li>\n<li>Scala 2.11+ (2.12.0)</li>\n<li>Python 2.7+ (2.7.16)</li>\n<li>Git 2.20+ (2.20.1)</li>\n</ul>\n<p>大家如上依赖配置如果有问题，可以在评论区留言。</p>\n<h1>安装PyFlink</h1>\n<p>我们要想利用Apache Flink Python API 进行业务开发，需要将PyFlink发布包进行安装。目前PyFlink并没有发布到Python仓库，比如著名的PyPI(正在社区讨论，<a href=\"http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Publish-the-PyFlink-into-PyPI-td30095.html\" target=\"_blank\" rel=\"noopener\">详见</a>)，所以我们需要从源码构建。</p>\n<h2>源码构建发布包</h2>\n<h3>下载flink源代码</h3>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/apache/flink.git</span><br></pre></td></tr></table></figure></p>\n<p>签出release-1.9分支(1.9版本是Apache Flink Python Table API的第一个版本）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin release-1.9</span><br><span class=\"line\">git checkout -b release-1.9 origin/release-1.9</span><br></pre></td></tr></table></figure></p>\n<p>输出如下证明已经签出了release-1.9代码：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Branch &apos;release-1.9&apos; set up to track remote branch &apos;release-1.9&apos; from &apos;origin&apos;.</span><br><span class=\"line\">Switched to a new branch &apos;release-1.9&apos;</span><br></pre></td></tr></table></figure></p>\n<h3>构建Java二进制包</h3>\n<p>查看我们在relese-1.9分支并进行源码构建发布包（JAVA）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean install -DskipTests -Dfast</span><br></pre></td></tr></table></figure></p>\n<p>上面命令会执行一段时间(时间长短取决于你机器配置),最终输出如下信息，证明已经成功的完成发布包的构建：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">[INFO] flink-ml-lib ....................................... SUCCESS [  0.190 s]</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time: 14:47 min</span><br><span class=\"line\">[INFO] Finished at: 2019-07-25T17:38:06+08:00</span><br><span class=\"line\">[INFO] Final Memory: 537M/2085M</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></p>\n<p>在flink-dist/target/下的lib目录和opt目录中flink-python*.jar 和 flink-table*.jar 都是PyFlink必须依赖的JARs, 在下面构建PyFlink包的时候，会将这些依赖JRAs也包含在里面。</p>\n<h3>构建PyFlink发布包</h3>\n<p>一般情况我们期望以pip install的方式安装python的类库，我们要想安装PyFlink的类库，也需要构建可用于pip install的发布包。执行如下命令：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-python; python setup.py sdist</span><br></pre></td></tr></table></figure></p>\n<p>最终输出如下信息，证明是成功的：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">copying pyflink/table/types.py -&gt; apache-flink-1.9.dev0/pyflink/table</span><br><span class=\"line\">copying pyflink/table/window.py -&gt; apache-flink-1.9.dev0/pyflink/table</span><br><span class=\"line\">copying pyflink/util/__init__.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/exceptions.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/utils.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">Writing apache-flink-1.9.dev0/setup.cfg</span><br><span class=\"line\">creating dist</span><br><span class=\"line\">Creating tar archive</span><br><span class=\"line\">removing &apos;apache-flink-1.9.dev0&apos; (and everything under it)</span><br></pre></td></tr></table></figure></p>\n<p>在dist目录的apache-flink-1.9.dev0.tar.gz就是我们可以用于pip install的PyFlink包.</p>\n<h2>安装PyFlink</h2>\n<p>上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install dist/*.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>输出如下信息，证明已经安装成功：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Building wheels for collected packages: apache-flink</span><br><span class=\"line\">  Building wheel for apache-flink (setup.py) ... done</span><br><span class=\"line\">  Stored in directory: /Users/jincheng.sunjc/Library/Caches/pip/wheels/34/b1/23/fca6e31a6de419c9c9d75a6d11df197d538c2ef67e017d79ea</span><br><span class=\"line\">Successfully built apache-flink</span><br><span class=\"line\">Installing collected packages: apache-flink</span><br><span class=\"line\">Successfully installed apache-flink-1.9.dev0</span><br></pre></td></tr></table></figure></p>\n<p>用pip命令检查是否安装成功：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IT-C02YL16BJHD2:flink-python jincheng.sunjc$ pip list</span><br><span class=\"line\">Package         Version    </span><br><span class=\"line\">--------------- -----------</span><br><span class=\"line\">apache-flink    1.9.dev0   </span><br><span class=\"line\">configparser    3.7.4      </span><br><span class=\"line\">entrypoints     0.3        </span><br><span class=\"line\">enum34          1.1.6      </span><br><span class=\"line\">flake8          3.7.8      </span><br><span class=\"line\">functools32     3.2.3.post2</span><br><span class=\"line\">kafka-python    1.4.6      </span><br><span class=\"line\">mccabe          0.6.1      </span><br><span class=\"line\">pip             19.1.1     </span><br><span class=\"line\">py4j            0.10.8.1   </span><br><span class=\"line\">pycodestyle     2.5.0      </span><br><span class=\"line\">pyflakes        2.1.1      </span><br><span class=\"line\">python-dateutil 2.8.0      </span><br><span class=\"line\">setuptools      41.0.1     </span><br><span class=\"line\">six             1.12.0     </span><br><span class=\"line\">tld             0.9.3      </span><br><span class=\"line\">typing          3.7.4      </span><br><span class=\"line\">virtualenv      16.6.1     </span><br><span class=\"line\">wheel           0.33.4</span><br></pre></td></tr></table></figure></p>\n<p>其中 <code>apache-flink 1.9.dev0</code>就是我们刚才安装的版本。</p>\n<h1>配置IDE开发环境</h1>\n<p>开发Python最好用的集成开发环境应该是PyCharm了，我们接下来介绍如果配置PyCharm环境。</p>\n<h2>下载安装包</h2>\n<p>从 <a href=\"https://www.jetbrains.com/pycharm/\" target=\"_blank\" rel=\"noopener\">https://www.jetbrains.com/pycharm/</a> 选择你需要的版本，如果你需要mac版本，直接打开如下链接下载：\n<a href=\"https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg\" target=\"_blank\" rel=\"noopener\">https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg</a></p>\n<h2>安装配置并创建项目</h2>\n<p>上面下载的应该都是可以直接执行的，一路next之后，就可以对PyCharm进行配置了。我们在启动界面选择创建一个项目，如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png\" alt>\n点击 &quot;Create New Project&quot;之后，出现如下界面：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png\" alt></p>\n<ol>\n<li>我们要项目取一个名字，比如&quot;myPyFlink&quot;。</li>\n<li>选择&quot;Existing interpreter&quot;。</li>\n<li>配置&quot;interpreter&quot; <strong>非常重要</strong>,这个配置必须和你刚才安装PyFlink的时候(pip install)的python完全一致。</li>\n<li>如果没有你需要的python版本，可以点击 4 所示位置进行添加。</li>\n<li>如何知道你pip install对应的python，请使用5所示的命令&quot;which is python&quot;进行查看。</li>\n</ol>\n<p>如果需要进行4所需要的操作，点击之后界面如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/8308BB64-2AB2-480C-9B2E-4AADD515F183.png\" alt>\n选择配置&quot;System Interpreter&quot;,并配置&quot;which is python&quot;说输出的路径。\n点击 &quot;OK&quot;。\n在顺利完成上面配置之后，我们点击 &quot;Create&quot;，完成项目的创建，如下图：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/77394C3C-D401-4DE6-A005-D4ADFFBFB892.png\" alt>\n我们可以核实一下，我们依赖的External Libraries是\n&quot;which is python&quot;说输出的Python。</p>\n<h1>开发WordCount</h1>\n<p>如果上面的过程一切顺利，那么我们的器已经很锋利了，开始善其事了，新建一个package,名为enjoyment,然后新建一个word_count.py。如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png\" alt>\n然后，在弹出的界面写入word_count之后，点击 &quot;OK&quot;。\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/E371FA28-BAAE-448F-A930-E795CBF09B0E.png\" alt></p>\n<h2>完整代码</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding: utf-8 -*-</span><br><span class=\"line\"></span><br><span class=\"line\">from pyflink.dataset import ExecutionEnvironment</span><br><span class=\"line\">from pyflink.table import TableConfig, DataTypes, BatchTableEnvironment</span><br><span class=\"line\">from pyflink.table.descriptors import Schema, OldCsv, FileSystem</span><br><span class=\"line\"></span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\"># 数据源文件</span><br><span class=\"line\">source_file = &apos;source.csv&apos;</span><br><span class=\"line\">#计算结果文件</span><br><span class=\"line\">sink_file = &apos;sink.csv&apos;</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建执行环境</span><br><span class=\"line\">exec_env = ExecutionEnvironment.get_execution_environment()</span><br><span class=\"line\"># 设置并发为1 方便调试</span><br><span class=\"line\">exec_env.set_parallelism(1)</span><br><span class=\"line\">t_config = TableConfig()</span><br><span class=\"line\">t_env = BatchTableEnvironment.create(exec_env, t_config)</span><br><span class=\"line\"></span><br><span class=\"line\"># 如果结果文件已经存在,就先删除</span><br><span class=\"line\">if os.path.exists(sink_file):</span><br><span class=\"line\">    os.remove(sink_file)</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建数据源表</span><br><span class=\"line\">t_env.connect(FileSystem().path(source_file)) \\</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">                 .line_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())) \\</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())) \\</span><br><span class=\"line\">    .register_table_source(&apos;mySource&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建结果表</span><br><span class=\"line\">t_env.connect(FileSystem().path(sink_file)) \\</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">                 .field_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())</span><br><span class=\"line\">                 .field(&apos;count&apos;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())</span><br><span class=\"line\">                 .field(&apos;count&apos;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">    .register_table_sink(&apos;mySink&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 非常简单的word_count计算逻辑</span><br><span class=\"line\">t_env.scan(&apos;mySource&apos;) \\</span><br><span class=\"line\">    .group_by(&apos;word&apos;) \\</span><br><span class=\"line\">    .select(&apos;word, count(1)&apos;) \\</span><br><span class=\"line\">    .insert_into(&apos;mySink&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 执行Job</span><br><span class=\"line\">t_env.execute(&quot;wordcount&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>在执行job之前我们需要先创建一个数据源文件,在enjoyment包目录下，创建 source.csv文件，内容如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">enjoyment,Apache Flink,cool,Apache Flink</span><br></pre></td></tr></table></figure></p>\n<p>现在我们可以执行，job了：</p>\n<p><img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png\" alt></p>\n<ol>\n<li>右键word_count.py, 2. 点击运行'word_count'。\n运行之后，当控制台出现：&quot;Process finished with exit code 0&quot;，并且在enjoyment目录下出现sink.csv文件，说明我们第一个word_count就成功运行了:)\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/34546866-F865-4660-8177-A64C5A14C002.png\" alt>\n我们双击 &quot;sink.csv&quot;,查看运行结果，如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/0B58FCBF-4D98-41F9-98A3-13B31F939916.png\" alt></li>\n</ol>\n<p>到此我们Python Table API的开发环境搭建完成。</p>\n<h1>项目源代码</h1>\n<p>为了方便大家快速体验本篇内容，可以下载本篇涉及到的python项目，git地址如下：<a href=\"https://github.com/sunjincheng121/enjoyment.code\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code</a></p>\n<h1>小结</h1>\n<p>本篇是为后续Flink Python Table API 的算子介绍做铺垫，介绍如何搭建开发环境。最后以一个最简单，也是最经典的WordCount具体示例来验证开发环境。同时在开篇说道部分简单为大家分享了老子所宣扬的&quot;曲则全&quot;的人生智慧，希望大家在学习技术的同时也能学到为人的智慧！谢谢大家的对本篇博文的查阅！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/comment.png\" alt></p>\n","site":{"data":{}},"excerpt":"","more":"<h1>开篇说道 - 曲则全</h1>\n<p>&quot;曲则全，枉则直，洼则盈，敝则新，少则得，多则惑&quot; 语出《道德经》第二十二章，整句充分反映了老子的辩证思想，老子用曲则全，枉则直，洼则盈，敝则新，少则得和多则惑来讲述了&quot;道&quot;顺其自然的理论。</p>\n<p>所谓的顺其自然，有很多方面，比如前面一篇讲的&quot;上善若水&quot;，水自高而下的流淌也是顺其自然的体现，如果不顺其自然,而是阻止水的自上而下的流淌会发生什么？那就是 灾难，水灾。大家知道在 大禹治水 之前人类是如何处理洪水的？&quot;拦截&quot; 无目的的随处拦截，不是拦截不好，而是只拦截不疏通就违背自然之道了。再想想大禹治水，则是在顺应自然的同时，在疏通水道的同时在恰当的地方进行拦截，使得在不违背水自上而下的自然本性的同时合理影响水流的方向，进而造福人类。</p>\n<p>那么本篇的&quot;曲则全，枉则直，洼则盈，敝则新，少则得，多则惑&quot;如何理解呢？直译一下就是：&quot;弯曲才可保全，委屈才可伸展，低洼将可充满，敝旧将可更新，少取反而获得，多取反而迷惑&quot;。其实不难理解，比如:&quot;洼则盈&quot;, &quot;洼&quot;也就是低洼，&quot;盈&quot;就是满，只有低洼的的地势才能赢得水流，也就是只有空的杯子才能装的进水。满的杯子很难再容下其他物品。其实老子无时无刻不在论证为人之道，谦卑将自己放空，才能容纳他人，聚贤聚德。下面我们再以一个简短的故事解释&quot;曲则全&quot;来结束本篇的论道：</p>\n<p>春秋时期，齐国君主齐景公非常喜欢捕鸟，当时有个名叫 烛雏 的大臣专门为其捕鸟。但有一次由于 烛雏 的疏忽将捕来的鸟弄飞了。这使得齐景公勃然大怒，要杀了 烛雏。当时 晏子 也附和齐景王对 烛雏 说：你有三大罪状：1. 你的疏忽把鸟放跑了，这是第一罪状，2. 因为鸟飞了使得大王要去杀人，使大王背上了喜欢杀人的坏名声，这是你第二大罪状，3. 这件事情传出去天下人会认为大王把鸟看的比人命还重要，破坏了大王的威望，这是你第三大罪状。所以，大王请将 烛雏 处死吧？可想而知 齐景王并没有完全失去理智，虽然 晏子 没有直接劝阻他不要杀死 烛雏，但他听出了 晏子 的用意，所以自然赦免了 烛雏。</p>\n<p>所以真正的智者会顺势而为，见机行事，无法直言的时候就要学会迂回，这也是对老子&quot;曲则全&quot;理论的典型运用！</p>\n<h1>概要</h1>\n<p>欲善其事必利其器，要想利用Apache Flink Python Table API进行业务开发，首先要先搭建一个开发环境。本文将细致的叙述如何搭建开发环境，最后在编写一个简单的WordCount示例程序。</p>\n<h1>依赖环境</h1>\n<ul>\n<li>JDK 1.8+ (1.8.0_211)</li>\n<li>Maven 3.x (3.2.5)</li>\n<li>Scala 2.11+ (2.12.0)</li>\n<li>Python 2.7+ (2.7.16)</li>\n<li>Git 2.20+ (2.20.1)</li>\n</ul>\n<p>大家如上依赖配置如果有问题，可以在评论区留言。</p>\n<h1>安装PyFlink</h1>\n<p>我们要想利用Apache Flink Python API 进行业务开发，需要将PyFlink发布包进行安装。目前PyFlink并没有发布到Python仓库，比如著名的PyPI(正在社区讨论，<a href=\"http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Publish-the-PyFlink-into-PyPI-td30095.html\" target=\"_blank\" rel=\"noopener\">详见</a>)，所以我们需要从源码构建。</p>\n<h2>源码构建发布包</h2>\n<h3>下载flink源代码</h3>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/apache/flink.git</span><br></pre></td></tr></table></figure></p>\n<p>签出release-1.9分支(1.9版本是Apache Flink Python Table API的第一个版本）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin release-1.9</span><br><span class=\"line\">git checkout -b release-1.9 origin/release-1.9</span><br></pre></td></tr></table></figure></p>\n<p>输出如下证明已经签出了release-1.9代码：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Branch &apos;release-1.9&apos; set up to track remote branch &apos;release-1.9&apos; from &apos;origin&apos;.</span><br><span class=\"line\">Switched to a new branch &apos;release-1.9&apos;</span><br></pre></td></tr></table></figure></p>\n<h3>构建Java二进制包</h3>\n<p>查看我们在relese-1.9分支并进行源码构建发布包（JAVA）\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean install -DskipTests -Dfast</span><br></pre></td></tr></table></figure></p>\n<p>上面命令会执行一段时间(时间长短取决于你机器配置),最终输出如下信息，证明已经成功的完成发布包的构建：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">[INFO] flink-ml-lib ....................................... SUCCESS [  0.190 s]</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time: 14:47 min</span><br><span class=\"line\">[INFO] Finished at: 2019-07-25T17:38:06+08:00</span><br><span class=\"line\">[INFO] Final Memory: 537M/2085M</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></p>\n<p>在flink-dist/target/下的lib目录和opt目录中flink-python*.jar 和 flink-table*.jar 都是PyFlink必须依赖的JARs, 在下面构建PyFlink包的时候，会将这些依赖JRAs也包含在里面。</p>\n<h3>构建PyFlink发布包</h3>\n<p>一般情况我们期望以pip install的方式安装python的类库，我们要想安装PyFlink的类库，也需要构建可用于pip install的发布包。执行如下命令：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-python; python setup.py sdist</span><br></pre></td></tr></table></figure></p>\n<p>最终输出如下信息，证明是成功的：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">copying pyflink/table/types.py -&gt; apache-flink-1.9.dev0/pyflink/table</span><br><span class=\"line\">copying pyflink/table/window.py -&gt; apache-flink-1.9.dev0/pyflink/table</span><br><span class=\"line\">copying pyflink/util/__init__.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/exceptions.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/utils.py -&gt; apache-flink-1.9.dev0/pyflink/util</span><br><span class=\"line\">Writing apache-flink-1.9.dev0/setup.cfg</span><br><span class=\"line\">creating dist</span><br><span class=\"line\">Creating tar archive</span><br><span class=\"line\">removing &apos;apache-flink-1.9.dev0&apos; (and everything under it)</span><br></pre></td></tr></table></figure></p>\n<p>在dist目录的apache-flink-1.9.dev0.tar.gz就是我们可以用于pip install的PyFlink包.</p>\n<h2>安装PyFlink</h2>\n<p>上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install dist/*.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>输出如下信息，证明已经安装成功：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Building wheels for collected packages: apache-flink</span><br><span class=\"line\">  Building wheel for apache-flink (setup.py) ... done</span><br><span class=\"line\">  Stored in directory: /Users/jincheng.sunjc/Library/Caches/pip/wheels/34/b1/23/fca6e31a6de419c9c9d75a6d11df197d538c2ef67e017d79ea</span><br><span class=\"line\">Successfully built apache-flink</span><br><span class=\"line\">Installing collected packages: apache-flink</span><br><span class=\"line\">Successfully installed apache-flink-1.9.dev0</span><br></pre></td></tr></table></figure></p>\n<p>用pip命令检查是否安装成功：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IT-C02YL16BJHD2:flink-python jincheng.sunjc$ pip list</span><br><span class=\"line\">Package         Version    </span><br><span class=\"line\">--------------- -----------</span><br><span class=\"line\">apache-flink    1.9.dev0   </span><br><span class=\"line\">configparser    3.7.4      </span><br><span class=\"line\">entrypoints     0.3        </span><br><span class=\"line\">enum34          1.1.6      </span><br><span class=\"line\">flake8          3.7.8      </span><br><span class=\"line\">functools32     3.2.3.post2</span><br><span class=\"line\">kafka-python    1.4.6      </span><br><span class=\"line\">mccabe          0.6.1      </span><br><span class=\"line\">pip             19.1.1     </span><br><span class=\"line\">py4j            0.10.8.1   </span><br><span class=\"line\">pycodestyle     2.5.0      </span><br><span class=\"line\">pyflakes        2.1.1      </span><br><span class=\"line\">python-dateutil 2.8.0      </span><br><span class=\"line\">setuptools      41.0.1     </span><br><span class=\"line\">six             1.12.0     </span><br><span class=\"line\">tld             0.9.3      </span><br><span class=\"line\">typing          3.7.4      </span><br><span class=\"line\">virtualenv      16.6.1     </span><br><span class=\"line\">wheel           0.33.4</span><br></pre></td></tr></table></figure></p>\n<p>其中 <code>apache-flink 1.9.dev0</code>就是我们刚才安装的版本。</p>\n<h1>配置IDE开发环境</h1>\n<p>开发Python最好用的集成开发环境应该是PyCharm了，我们接下来介绍如果配置PyCharm环境。</p>\n<h2>下载安装包</h2>\n<p>从 <a href=\"https://www.jetbrains.com/pycharm/\" target=\"_blank\" rel=\"noopener\">https://www.jetbrains.com/pycharm/</a> 选择你需要的版本，如果你需要mac版本，直接打开如下链接下载：\n<a href=\"https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg\" target=\"_blank\" rel=\"noopener\">https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg</a></p>\n<h2>安装配置并创建项目</h2>\n<p>上面下载的应该都是可以直接执行的，一路next之后，就可以对PyCharm进行配置了。我们在启动界面选择创建一个项目，如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png\" alt>\n点击 &quot;Create New Project&quot;之后，出现如下界面：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png\" alt></p>\n<ol>\n<li>我们要项目取一个名字，比如&quot;myPyFlink&quot;。</li>\n<li>选择&quot;Existing interpreter&quot;。</li>\n<li>配置&quot;interpreter&quot; <strong>非常重要</strong>,这个配置必须和你刚才安装PyFlink的时候(pip install)的python完全一致。</li>\n<li>如果没有你需要的python版本，可以点击 4 所示位置进行添加。</li>\n<li>如何知道你pip install对应的python，请使用5所示的命令&quot;which is python&quot;进行查看。</li>\n</ol>\n<p>如果需要进行4所需要的操作，点击之后界面如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/8308BB64-2AB2-480C-9B2E-4AADD515F183.png\" alt>\n选择配置&quot;System Interpreter&quot;,并配置&quot;which is python&quot;说输出的路径。\n点击 &quot;OK&quot;。\n在顺利完成上面配置之后，我们点击 &quot;Create&quot;，完成项目的创建，如下图：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/77394C3C-D401-4DE6-A005-D4ADFFBFB892.png\" alt>\n我们可以核实一下，我们依赖的External Libraries是\n&quot;which is python&quot;说输出的Python。</p>\n<h1>开发WordCount</h1>\n<p>如果上面的过程一切顺利，那么我们的器已经很锋利了，开始善其事了，新建一个package,名为enjoyment,然后新建一个word_count.py。如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png\" alt>\n然后，在弹出的界面写入word_count之后，点击 &quot;OK&quot;。\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/E371FA28-BAAE-448F-A930-E795CBF09B0E.png\" alt></p>\n<h2>完整代码</h2>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding: utf-8 -*-</span><br><span class=\"line\"></span><br><span class=\"line\">from pyflink.dataset import ExecutionEnvironment</span><br><span class=\"line\">from pyflink.table import TableConfig, DataTypes, BatchTableEnvironment</span><br><span class=\"line\">from pyflink.table.descriptors import Schema, OldCsv, FileSystem</span><br><span class=\"line\"></span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\"># 数据源文件</span><br><span class=\"line\">source_file = &apos;source.csv&apos;</span><br><span class=\"line\">#计算结果文件</span><br><span class=\"line\">sink_file = &apos;sink.csv&apos;</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建执行环境</span><br><span class=\"line\">exec_env = ExecutionEnvironment.get_execution_environment()</span><br><span class=\"line\"># 设置并发为1 方便调试</span><br><span class=\"line\">exec_env.set_parallelism(1)</span><br><span class=\"line\">t_config = TableConfig()</span><br><span class=\"line\">t_env = BatchTableEnvironment.create(exec_env, t_config)</span><br><span class=\"line\"></span><br><span class=\"line\"># 如果结果文件已经存在,就先删除</span><br><span class=\"line\">if os.path.exists(sink_file):</span><br><span class=\"line\">    os.remove(sink_file)</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建数据源表</span><br><span class=\"line\">t_env.connect(FileSystem().path(source_file)) \\</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">                 .line_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())) \\</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())) \\</span><br><span class=\"line\">    .register_table_source(&apos;mySource&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建结果表</span><br><span class=\"line\">t_env.connect(FileSystem().path(sink_file)) \\</span><br><span class=\"line\">    .with_format(OldCsv()</span><br><span class=\"line\">                 .field_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())</span><br><span class=\"line\">                 .field(&apos;count&apos;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">    .with_schema(Schema()</span><br><span class=\"line\">                 .field(&apos;word&apos;, DataTypes.STRING())</span><br><span class=\"line\">                 .field(&apos;count&apos;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">    .register_table_sink(&apos;mySink&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 非常简单的word_count计算逻辑</span><br><span class=\"line\">t_env.scan(&apos;mySource&apos;) \\</span><br><span class=\"line\">    .group_by(&apos;word&apos;) \\</span><br><span class=\"line\">    .select(&apos;word, count(1)&apos;) \\</span><br><span class=\"line\">    .insert_into(&apos;mySink&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 执行Job</span><br><span class=\"line\">t_env.execute(&quot;wordcount&quot;)</span><br></pre></td></tr></table></figure></p>\n<h2>执行并查看运行结果</h2>\n<p>在执行job之前我们需要先创建一个数据源文件,在enjoyment包目录下，创建 source.csv文件，内容如下：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">enjoyment,Apache Flink,cool,Apache Flink</span><br></pre></td></tr></table></figure></p>\n<p>现在我们可以执行，job了：</p>\n<p><img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png\" alt></p>\n<ol>\n<li>右键word_count.py, 2. 点击运行'word_count'。\n运行之后，当控制台出现：&quot;Process finished with exit code 0&quot;，并且在enjoyment目录下出现sink.csv文件，说明我们第一个word_count就成功运行了:)\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/34546866-F865-4660-8177-A64C5A14C002.png\" alt>\n我们双击 &quot;sink.csv&quot;,查看运行结果，如下：\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/0B58FCBF-4D98-41F9-98A3-13B31F939916.png\" alt></li>\n</ol>\n<p>到此我们Python Table API的开发环境搭建完成。</p>\n<h1>项目源代码</h1>\n<p>为了方便大家快速体验本篇内容，可以下载本篇涉及到的python项目，git地址如下：<a href=\"https://github.com/sunjincheng121/enjoyment.code\" target=\"_blank\" rel=\"noopener\">https://github.com/sunjincheng121/enjoyment.code</a></p>\n<h1>小结</h1>\n<p>本篇是为后续Flink Python Table API 的算子介绍做铺垫，介绍如何搭建开发环境。最后以一个最简单，也是最经典的WordCount具体示例来验证开发环境。同时在开篇说道部分简单为大家分享了老子所宣扬的&quot;曲则全&quot;的人生智慧，希望大家在学习技术的同时也能学到为人的智慧！谢谢大家的对本篇博文的查阅！</p>\n<h1>关于评论</h1>\n<p>如果你没有看到下面的评论区域，请进行翻墙！\n<img src=\"/2019/07/25/Apache Flink 说道系列- Python Table API 开发环境搭建/comment.png\" alt></p>\n"},{"title":"Apache Flink 说道系列 - PyFlink 作业的多种部署模式","date":"2020-01-02T09:15:18.000Z","_content":"\n# 开篇说道\n老子说：“致虚极，守静笃。万物并作，吾以观其复\"。 字里行间是告诉人们要尽量使心灵达到虚寂的极致，持之以恒的坚守宁静。老子认为“虚”和“静”是心灵的本初的状态，也应该是一种常态，即，是一种没有心机，没有成见，没有外界名利，物质诱惑，而保持的状态。当达到这个状态之后，就可以静观万物的蓬勃发展，了解万物的循环往复的变化，通晓自然之理，体悟自然之道。\n\n\"致虚极，守静笃\" 并不代表对任何事情无动于衷，而是通过 \"致虚极，守静笃\" 的方式了解万物变化的根源，就好比，看到新芽不惊，看到落叶不哀，因为我们知道，落叶归根，重归泥土之后，还会再破土而出，开始新的轮回。\n\n在2020年1月1日零点我发了一条朋友圈，对自己的祝福是：\"㊗️ 2020 安\"。\n![](15776987959455/15779569378844.jpg)\n当然这里我也祝福所有看到这篇博客的你: \"㊗️ 2020 安\"。 祝福大家 \"安\"，也即祝福大家 能够在新的一年，追求内心的修炼，摒去纷纷复杂的物欲干扰，拂去心灵的灰尘，保持自己的赤子之心！ \n\n# 道破 \"天机\"\n前面一些博客大多介绍PyFlink的功能开发，比如，如何使用各种算子(Join/Window/AGG etc.)，如何使用各种Connector(Kafka, CSV, Socket etc.)，还有一些实际的案例。这些都停留在开发阶段，一旦开发完成，我们就面临激动人心的时刻，那就是 将我们精心设计开发的作业进行部署，那么问题来了，你知道怎样部署PyFlink的作业吗？这篇将为大家全面介绍部署PyFlink作业的各种模式。\n\n# 组件栈回顾\n![](15776987959455/15777810183663.jpg)\n\n\n上面的组件栈除了PyFlink是第一次添加上去，其他部分大家应该非常熟悉了。目前PyFlink基于Java的Table API之上，同时在Runtime层面有Python的算子和执行容器。那么我们聚焦重点，看最底层的 Deploy部分，上图我们分成了三种部署模式，Local/Cluster/Cloud，其中Local模式还有2种不同方式，一是SingleJVM，也即是MiniCluster, 前面博客里面运行示例所使用的就是MiniCluster。二是SingleNode，也就是虽然是集群模式，但是所有角色都在一台机器上。下面我们简单介绍一下上面这几种部署模式的区别：\n- Local-SingleJVM 模式- 该模式大多是开发测试阶段使用的方式，所有角色TM，JM等都在同一个JVM里面。\n- Local-SingleNode 模式 - 意在所有角色都运行在同一台机器，直白一点就是从运行的架构上看，这种模式虽然是分布式的，但集群节点只有1个，该模式大多是测试和IoT设备上进行部署使用。\n- Cluster 模式 - 也就是我们经常用于投产的分布式部署方式，上图根据对资源管理的方式不同又分为了多种，如：Standalone 是Flink自身进行资源管理，YARN，顾名思义就是利用资源管理框架Yarn来负责Flink运行资源的分配，还有结合Kubernetes等等。\n- Cloud 模式- 该部署模式是结合其他云平台进行部署。\n\n接下来我们看看PyFlink的作业可以进行怎样的模式部署？\n\n# 环境依赖\n- JDK 1.8+ (1.8.0_211)\n- Maven 3.x (3.2.5)\n- Scala 2.11+ (2.12.0)\n- Python 3.5+ (3.7.6)\n- Git 2.20+ (2.20.1)\n\n# 源码构建及安装\n在Apache Flink 1.10 发布之后，我们除了源码构建之外，还支持直接利用pip install 安装PyFlink。那么现在我们还是以源码构建的方式进行今天的介绍。\n- 下载源码\n\n```\ngit clone https://github.com/apache/flink.git\n```\n- 签出release-1.10分支(1.10版本是PyFlink的第二个版本）\n\n```\ngit fetch origin release-1.10\ngit checkout -b release-1.10 origin/release-1.10\n```\n- 构建编译\n\n```\nmvn clean package -DskipTests \n```\n如果一起顺利，你会最终看到如下信息：\n\n```\n...\n...\n[INFO] flink-walkthrough-table-scala ...................... SUCCESS [  0.070 s]\n[INFO] flink-walkthrough-datastream-java .................. SUCCESS [  0.081 s]\n[INFO] flink-walkthrough-datastream-scala ................. SUCCESS [  0.067 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  16:22 min\n[INFO] Finished at: 2019-12-31T10:37:21+08:00\n[INFO] ------------------------------------------------------------------------\n```\n- 构建PyFlink发布包\n\n上面我们构建了Java的发布包，接下来我们构建PyFlink的发布包,如下：\n\n```\ncd flink-Python; Python setup.py sdist\n```\n最终输出如下信息，证明是成功的：\n\n```\ncopying pyflink/util/exceptions.py -> apache-flink-1.10.dev0/pyflink/util\ncopying pyflink/util/utils.py -> apache-flink-1.10.dev0/pyflink/util\nWriting apache-flink-1.10.dev0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'apache-flink-1.10.dev0' (and everything under it)\n```\n在dist目录的apache-flink-1.10.dev0.tar.gz就是我们可以用于`pip install`的PyFlink包.\n\n- 安装PyFlink\n\n上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，检测是否之前已经安装过PyFlink，如下命令：\n\n```\npip3 list|grep flink\n...\nflink                         1.0      \npyflink-demo-connector        0.1 \n```\n上面信息说明我本机已经安装过PyFlink，我们要先删除，如下：\n\n\n```\npip3 uninstall flink\n```\n删除以前的安装之后，我们再安装新的如下：\n```\npip3 install dist/*.tar.gz\n\n...\nSuccessfully built apache-flink\nInstalling collected packages: apache-flink\nSuccessfully installed apache-flink-1.10.dev0\n```\n我们再用list命令检查一遍：\n\n\n```\npip3 list|grep flink\n\n...\napache-flink                  1.10.dev0\npyflink-demo-connector        0.1 \n```\n其中`pyflink-demo-connector` 是我以前做实验时候的安装，对本篇没有影响。\n\n# 安装Apache Beam 依赖\n\n我们需要使用Python3.5+ 版本，检验一下Python版本，如下：\n```\njincheng.sunjc$ Python --version\nPython 3.7.6\n```\n\n我本机是Python3.7.6，现在我们需要安装Apache Beam，如下：\n\n```\npython -m pip install apache-beam==2.15.0\n\n...\nInstalling collected packages: apache-beam\nSuccessfully installed apache-beam-2.15.0\n```\n如果顺利的出现上面信息，说明Apache-beam已经安装成功。\n\n# PyFlink示例作业\n\n接下来我们开发一个简单的PyFlink作业，源码如下：\n\n```\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nfrom pyflink.table import BatchTableEnvironment, EnvironmentSettings\nfrom pyflink.table.descriptors import FileSystem, OldCsv, Schema\nfrom pyflink.table.types import DataTypes\nfrom pyflink.table.udf import udf\n\n\ndef word_count():\n   environment_settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()\n   t_env = BatchTableEnvironment.create(environment_settings=environment_settings)\n\n   # register Results table in table environment\n   tmp_dir = tempfile.gettempdir()\n   result_path = tmp_dir + '/result'\n   if os.path.exists(result_path):\n       try:\n           if os.path.isfile(result_path):\n               os.remove(result_path)\n           else:\n               shutil.rmtree(result_path)\n       except OSError as e:\n           logging.error(\"Error removing directory: %s - %s.\", e.filename, e.strerror)\n\n   logging.info(\"Results directory: %s\", result_path)\n\n   # we should set the Python verison here if `Python` not point\n   t_env.get_config().set_Python_executable(\"Python3\")\n\n   t_env.connect(FileSystem().path(result_path)) \\\n       .with_format(OldCsv()\n                    .field_delimiter(',')\n                    .field(\"city\", DataTypes.STRING())\n                    .field(\"sales_volume\", DataTypes.BIGINT())\n                    .field(\"sales\", DataTypes.BIGINT())) \\\n       .with_schema(Schema()\n                    .field(\"city\", DataTypes.STRING())\n                    .field(\"sales_volume\", DataTypes.BIGINT())\n                    .field(\"sales\", DataTypes.BIGINT())) \\\n       .register_table_sink(\"Results\")\n\n   @udf(input_types=DataTypes.STRING(), result_type=DataTypes.ARRAY(DataTypes.STRING()))\n   def split(input_str: str):\n       return input_str.split(\",\")\n\n   @udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())\n   def get(arr, index):\n       return arr[index]\n\n   t_env.register_function(\"split\", split)\n   t_env.register_function(\"get\", get)\n\n   t_env.get_config().get_configuration().set_string(\"parallelism.default\", \"1\")\n\n   data = [(\"iPhone 11,30,5499,Beijing\", ),\n           (\"iPhone 11 Pro,20,8699,Guangzhou\", ),\n           (\"MacBook Pro,10,9999,Beijing\", ),\n           (\"AirPods Pro,50,1999,Beijing\", ),\n           (\"MacBook Pro,10,11499,Shanghai\", ),\n           (\"iPhone 11,30,5999,Shanghai\", ),\n           (\"iPhone 11 Pro,20,9999,Shenzhen\", ),\n           (\"MacBook Pro,10,13899,Hangzhou\", ),\n           (\"iPhone 11,10,6799,Beijing\", ),\n           (\"MacBook Pro,10,18999,Beijing\", ),\n           (\"iPhone 11 Pro,10,11799,Shenzhen\", ),\n           (\"MacBook Pro,10,22199,Shanghai\", ),\n           (\"AirPods Pro,40,1999,Shanghai\", )]\n   t_env.from_elements(data, [\"line\"]) \\\n       .select(\"split(line) as str_array\") \\\n       .select(\"get(str_array, 3) as city, \"\n               \"get(str_array, 1).cast(LONG) as count, \"\n               \"get(str_array, 2).cast(LONG) as unit_price\") \\\n       .select(\"city, count, count * unit_price as total_price\") \\\n       .group_by(\"city\") \\\n       .select(\"city, \"\n               \"sum(count) as sales_volume, \"\n               \"sum(total_price) as sales\") \\\n       .insert_into(\"Results\")\n\n   t_env.execute(\"word_count\")\n\n\nif __name__ == '__main__':\n   logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n   word_count()\n```\n接下来我们就介绍如何用不同部署模式运行PyFlink作业！\n\n# Local-SingleJVM 模式部署\n该模式多用于开发测试阶段，简单的利用`Python pyflink_job.py`命令，PyFlink就会默认启动一个Local-SingleJVM的Flink环境来执行作业，如下：\n\n![](15776987959455/15777784863901.jpg)\n首先确认你Python是3.5+，然后执行上面的PyFlink作业`Python deploy_demo.py`,结果写入到本地文件，然后`cat`计算结果，如果出现如图所示的结果，则说明准备工作已经就绪。:),如果有问题，欢迎留言！\n这里运行时SingleJVM，在运行这个job时候大家可以查看java进程：\n\n![](15776987959455/15777821361064.jpg)\n我们发现只有一个JVM进程，里面包含了所有Flink所需角色。\n\n\n# Local-SingleNode 模式部署\n这种模式一般用在单机环境中进行部署，如IoT设备中，我们从0开始进行该模式的部署操作。我们进入到`flink/build-target`目录，执行如下命令(个人爱好，我把端口改成了8888)：\n\n``` \njincheng:build-target jincheng.sunjc$ bin/start-cluster.sh \n...\nStarting cluster.\nStarting standalonesession daemon on host jincheng.local.\n```\n查看一下Flink的进程：\n\n![](15776987959455/15777817105172.jpg)\n\n我们发现有TM和JM两个进程，虽然在一台机器(Local）但是也是一个集群的架构。\n上面信息证明已经启动完成，我们可以查看web界面：[http://localhost:8888/](http://localhost:8888/)（我个人爱好端口是8888，默认是8080）, 如下：\n![](15776987959455/15777816500228.jpg)\n\n目前集群环境已经准备完成，我们看如果将作业部署到集群中，一条简单的命令，如下：\n\n```\nbin/flink run -m localhost:8888 -py ~/deploy_demo.py\n```\n这里如果你不更改端口可以不添加`-m` 选项。如果一切顺利，你会得到如下输出：\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -m localhost:8888 -py ~/deploy_demo.py \nResults directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\nJob has been submitted with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6\nProgram execution finished\nJob with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6 has finished.\nJob Runtime: 5389 ms\n```\n其中`/var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result` 目录是计算结果目录，我们可以产看一下，如下：\n\n```\njincheng:build-target jincheng.sunjc$  cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\nBeijing,110,622890\nGuangzhou,20,173980\nShanghai,90,596910\nShenzhen,30,317970\nHangzhou,10,138990\n```\n同时我们也可以在WebUI上面进行查看，在完成的job列表中，显示如下：\n![](15776987959455/15777853568978.jpg)\n到此，我们完成了在Local模式，其实也是只有一个节点的Standalone模式下完成PyFlink的部署。\n最后我们为了继续下面的操作，请停止集群：\n\n```\njincheng:build-target jincheng.sunjc$ bin/stop-cluster.sh\nStopping taskexecutor daemon (pid: 45714) on host jincheng.local.\nStopping standalonesession daemon (pid: 45459) on host jincheng.local.\n```\n\n# Cluster YARN 模式部署\n这个模式部署，我们需要一个YARN环境，我们一切从简，以单机部署的方式准备YARN环境，然后再与Flink进行集成。\n## 准备YARN环境\n- 安装Hadoop\n\n我本机是mac系统，所以我偷懒一下直接用brew进行安装：\n\n```\njincheng:bin jincheng.sunjc$ brew install Hadoop\nUpdating Homebrew...\n==> Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==> Updated Formulae\nPython ✔        doxygen         minio           ntopng          typescript\ncertbot         libngspice      mitmproxy       ooniprobe\ndoitlive        minimal-racket  ngspice         openimageio\n\n==> Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-\n==> Downloading from http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/\n######################################################################## 100.0%\n\n🍺  /usr/local/Cellar/Hadoop/3.2.1: 22,397 files, 815.6MB, built in 5 minutes 12 seconds\n```\n完成之后，检验一下hadoop版本：\n\n```\njincheng:bin jincheng.sunjc$ hadoop version\nHadoop 3.2.1\n```\n超级顺利，hadoop被安装到了`/usr/local/Cellar/hadoop/3.2.1/`目录下，`brew`还是很能提高生产力啊～\n\n- 配置免登(SSH)\n\nMac系统自带了ssh，我们可以简单配置一下即可，我们先打开远程登录。 `系统偏好设置` -> `共享` 中，左边勾选 `远程登录` ，右边选择 `仅这些用户` （选择`所有用户`更宽松），并添加当前用户。\n\n```\njincheng:bin jincheng.sunjc$ whoami\njincheng.sunjc\n```\n我当前用户是 `jincheng.sunjc`。 配置图如下：\n\n![](15776987959455/15777878055973.jpg)\n\n然后生产证书，如下操作：\n\n```\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\n/Users/jincheng.sunjc/.ssh/id_rsa already exists.\nOverwrite (y/n)? y\nYour identification has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.\nYour public key has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:IkjKkOjfMx1fxWlwtQYg8hThph7Xlm9kPutAYFmQR0A jincheng.sunjc@jincheng.local\nThe key's randomart image is:\n+---[RSA 2048]----+\n|       ..EB=.o.. |\n|..      =.+.+ o .|\n|+ .      B.  = o |\n|+o .    + o + .  |\n|.o. . .+S. * o   |\n|  . ..o.= + =    |\n|   . + o . . =   |\n|      o     o o  |\n|            .o   |\n+----[SHA256]-----+\n```\n接下来将公钥追加到如下文件，并修改文件权限：\n\n```\njincheng.sunjc$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\njincheng.sunjc$ chmod 0600 ~/.ssh/authorized_keys\n```\n利用ssh localhost验证，看到 Last login: 字样为ssh成功\n \n```\njincheng:~ jincheng.sunjc$ ssh localhost\nPassword:\nLast login: Tue Dec 31 18:26:48 2019 from ::1\n```\n- 设置环境变量\n\n设置JAVA_HOME,HADOOP_HOME和HADOOP_CONF_DIR，`vi ~/.bashrc`:\n  \n```\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home\n\nexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexec\n\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n```\nNOTE: 后续操作要确保的terminal环境变量是生效哦， 如果不生效可以执行 `source ~/.bashrc`。:)\n\n- 修改配置\n\n    1) 修改core-site.xml\n     \n```\n<configuration>\n   <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/tmp</value>\n   </property>\n   <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n```\n\n    2) 修改hdfs-site.xml\n    \n```\n<configuration>\n\n    <property>\n        <name>dfs.namenode.name.dir</name>\n        <value>/tmp/hadoop/name</value>\n    </property>\n    \n    <property>\n        <name>dfs.datanode.data.dir</name>\n        <value>/tmp/hadoop/data</value>\n    </property>\n    \n</configuration>\n```\n\n    3) 修改yarn-site.xml\n\n配置 YARN 作为资源管理框架：\n```\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>    \n    </property>\n    <property>\n        <name>yarn.nodemanager.env-whitelist</name>  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n    </property>\n</configuration>\n```\n\n简单的配置已经完成，我们执行一下简单命令启动环境：\n\n- 格式化文档系统：\n\n```\njincheng:libexec jincheng.sunjc$ hadoop namenode -format\n...\n...\n2019-12-31 18:58:53,260 INFO namenode.NameNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at jincheng.local/127.0.0.1\n************************************************************/\n\n```\n\n- 启动服务：\n\n我们先启动hdf再启动yarn，如下图：\n![](15776987959455/15778389671396.jpg)\n\nOkay,一切顺利的话，我们会启动namenodes，datanodes，resourcemanager和nodemanagers。我们有几个web界面可以查看，如下：\n\n 1). Overview 界面， [http://localhost:9870 ](http://localhost:9870 )如下：\n![](15776987959455/15778373195478.jpg)\n\n\n 2). NodeManager界面， [http://localhost:8042](http://localhost:8042)，如下：\n![-w649](15776987959455/15778373437959.jpg)\n\n 3). ResourceManager 管理界面 [http://localhost:8088/](http://localhost:8088/),如下：\n![-w1054](15776987959455/15777972228895.jpg)\n\n目前YARN的环境已经准备完成，我们接下来看如何与Flink进行集成。\n\n## Flink集成Hadoop包\n切换到编译结果目录下`flink/build-target`,并将haddop的JAR包放到`lib`目录。\n在官网下载hadoop包，\n\n```\ncd lib;\ncurl https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar > flink-shaded-hadoop-2-uber-2.8.3-7.0.jar\n```\n下载后，lib目录下文件如下：\n![](15776987959455/15777916683830.jpg)\n\n到现在为止我们可以提交PyFlink的作业到由YARN进行资源分配的集群了。但为了确保集群上有正确的Python环境我们最好打包一个Python环境到集群上面。因为大部分情况下我们无法得知yarn集群上的Python版本是否符合我们的要求(Python 3.5+，装有apache-beam 2.15.0)，因此我们需要打包一个符合条件的Python环境，并随job文件提交到yarn集群上。\n\n## 打包Python环境\n再次检查一下当前Python的版本是否3.5+，如下：\n```\njincheng:lib jincheng.sunjc$ Python\nPython 3.7.6 (default, Dec 31 2019, 09:48:30) \n```\n由于这个Python环境是用于集群的，所以打包时的系统需要和集群一致。如果不一致，比如集群是linux而本机是mac，我们需要在虚拟机或者docker中打包。以下列出两种情况的示范方法，读者根据需求选择一种即可。\n\n### 本地打包（集群和本机操作系统一致时）\n如果集群所在机器的操作系统和本地一致（都是mac或者都是linux），直接通过virtualenv打包一个符合条件的Python环境：\n- 安装virtualenv\n使用`python -m pip install virtualenv` 进行安装如下：\n\n```\njincheng:tmp jincheng.sunjc$ python -m pip install virtualenv\nCollecting virtualenv\n  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)\n     |████████████████████████████████| 3.4MB 2.0MB/s \nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.7.9\n```\n我本地环境已经成功安装。\n\n- 创建Python环境\n用virtualenv以always-copy方式建立一个全新的Python环境，名字随意，以venv为例，`virtualenv --always-copy venv`:\n\n```\njincheng:tmp jincheng.sunjc$ virtualenv --always-copy venv\nUsing base prefix '/usr/local/Cellar/Python/3.7.6/Frameworks/Python.framework/Versions/3.7'\nNew Python executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python3.7\nAlso creating executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python\nInstalling setuptools, pip, wheel...\ndone.\n```\n- 在新环境中安装apache-beam 2.15.0\n使用`venv/bin/pip install apache-beam==2.15.0`进行安装：\n\n```\njincheng:tmp jincheng.sunjc$ venv/bin/pip install apache-beam==2.15.0\nCollecting apache-beam==2.15.0\n...\n...\nSuccessfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7\n```\n上面信息已经说明我们成功的在Python环境中安装了apache-beam==2.15.0。接下来我们打包Python环境。\n- 打包Python环境\n 我们将Python打包成zip文件，`zip -r venv.zip venv` 如下：\n \n```\nzip -r venv.zip venv\n...\n...\n  adding: venv/lib/Python3.7/re.py (deflated 68%)\n  adding: venv/lib/Python3.7/struct.py (deflated 46%)\n  adding: venv/lib/Python3.7/sre_parse.py (deflated 80%)\n  adding: venv/lib/Python3.7/abc.py (deflated 72%)\n  adding: venv/lib/Python3.7/_bootlocale.py (deflated 63%)\n```\n查看一下zip大小：\n\n```\njincheng:tmp jincheng.sunjc$ du -sh venv.zip \n 81M\tvenv.zip\n```\n这个大小实在太大了，核心问题是Beam的包非常大，后面我会持续在Beam社区提出优化建议。我们先忍一下:(。\n\n### Docker中打包（比如集群为linux，本机为mac时）\n我们选择在docker中打包，可以从以下链接下载最新版docker并安装：\n[https://download.docker.com/mac/stable/Docker.dmg](https://download.docker.com/mac/stable/Docker.dmg)安装完毕后重启终端，执行`docker version`确认docker安装成功：\n\n```\njincheng:tmp jincheng.sunjc$ docker version\nClient: Docker Engine - Community\n Version:           19.03.4\n API version:       1.40\n Go version:        go1.12.10\n Git commit:        9013bf5\n Built:             Thu Oct 17 23:44:48 2019\n OS/Arch:           darwin/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.4\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.12.10\n  Git commit:       9013bf5\n  Built:            Thu Oct 17 23:50:38 2019\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.2.10\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\n runc:\n  Version:          1.0.0-rc8+dev\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n```\n\n- 启动容器\n我们启动一个Python 3.7版本的容器如果是第一次启动可能需要较长时间来拉取镜像：\n`docker run -it Python:3.7 /bin/bash`, 如下：\n\n```\njincheng:libexec jincheng.sunjc$  docker run -it Python:3.7 /bin/bash\nUnable to find image 'Python:3.7' locally\n3.7: Pulling from library/Python\n8f0fdd3eaac0: Pull complete \nd918eaefd9de: Pull complete \n43bf3e3107f5: Pull complete \n27622921edb2: Pull complete \ndcfa0aa1ae2c: Pull complete \nbf6840af9e70: Pull complete \n167665d59281: Pull complete \nffc544588c7f: Pull complete \n4ebe99df65fe: Pull complete \nDigest: sha256:40d615d7617f0f3b54614fd228d41a891949b988ae2b452c0aaac5bee924888d\nStatus: Downloaded newer image for Python:3.7\n```\n\n- 容器中安装virtualenv\n我们在刚才启动的容器中安装virtualenv， `pip install virtualenv`,如下：\n```\nroot@1b48d2b526ae:/# pip install virtualenv\nCollecting virtualenv\n  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)\n     |████████████████████████████████| 3.4MB 2.0MB/s \nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.7.9\nroot@1b48d2b526ae:/# \n```\n\n- 创建Python环境\n以always copy方式建立一个全新的Python环境，名字随意，以venv为例，`virtualenv --always-copy venv`, 如下：\n\n\n```\nroot@1b48d2b526ae:/# virtualenv --always-copy venv\nUsing base prefix '/usr/local'\nNew Python executable in /venv/bin/Python\nInstalling setuptools, pip, wheel...\ndone.\nroot@1b48d2b526ae:/# \n```\n\n- 安装Apache Beam\n 在新的Python环境中安装apache-beam 2.15.0，`venv/bin/pip install apache-beam==2.15.0`,如下：\n \n```\nroot@1b48d2b526ae:/# venv/bin/pip install apache-beam==2.15.0\nCollecting apache-beam==2.15.0\n...\n...\nSuccessfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7\n```\n- 查看docker中的Python环境\n用`exit`命令退出容器，用`docker ps -a`找到docker容器的id，用于拷贝文件,如下：\n\n```\nroot@1b48d2b526ae:/# exit\nexit\njincheng:libexec jincheng.sunjc$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES\n1b48d2b526ae        Python:3.7          \"/bin/bash\"         7 minutes ago       Exited (0) 8 seconds ago                       elated_visvesvaraya\n```\n由于刚刚结束，一般来说是列表中的第一条，可以根据容器的镜像名Python:3.7来分辨。我们记下最左边的容器ID。如上是`1b48d2b526ae`。\n\n- 打包Python环境\n 从将容器中的Python环境拷贝出来，我们切换到`flink/build-target`录下，拷贝 `docker cp 1b48d2b526ae:/venv ./`并打包`zip -r venv.zip venv`。\n 最终`flink/build-target`录下生成`venv.zip`。\n \n\n## 部署作业\n终于到部署作业的环节了:), flink on yarn支持两种模式，per-job和session。per-job模式在提交job时会为每个job单独起一个flink集群，session模式先在yarn上起一个flink集群，之后提交job都提交到这个flink集群。\n\n### Pre-Job 模式部署作业\n执行以下命令，以Pre-Job模式部署PyFlink作业：\n`bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py`,如下：\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py\n2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\n2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\nResults directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\n2020-01-02 13:04:55,945 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032\n2020-01-02 13:04:56,049 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar\n2020-01-02 13:05:01,153 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.\n2020-01-02 13:05:01,177 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1}\n2020-01-02 13:05:01,294 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is 'file'. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system\n2020-01-02 13:05:02,600 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0004\n2020-01-02 13:05:02,971 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0004\n2020-01-02 13:05:02,972 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated\n2020-01-02 13:05:02,975 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED\n2020-01-02 13:05:23,138 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.\n2020-01-02 13:05:23,140 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:61616 of application 'application_1577936885434_0004'.\nJob has been submitted with JobID a41d82194a500809fd715da8f29894a0\nProgram execution finished\nJob with JobID a41d82194a500809fd715da8f29894a0 has finished.\nJob Runtime: 35576 ms\n```\n上面信息已经显示运行完成，在Web界面可以看到作业状态：\n![](15776987959455/15779417223687.jpg)\n\n我们再检验一下计算结果`cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result`：\n![](15776987959455/15779417681405.jpg)\n\n到这里，我们以Pre-Job的方式成功部署了PyFlink的作业！相比提交到本地Standalone集群，多了三个参数，我们简单说明如下：\n\n| 参数                               | 说明                                                  |\n|----------------------------------|-----------------------------------------------------|\n| -m yarn-cluster                  | 以Per-Job模式部署到yarn集群                                 |\n| -pyarch venv.zip                 | 将当前目录下的venv.zip上传到yarn集群                            |\n| -pyexec venv.zip/venv/bin/Python | 指定venv.zip中的Python解释器来执行Python UDF，路径需要和zip包内部结构一致。 |\n\n### Session 模式部署作业\n以Session模式部署作业也非常简单，我们实际操作一下：\n\n```\njincheng:build-target jincheng.sunjc$ bin/yarn-session.sh \n2020-01-02 13:58:53,049 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.memory.process.size, 1024m\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1\n2020-01-02 13:58:53,051 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.execution.failover-strategy, region\n2020-01-02 13:58:53,413 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2020-01-02 13:58:53,476 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to jincheng.sunjc (auth:SIMPLE)\n2020-01-02 13:58:53,509 INFO  org.apache.flink.runtime.security.modules.JaasModule          - Jaas file will be created as /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/jaas-3848984206030141476.conf.\n2020-01-02 13:58:53,521 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\n2020-01-02 13:58:53,562 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032\n2020-01-02 13:58:58,803 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.\n2020-01-02 13:58:58,824 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1}\n2020-01-02 13:59:03,975 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is 'file'. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system\n2020-01-02 13:59:04,779 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0005\n2020-01-02 13:59:04,799 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0005\n2020-01-02 13:59:04,799 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated\n2020-01-02 13:59:04,801 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED\n2020-01-02 13:59:24,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.\n2020-01-02 13:59:24,713 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application 'application_1577936885434_0005'.\nJobManager Web Interface: http://localhost:62247\n\n```\n执行成功后不会返回，但会启动一个JoBManager Web，地址如上`http://localhost:62247`，可复制到浏览器查看:\n![](15776987959455/15779449909178.jpg)\n\n\n我们可以修改conf/flink-conf.yaml中的配置参数。如果要更改某些内容，请参考[官方文档](https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html)。接下来我们提交作业，首先按组合键Ctrl+Z将yarn-session.sh进程切换到后台，并执行bg指令让其在后台继续执行, 然后执行以下命令，即可向Session模式的flink集群提交job `bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py`：\n\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py\n\n2020-01-02 14:10:48,285 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application 'application_1577936885434_0005'.\nJob has been submitted with JobID bea33b7aa07c0f62153ab5f6e134b6bf\nProgram execution finished\nJob with JobID bea33b7aa07c0f62153ab5f6e134b6bf has finished.\nJob Runtime: 34405 ms\n```\n如果在打印`finished`之前查看之前的web页面，我们会发现Session集群会有一个正确运行的作业，如下：\n![](15776987959455/15779454827330.jpg)\n如果已经运行完成，那么我们应该会看到状态也变成结束：\n![](15776987959455/15779456315383.jpg)\n\n相比per job模式提交，少了”-m”参数。因为之前已经启动了yarn-session.sh，所以flink默认会向yarn-session.sh启动的集群上提交job。执行完毕后，别忘了关闭yarn-session.sh（session 模式）：先将yarn-session.sh调到前台，执行`fg`,然后在再按Ctrl+C结束进程或者执行`stop`，结束时yarn上的集群也会被关闭。\n\n# Docker 模式部署\n我们还可以将Flink Python job打包成docker镜像，然后使用docker-compose或者Kubernetes部署执行，由于现在的docker镜像打包工具并没有完美支持运行Python UDF，因此我们需要往里面添加一些额外的文件。首先是一个仅包含PythonDriver类的jar包. 我们在`build-target`目录下执行如下命令：\n\n```\njincheng:build-target jincheng.sunjc$ mkdir temp\njincheng:build-target jincheng.sunjc$ cd temp\njincheng:temp jincheng.sunjc$ unzip ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar org/apache/flink/client/Python/PythonDriver.class\nArchive:  ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar\n  inflating: org/apache/flink/client/Python/PythonDriver.class \n```\n解压之后，我们在进行压缩打包：\n\n```\njincheng:temp jincheng.sunjc$ zip Python-driver.jar org/apache/flink/client/Python/PythonDriver.class\n  adding: org/apache/flink/client/Python/PythonDriver.class (deflated 56%)\n```\n![](15776987959455/15779465411786.jpg)\n我们得到`Python-driver.jar`。然后下载一个pyArrow的安装文件(我准备了一个大家下载直接使用即可 [pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl](https://github.com/sunjincheng121/enjoyment.code/blob/master/share_resource/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl)。执行以下命令构建Docker镜像，需要作为artifacts引入的文件有作业文件，Python-driver的jar包和pyarrow安装文件,`./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist`(进入`flink/flink-container/docker`目录)\n\n```\njincheng:docker jincheng.sunjc$ ./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist\nUsing flink dist: ../../flink-dist/target/flink-*-bin\na .\na ./flink-1.10-SNAPSHOT\na ./flink-1.10-SNAPSHOT/temp\n...\n...\nRemoving intermediate container a0558bbcbdd1\n ---> 00ecda6117b7\nSuccessfully built 00ecda6117b7\nSuccessfully tagged flink-job:latest\n```\n构建Docker镜像需要较长时间，请耐心等待。构建完毕之后，可以输入docker images命令在镜像列表中找到构建结果`docker images\n`：\n![-w765](15776987959455/15779503938890.jpg)\n然后我们在构建好的镜像基础上安装好Python udf所需依赖，并删除过程中产生的临时文件：\n- 启动docker容器\n`docker run -it --user root --entrypoint /bin/bash --name flink-job-container flink-job`\n- 安装一些依赖\n`apk add --no-cache g++ Python3-dev musl-dev`\n- 安装PyArrow\n`python -m pip3 install /opt/artifacts/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl`\n\n- 安装Apache Beam\n`python -m pip3 install apache-beam==2.15.0`\n\n- 删除临时文件\n`rm -rf /root/.cache/pip`\n\n执行完如上命令我可以执行`exit`退出容器了，然后把这个容器提交为新的flink-job镜像`docker commit -c 'CMD [\"--help\"]' -c \"USER flink\" -c 'ENTRYPOINT [\"/docker-entrypoint.sh\"]' flink-job-container flink-job:latest \n`：\n\n```\njincheng:docker jincheng.sunjc$ docker commit -c 'CMD [\"--help\"]' -c \"USER flink\" -c 'ENTRYPOINT [\"/docker-entrypoint.sh\"]' flink-job-container flink-job:latest \nsha256:0740a635e2b0342ddf776f33692df263ebf0437d6373f156821f4dd044ad648b\n```\n到这里包含Python UDF 作业的Docker镜像就制作好了，这个Docker镜像既可以以docker-compose使用，也可以结合kubernetes中使用。\n\n我们以使用docker-compose执行为例，mac版docker自带docker-compose，用户可以直接使用，在`flink/flink-container/docker`目录下，使用以下命令启动作业,`FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=\"-py /opt/artifacts/deploy_demo.py\" docker-compose up`:\n\n\n```\njincheng:docker jincheng.sunjc$ FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=\"-py /opt/artifacts/deploy_demo.py\" docker-compose up\nWARNING: The SAVEPOINT_OPTIONS variable is not set. Defaulting to a blank string.\nRecreating docker_job-cluster_1 ... done\nStarting docker_taskmanager_1   ... done\nAttaching to docker_taskmanager_1, docker_job-cluster_1\ntaskmanager_1  | Starting the task-manager\njob-cluster_1  | Starting the job-cluster\n...\n...\njob-cluster_1  | 2020-01-02 08:35:03,796 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Terminating cluster entrypoint process StandaloneJobClusterEntryPoint with exit code 0.\ndocker_job-cluster_1 exited with code 0\n```\n在log中出现“docker_job-cluster_1 exited with code 0”表示job已执行成功，JobManager已经退出。TaskManager还需要较长的时间等待超时后才会退出，我们可以直接按快捷键Ctrl+C提前退出。\n\n查看执行结果，可以从TaskManager的容器中将结果文件拷贝出来查看,执行 `docker cp docker_taskmanager_1:/tmp/result ./; cat result `\n![](15776987959455/15779542617667.jpg)\n\nOkay, 到这里本篇要与大家分享的内容已经接近尾声了，如果你期间也很顺利的成功了，可以 Cheers 了:)\n\n# 小结\n本篇核心向大家分享了如何以多种方式部署PyFlink作业。期望在PyFlink1.10发布之后，大家能有一个顺利快速体验的快感！在开篇说道部分，为大家分享了老子倡导大家的 “致虚极，守静笃。万物并作，吾以观其复”的大道，同时也给大家带来了2020的祝福，祝福大家 \"2020 安！\"。\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式.md","raw":"---\ntitle: Apache Flink 说道系列 - PyFlink 作业的多种部署模式\ndate: 2020-01-02 17:15:18\ncategories: Apache Flink 说道\ntags: [Flink, 道德经, 致虚极守静笃]\n---\n\n# 开篇说道\n老子说：“致虚极，守静笃。万物并作，吾以观其复\"。 字里行间是告诉人们要尽量使心灵达到虚寂的极致，持之以恒的坚守宁静。老子认为“虚”和“静”是心灵的本初的状态，也应该是一种常态，即，是一种没有心机，没有成见，没有外界名利，物质诱惑，而保持的状态。当达到这个状态之后，就可以静观万物的蓬勃发展，了解万物的循环往复的变化，通晓自然之理，体悟自然之道。\n\n\"致虚极，守静笃\" 并不代表对任何事情无动于衷，而是通过 \"致虚极，守静笃\" 的方式了解万物变化的根源，就好比，看到新芽不惊，看到落叶不哀，因为我们知道，落叶归根，重归泥土之后，还会再破土而出，开始新的轮回。\n\n在2020年1月1日零点我发了一条朋友圈，对自己的祝福是：\"㊗️ 2020 安\"。\n![](15776987959455/15779569378844.jpg)\n当然这里我也祝福所有看到这篇博客的你: \"㊗️ 2020 安\"。 祝福大家 \"安\"，也即祝福大家 能够在新的一年，追求内心的修炼，摒去纷纷复杂的物欲干扰，拂去心灵的灰尘，保持自己的赤子之心！ \n\n# 道破 \"天机\"\n前面一些博客大多介绍PyFlink的功能开发，比如，如何使用各种算子(Join/Window/AGG etc.)，如何使用各种Connector(Kafka, CSV, Socket etc.)，还有一些实际的案例。这些都停留在开发阶段，一旦开发完成，我们就面临激动人心的时刻，那就是 将我们精心设计开发的作业进行部署，那么问题来了，你知道怎样部署PyFlink的作业吗？这篇将为大家全面介绍部署PyFlink作业的各种模式。\n\n# 组件栈回顾\n![](15776987959455/15777810183663.jpg)\n\n\n上面的组件栈除了PyFlink是第一次添加上去，其他部分大家应该非常熟悉了。目前PyFlink基于Java的Table API之上，同时在Runtime层面有Python的算子和执行容器。那么我们聚焦重点，看最底层的 Deploy部分，上图我们分成了三种部署模式，Local/Cluster/Cloud，其中Local模式还有2种不同方式，一是SingleJVM，也即是MiniCluster, 前面博客里面运行示例所使用的就是MiniCluster。二是SingleNode，也就是虽然是集群模式，但是所有角色都在一台机器上。下面我们简单介绍一下上面这几种部署模式的区别：\n- Local-SingleJVM 模式- 该模式大多是开发测试阶段使用的方式，所有角色TM，JM等都在同一个JVM里面。\n- Local-SingleNode 模式 - 意在所有角色都运行在同一台机器，直白一点就是从运行的架构上看，这种模式虽然是分布式的，但集群节点只有1个，该模式大多是测试和IoT设备上进行部署使用。\n- Cluster 模式 - 也就是我们经常用于投产的分布式部署方式，上图根据对资源管理的方式不同又分为了多种，如：Standalone 是Flink自身进行资源管理，YARN，顾名思义就是利用资源管理框架Yarn来负责Flink运行资源的分配，还有结合Kubernetes等等。\n- Cloud 模式- 该部署模式是结合其他云平台进行部署。\n\n接下来我们看看PyFlink的作业可以进行怎样的模式部署？\n\n# 环境依赖\n- JDK 1.8+ (1.8.0_211)\n- Maven 3.x (3.2.5)\n- Scala 2.11+ (2.12.0)\n- Python 3.5+ (3.7.6)\n- Git 2.20+ (2.20.1)\n\n# 源码构建及安装\n在Apache Flink 1.10 发布之后，我们除了源码构建之外，还支持直接利用pip install 安装PyFlink。那么现在我们还是以源码构建的方式进行今天的介绍。\n- 下载源码\n\n```\ngit clone https://github.com/apache/flink.git\n```\n- 签出release-1.10分支(1.10版本是PyFlink的第二个版本）\n\n```\ngit fetch origin release-1.10\ngit checkout -b release-1.10 origin/release-1.10\n```\n- 构建编译\n\n```\nmvn clean package -DskipTests \n```\n如果一起顺利，你会最终看到如下信息：\n\n```\n...\n...\n[INFO] flink-walkthrough-table-scala ...................... SUCCESS [  0.070 s]\n[INFO] flink-walkthrough-datastream-java .................. SUCCESS [  0.081 s]\n[INFO] flink-walkthrough-datastream-scala ................. SUCCESS [  0.067 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  16:22 min\n[INFO] Finished at: 2019-12-31T10:37:21+08:00\n[INFO] ------------------------------------------------------------------------\n```\n- 构建PyFlink发布包\n\n上面我们构建了Java的发布包，接下来我们构建PyFlink的发布包,如下：\n\n```\ncd flink-Python; Python setup.py sdist\n```\n最终输出如下信息，证明是成功的：\n\n```\ncopying pyflink/util/exceptions.py -> apache-flink-1.10.dev0/pyflink/util\ncopying pyflink/util/utils.py -> apache-flink-1.10.dev0/pyflink/util\nWriting apache-flink-1.10.dev0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'apache-flink-1.10.dev0' (and everything under it)\n```\n在dist目录的apache-flink-1.10.dev0.tar.gz就是我们可以用于`pip install`的PyFlink包.\n\n- 安装PyFlink\n\n上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，检测是否之前已经安装过PyFlink，如下命令：\n\n```\npip3 list|grep flink\n...\nflink                         1.0      \npyflink-demo-connector        0.1 \n```\n上面信息说明我本机已经安装过PyFlink，我们要先删除，如下：\n\n\n```\npip3 uninstall flink\n```\n删除以前的安装之后，我们再安装新的如下：\n```\npip3 install dist/*.tar.gz\n\n...\nSuccessfully built apache-flink\nInstalling collected packages: apache-flink\nSuccessfully installed apache-flink-1.10.dev0\n```\n我们再用list命令检查一遍：\n\n\n```\npip3 list|grep flink\n\n...\napache-flink                  1.10.dev0\npyflink-demo-connector        0.1 \n```\n其中`pyflink-demo-connector` 是我以前做实验时候的安装，对本篇没有影响。\n\n# 安装Apache Beam 依赖\n\n我们需要使用Python3.5+ 版本，检验一下Python版本，如下：\n```\njincheng.sunjc$ Python --version\nPython 3.7.6\n```\n\n我本机是Python3.7.6，现在我们需要安装Apache Beam，如下：\n\n```\npython -m pip install apache-beam==2.15.0\n\n...\nInstalling collected packages: apache-beam\nSuccessfully installed apache-beam-2.15.0\n```\n如果顺利的出现上面信息，说明Apache-beam已经安装成功。\n\n# PyFlink示例作业\n\n接下来我们开发一个简单的PyFlink作业，源码如下：\n\n```\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nfrom pyflink.table import BatchTableEnvironment, EnvironmentSettings\nfrom pyflink.table.descriptors import FileSystem, OldCsv, Schema\nfrom pyflink.table.types import DataTypes\nfrom pyflink.table.udf import udf\n\n\ndef word_count():\n   environment_settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()\n   t_env = BatchTableEnvironment.create(environment_settings=environment_settings)\n\n   # register Results table in table environment\n   tmp_dir = tempfile.gettempdir()\n   result_path = tmp_dir + '/result'\n   if os.path.exists(result_path):\n       try:\n           if os.path.isfile(result_path):\n               os.remove(result_path)\n           else:\n               shutil.rmtree(result_path)\n       except OSError as e:\n           logging.error(\"Error removing directory: %s - %s.\", e.filename, e.strerror)\n\n   logging.info(\"Results directory: %s\", result_path)\n\n   # we should set the Python verison here if `Python` not point\n   t_env.get_config().set_Python_executable(\"Python3\")\n\n   t_env.connect(FileSystem().path(result_path)) \\\n       .with_format(OldCsv()\n                    .field_delimiter(',')\n                    .field(\"city\", DataTypes.STRING())\n                    .field(\"sales_volume\", DataTypes.BIGINT())\n                    .field(\"sales\", DataTypes.BIGINT())) \\\n       .with_schema(Schema()\n                    .field(\"city\", DataTypes.STRING())\n                    .field(\"sales_volume\", DataTypes.BIGINT())\n                    .field(\"sales\", DataTypes.BIGINT())) \\\n       .register_table_sink(\"Results\")\n\n   @udf(input_types=DataTypes.STRING(), result_type=DataTypes.ARRAY(DataTypes.STRING()))\n   def split(input_str: str):\n       return input_str.split(\",\")\n\n   @udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())\n   def get(arr, index):\n       return arr[index]\n\n   t_env.register_function(\"split\", split)\n   t_env.register_function(\"get\", get)\n\n   t_env.get_config().get_configuration().set_string(\"parallelism.default\", \"1\")\n\n   data = [(\"iPhone 11,30,5499,Beijing\", ),\n           (\"iPhone 11 Pro,20,8699,Guangzhou\", ),\n           (\"MacBook Pro,10,9999,Beijing\", ),\n           (\"AirPods Pro,50,1999,Beijing\", ),\n           (\"MacBook Pro,10,11499,Shanghai\", ),\n           (\"iPhone 11,30,5999,Shanghai\", ),\n           (\"iPhone 11 Pro,20,9999,Shenzhen\", ),\n           (\"MacBook Pro,10,13899,Hangzhou\", ),\n           (\"iPhone 11,10,6799,Beijing\", ),\n           (\"MacBook Pro,10,18999,Beijing\", ),\n           (\"iPhone 11 Pro,10,11799,Shenzhen\", ),\n           (\"MacBook Pro,10,22199,Shanghai\", ),\n           (\"AirPods Pro,40,1999,Shanghai\", )]\n   t_env.from_elements(data, [\"line\"]) \\\n       .select(\"split(line) as str_array\") \\\n       .select(\"get(str_array, 3) as city, \"\n               \"get(str_array, 1).cast(LONG) as count, \"\n               \"get(str_array, 2).cast(LONG) as unit_price\") \\\n       .select(\"city, count, count * unit_price as total_price\") \\\n       .group_by(\"city\") \\\n       .select(\"city, \"\n               \"sum(count) as sales_volume, \"\n               \"sum(total_price) as sales\") \\\n       .insert_into(\"Results\")\n\n   t_env.execute(\"word_count\")\n\n\nif __name__ == '__main__':\n   logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n   word_count()\n```\n接下来我们就介绍如何用不同部署模式运行PyFlink作业！\n\n# Local-SingleJVM 模式部署\n该模式多用于开发测试阶段，简单的利用`Python pyflink_job.py`命令，PyFlink就会默认启动一个Local-SingleJVM的Flink环境来执行作业，如下：\n\n![](15776987959455/15777784863901.jpg)\n首先确认你Python是3.5+，然后执行上面的PyFlink作业`Python deploy_demo.py`,结果写入到本地文件，然后`cat`计算结果，如果出现如图所示的结果，则说明准备工作已经就绪。:),如果有问题，欢迎留言！\n这里运行时SingleJVM，在运行这个job时候大家可以查看java进程：\n\n![](15776987959455/15777821361064.jpg)\n我们发现只有一个JVM进程，里面包含了所有Flink所需角色。\n\n\n# Local-SingleNode 模式部署\n这种模式一般用在单机环境中进行部署，如IoT设备中，我们从0开始进行该模式的部署操作。我们进入到`flink/build-target`目录，执行如下命令(个人爱好，我把端口改成了8888)：\n\n``` \njincheng:build-target jincheng.sunjc$ bin/start-cluster.sh \n...\nStarting cluster.\nStarting standalonesession daemon on host jincheng.local.\n```\n查看一下Flink的进程：\n\n![](15776987959455/15777817105172.jpg)\n\n我们发现有TM和JM两个进程，虽然在一台机器(Local）但是也是一个集群的架构。\n上面信息证明已经启动完成，我们可以查看web界面：[http://localhost:8888/](http://localhost:8888/)（我个人爱好端口是8888，默认是8080）, 如下：\n![](15776987959455/15777816500228.jpg)\n\n目前集群环境已经准备完成，我们看如果将作业部署到集群中，一条简单的命令，如下：\n\n```\nbin/flink run -m localhost:8888 -py ~/deploy_demo.py\n```\n这里如果你不更改端口可以不添加`-m` 选项。如果一切顺利，你会得到如下输出：\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -m localhost:8888 -py ~/deploy_demo.py \nResults directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\nJob has been submitted with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6\nProgram execution finished\nJob with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6 has finished.\nJob Runtime: 5389 ms\n```\n其中`/var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result` 目录是计算结果目录，我们可以产看一下，如下：\n\n```\njincheng:build-target jincheng.sunjc$  cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\nBeijing,110,622890\nGuangzhou,20,173980\nShanghai,90,596910\nShenzhen,30,317970\nHangzhou,10,138990\n```\n同时我们也可以在WebUI上面进行查看，在完成的job列表中，显示如下：\n![](15776987959455/15777853568978.jpg)\n到此，我们完成了在Local模式，其实也是只有一个节点的Standalone模式下完成PyFlink的部署。\n最后我们为了继续下面的操作，请停止集群：\n\n```\njincheng:build-target jincheng.sunjc$ bin/stop-cluster.sh\nStopping taskexecutor daemon (pid: 45714) on host jincheng.local.\nStopping standalonesession daemon (pid: 45459) on host jincheng.local.\n```\n\n# Cluster YARN 模式部署\n这个模式部署，我们需要一个YARN环境，我们一切从简，以单机部署的方式准备YARN环境，然后再与Flink进行集成。\n## 准备YARN环境\n- 安装Hadoop\n\n我本机是mac系统，所以我偷懒一下直接用brew进行安装：\n\n```\njincheng:bin jincheng.sunjc$ brew install Hadoop\nUpdating Homebrew...\n==> Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==> Updated Formulae\nPython ✔        doxygen         minio           ntopng          typescript\ncertbot         libngspice      mitmproxy       ooniprobe\ndoitlive        minimal-racket  ngspice         openimageio\n\n==> Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-\n==> Downloading from http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/\n######################################################################## 100.0%\n\n🍺  /usr/local/Cellar/Hadoop/3.2.1: 22,397 files, 815.6MB, built in 5 minutes 12 seconds\n```\n完成之后，检验一下hadoop版本：\n\n```\njincheng:bin jincheng.sunjc$ hadoop version\nHadoop 3.2.1\n```\n超级顺利，hadoop被安装到了`/usr/local/Cellar/hadoop/3.2.1/`目录下，`brew`还是很能提高生产力啊～\n\n- 配置免登(SSH)\n\nMac系统自带了ssh，我们可以简单配置一下即可，我们先打开远程登录。 `系统偏好设置` -> `共享` 中，左边勾选 `远程登录` ，右边选择 `仅这些用户` （选择`所有用户`更宽松），并添加当前用户。\n\n```\njincheng:bin jincheng.sunjc$ whoami\njincheng.sunjc\n```\n我当前用户是 `jincheng.sunjc`。 配置图如下：\n\n![](15776987959455/15777878055973.jpg)\n\n然后生产证书，如下操作：\n\n```\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\n/Users/jincheng.sunjc/.ssh/id_rsa already exists.\nOverwrite (y/n)? y\nYour identification has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.\nYour public key has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:IkjKkOjfMx1fxWlwtQYg8hThph7Xlm9kPutAYFmQR0A jincheng.sunjc@jincheng.local\nThe key's randomart image is:\n+---[RSA 2048]----+\n|       ..EB=.o.. |\n|..      =.+.+ o .|\n|+ .      B.  = o |\n|+o .    + o + .  |\n|.o. . .+S. * o   |\n|  . ..o.= + =    |\n|   . + o . . =   |\n|      o     o o  |\n|            .o   |\n+----[SHA256]-----+\n```\n接下来将公钥追加到如下文件，并修改文件权限：\n\n```\njincheng.sunjc$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\njincheng.sunjc$ chmod 0600 ~/.ssh/authorized_keys\n```\n利用ssh localhost验证，看到 Last login: 字样为ssh成功\n \n```\njincheng:~ jincheng.sunjc$ ssh localhost\nPassword:\nLast login: Tue Dec 31 18:26:48 2019 from ::1\n```\n- 设置环境变量\n\n设置JAVA_HOME,HADOOP_HOME和HADOOP_CONF_DIR，`vi ~/.bashrc`:\n  \n```\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home\n\nexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexec\n\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n```\nNOTE: 后续操作要确保的terminal环境变量是生效哦， 如果不生效可以执行 `source ~/.bashrc`。:)\n\n- 修改配置\n\n    1) 修改core-site.xml\n     \n```\n<configuration>\n   <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/tmp</value>\n   </property>\n   <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n```\n\n    2) 修改hdfs-site.xml\n    \n```\n<configuration>\n\n    <property>\n        <name>dfs.namenode.name.dir</name>\n        <value>/tmp/hadoop/name</value>\n    </property>\n    \n    <property>\n        <name>dfs.datanode.data.dir</name>\n        <value>/tmp/hadoop/data</value>\n    </property>\n    \n</configuration>\n```\n\n    3) 修改yarn-site.xml\n\n配置 YARN 作为资源管理框架：\n```\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>    \n    </property>\n    <property>\n        <name>yarn.nodemanager.env-whitelist</name>  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n    </property>\n</configuration>\n```\n\n简单的配置已经完成，我们执行一下简单命令启动环境：\n\n- 格式化文档系统：\n\n```\njincheng:libexec jincheng.sunjc$ hadoop namenode -format\n...\n...\n2019-12-31 18:58:53,260 INFO namenode.NameNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at jincheng.local/127.0.0.1\n************************************************************/\n\n```\n\n- 启动服务：\n\n我们先启动hdf再启动yarn，如下图：\n![](15776987959455/15778389671396.jpg)\n\nOkay,一切顺利的话，我们会启动namenodes，datanodes，resourcemanager和nodemanagers。我们有几个web界面可以查看，如下：\n\n 1). Overview 界面， [http://localhost:9870 ](http://localhost:9870 )如下：\n![](15776987959455/15778373195478.jpg)\n\n\n 2). NodeManager界面， [http://localhost:8042](http://localhost:8042)，如下：\n![-w649](15776987959455/15778373437959.jpg)\n\n 3). ResourceManager 管理界面 [http://localhost:8088/](http://localhost:8088/),如下：\n![-w1054](15776987959455/15777972228895.jpg)\n\n目前YARN的环境已经准备完成，我们接下来看如何与Flink进行集成。\n\n## Flink集成Hadoop包\n切换到编译结果目录下`flink/build-target`,并将haddop的JAR包放到`lib`目录。\n在官网下载hadoop包，\n\n```\ncd lib;\ncurl https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar > flink-shaded-hadoop-2-uber-2.8.3-7.0.jar\n```\n下载后，lib目录下文件如下：\n![](15776987959455/15777916683830.jpg)\n\n到现在为止我们可以提交PyFlink的作业到由YARN进行资源分配的集群了。但为了确保集群上有正确的Python环境我们最好打包一个Python环境到集群上面。因为大部分情况下我们无法得知yarn集群上的Python版本是否符合我们的要求(Python 3.5+，装有apache-beam 2.15.0)，因此我们需要打包一个符合条件的Python环境，并随job文件提交到yarn集群上。\n\n## 打包Python环境\n再次检查一下当前Python的版本是否3.5+，如下：\n```\njincheng:lib jincheng.sunjc$ Python\nPython 3.7.6 (default, Dec 31 2019, 09:48:30) \n```\n由于这个Python环境是用于集群的，所以打包时的系统需要和集群一致。如果不一致，比如集群是linux而本机是mac，我们需要在虚拟机或者docker中打包。以下列出两种情况的示范方法，读者根据需求选择一种即可。\n\n### 本地打包（集群和本机操作系统一致时）\n如果集群所在机器的操作系统和本地一致（都是mac或者都是linux），直接通过virtualenv打包一个符合条件的Python环境：\n- 安装virtualenv\n使用`python -m pip install virtualenv` 进行安装如下：\n\n```\njincheng:tmp jincheng.sunjc$ python -m pip install virtualenv\nCollecting virtualenv\n  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)\n     |████████████████████████████████| 3.4MB 2.0MB/s \nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.7.9\n```\n我本地环境已经成功安装。\n\n- 创建Python环境\n用virtualenv以always-copy方式建立一个全新的Python环境，名字随意，以venv为例，`virtualenv --always-copy venv`:\n\n```\njincheng:tmp jincheng.sunjc$ virtualenv --always-copy venv\nUsing base prefix '/usr/local/Cellar/Python/3.7.6/Frameworks/Python.framework/Versions/3.7'\nNew Python executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python3.7\nAlso creating executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python\nInstalling setuptools, pip, wheel...\ndone.\n```\n- 在新环境中安装apache-beam 2.15.0\n使用`venv/bin/pip install apache-beam==2.15.0`进行安装：\n\n```\njincheng:tmp jincheng.sunjc$ venv/bin/pip install apache-beam==2.15.0\nCollecting apache-beam==2.15.0\n...\n...\nSuccessfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7\n```\n上面信息已经说明我们成功的在Python环境中安装了apache-beam==2.15.0。接下来我们打包Python环境。\n- 打包Python环境\n 我们将Python打包成zip文件，`zip -r venv.zip venv` 如下：\n \n```\nzip -r venv.zip venv\n...\n...\n  adding: venv/lib/Python3.7/re.py (deflated 68%)\n  adding: venv/lib/Python3.7/struct.py (deflated 46%)\n  adding: venv/lib/Python3.7/sre_parse.py (deflated 80%)\n  adding: venv/lib/Python3.7/abc.py (deflated 72%)\n  adding: venv/lib/Python3.7/_bootlocale.py (deflated 63%)\n```\n查看一下zip大小：\n\n```\njincheng:tmp jincheng.sunjc$ du -sh venv.zip \n 81M\tvenv.zip\n```\n这个大小实在太大了，核心问题是Beam的包非常大，后面我会持续在Beam社区提出优化建议。我们先忍一下:(。\n\n### Docker中打包（比如集群为linux，本机为mac时）\n我们选择在docker中打包，可以从以下链接下载最新版docker并安装：\n[https://download.docker.com/mac/stable/Docker.dmg](https://download.docker.com/mac/stable/Docker.dmg)安装完毕后重启终端，执行`docker version`确认docker安装成功：\n\n```\njincheng:tmp jincheng.sunjc$ docker version\nClient: Docker Engine - Community\n Version:           19.03.4\n API version:       1.40\n Go version:        go1.12.10\n Git commit:        9013bf5\n Built:             Thu Oct 17 23:44:48 2019\n OS/Arch:           darwin/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.4\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.12.10\n  Git commit:       9013bf5\n  Built:            Thu Oct 17 23:50:38 2019\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.2.10\n  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339\n runc:\n  Version:          1.0.0-rc8+dev\n  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n```\n\n- 启动容器\n我们启动一个Python 3.7版本的容器如果是第一次启动可能需要较长时间来拉取镜像：\n`docker run -it Python:3.7 /bin/bash`, 如下：\n\n```\njincheng:libexec jincheng.sunjc$  docker run -it Python:3.7 /bin/bash\nUnable to find image 'Python:3.7' locally\n3.7: Pulling from library/Python\n8f0fdd3eaac0: Pull complete \nd918eaefd9de: Pull complete \n43bf3e3107f5: Pull complete \n27622921edb2: Pull complete \ndcfa0aa1ae2c: Pull complete \nbf6840af9e70: Pull complete \n167665d59281: Pull complete \nffc544588c7f: Pull complete \n4ebe99df65fe: Pull complete \nDigest: sha256:40d615d7617f0f3b54614fd228d41a891949b988ae2b452c0aaac5bee924888d\nStatus: Downloaded newer image for Python:3.7\n```\n\n- 容器中安装virtualenv\n我们在刚才启动的容器中安装virtualenv， `pip install virtualenv`,如下：\n```\nroot@1b48d2b526ae:/# pip install virtualenv\nCollecting virtualenv\n  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)\n     |████████████████████████████████| 3.4MB 2.0MB/s \nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.7.9\nroot@1b48d2b526ae:/# \n```\n\n- 创建Python环境\n以always copy方式建立一个全新的Python环境，名字随意，以venv为例，`virtualenv --always-copy venv`, 如下：\n\n\n```\nroot@1b48d2b526ae:/# virtualenv --always-copy venv\nUsing base prefix '/usr/local'\nNew Python executable in /venv/bin/Python\nInstalling setuptools, pip, wheel...\ndone.\nroot@1b48d2b526ae:/# \n```\n\n- 安装Apache Beam\n 在新的Python环境中安装apache-beam 2.15.0，`venv/bin/pip install apache-beam==2.15.0`,如下：\n \n```\nroot@1b48d2b526ae:/# venv/bin/pip install apache-beam==2.15.0\nCollecting apache-beam==2.15.0\n...\n...\nSuccessfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7\n```\n- 查看docker中的Python环境\n用`exit`命令退出容器，用`docker ps -a`找到docker容器的id，用于拷贝文件,如下：\n\n```\nroot@1b48d2b526ae:/# exit\nexit\njincheng:libexec jincheng.sunjc$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES\n1b48d2b526ae        Python:3.7          \"/bin/bash\"         7 minutes ago       Exited (0) 8 seconds ago                       elated_visvesvaraya\n```\n由于刚刚结束，一般来说是列表中的第一条，可以根据容器的镜像名Python:3.7来分辨。我们记下最左边的容器ID。如上是`1b48d2b526ae`。\n\n- 打包Python环境\n 从将容器中的Python环境拷贝出来，我们切换到`flink/build-target`录下，拷贝 `docker cp 1b48d2b526ae:/venv ./`并打包`zip -r venv.zip venv`。\n 最终`flink/build-target`录下生成`venv.zip`。\n \n\n## 部署作业\n终于到部署作业的环节了:), flink on yarn支持两种模式，per-job和session。per-job模式在提交job时会为每个job单独起一个flink集群，session模式先在yarn上起一个flink集群，之后提交job都提交到这个flink集群。\n\n### Pre-Job 模式部署作业\n执行以下命令，以Pre-Job模式部署PyFlink作业：\n`bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py`,如下：\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py\n2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\n2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\nResults directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result\n2020-01-02 13:04:55,945 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032\n2020-01-02 13:04:56,049 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar\n2020-01-02 13:05:01,153 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.\n2020-01-02 13:05:01,177 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1}\n2020-01-02 13:05:01,294 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is 'file'. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system\n2020-01-02 13:05:02,600 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0004\n2020-01-02 13:05:02,971 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0004\n2020-01-02 13:05:02,972 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated\n2020-01-02 13:05:02,975 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED\n2020-01-02 13:05:23,138 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.\n2020-01-02 13:05:23,140 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:61616 of application 'application_1577936885434_0004'.\nJob has been submitted with JobID a41d82194a500809fd715da8f29894a0\nProgram execution finished\nJob with JobID a41d82194a500809fd715da8f29894a0 has finished.\nJob Runtime: 35576 ms\n```\n上面信息已经显示运行完成，在Web界面可以看到作业状态：\n![](15776987959455/15779417223687.jpg)\n\n我们再检验一下计算结果`cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result`：\n![](15776987959455/15779417681405.jpg)\n\n到这里，我们以Pre-Job的方式成功部署了PyFlink的作业！相比提交到本地Standalone集群，多了三个参数，我们简单说明如下：\n\n| 参数                               | 说明                                                  |\n|----------------------------------|-----------------------------------------------------|\n| -m yarn-cluster                  | 以Per-Job模式部署到yarn集群                                 |\n| -pyarch venv.zip                 | 将当前目录下的venv.zip上传到yarn集群                            |\n| -pyexec venv.zip/venv/bin/Python | 指定venv.zip中的Python解释器来执行Python UDF，路径需要和zip包内部结构一致。 |\n\n### Session 模式部署作业\n以Session模式部署作业也非常简单，我们实际操作一下：\n\n```\njincheng:build-target jincheng.sunjc$ bin/yarn-session.sh \n2020-01-02 13:58:53,049 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.memory.process.size, 1024m\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1\n2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1\n2020-01-02 13:58:53,051 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.execution.failover-strategy, region\n2020-01-02 13:58:53,413 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2020-01-02 13:58:53,476 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to jincheng.sunjc (auth:SIMPLE)\n2020-01-02 13:58:53,509 INFO  org.apache.flink.runtime.security.modules.JaasModule          - Jaas file will be created as /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/jaas-3848984206030141476.conf.\n2020-01-02 13:58:53,521 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.\n2020-01-02 13:58:53,562 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032\n2020-01-02 13:58:58,803 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.\n2020-01-02 13:58:58,824 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1}\n2020-01-02 13:59:03,975 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is 'file'. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system\n2020-01-02 13:59:04,779 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0005\n2020-01-02 13:59:04,799 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0005\n2020-01-02 13:59:04,799 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated\n2020-01-02 13:59:04,801 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED\n2020-01-02 13:59:24,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.\n2020-01-02 13:59:24,713 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application 'application_1577936885434_0005'.\nJobManager Web Interface: http://localhost:62247\n\n```\n执行成功后不会返回，但会启动一个JoBManager Web，地址如上`http://localhost:62247`，可复制到浏览器查看:\n![](15776987959455/15779449909178.jpg)\n\n\n我们可以修改conf/flink-conf.yaml中的配置参数。如果要更改某些内容，请参考[官方文档](https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html)。接下来我们提交作业，首先按组合键Ctrl+Z将yarn-session.sh进程切换到后台，并执行bg指令让其在后台继续执行, 然后执行以下命令，即可向Session模式的flink集群提交job `bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py`：\n\n\n```\njincheng:build-target jincheng.sunjc$ bin/flink run -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py\n\n2020-01-02 14:10:48,285 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application 'application_1577936885434_0005'.\nJob has been submitted with JobID bea33b7aa07c0f62153ab5f6e134b6bf\nProgram execution finished\nJob with JobID bea33b7aa07c0f62153ab5f6e134b6bf has finished.\nJob Runtime: 34405 ms\n```\n如果在打印`finished`之前查看之前的web页面，我们会发现Session集群会有一个正确运行的作业，如下：\n![](15776987959455/15779454827330.jpg)\n如果已经运行完成，那么我们应该会看到状态也变成结束：\n![](15776987959455/15779456315383.jpg)\n\n相比per job模式提交，少了”-m”参数。因为之前已经启动了yarn-session.sh，所以flink默认会向yarn-session.sh启动的集群上提交job。执行完毕后，别忘了关闭yarn-session.sh（session 模式）：先将yarn-session.sh调到前台，执行`fg`,然后在再按Ctrl+C结束进程或者执行`stop`，结束时yarn上的集群也会被关闭。\n\n# Docker 模式部署\n我们还可以将Flink Python job打包成docker镜像，然后使用docker-compose或者Kubernetes部署执行，由于现在的docker镜像打包工具并没有完美支持运行Python UDF，因此我们需要往里面添加一些额外的文件。首先是一个仅包含PythonDriver类的jar包. 我们在`build-target`目录下执行如下命令：\n\n```\njincheng:build-target jincheng.sunjc$ mkdir temp\njincheng:build-target jincheng.sunjc$ cd temp\njincheng:temp jincheng.sunjc$ unzip ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar org/apache/flink/client/Python/PythonDriver.class\nArchive:  ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar\n  inflating: org/apache/flink/client/Python/PythonDriver.class \n```\n解压之后，我们在进行压缩打包：\n\n```\njincheng:temp jincheng.sunjc$ zip Python-driver.jar org/apache/flink/client/Python/PythonDriver.class\n  adding: org/apache/flink/client/Python/PythonDriver.class (deflated 56%)\n```\n![](15776987959455/15779465411786.jpg)\n我们得到`Python-driver.jar`。然后下载一个pyArrow的安装文件(我准备了一个大家下载直接使用即可 [pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl](https://github.com/sunjincheng121/enjoyment.code/blob/master/share_resource/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl)。执行以下命令构建Docker镜像，需要作为artifacts引入的文件有作业文件，Python-driver的jar包和pyarrow安装文件,`./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist`(进入`flink/flink-container/docker`目录)\n\n```\njincheng:docker jincheng.sunjc$ ./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist\nUsing flink dist: ../../flink-dist/target/flink-*-bin\na .\na ./flink-1.10-SNAPSHOT\na ./flink-1.10-SNAPSHOT/temp\n...\n...\nRemoving intermediate container a0558bbcbdd1\n ---> 00ecda6117b7\nSuccessfully built 00ecda6117b7\nSuccessfully tagged flink-job:latest\n```\n构建Docker镜像需要较长时间，请耐心等待。构建完毕之后，可以输入docker images命令在镜像列表中找到构建结果`docker images\n`：\n![-w765](15776987959455/15779503938890.jpg)\n然后我们在构建好的镜像基础上安装好Python udf所需依赖，并删除过程中产生的临时文件：\n- 启动docker容器\n`docker run -it --user root --entrypoint /bin/bash --name flink-job-container flink-job`\n- 安装一些依赖\n`apk add --no-cache g++ Python3-dev musl-dev`\n- 安装PyArrow\n`python -m pip3 install /opt/artifacts/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl`\n\n- 安装Apache Beam\n`python -m pip3 install apache-beam==2.15.0`\n\n- 删除临时文件\n`rm -rf /root/.cache/pip`\n\n执行完如上命令我可以执行`exit`退出容器了，然后把这个容器提交为新的flink-job镜像`docker commit -c 'CMD [\"--help\"]' -c \"USER flink\" -c 'ENTRYPOINT [\"/docker-entrypoint.sh\"]' flink-job-container flink-job:latest \n`：\n\n```\njincheng:docker jincheng.sunjc$ docker commit -c 'CMD [\"--help\"]' -c \"USER flink\" -c 'ENTRYPOINT [\"/docker-entrypoint.sh\"]' flink-job-container flink-job:latest \nsha256:0740a635e2b0342ddf776f33692df263ebf0437d6373f156821f4dd044ad648b\n```\n到这里包含Python UDF 作业的Docker镜像就制作好了，这个Docker镜像既可以以docker-compose使用，也可以结合kubernetes中使用。\n\n我们以使用docker-compose执行为例，mac版docker自带docker-compose，用户可以直接使用，在`flink/flink-container/docker`目录下，使用以下命令启动作业,`FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=\"-py /opt/artifacts/deploy_demo.py\" docker-compose up`:\n\n\n```\njincheng:docker jincheng.sunjc$ FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=\"-py /opt/artifacts/deploy_demo.py\" docker-compose up\nWARNING: The SAVEPOINT_OPTIONS variable is not set. Defaulting to a blank string.\nRecreating docker_job-cluster_1 ... done\nStarting docker_taskmanager_1   ... done\nAttaching to docker_taskmanager_1, docker_job-cluster_1\ntaskmanager_1  | Starting the task-manager\njob-cluster_1  | Starting the job-cluster\n...\n...\njob-cluster_1  | 2020-01-02 08:35:03,796 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Terminating cluster entrypoint process StandaloneJobClusterEntryPoint with exit code 0.\ndocker_job-cluster_1 exited with code 0\n```\n在log中出现“docker_job-cluster_1 exited with code 0”表示job已执行成功，JobManager已经退出。TaskManager还需要较长的时间等待超时后才会退出，我们可以直接按快捷键Ctrl+C提前退出。\n\n查看执行结果，可以从TaskManager的容器中将结果文件拷贝出来查看,执行 `docker cp docker_taskmanager_1:/tmp/result ./; cat result `\n![](15776987959455/15779542617667.jpg)\n\nOkay, 到这里本篇要与大家分享的内容已经接近尾声了，如果你期间也很顺利的成功了，可以 Cheers 了:)\n\n# 小结\n本篇核心向大家分享了如何以多种方式部署PyFlink作业。期望在PyFlink1.10发布之后，大家能有一个顺利快速体验的快感！在开篇说道部分，为大家分享了老子倡导大家的 “致虚极，守静笃。万物并作，吾以观其复”的大道，同时也给大家带来了2020的祝福，祝福大家 \"2020 安！\"。\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Apache-Flink-说道系列-PyFlink-作业的多种部署模式","published":1,"updated":"2020-01-02T10:31:42.326Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck4wllksc00005u4hj1k6sj5v","content":"<h1 id=\"开篇说道\"><a href=\"#开篇说道\" class=\"headerlink\" title=\"开篇说道\"></a>开篇说道</h1><p>老子说：“致虚极，守静笃。万物并作，吾以观其复”。 字里行间是告诉人们要尽量使心灵达到虚寂的极致，持之以恒的坚守宁静。老子认为“虚”和“静”是心灵的本初的状态，也应该是一种常态，即，是一种没有心机，没有成见，没有外界名利，物质诱惑，而保持的状态。当达到这个状态之后，就可以静观万物的蓬勃发展，了解万物的循环往复的变化，通晓自然之理，体悟自然之道。</p>\n<p>“致虚极，守静笃” 并不代表对任何事情无动于衷，而是通过 “致虚极，守静笃” 的方式了解万物变化的根源，就好比，看到新芽不惊，看到落叶不哀，因为我们知道，落叶归根，重归泥土之后，还会再破土而出，开始新的轮回。</p>\n<p>在2020年1月1日零点我发了一条朋友圈，对自己的祝福是：”㊗️ 2020 安”。<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779569378844.jpg\" alt><br>当然这里我也祝福所有看到这篇博客的你: “㊗️ 2020 安”。 祝福大家 “安”，也即祝福大家 能够在新的一年，追求内心的修炼，摒去纷纷复杂的物欲干扰，拂去心灵的灰尘，保持自己的赤子之心！ </p>\n<h1 id=\"道破-“天机”\"><a href=\"#道破-“天机”\" class=\"headerlink\" title=\"道破 “天机”\"></a>道破 “天机”</h1><p>前面一些博客大多介绍PyFlink的功能开发，比如，如何使用各种算子(Join/Window/AGG etc.)，如何使用各种Connector(Kafka, CSV, Socket etc.)，还有一些实际的案例。这些都停留在开发阶段，一旦开发完成，我们就面临激动人心的时刻，那就是 将我们精心设计开发的作业进行部署，那么问题来了，你知道怎样部署PyFlink的作业吗？这篇将为大家全面介绍部署PyFlink作业的各种模式。</p>\n<h1 id=\"组件栈回顾\"><a href=\"#组件栈回顾\" class=\"headerlink\" title=\"组件栈回顾\"></a>组件栈回顾</h1><p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777810183663.jpg\" alt></p>\n<p>上面的组件栈除了PyFlink是第一次添加上去，其他部分大家应该非常熟悉了。目前PyFlink基于Java的Table API之上，同时在Runtime层面有Python的算子和执行容器。那么我们聚焦重点，看最底层的 Deploy部分，上图我们分成了三种部署模式，Local/Cluster/Cloud，其中Local模式还有2种不同方式，一是SingleJVM，也即是MiniCluster, 前面博客里面运行示例所使用的就是MiniCluster。二是SingleNode，也就是虽然是集群模式，但是所有角色都在一台机器上。下面我们简单介绍一下上面这几种部署模式的区别：</p>\n<ul>\n<li>Local-SingleJVM 模式- 该模式大多是开发测试阶段使用的方式，所有角色TM，JM等都在同一个JVM里面。</li>\n<li>Local-SingleNode 模式 - 意在所有角色都运行在同一台机器，直白一点就是从运行的架构上看，这种模式虽然是分布式的，但集群节点只有1个，该模式大多是测试和IoT设备上进行部署使用。</li>\n<li>Cluster 模式 - 也就是我们经常用于投产的分布式部署方式，上图根据对资源管理的方式不同又分为了多种，如：Standalone 是Flink自身进行资源管理，YARN，顾名思义就是利用资源管理框架Yarn来负责Flink运行资源的分配，还有结合Kubernetes等等。</li>\n<li>Cloud 模式- 该部署模式是结合其他云平台进行部署。</li>\n</ul>\n<p>接下来我们看看PyFlink的作业可以进行怎样的模式部署？</p>\n<h1 id=\"环境依赖\"><a href=\"#环境依赖\" class=\"headerlink\" title=\"环境依赖\"></a>环境依赖</h1><ul>\n<li>JDK 1.8+ (1.8.0_211)</li>\n<li>Maven 3.x (3.2.5)</li>\n<li>Scala 2.11+ (2.12.0)</li>\n<li>Python 3.5+ (3.7.6)</li>\n<li>Git 2.20+ (2.20.1)</li>\n</ul>\n<h1 id=\"源码构建及安装\"><a href=\"#源码构建及安装\" class=\"headerlink\" title=\"源码构建及安装\"></a>源码构建及安装</h1><p>在Apache Flink 1.10 发布之后，我们除了源码构建之外，还支持直接利用pip install 安装PyFlink。那么现在我们还是以源码构建的方式进行今天的介绍。</p>\n<ul>\n<li>下载源码</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/apache/flink.git</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>签出release-1.10分支(1.10版本是PyFlink的第二个版本）</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin release-1.10</span><br><span class=\"line\">git checkout -b release-1.10 origin/release-1.10</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建编译</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>\n\n<p>如果一起顺利，你会最终看到如下信息：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">[INFO] flink-walkthrough-table-scala ...................... SUCCESS [  0.070 s]</span><br><span class=\"line\">[INFO] flink-walkthrough-datastream-java .................. SUCCESS [  0.081 s]</span><br><span class=\"line\">[INFO] flink-walkthrough-datastream-scala ................. SUCCESS [  0.067 s]</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time:  16:22 min</span><br><span class=\"line\">[INFO] Finished at: 2019-12-31T10:37:21+08:00</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建PyFlink发布包</li>\n</ul>\n<p>上面我们构建了Java的发布包，接下来我们构建PyFlink的发布包,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-Python; Python setup.py sdist</span><br></pre></td></tr></table></figure>\n\n<p>最终输出如下信息，证明是成功的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">copying pyflink/util/exceptions.py -&gt; apache-flink-1.10.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/utils.py -&gt; apache-flink-1.10.dev0/pyflink/util</span><br><span class=\"line\">Writing apache-flink-1.10.dev0/setup.cfg</span><br><span class=\"line\">creating dist</span><br><span class=\"line\">Creating tar archive</span><br><span class=\"line\">removing &apos;apache-flink-1.10.dev0&apos; (and everything under it)</span><br></pre></td></tr></table></figure>\n\n<p>在dist目录的apache-flink-1.10.dev0.tar.gz就是我们可以用于<code>pip install</code>的PyFlink包.</p>\n<ul>\n<li>安装PyFlink</li>\n</ul>\n<p>上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，检测是否之前已经安装过PyFlink，如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 list|grep flink</span><br><span class=\"line\">...</span><br><span class=\"line\">flink                         1.0      </span><br><span class=\"line\">pyflink-demo-connector        0.1</span><br></pre></td></tr></table></figure>\n\n<p>上面信息说明我本机已经安装过PyFlink，我们要先删除，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 uninstall flink</span><br></pre></td></tr></table></figure>\n\n<p>删除以前的安装之后，我们再安装新的如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install dist/*.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully built apache-flink</span><br><span class=\"line\">Installing collected packages: apache-flink</span><br><span class=\"line\">Successfully installed apache-flink-1.10.dev0</span><br></pre></td></tr></table></figure>\n\n<p>我们再用list命令检查一遍：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 list|grep flink</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">apache-flink                  1.10.dev0</span><br><span class=\"line\">pyflink-demo-connector        0.1</span><br></pre></td></tr></table></figure>\n\n<p>其中<code>pyflink-demo-connector</code> 是我以前做实验时候的安装，对本篇没有影响。</p>\n<h1 id=\"安装Apache-Beam-依赖\"><a href=\"#安装Apache-Beam-依赖\" class=\"headerlink\" title=\"安装Apache Beam 依赖\"></a>安装Apache Beam 依赖</h1><p>我们需要使用Python3.5+ 版本，检验一下Python版本，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng.sunjc$ Python --version</span><br><span class=\"line\">Python 3.7.6</span><br></pre></td></tr></table></figure>\n\n<p>我本机是Python3.7.6，现在我们需要安装Apache Beam，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m pip install apache-beam==2.15.0</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">Installing collected packages: apache-beam</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0</span><br></pre></td></tr></table></figure>\n\n<p>如果顺利的出现上面信息，说明Apache-beam已经安装成功。</p>\n<h1 id=\"PyFlink示例作业\"><a href=\"#PyFlink示例作业\" class=\"headerlink\" title=\"PyFlink示例作业\"></a>PyFlink示例作业</h1><p>接下来我们开发一个简单的PyFlink作业，源码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import logging</span><br><span class=\"line\">import os</span><br><span class=\"line\">import shutil</span><br><span class=\"line\">import sys</span><br><span class=\"line\">import tempfile</span><br><span class=\"line\"></span><br><span class=\"line\">from pyflink.table import BatchTableEnvironment, EnvironmentSettings</span><br><span class=\"line\">from pyflink.table.descriptors import FileSystem, OldCsv, Schema</span><br><span class=\"line\">from pyflink.table.types import DataTypes</span><br><span class=\"line\">from pyflink.table.udf import udf</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def word_count():</span><br><span class=\"line\">   environment_settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()</span><br><span class=\"line\">   t_env = BatchTableEnvironment.create(environment_settings=environment_settings)</span><br><span class=\"line\"></span><br><span class=\"line\">   # register Results table in table environment</span><br><span class=\"line\">   tmp_dir = tempfile.gettempdir()</span><br><span class=\"line\">   result_path = tmp_dir + &apos;/result&apos;</span><br><span class=\"line\">   if os.path.exists(result_path):</span><br><span class=\"line\">       try:</span><br><span class=\"line\">           if os.path.isfile(result_path):</span><br><span class=\"line\">               os.remove(result_path)</span><br><span class=\"line\">           else:</span><br><span class=\"line\">               shutil.rmtree(result_path)</span><br><span class=\"line\">       except OSError as e:</span><br><span class=\"line\">           logging.error(&quot;Error removing directory: %s - %s.&quot;, e.filename, e.strerror)</span><br><span class=\"line\"></span><br><span class=\"line\">   logging.info(&quot;Results directory: %s&quot;, result_path)</span><br><span class=\"line\"></span><br><span class=\"line\">   # we should set the Python verison here if `Python` not point</span><br><span class=\"line\">   t_env.get_config().set_Python_executable(&quot;Python3&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.connect(FileSystem().path(result_path)) \\</span><br><span class=\"line\">       .with_format(OldCsv()</span><br><span class=\"line\">                    .field_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                    .field(&quot;city&quot;, DataTypes.STRING())</span><br><span class=\"line\">                    .field(&quot;sales_volume&quot;, DataTypes.BIGINT())</span><br><span class=\"line\">                    .field(&quot;sales&quot;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">       .with_schema(Schema()</span><br><span class=\"line\">                    .field(&quot;city&quot;, DataTypes.STRING())</span><br><span class=\"line\">                    .field(&quot;sales_volume&quot;, DataTypes.BIGINT())</span><br><span class=\"line\">                    .field(&quot;sales&quot;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">       .register_table_sink(&quot;Results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   @udf(input_types=DataTypes.STRING(), result_type=DataTypes.ARRAY(DataTypes.STRING()))</span><br><span class=\"line\">   def split(input_str: str):</span><br><span class=\"line\">       return input_str.split(&quot;,&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   @udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())</span><br><span class=\"line\">   def get(arr, index):</span><br><span class=\"line\">       return arr[index]</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.register_function(&quot;split&quot;, split)</span><br><span class=\"line\">   t_env.register_function(&quot;get&quot;, get)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.get_config().get_configuration().set_string(&quot;parallelism.default&quot;, &quot;1&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   data = [(&quot;iPhone 11,30,5499,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,20,8699,Guangzhou&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,9999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;AirPods Pro,50,1999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,11499,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11,30,5999,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,20,9999,Shenzhen&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,13899,Hangzhou&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11,10,6799,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,18999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,10,11799,Shenzhen&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,22199,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;AirPods Pro,40,1999,Shanghai&quot;, )]</span><br><span class=\"line\">   t_env.from_elements(data, [&quot;line&quot;]) \\</span><br><span class=\"line\">       .select(&quot;split(line) as str_array&quot;) \\</span><br><span class=\"line\">       .select(&quot;get(str_array, 3) as city, &quot;</span><br><span class=\"line\">               &quot;get(str_array, 1).cast(LONG) as count, &quot;</span><br><span class=\"line\">               &quot;get(str_array, 2).cast(LONG) as unit_price&quot;) \\</span><br><span class=\"line\">       .select(&quot;city, count, count * unit_price as total_price&quot;) \\</span><br><span class=\"line\">       .group_by(&quot;city&quot;) \\</span><br><span class=\"line\">       .select(&quot;city, &quot;</span><br><span class=\"line\">               &quot;sum(count) as sales_volume, &quot;</span><br><span class=\"line\">               &quot;sum(total_price) as sales&quot;) \\</span><br><span class=\"line\">       .insert_into(&quot;Results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.execute(&quot;word_count&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">   logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=&quot;%(message)s&quot;)</span><br><span class=\"line\">   word_count()</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们就介绍如何用不同部署模式运行PyFlink作业！</p>\n<h1 id=\"Local-SingleJVM-模式部署\"><a href=\"#Local-SingleJVM-模式部署\" class=\"headerlink\" title=\"Local-SingleJVM 模式部署\"></a>Local-SingleJVM 模式部署</h1><p>该模式多用于开发测试阶段，简单的利用<code>Python pyflink_job.py</code>命令，PyFlink就会默认启动一个Local-SingleJVM的Flink环境来执行作业，如下：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777784863901.jpg\" alt><br>首先确认你Python是3.5+，然后执行上面的PyFlink作业<code>Python deploy_demo.py</code>,结果写入到本地文件，然后<code>cat</code>计算结果，如果出现如图所示的结果，则说明准备工作已经就绪。:),如果有问题，欢迎留言！<br>这里运行时SingleJVM，在运行这个job时候大家可以查看java进程：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821361064.jpg\" alt><br>我们发现只有一个JVM进程，里面包含了所有Flink所需角色。</p>\n<h1 id=\"Local-SingleNode-模式部署\"><a href=\"#Local-SingleNode-模式部署\" class=\"headerlink\" title=\"Local-SingleNode 模式部署\"></a>Local-SingleNode 模式部署</h1><p>这种模式一般用在单机环境中进行部署，如IoT设备中，我们从0开始进行该模式的部署操作。我们进入到<code>flink/build-target</code>目录，执行如下命令(个人爱好，我把端口改成了8888)：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/start-cluster.sh </span><br><span class=\"line\">...</span><br><span class=\"line\">Starting cluster.</span><br><span class=\"line\">Starting standalonesession daemon on host jincheng.local.</span><br></pre></td></tr></table></figure>\n\n<p>查看一下Flink的进程：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777817105172.jpg\" alt></p>\n<p>我们发现有TM和JM两个进程，虽然在一台机器(Local）但是也是一个集群的架构。<br>上面信息证明已经启动完成，我们可以查看web界面：<a href=\"http://localhost:8888/\" target=\"_blank\" rel=\"noopener\">http://localhost:8888/</a>（我个人爱好端口是8888，默认是8080）, 如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777816500228.jpg\" alt></p>\n<p>目前集群环境已经准备完成，我们看如果将作业部署到集群中，一条简单的命令，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flink run -m localhost:8888 -py ~/deploy_demo.py</span><br></pre></td></tr></table></figure>\n\n<p>这里如果你不更改端口可以不添加<code>-m</code> 选项。如果一切顺利，你会得到如下输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -m localhost:8888 -py ~/deploy_demo.py </span><br><span class=\"line\">Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">Job has been submitted with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6 has finished.</span><br><span class=\"line\">Job Runtime: 5389 ms</span><br></pre></td></tr></table></figure>\n\n<p>其中<code>/var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</code> 目录是计算结果目录，我们可以产看一下，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$  cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">Beijing,110,622890</span><br><span class=\"line\">Guangzhou,20,173980</span><br><span class=\"line\">Shanghai,90,596910</span><br><span class=\"line\">Shenzhen,30,317970</span><br><span class=\"line\">Hangzhou,10,138990</span><br></pre></td></tr></table></figure>\n\n<p>同时我们也可以在WebUI上面进行查看，在完成的job列表中，显示如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777853568978.jpg\" alt><br>到此，我们完成了在Local模式，其实也是只有一个节点的Standalone模式下完成PyFlink的部署。<br>最后我们为了继续下面的操作，请停止集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/stop-cluster.sh</span><br><span class=\"line\">Stopping taskexecutor daemon (pid: 45714) on host jincheng.local.</span><br><span class=\"line\">Stopping standalonesession daemon (pid: 45459) on host jincheng.local.</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Cluster-YARN-模式部署\"><a href=\"#Cluster-YARN-模式部署\" class=\"headerlink\" title=\"Cluster YARN 模式部署\"></a>Cluster YARN 模式部署</h1><p>这个模式部署，我们需要一个YARN环境，我们一切从简，以单机部署的方式准备YARN环境，然后再与Flink进行集成。</p>\n<h2 id=\"准备YARN环境\"><a href=\"#准备YARN环境\" class=\"headerlink\" title=\"准备YARN环境\"></a>准备YARN环境</h2><ul>\n<li>安装Hadoop</li>\n</ul>\n<p>我本机是mac系统，所以我偷懒一下直接用brew进行安装：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ brew install Hadoop</span><br><span class=\"line\">Updating Homebrew...</span><br><span class=\"line\">==&gt; Auto-updated Homebrew!</span><br><span class=\"line\">Updated 2 taps (homebrew/core and homebrew/cask).</span><br><span class=\"line\">==&gt; Updated Formulae</span><br><span class=\"line\">Python ✔        doxygen         minio           ntopng          typescript</span><br><span class=\"line\">certbot         libngspice      mitmproxy       ooniprobe</span><br><span class=\"line\">doitlive        minimal-racket  ngspice         openimageio</span><br><span class=\"line\"></span><br><span class=\"line\">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-</span><br><span class=\"line\">==&gt; Downloading from http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/</span><br><span class=\"line\">######################################################################## 100.0%</span><br><span class=\"line\"></span><br><span class=\"line\">🍺  /usr/local/Cellar/Hadoop/3.2.1: 22,397 files, 815.6MB, built in 5 minutes 12 seconds</span><br></pre></td></tr></table></figure>\n\n<p>完成之后，检验一下hadoop版本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ hadoop version</span><br><span class=\"line\">Hadoop 3.2.1</span><br></pre></td></tr></table></figure>\n\n<p>超级顺利，hadoop被安装到了<code>/usr/local/Cellar/hadoop/3.2.1/</code>目录下，<code>brew</code>还是很能提高生产力啊～</p>\n<ul>\n<li>配置免登(SSH)</li>\n</ul>\n<p>Mac系统自带了ssh，我们可以简单配置一下即可，我们先打开远程登录。 <code>系统偏好设置</code> -&gt; <code>共享</code> 中，左边勾选 <code>远程登录</code> ，右边选择 <code>仅这些用户</code> （选择<code>所有用户</code>更宽松），并添加当前用户。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ whoami</span><br><span class=\"line\">jincheng.sunjc</span><br></pre></td></tr></table></figure>\n\n<p>我当前用户是 <code>jincheng.sunjc</code>。 配置图如下：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777878055973.jpg\" alt></p>\n<p>然后生产证书，如下操作：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class=\"line\">Generating public/private rsa key pair.</span><br><span class=\"line\">/Users/jincheng.sunjc/.ssh/id_rsa already exists.</span><br><span class=\"line\">Overwrite (y/n)? y</span><br><span class=\"line\">Your identification has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.</span><br><span class=\"line\">Your public key has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.pub.</span><br><span class=\"line\">The key fingerprint is:</span><br><span class=\"line\">SHA256:IkjKkOjfMx1fxWlwtQYg8hThph7Xlm9kPutAYFmQR0A jincheng.sunjc@jincheng.local</span><br><span class=\"line\">The key&apos;s randomart image is:</span><br><span class=\"line\">+---[RSA 2048]----+</span><br><span class=\"line\">|       ..EB=.o.. |</span><br><span class=\"line\">|..      =.+.+ o .|</span><br><span class=\"line\">|+ .      B.  = o |</span><br><span class=\"line\">|+o .    + o + .  |</span><br><span class=\"line\">|.o. . .+S. * o   |</span><br><span class=\"line\">|  . ..o.= + =    |</span><br><span class=\"line\">|   . + o . . =   |</span><br><span class=\"line\">|      o     o o  |</span><br><span class=\"line\">|            .o   |</span><br><span class=\"line\">+----[SHA256]-----+</span><br></pre></td></tr></table></figure>\n\n<p>接下来将公钥追加到如下文件，并修改文件权限：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng.sunjc$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class=\"line\">jincheng.sunjc$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>\n\n<p>利用ssh localhost验证，看到 Last login: 字样为ssh成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:~ jincheng.sunjc$ ssh localhost</span><br><span class=\"line\">Password:</span><br><span class=\"line\">Last login: Tue Dec 31 18:26:48 2019 from ::1</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>设置环境变量</li>\n</ul>\n<p>设置JAVA_HOME,HADOOP_HOME和HADOOP_CONF_DIR，<code>vi ~/.bashrc</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home</span><br><span class=\"line\"></span><br><span class=\"line\">export HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexec</span><br><span class=\"line\"></span><br><span class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br></pre></td></tr></table></figure>\n\n<p>NOTE: 后续操作要确保的terminal环境变量是生效哦， 如果不生效可以执行 <code>source ~/.bashrc</code>。:)</p>\n<ul>\n<li><p>修改配置</p>\n<p>  1) 修改core-site.xml</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class=\"line\">      &lt;value&gt;/tmp&lt;/value&gt;</span><br><span class=\"line\">   &lt;/property&gt;</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">      &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class=\"line\">      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<pre><code>2) 修改hdfs-site.xml</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/tmp/hadoop/name&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/tmp/hadoop/data&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<pre><code>3) 修改yarn-site.xml</code></pre><p>配置 YARN 作为资源管理框架：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    </span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;  &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<p>简单的配置已经完成，我们执行一下简单命令启动环境：</p>\n<ul>\n<li>格式化文档系统：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:libexec jincheng.sunjc$ hadoop namenode -format</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">2019-12-31 18:58:53,260 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class=\"line\">/************************************************************</span><br><span class=\"line\">SHUTDOWN_MSG: Shutting down NameNode at jincheng.local/127.0.0.1</span><br><span class=\"line\">************************************************************/</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>启动服务：</li>\n</ul>\n<p>我们先启动hdf再启动yarn，如下图：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778389671396.jpg\" alt></p>\n<p>Okay,一切顺利的话，我们会启动namenodes，datanodes，resourcemanager和nodemanagers。我们有几个web界面可以查看，如下：</p>\n<p> 1). Overview 界面， <a href=\"http://localhost:9870\" target=\"_blank\" rel=\"noopener\">http://localhost:9870 </a>如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373195478.jpg\" alt></p>\n<p> 2). NodeManager界面， <a href=\"http://localhost:8042\" target=\"_blank\" rel=\"noopener\">http://localhost:8042</a>，如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373437959.jpg\" alt=\"-w649\"></p>\n<p> 3). ResourceManager 管理界面 <a href=\"http://localhost:8088/\" target=\"_blank\" rel=\"noopener\">http://localhost:8088/</a>,如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777972228895.jpg\" alt=\"-w1054\"></p>\n<p>目前YARN的环境已经准备完成，我们接下来看如何与Flink进行集成。</p>\n<h2 id=\"Flink集成Hadoop包\"><a href=\"#Flink集成Hadoop包\" class=\"headerlink\" title=\"Flink集成Hadoop包\"></a>Flink集成Hadoop包</h2><p>切换到编译结果目录下<code>flink/build-target</code>,并将haddop的JAR包放到<code>lib</code>目录。<br>在官网下载hadoop包，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd lib;</span><br><span class=\"line\">curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar &gt; flink-shaded-hadoop-2-uber-2.8.3-7.0.jar</span><br></pre></td></tr></table></figure>\n\n<p>下载后，lib目录下文件如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777916683830.jpg\" alt></p>\n<p>到现在为止我们可以提交PyFlink的作业到由YARN进行资源分配的集群了。但为了确保集群上有正确的Python环境我们最好打包一个Python环境到集群上面。因为大部分情况下我们无法得知yarn集群上的Python版本是否符合我们的要求(Python 3.5+，装有apache-beam 2.15.0)，因此我们需要打包一个符合条件的Python环境，并随job文件提交到yarn集群上。</p>\n<h2 id=\"打包Python环境\"><a href=\"#打包Python环境\" class=\"headerlink\" title=\"打包Python环境\"></a>打包Python环境</h2><p>再次检查一下当前Python的版本是否3.5+，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:lib jincheng.sunjc$ Python</span><br><span class=\"line\">Python 3.7.6 (default, Dec 31 2019, 09:48:30)</span><br></pre></td></tr></table></figure>\n\n<p>由于这个Python环境是用于集群的，所以打包时的系统需要和集群一致。如果不一致，比如集群是linux而本机是mac，我们需要在虚拟机或者docker中打包。以下列出两种情况的示范方法，读者根据需求选择一种即可。</p>\n<h3 id=\"本地打包（集群和本机操作系统一致时）\"><a href=\"#本地打包（集群和本机操作系统一致时）\" class=\"headerlink\" title=\"本地打包（集群和本机操作系统一致时）\"></a>本地打包（集群和本机操作系统一致时）</h3><p>如果集群所在机器的操作系统和本地一致（都是mac或者都是linux），直接通过virtualenv打包一个符合条件的Python环境：</p>\n<ul>\n<li>安装virtualenv<br>使用<code>python -m pip install virtualenv</code> 进行安装如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ python -m pip install virtualenv</span><br><span class=\"line\">Collecting virtualenv</span><br><span class=\"line\">  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)</span><br><span class=\"line\">     |████████████████████████████████| 3.4MB 2.0MB/s </span><br><span class=\"line\">Installing collected packages: virtualenv</span><br><span class=\"line\">Successfully installed virtualenv-16.7.9</span><br></pre></td></tr></table></figure>\n\n<p>我本地环境已经成功安装。</p>\n<ul>\n<li>创建Python环境<br>用virtualenv以always-copy方式建立一个全新的Python环境，名字随意，以venv为例，<code>virtualenv --always-copy venv</code>:</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ virtualenv --always-copy venv</span><br><span class=\"line\">Using base prefix &apos;/usr/local/Cellar/Python/3.7.6/Frameworks/Python.framework/Versions/3.7&apos;</span><br><span class=\"line\">New Python executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python3.7</span><br><span class=\"line\">Also creating executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python</span><br><span class=\"line\">Installing setuptools, pip, wheel...</span><br><span class=\"line\">done.</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>在新环境中安装apache-beam 2.15.0<br>使用<code>venv/bin/pip install apache-beam==2.15.0</code>进行安装：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ venv/bin/pip install apache-beam==2.15.0</span><br><span class=\"line\">Collecting apache-beam==2.15.0</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7</span><br></pre></td></tr></table></figure>\n\n<p>上面信息已经说明我们成功的在Python环境中安装了apache-beam==2.15.0。接下来我们打包Python环境。</p>\n<ul>\n<li>打包Python环境<br>我们将Python打包成zip文件，<code>zip -r venv.zip venv</code> 如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip -r venv.zip venv</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">  adding: venv/lib/Python3.7/re.py (deflated 68%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/struct.py (deflated 46%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/sre_parse.py (deflated 80%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/abc.py (deflated 72%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/_bootlocale.py (deflated 63%)</span><br></pre></td></tr></table></figure>\n\n<p>查看一下zip大小：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ du -sh venv.zip </span><br><span class=\"line\"> 81M\tvenv.zip</span><br></pre></td></tr></table></figure>\n\n<p>这个大小实在太大了，核心问题是Beam的包非常大，后面我会持续在Beam社区提出优化建议。我们先忍一下:(。</p>\n<h3 id=\"Docker中打包（比如集群为linux，本机为mac时）\"><a href=\"#Docker中打包（比如集群为linux，本机为mac时）\" class=\"headerlink\" title=\"Docker中打包（比如集群为linux，本机为mac时）\"></a>Docker中打包（比如集群为linux，本机为mac时）</h3><p>我们选择在docker中打包，可以从以下链接下载最新版docker并安装：<br><a href=\"https://download.docker.com/mac/stable/Docker.dmg\" target=\"_blank\" rel=\"noopener\">https://download.docker.com/mac/stable/Docker.dmg</a>安装完毕后重启终端，执行<code>docker version</code>确认docker安装成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ docker version</span><br><span class=\"line\">Client: Docker Engine - Community</span><br><span class=\"line\"> Version:           19.03.4</span><br><span class=\"line\"> API version:       1.40</span><br><span class=\"line\"> Go version:        go1.12.10</span><br><span class=\"line\"> Git commit:        9013bf5</span><br><span class=\"line\"> Built:             Thu Oct 17 23:44:48 2019</span><br><span class=\"line\"> OS/Arch:           darwin/amd64</span><br><span class=\"line\"> Experimental:      false</span><br><span class=\"line\"></span><br><span class=\"line\">Server: Docker Engine - Community</span><br><span class=\"line\"> Engine:</span><br><span class=\"line\">  Version:          19.03.4</span><br><span class=\"line\">  API version:      1.40 (minimum version 1.12)</span><br><span class=\"line\">  Go version:       go1.12.10</span><br><span class=\"line\">  Git commit:       9013bf5</span><br><span class=\"line\">  Built:            Thu Oct 17 23:50:38 2019</span><br><span class=\"line\">  OS/Arch:          linux/amd64</span><br><span class=\"line\">  Experimental:     false</span><br><span class=\"line\"> containerd:</span><br><span class=\"line\">  Version:          v1.2.10</span><br><span class=\"line\">  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339</span><br><span class=\"line\"> runc:</span><br><span class=\"line\">  Version:          1.0.0-rc8+dev</span><br><span class=\"line\">  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657</span><br><span class=\"line\"> docker-init:</span><br><span class=\"line\">  Version:          0.18.0</span><br><span class=\"line\">  GitCommit:        fec3683</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>启动容器<br>我们启动一个Python 3.7版本的容器如果是第一次启动可能需要较长时间来拉取镜像：<br><code>docker run -it Python:3.7 /bin/bash</code>, 如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:libexec jincheng.sunjc$  docker run -it Python:3.7 /bin/bash</span><br><span class=\"line\">Unable to find image &apos;Python:3.7&apos; locally</span><br><span class=\"line\">3.7: Pulling from library/Python</span><br><span class=\"line\">8f0fdd3eaac0: Pull complete </span><br><span class=\"line\">d918eaefd9de: Pull complete </span><br><span class=\"line\">43bf3e3107f5: Pull complete </span><br><span class=\"line\">27622921edb2: Pull complete </span><br><span class=\"line\">dcfa0aa1ae2c: Pull complete </span><br><span class=\"line\">bf6840af9e70: Pull complete </span><br><span class=\"line\">167665d59281: Pull complete </span><br><span class=\"line\">ffc544588c7f: Pull complete </span><br><span class=\"line\">4ebe99df65fe: Pull complete </span><br><span class=\"line\">Digest: sha256:40d615d7617f0f3b54614fd228d41a891949b988ae2b452c0aaac5bee924888d</span><br><span class=\"line\">Status: Downloaded newer image for Python:3.7</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>容器中安装virtualenv<br>我们在刚才启动的容器中安装virtualenv， <code>pip install virtualenv</code>,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# pip install virtualenv</span><br><span class=\"line\">Collecting virtualenv</span><br><span class=\"line\">  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)</span><br><span class=\"line\">     |████████████████████████████████| 3.4MB 2.0MB/s </span><br><span class=\"line\">Installing collected packages: virtualenv</span><br><span class=\"line\">Successfully installed virtualenv-16.7.9</span><br><span class=\"line\">root@1b48d2b526ae:/#</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建Python环境<br>以always copy方式建立一个全新的Python环境，名字随意，以venv为例，<code>virtualenv --always-copy venv</code>, 如下：</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# virtualenv --always-copy venv</span><br><span class=\"line\">Using base prefix &apos;/usr/local&apos;</span><br><span class=\"line\">New Python executable in /venv/bin/Python</span><br><span class=\"line\">Installing setuptools, pip, wheel...</span><br><span class=\"line\">done.</span><br><span class=\"line\">root@1b48d2b526ae:/#</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>安装Apache Beam<br>在新的Python环境中安装apache-beam 2.15.0，<code>venv/bin/pip install apache-beam==2.15.0</code>,如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# venv/bin/pip install apache-beam==2.15.0</span><br><span class=\"line\">Collecting apache-beam==2.15.0</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看docker中的Python环境<br>用<code>exit</code>命令退出容器，用<code>docker ps -a</code>找到docker容器的id，用于拷贝文件,如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# exit</span><br><span class=\"line\">exit</span><br><span class=\"line\">jincheng:libexec jincheng.sunjc$ docker ps -a</span><br><span class=\"line\">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES</span><br><span class=\"line\">1b48d2b526ae        Python:3.7          &quot;/bin/bash&quot;         7 minutes ago       Exited (0) 8 seconds ago                       elated_visvesvaraya</span><br></pre></td></tr></table></figure>\n\n<p>由于刚刚结束，一般来说是列表中的第一条，可以根据容器的镜像名Python:3.7来分辨。我们记下最左边的容器ID。如上是<code>1b48d2b526ae</code>。</p>\n<ul>\n<li>打包Python环境<br>从将容器中的Python环境拷贝出来，我们切换到<code>flink/build-target</code>录下，拷贝 <code>docker cp 1b48d2b526ae:/venv ./</code>并打包<code>zip -r venv.zip venv</code>。<br>最终<code>flink/build-target</code>录下生成<code>venv.zip</code>。</li>\n</ul>\n<h2 id=\"部署作业\"><a href=\"#部署作业\" class=\"headerlink\" title=\"部署作业\"></a>部署作业</h2><p>终于到部署作业的环节了:), flink on yarn支持两种模式，per-job和session。per-job模式在提交job时会为每个job单独起一个flink集群，session模式先在yarn上起一个flink集群，之后提交job都提交到这个flink集群。</p>\n<h3 id=\"Pre-Job-模式部署作业\"><a href=\"#Pre-Job-模式部署作业\" class=\"headerlink\" title=\"Pre-Job 模式部署作业\"></a>Pre-Job 模式部署作业</h3><p>执行以下命令，以Pre-Job模式部署PyFlink作业：<br><code>bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</code>,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</span><br><span class=\"line\">2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">2020-01-02 13:04:55,945 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class=\"line\">2020-01-02 13:04:56,049 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class=\"line\">2020-01-02 13:05:01,153 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.</span><br><span class=\"line\">2020-01-02 13:05:01,177 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class=\"line\">2020-01-02 13:05:01,294 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system</span><br><span class=\"line\">2020-01-02 13:05:02,600 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0004</span><br><span class=\"line\">2020-01-02 13:05:02,971 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0004</span><br><span class=\"line\">2020-01-02 13:05:02,972 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated</span><br><span class=\"line\">2020-01-02 13:05:02,975 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED</span><br><span class=\"line\">2020-01-02 13:05:23,138 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.</span><br><span class=\"line\">2020-01-02 13:05:23,140 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:61616 of application &apos;application_1577936885434_0004&apos;.</span><br><span class=\"line\">Job has been submitted with JobID a41d82194a500809fd715da8f29894a0</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID a41d82194a500809fd715da8f29894a0 has finished.</span><br><span class=\"line\">Job Runtime: 35576 ms</span><br></pre></td></tr></table></figure>\n\n<p>上面信息已经显示运行完成，在Web界面可以看到作业状态：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417223687.jpg\" alt></p>\n<p>我们再检验一下计算结果<code>cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</code>：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417681405.jpg\" alt></p>\n<p>到这里，我们以Pre-Job的方式成功部署了PyFlink的作业！相比提交到本地Standalone集群，多了三个参数，我们简单说明如下：</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-m yarn-cluster</td>\n<td>以Per-Job模式部署到yarn集群</td>\n</tr>\n<tr>\n<td>-pyarch venv.zip</td>\n<td>将当前目录下的venv.zip上传到yarn集群</td>\n</tr>\n<tr>\n<td>-pyexec venv.zip/venv/bin/Python</td>\n<td>指定venv.zip中的Python解释器来执行Python UDF，路径需要和zip包内部结构一致。</td>\n</tr>\n</tbody></table>\n<h3 id=\"Session-模式部署作业\"><a href=\"#Session-模式部署作业\" class=\"headerlink\" title=\"Session 模式部署作业\"></a>Session 模式部署作业</h3><p>以Session模式部署作业也非常简单，我们实际操作一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/yarn-session.sh </span><br><span class=\"line\">2020-01-02 13:58:53,049 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.memory.process.size, 1024m</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class=\"line\">2020-01-02 13:58:53,051 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.execution.failover-strategy, region</span><br><span class=\"line\">2020-01-02 13:58:53,413 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class=\"line\">2020-01-02 13:58:53,476 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to jincheng.sunjc (auth:SIMPLE)</span><br><span class=\"line\">2020-01-02 13:58:53,509 INFO  org.apache.flink.runtime.security.modules.JaasModule          - Jaas file will be created as /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/jaas-3848984206030141476.conf.</span><br><span class=\"line\">2020-01-02 13:58:53,521 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">2020-01-02 13:58:53,562 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class=\"line\">2020-01-02 13:58:58,803 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.</span><br><span class=\"line\">2020-01-02 13:58:58,824 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class=\"line\">2020-01-02 13:59:03,975 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system</span><br><span class=\"line\">2020-01-02 13:59:04,779 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0005</span><br><span class=\"line\">2020-01-02 13:59:04,799 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0005</span><br><span class=\"line\">2020-01-02 13:59:04,799 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated</span><br><span class=\"line\">2020-01-02 13:59:04,801 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED</span><br><span class=\"line\">2020-01-02 13:59:24,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.</span><br><span class=\"line\">2020-01-02 13:59:24,713 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.</span><br><span class=\"line\">JobManager Web Interface: http://localhost:62247</span><br></pre></td></tr></table></figure>\n\n<p>执行成功后不会返回，但会启动一个JoBManager Web，地址如上<code>http://localhost:62247</code>，可复制到浏览器查看:<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779449909178.jpg\" alt></p>\n<p>我们可以修改conf/flink-conf.yaml中的配置参数。如果要更改某些内容，请参考<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>。接下来我们提交作业，首先按组合键Ctrl+Z将yarn-session.sh进程切换到后台，并执行bg指令让其在后台继续执行, 然后执行以下命令，即可向Session模式的flink集群提交job <code>bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</span><br><span class=\"line\"></span><br><span class=\"line\">2020-01-02 14:10:48,285 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.</span><br><span class=\"line\">Job has been submitted with JobID bea33b7aa07c0f62153ab5f6e134b6bf</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID bea33b7aa07c0f62153ab5f6e134b6bf has finished.</span><br><span class=\"line\">Job Runtime: 34405 ms</span><br></pre></td></tr></table></figure>\n\n<p>如果在打印<code>finished</code>之前查看之前的web页面，我们会发现Session集群会有一个正确运行的作业，如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779454827330.jpg\" alt><br>如果已经运行完成，那么我们应该会看到状态也变成结束：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779456315383.jpg\" alt></p>\n<p>相比per job模式提交，少了”-m”参数。因为之前已经启动了yarn-session.sh，所以flink默认会向yarn-session.sh启动的集群上提交job。执行完毕后，别忘了关闭yarn-session.sh（session 模式）：先将yarn-session.sh调到前台，执行<code>fg</code>,然后在再按Ctrl+C结束进程或者执行<code>stop</code>，结束时yarn上的集群也会被关闭。</p>\n<h1 id=\"Docker-模式部署\"><a href=\"#Docker-模式部署\" class=\"headerlink\" title=\"Docker 模式部署\"></a>Docker 模式部署</h1><p>我们还可以将Flink Python job打包成docker镜像，然后使用docker-compose或者Kubernetes部署执行，由于现在的docker镜像打包工具并没有完美支持运行Python UDF，因此我们需要往里面添加一些额外的文件。首先是一个仅包含PythonDriver类的jar包. 我们在<code>build-target</code>目录下执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ mkdir temp</span><br><span class=\"line\">jincheng:build-target jincheng.sunjc$ cd temp</span><br><span class=\"line\">jincheng:temp jincheng.sunjc$ unzip ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar org/apache/flink/client/Python/PythonDriver.class</span><br><span class=\"line\">Archive:  ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar</span><br><span class=\"line\">  inflating: org/apache/flink/client/Python/PythonDriver.class</span><br></pre></td></tr></table></figure>\n\n<p>解压之后，我们在进行压缩打包：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:temp jincheng.sunjc$ zip Python-driver.jar org/apache/flink/client/Python/PythonDriver.class</span><br><span class=\"line\">  adding: org/apache/flink/client/Python/PythonDriver.class (deflated 56%)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779465411786.jpg\" alt><br>我们得到<code>Python-driver.jar</code>。然后下载一个pyArrow的安装文件(我准备了一个大家下载直接使用即可 <a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/share_resource/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl\" target=\"_blank\" rel=\"noopener\">pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl</a>。执行以下命令构建Docker镜像，需要作为artifacts引入的文件有作业文件，Python-driver的jar包和pyarrow安装文件,<code>./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist</code>(进入<code>flink/flink-container/docker</code>目录)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ ./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist</span><br><span class=\"line\">Using flink dist: ../../flink-dist/target/flink-*-bin</span><br><span class=\"line\">a .</span><br><span class=\"line\">a ./flink-1.10-SNAPSHOT</span><br><span class=\"line\">a ./flink-1.10-SNAPSHOT/temp</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Removing intermediate container a0558bbcbdd1</span><br><span class=\"line\"> ---&gt; 00ecda6117b7</span><br><span class=\"line\">Successfully built 00ecda6117b7</span><br><span class=\"line\">Successfully tagged flink-job:latest</span><br></pre></td></tr></table></figure>\n\n<p>构建Docker镜像需要较长时间，请耐心等待。构建完毕之后，可以输入docker images命令在镜像列表中找到构建结果<code>docker images</code>：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779503938890.jpg\" alt=\"-w765\"><br>然后我们在构建好的镜像基础上安装好Python udf所需依赖，并删除过程中产生的临时文件：</p>\n<ul>\n<li><p>启动docker容器<br><code>docker run -it --user root --entrypoint /bin/bash --name flink-job-container flink-job</code></p>\n</li>\n<li><p>安装一些依赖<br><code>apk add --no-cache g++ Python3-dev musl-dev</code></p>\n</li>\n<li><p>安装PyArrow<br><code>python -m pip3 install /opt/artifacts/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl</code></p>\n</li>\n<li><p>安装Apache Beam<br><code>python -m pip3 install apache-beam==2.15.0</code></p>\n</li>\n<li><p>删除临时文件<br><code>rm -rf /root/.cache/pip</code></p>\n</li>\n</ul>\n<p>执行完如上命令我可以执行<code>exit</code>退出容器了，然后把这个容器提交为新的flink-job镜像<code>docker commit -c &#39;CMD [&quot;--help&quot;]&#39; -c &quot;USER flink&quot; -c &#39;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&#39; flink-job-container flink-job:latest</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ docker commit -c &apos;CMD [&quot;--help&quot;]&apos; -c &quot;USER flink&quot; -c &apos;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&apos; flink-job-container flink-job:latest </span><br><span class=\"line\">sha256:0740a635e2b0342ddf776f33692df263ebf0437d6373f156821f4dd044ad648b</span><br></pre></td></tr></table></figure>\n\n<p>到这里包含Python UDF 作业的Docker镜像就制作好了，这个Docker镜像既可以以docker-compose使用，也可以结合kubernetes中使用。</p>\n<p>我们以使用docker-compose执行为例，mac版docker自带docker-compose，用户可以直接使用，在<code>flink/flink-container/docker</code>目录下，使用以下命令启动作业,<code>FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose up</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose up</span><br><span class=\"line\">WARNING: The SAVEPOINT_OPTIONS variable is not set. Defaulting to a blank string.</span><br><span class=\"line\">Recreating docker_job-cluster_1 ... done</span><br><span class=\"line\">Starting docker_taskmanager_1   ... done</span><br><span class=\"line\">Attaching to docker_taskmanager_1, docker_job-cluster_1</span><br><span class=\"line\">taskmanager_1  | Starting the task-manager</span><br><span class=\"line\">job-cluster_1  | Starting the job-cluster</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">job-cluster_1  | 2020-01-02 08:35:03,796 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Terminating cluster entrypoint process StandaloneJobClusterEntryPoint with exit code 0.</span><br><span class=\"line\">docker_job-cluster_1 exited with code 0</span><br></pre></td></tr></table></figure>\n\n<p>在log中出现“docker_job-cluster_1 exited with code 0”表示job已执行成功，JobManager已经退出。TaskManager还需要较长的时间等待超时后才会退出，我们可以直接按快捷键Ctrl+C提前退出。</p>\n<p>查看执行结果，可以从TaskManager的容器中将结果文件拷贝出来查看,执行 <code>docker cp docker_taskmanager_1:/tmp/result ./; cat result</code><br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779542617667.jpg\" alt></p>\n<p>Okay, 到这里本篇要与大家分享的内容已经接近尾声了，如果你期间也很顺利的成功了，可以 Cheers 了:)</p>\n<h1 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h1><p>本篇核心向大家分享了如何以多种方式部署PyFlink作业。期望在PyFlink1.10发布之后，大家能有一个顺利快速体验的快感！在开篇说道部分，为大家分享了老子倡导大家的 “致虚极，守静笃。万物并作，吾以观其复”的大道，同时也给大家带来了2020的祝福，祝福大家 “2020 安！”。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"开篇说道\"><a href=\"#开篇说道\" class=\"headerlink\" title=\"开篇说道\"></a>开篇说道</h1><p>老子说：“致虚极，守静笃。万物并作，吾以观其复”。 字里行间是告诉人们要尽量使心灵达到虚寂的极致，持之以恒的坚守宁静。老子认为“虚”和“静”是心灵的本初的状态，也应该是一种常态，即，是一种没有心机，没有成见，没有外界名利，物质诱惑，而保持的状态。当达到这个状态之后，就可以静观万物的蓬勃发展，了解万物的循环往复的变化，通晓自然之理，体悟自然之道。</p>\n<p>“致虚极，守静笃” 并不代表对任何事情无动于衷，而是通过 “致虚极，守静笃” 的方式了解万物变化的根源，就好比，看到新芽不惊，看到落叶不哀，因为我们知道，落叶归根，重归泥土之后，还会再破土而出，开始新的轮回。</p>\n<p>在2020年1月1日零点我发了一条朋友圈，对自己的祝福是：”㊗️ 2020 安”。<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779569378844.jpg\" alt><br>当然这里我也祝福所有看到这篇博客的你: “㊗️ 2020 安”。 祝福大家 “安”，也即祝福大家 能够在新的一年，追求内心的修炼，摒去纷纷复杂的物欲干扰，拂去心灵的灰尘，保持自己的赤子之心！ </p>\n<h1 id=\"道破-“天机”\"><a href=\"#道破-“天机”\" class=\"headerlink\" title=\"道破 “天机”\"></a>道破 “天机”</h1><p>前面一些博客大多介绍PyFlink的功能开发，比如，如何使用各种算子(Join/Window/AGG etc.)，如何使用各种Connector(Kafka, CSV, Socket etc.)，还有一些实际的案例。这些都停留在开发阶段，一旦开发完成，我们就面临激动人心的时刻，那就是 将我们精心设计开发的作业进行部署，那么问题来了，你知道怎样部署PyFlink的作业吗？这篇将为大家全面介绍部署PyFlink作业的各种模式。</p>\n<h1 id=\"组件栈回顾\"><a href=\"#组件栈回顾\" class=\"headerlink\" title=\"组件栈回顾\"></a>组件栈回顾</h1><p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777810183663.jpg\" alt></p>\n<p>上面的组件栈除了PyFlink是第一次添加上去，其他部分大家应该非常熟悉了。目前PyFlink基于Java的Table API之上，同时在Runtime层面有Python的算子和执行容器。那么我们聚焦重点，看最底层的 Deploy部分，上图我们分成了三种部署模式，Local/Cluster/Cloud，其中Local模式还有2种不同方式，一是SingleJVM，也即是MiniCluster, 前面博客里面运行示例所使用的就是MiniCluster。二是SingleNode，也就是虽然是集群模式，但是所有角色都在一台机器上。下面我们简单介绍一下上面这几种部署模式的区别：</p>\n<ul>\n<li>Local-SingleJVM 模式- 该模式大多是开发测试阶段使用的方式，所有角色TM，JM等都在同一个JVM里面。</li>\n<li>Local-SingleNode 模式 - 意在所有角色都运行在同一台机器，直白一点就是从运行的架构上看，这种模式虽然是分布式的，但集群节点只有1个，该模式大多是测试和IoT设备上进行部署使用。</li>\n<li>Cluster 模式 - 也就是我们经常用于投产的分布式部署方式，上图根据对资源管理的方式不同又分为了多种，如：Standalone 是Flink自身进行资源管理，YARN，顾名思义就是利用资源管理框架Yarn来负责Flink运行资源的分配，还有结合Kubernetes等等。</li>\n<li>Cloud 模式- 该部署模式是结合其他云平台进行部署。</li>\n</ul>\n<p>接下来我们看看PyFlink的作业可以进行怎样的模式部署？</p>\n<h1 id=\"环境依赖\"><a href=\"#环境依赖\" class=\"headerlink\" title=\"环境依赖\"></a>环境依赖</h1><ul>\n<li>JDK 1.8+ (1.8.0_211)</li>\n<li>Maven 3.x (3.2.5)</li>\n<li>Scala 2.11+ (2.12.0)</li>\n<li>Python 3.5+ (3.7.6)</li>\n<li>Git 2.20+ (2.20.1)</li>\n</ul>\n<h1 id=\"源码构建及安装\"><a href=\"#源码构建及安装\" class=\"headerlink\" title=\"源码构建及安装\"></a>源码构建及安装</h1><p>在Apache Flink 1.10 发布之后，我们除了源码构建之外，还支持直接利用pip install 安装PyFlink。那么现在我们还是以源码构建的方式进行今天的介绍。</p>\n<ul>\n<li>下载源码</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/apache/flink.git</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>签出release-1.10分支(1.10版本是PyFlink的第二个版本）</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin release-1.10</span><br><span class=\"line\">git checkout -b release-1.10 origin/release-1.10</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建编译</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>\n\n<p>如果一起顺利，你会最终看到如下信息：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">[INFO] flink-walkthrough-table-scala ...................... SUCCESS [  0.070 s]</span><br><span class=\"line\">[INFO] flink-walkthrough-datastream-java .................. SUCCESS [  0.081 s]</span><br><span class=\"line\">[INFO] flink-walkthrough-datastream-scala ................. SUCCESS [  0.067 s]</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] BUILD SUCCESS</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br><span class=\"line\">[INFO] Total time:  16:22 min</span><br><span class=\"line\">[INFO] Finished at: 2019-12-31T10:37:21+08:00</span><br><span class=\"line\">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>构建PyFlink发布包</li>\n</ul>\n<p>上面我们构建了Java的发布包，接下来我们构建PyFlink的发布包,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd flink-Python; Python setup.py sdist</span><br></pre></td></tr></table></figure>\n\n<p>最终输出如下信息，证明是成功的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">copying pyflink/util/exceptions.py -&gt; apache-flink-1.10.dev0/pyflink/util</span><br><span class=\"line\">copying pyflink/util/utils.py -&gt; apache-flink-1.10.dev0/pyflink/util</span><br><span class=\"line\">Writing apache-flink-1.10.dev0/setup.cfg</span><br><span class=\"line\">creating dist</span><br><span class=\"line\">Creating tar archive</span><br><span class=\"line\">removing &apos;apache-flink-1.10.dev0&apos; (and everything under it)</span><br></pre></td></tr></table></figure>\n\n<p>在dist目录的apache-flink-1.10.dev0.tar.gz就是我们可以用于<code>pip install</code>的PyFlink包.</p>\n<ul>\n<li>安装PyFlink</li>\n</ul>\n<p>上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，检测是否之前已经安装过PyFlink，如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 list|grep flink</span><br><span class=\"line\">...</span><br><span class=\"line\">flink                         1.0      </span><br><span class=\"line\">pyflink-demo-connector        0.1</span><br></pre></td></tr></table></figure>\n\n<p>上面信息说明我本机已经安装过PyFlink，我们要先删除，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 uninstall flink</span><br></pre></td></tr></table></figure>\n\n<p>删除以前的安装之后，我们再安装新的如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install dist/*.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully built apache-flink</span><br><span class=\"line\">Installing collected packages: apache-flink</span><br><span class=\"line\">Successfully installed apache-flink-1.10.dev0</span><br></pre></td></tr></table></figure>\n\n<p>我们再用list命令检查一遍：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 list|grep flink</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">apache-flink                  1.10.dev0</span><br><span class=\"line\">pyflink-demo-connector        0.1</span><br></pre></td></tr></table></figure>\n\n<p>其中<code>pyflink-demo-connector</code> 是我以前做实验时候的安装，对本篇没有影响。</p>\n<h1 id=\"安装Apache-Beam-依赖\"><a href=\"#安装Apache-Beam-依赖\" class=\"headerlink\" title=\"安装Apache Beam 依赖\"></a>安装Apache Beam 依赖</h1><p>我们需要使用Python3.5+ 版本，检验一下Python版本，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng.sunjc$ Python --version</span><br><span class=\"line\">Python 3.7.6</span><br></pre></td></tr></table></figure>\n\n<p>我本机是Python3.7.6，现在我们需要安装Apache Beam，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m pip install apache-beam==2.15.0</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\">Installing collected packages: apache-beam</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0</span><br></pre></td></tr></table></figure>\n\n<p>如果顺利的出现上面信息，说明Apache-beam已经安装成功。</p>\n<h1 id=\"PyFlink示例作业\"><a href=\"#PyFlink示例作业\" class=\"headerlink\" title=\"PyFlink示例作业\"></a>PyFlink示例作业</h1><p>接下来我们开发一个简单的PyFlink作业，源码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import logging</span><br><span class=\"line\">import os</span><br><span class=\"line\">import shutil</span><br><span class=\"line\">import sys</span><br><span class=\"line\">import tempfile</span><br><span class=\"line\"></span><br><span class=\"line\">from pyflink.table import BatchTableEnvironment, EnvironmentSettings</span><br><span class=\"line\">from pyflink.table.descriptors import FileSystem, OldCsv, Schema</span><br><span class=\"line\">from pyflink.table.types import DataTypes</span><br><span class=\"line\">from pyflink.table.udf import udf</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def word_count():</span><br><span class=\"line\">   environment_settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()</span><br><span class=\"line\">   t_env = BatchTableEnvironment.create(environment_settings=environment_settings)</span><br><span class=\"line\"></span><br><span class=\"line\">   # register Results table in table environment</span><br><span class=\"line\">   tmp_dir = tempfile.gettempdir()</span><br><span class=\"line\">   result_path = tmp_dir + &apos;/result&apos;</span><br><span class=\"line\">   if os.path.exists(result_path):</span><br><span class=\"line\">       try:</span><br><span class=\"line\">           if os.path.isfile(result_path):</span><br><span class=\"line\">               os.remove(result_path)</span><br><span class=\"line\">           else:</span><br><span class=\"line\">               shutil.rmtree(result_path)</span><br><span class=\"line\">       except OSError as e:</span><br><span class=\"line\">           logging.error(&quot;Error removing directory: %s - %s.&quot;, e.filename, e.strerror)</span><br><span class=\"line\"></span><br><span class=\"line\">   logging.info(&quot;Results directory: %s&quot;, result_path)</span><br><span class=\"line\"></span><br><span class=\"line\">   # we should set the Python verison here if `Python` not point</span><br><span class=\"line\">   t_env.get_config().set_Python_executable(&quot;Python3&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.connect(FileSystem().path(result_path)) \\</span><br><span class=\"line\">       .with_format(OldCsv()</span><br><span class=\"line\">                    .field_delimiter(&apos;,&apos;)</span><br><span class=\"line\">                    .field(&quot;city&quot;, DataTypes.STRING())</span><br><span class=\"line\">                    .field(&quot;sales_volume&quot;, DataTypes.BIGINT())</span><br><span class=\"line\">                    .field(&quot;sales&quot;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">       .with_schema(Schema()</span><br><span class=\"line\">                    .field(&quot;city&quot;, DataTypes.STRING())</span><br><span class=\"line\">                    .field(&quot;sales_volume&quot;, DataTypes.BIGINT())</span><br><span class=\"line\">                    .field(&quot;sales&quot;, DataTypes.BIGINT())) \\</span><br><span class=\"line\">       .register_table_sink(&quot;Results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   @udf(input_types=DataTypes.STRING(), result_type=DataTypes.ARRAY(DataTypes.STRING()))</span><br><span class=\"line\">   def split(input_str: str):</span><br><span class=\"line\">       return input_str.split(&quot;,&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   @udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())</span><br><span class=\"line\">   def get(arr, index):</span><br><span class=\"line\">       return arr[index]</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.register_function(&quot;split&quot;, split)</span><br><span class=\"line\">   t_env.register_function(&quot;get&quot;, get)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.get_config().get_configuration().set_string(&quot;parallelism.default&quot;, &quot;1&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   data = [(&quot;iPhone 11,30,5499,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,20,8699,Guangzhou&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,9999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;AirPods Pro,50,1999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,11499,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11,30,5999,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,20,9999,Shenzhen&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,13899,Hangzhou&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11,10,6799,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,18999,Beijing&quot;, ),</span><br><span class=\"line\">           (&quot;iPhone 11 Pro,10,11799,Shenzhen&quot;, ),</span><br><span class=\"line\">           (&quot;MacBook Pro,10,22199,Shanghai&quot;, ),</span><br><span class=\"line\">           (&quot;AirPods Pro,40,1999,Shanghai&quot;, )]</span><br><span class=\"line\">   t_env.from_elements(data, [&quot;line&quot;]) \\</span><br><span class=\"line\">       .select(&quot;split(line) as str_array&quot;) \\</span><br><span class=\"line\">       .select(&quot;get(str_array, 3) as city, &quot;</span><br><span class=\"line\">               &quot;get(str_array, 1).cast(LONG) as count, &quot;</span><br><span class=\"line\">               &quot;get(str_array, 2).cast(LONG) as unit_price&quot;) \\</span><br><span class=\"line\">       .select(&quot;city, count, count * unit_price as total_price&quot;) \\</span><br><span class=\"line\">       .group_by(&quot;city&quot;) \\</span><br><span class=\"line\">       .select(&quot;city, &quot;</span><br><span class=\"line\">               &quot;sum(count) as sales_volume, &quot;</span><br><span class=\"line\">               &quot;sum(total_price) as sales&quot;) \\</span><br><span class=\"line\">       .insert_into(&quot;Results&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   t_env.execute(&quot;word_count&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">   logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=&quot;%(message)s&quot;)</span><br><span class=\"line\">   word_count()</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们就介绍如何用不同部署模式运行PyFlink作业！</p>\n<h1 id=\"Local-SingleJVM-模式部署\"><a href=\"#Local-SingleJVM-模式部署\" class=\"headerlink\" title=\"Local-SingleJVM 模式部署\"></a>Local-SingleJVM 模式部署</h1><p>该模式多用于开发测试阶段，简单的利用<code>Python pyflink_job.py</code>命令，PyFlink就会默认启动一个Local-SingleJVM的Flink环境来执行作业，如下：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777784863901.jpg\" alt><br>首先确认你Python是3.5+，然后执行上面的PyFlink作业<code>Python deploy_demo.py</code>,结果写入到本地文件，然后<code>cat</code>计算结果，如果出现如图所示的结果，则说明准备工作已经就绪。:),如果有问题，欢迎留言！<br>这里运行时SingleJVM，在运行这个job时候大家可以查看java进程：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821361064.jpg\" alt><br>我们发现只有一个JVM进程，里面包含了所有Flink所需角色。</p>\n<h1 id=\"Local-SingleNode-模式部署\"><a href=\"#Local-SingleNode-模式部署\" class=\"headerlink\" title=\"Local-SingleNode 模式部署\"></a>Local-SingleNode 模式部署</h1><p>这种模式一般用在单机环境中进行部署，如IoT设备中，我们从0开始进行该模式的部署操作。我们进入到<code>flink/build-target</code>目录，执行如下命令(个人爱好，我把端口改成了8888)：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/start-cluster.sh </span><br><span class=\"line\">...</span><br><span class=\"line\">Starting cluster.</span><br><span class=\"line\">Starting standalonesession daemon on host jincheng.local.</span><br></pre></td></tr></table></figure>\n\n<p>查看一下Flink的进程：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777817105172.jpg\" alt></p>\n<p>我们发现有TM和JM两个进程，虽然在一台机器(Local）但是也是一个集群的架构。<br>上面信息证明已经启动完成，我们可以查看web界面：<a href=\"http://localhost:8888/\" target=\"_blank\" rel=\"noopener\">http://localhost:8888/</a>（我个人爱好端口是8888，默认是8080）, 如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777816500228.jpg\" alt></p>\n<p>目前集群环境已经准备完成，我们看如果将作业部署到集群中，一条简单的命令，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/flink run -m localhost:8888 -py ~/deploy_demo.py</span><br></pre></td></tr></table></figure>\n\n<p>这里如果你不更改端口可以不添加<code>-m</code> 选项。如果一切顺利，你会得到如下输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -m localhost:8888 -py ~/deploy_demo.py </span><br><span class=\"line\">Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">Job has been submitted with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6 has finished.</span><br><span class=\"line\">Job Runtime: 5389 ms</span><br></pre></td></tr></table></figure>\n\n<p>其中<code>/var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</code> 目录是计算结果目录，我们可以产看一下，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$  cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">Beijing,110,622890</span><br><span class=\"line\">Guangzhou,20,173980</span><br><span class=\"line\">Shanghai,90,596910</span><br><span class=\"line\">Shenzhen,30,317970</span><br><span class=\"line\">Hangzhou,10,138990</span><br></pre></td></tr></table></figure>\n\n<p>同时我们也可以在WebUI上面进行查看，在完成的job列表中，显示如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777853568978.jpg\" alt><br>到此，我们完成了在Local模式，其实也是只有一个节点的Standalone模式下完成PyFlink的部署。<br>最后我们为了继续下面的操作，请停止集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/stop-cluster.sh</span><br><span class=\"line\">Stopping taskexecutor daemon (pid: 45714) on host jincheng.local.</span><br><span class=\"line\">Stopping standalonesession daemon (pid: 45459) on host jincheng.local.</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Cluster-YARN-模式部署\"><a href=\"#Cluster-YARN-模式部署\" class=\"headerlink\" title=\"Cluster YARN 模式部署\"></a>Cluster YARN 模式部署</h1><p>这个模式部署，我们需要一个YARN环境，我们一切从简，以单机部署的方式准备YARN环境，然后再与Flink进行集成。</p>\n<h2 id=\"准备YARN环境\"><a href=\"#准备YARN环境\" class=\"headerlink\" title=\"准备YARN环境\"></a>准备YARN环境</h2><ul>\n<li>安装Hadoop</li>\n</ul>\n<p>我本机是mac系统，所以我偷懒一下直接用brew进行安装：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ brew install Hadoop</span><br><span class=\"line\">Updating Homebrew...</span><br><span class=\"line\">==&gt; Auto-updated Homebrew!</span><br><span class=\"line\">Updated 2 taps (homebrew/core and homebrew/cask).</span><br><span class=\"line\">==&gt; Updated Formulae</span><br><span class=\"line\">Python ✔        doxygen         minio           ntopng          typescript</span><br><span class=\"line\">certbot         libngspice      mitmproxy       ooniprobe</span><br><span class=\"line\">doitlive        minimal-racket  ngspice         openimageio</span><br><span class=\"line\"></span><br><span class=\"line\">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-</span><br><span class=\"line\">==&gt; Downloading from http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/</span><br><span class=\"line\">######################################################################## 100.0%</span><br><span class=\"line\"></span><br><span class=\"line\">🍺  /usr/local/Cellar/Hadoop/3.2.1: 22,397 files, 815.6MB, built in 5 minutes 12 seconds</span><br></pre></td></tr></table></figure>\n\n<p>完成之后，检验一下hadoop版本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ hadoop version</span><br><span class=\"line\">Hadoop 3.2.1</span><br></pre></td></tr></table></figure>\n\n<p>超级顺利，hadoop被安装到了<code>/usr/local/Cellar/hadoop/3.2.1/</code>目录下，<code>brew</code>还是很能提高生产力啊～</p>\n<ul>\n<li>配置免登(SSH)</li>\n</ul>\n<p>Mac系统自带了ssh，我们可以简单配置一下即可，我们先打开远程登录。 <code>系统偏好设置</code> -&gt; <code>共享</code> 中，左边勾选 <code>远程登录</code> ，右边选择 <code>仅这些用户</code> （选择<code>所有用户</code>更宽松），并添加当前用户。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:bin jincheng.sunjc$ whoami</span><br><span class=\"line\">jincheng.sunjc</span><br></pre></td></tr></table></figure>\n\n<p>我当前用户是 <code>jincheng.sunjc</code>。 配置图如下：</p>\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777878055973.jpg\" alt></p>\n<p>然后生产证书，如下操作：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class=\"line\">Generating public/private rsa key pair.</span><br><span class=\"line\">/Users/jincheng.sunjc/.ssh/id_rsa already exists.</span><br><span class=\"line\">Overwrite (y/n)? y</span><br><span class=\"line\">Your identification has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.</span><br><span class=\"line\">Your public key has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.pub.</span><br><span class=\"line\">The key fingerprint is:</span><br><span class=\"line\">SHA256:IkjKkOjfMx1fxWlwtQYg8hThph7Xlm9kPutAYFmQR0A jincheng.sunjc@jincheng.local</span><br><span class=\"line\">The key&apos;s randomart image is:</span><br><span class=\"line\">+---[RSA 2048]----+</span><br><span class=\"line\">|       ..EB=.o.. |</span><br><span class=\"line\">|..      =.+.+ o .|</span><br><span class=\"line\">|+ .      B.  = o |</span><br><span class=\"line\">|+o .    + o + .  |</span><br><span class=\"line\">|.o. . .+S. * o   |</span><br><span class=\"line\">|  . ..o.= + =    |</span><br><span class=\"line\">|   . + o . . =   |</span><br><span class=\"line\">|      o     o o  |</span><br><span class=\"line\">|            .o   |</span><br><span class=\"line\">+----[SHA256]-----+</span><br></pre></td></tr></table></figure>\n\n<p>接下来将公钥追加到如下文件，并修改文件权限：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng.sunjc$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class=\"line\">jincheng.sunjc$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>\n\n<p>利用ssh localhost验证，看到 Last login: 字样为ssh成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:~ jincheng.sunjc$ ssh localhost</span><br><span class=\"line\">Password:</span><br><span class=\"line\">Last login: Tue Dec 31 18:26:48 2019 from ::1</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>设置环境变量</li>\n</ul>\n<p>设置JAVA_HOME,HADOOP_HOME和HADOOP_CONF_DIR，<code>vi ~/.bashrc</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home</span><br><span class=\"line\"></span><br><span class=\"line\">export HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexec</span><br><span class=\"line\"></span><br><span class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br></pre></td></tr></table></figure>\n\n<p>NOTE: 后续操作要确保的terminal环境变量是生效哦， 如果不生效可以执行 <code>source ~/.bashrc</code>。:)</p>\n<ul>\n<li><p>修改配置</p>\n<p>  1) 修改core-site.xml</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class=\"line\">      &lt;value&gt;/tmp&lt;/value&gt;</span><br><span class=\"line\">   &lt;/property&gt;</span><br><span class=\"line\">   &lt;property&gt;</span><br><span class=\"line\">      &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class=\"line\">      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<pre><code>2) 修改hdfs-site.xml</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/tmp/hadoop/name&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/tmp/hadoop/data&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<pre><code>3) 修改yarn-site.xml</code></pre><p>配置 YARN 作为资源管理框架：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    </span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;  &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n\n<p>简单的配置已经完成，我们执行一下简单命令启动环境：</p>\n<ul>\n<li>格式化文档系统：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:libexec jincheng.sunjc$ hadoop namenode -format</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">2019-12-31 18:58:53,260 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class=\"line\">/************************************************************</span><br><span class=\"line\">SHUTDOWN_MSG: Shutting down NameNode at jincheng.local/127.0.0.1</span><br><span class=\"line\">************************************************************/</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>启动服务：</li>\n</ul>\n<p>我们先启动hdf再启动yarn，如下图：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778389671396.jpg\" alt></p>\n<p>Okay,一切顺利的话，我们会启动namenodes，datanodes，resourcemanager和nodemanagers。我们有几个web界面可以查看，如下：</p>\n<p> 1). Overview 界面， <a href=\"http://localhost:9870\" target=\"_blank\" rel=\"noopener\">http://localhost:9870 </a>如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373195478.jpg\" alt></p>\n<p> 2). NodeManager界面， <a href=\"http://localhost:8042\" target=\"_blank\" rel=\"noopener\">http://localhost:8042</a>，如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373437959.jpg\" alt=\"-w649\"></p>\n<p> 3). ResourceManager 管理界面 <a href=\"http://localhost:8088/\" target=\"_blank\" rel=\"noopener\">http://localhost:8088/</a>,如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777972228895.jpg\" alt=\"-w1054\"></p>\n<p>目前YARN的环境已经准备完成，我们接下来看如何与Flink进行集成。</p>\n<h2 id=\"Flink集成Hadoop包\"><a href=\"#Flink集成Hadoop包\" class=\"headerlink\" title=\"Flink集成Hadoop包\"></a>Flink集成Hadoop包</h2><p>切换到编译结果目录下<code>flink/build-target</code>,并将haddop的JAR包放到<code>lib</code>目录。<br>在官网下载hadoop包，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd lib;</span><br><span class=\"line\">curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar &gt; flink-shaded-hadoop-2-uber-2.8.3-7.0.jar</span><br></pre></td></tr></table></figure>\n\n<p>下载后，lib目录下文件如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777916683830.jpg\" alt></p>\n<p>到现在为止我们可以提交PyFlink的作业到由YARN进行资源分配的集群了。但为了确保集群上有正确的Python环境我们最好打包一个Python环境到集群上面。因为大部分情况下我们无法得知yarn集群上的Python版本是否符合我们的要求(Python 3.5+，装有apache-beam 2.15.0)，因此我们需要打包一个符合条件的Python环境，并随job文件提交到yarn集群上。</p>\n<h2 id=\"打包Python环境\"><a href=\"#打包Python环境\" class=\"headerlink\" title=\"打包Python环境\"></a>打包Python环境</h2><p>再次检查一下当前Python的版本是否3.5+，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:lib jincheng.sunjc$ Python</span><br><span class=\"line\">Python 3.7.6 (default, Dec 31 2019, 09:48:30)</span><br></pre></td></tr></table></figure>\n\n<p>由于这个Python环境是用于集群的，所以打包时的系统需要和集群一致。如果不一致，比如集群是linux而本机是mac，我们需要在虚拟机或者docker中打包。以下列出两种情况的示范方法，读者根据需求选择一种即可。</p>\n<h3 id=\"本地打包（集群和本机操作系统一致时）\"><a href=\"#本地打包（集群和本机操作系统一致时）\" class=\"headerlink\" title=\"本地打包（集群和本机操作系统一致时）\"></a>本地打包（集群和本机操作系统一致时）</h3><p>如果集群所在机器的操作系统和本地一致（都是mac或者都是linux），直接通过virtualenv打包一个符合条件的Python环境：</p>\n<ul>\n<li>安装virtualenv<br>使用<code>python -m pip install virtualenv</code> 进行安装如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ python -m pip install virtualenv</span><br><span class=\"line\">Collecting virtualenv</span><br><span class=\"line\">  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)</span><br><span class=\"line\">     |████████████████████████████████| 3.4MB 2.0MB/s </span><br><span class=\"line\">Installing collected packages: virtualenv</span><br><span class=\"line\">Successfully installed virtualenv-16.7.9</span><br></pre></td></tr></table></figure>\n\n<p>我本地环境已经成功安装。</p>\n<ul>\n<li>创建Python环境<br>用virtualenv以always-copy方式建立一个全新的Python环境，名字随意，以venv为例，<code>virtualenv --always-copy venv</code>:</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ virtualenv --always-copy venv</span><br><span class=\"line\">Using base prefix &apos;/usr/local/Cellar/Python/3.7.6/Frameworks/Python.framework/Versions/3.7&apos;</span><br><span class=\"line\">New Python executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python3.7</span><br><span class=\"line\">Also creating executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python</span><br><span class=\"line\">Installing setuptools, pip, wheel...</span><br><span class=\"line\">done.</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>在新环境中安装apache-beam 2.15.0<br>使用<code>venv/bin/pip install apache-beam==2.15.0</code>进行安装：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ venv/bin/pip install apache-beam==2.15.0</span><br><span class=\"line\">Collecting apache-beam==2.15.0</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7</span><br></pre></td></tr></table></figure>\n\n<p>上面信息已经说明我们成功的在Python环境中安装了apache-beam==2.15.0。接下来我们打包Python环境。</p>\n<ul>\n<li>打包Python环境<br>我们将Python打包成zip文件，<code>zip -r venv.zip venv</code> 如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zip -r venv.zip venv</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">  adding: venv/lib/Python3.7/re.py (deflated 68%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/struct.py (deflated 46%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/sre_parse.py (deflated 80%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/abc.py (deflated 72%)</span><br><span class=\"line\">  adding: venv/lib/Python3.7/_bootlocale.py (deflated 63%)</span><br></pre></td></tr></table></figure>\n\n<p>查看一下zip大小：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ du -sh venv.zip </span><br><span class=\"line\"> 81M\tvenv.zip</span><br></pre></td></tr></table></figure>\n\n<p>这个大小实在太大了，核心问题是Beam的包非常大，后面我会持续在Beam社区提出优化建议。我们先忍一下:(。</p>\n<h3 id=\"Docker中打包（比如集群为linux，本机为mac时）\"><a href=\"#Docker中打包（比如集群为linux，本机为mac时）\" class=\"headerlink\" title=\"Docker中打包（比如集群为linux，本机为mac时）\"></a>Docker中打包（比如集群为linux，本机为mac时）</h3><p>我们选择在docker中打包，可以从以下链接下载最新版docker并安装：<br><a href=\"https://download.docker.com/mac/stable/Docker.dmg\" target=\"_blank\" rel=\"noopener\">https://download.docker.com/mac/stable/Docker.dmg</a>安装完毕后重启终端，执行<code>docker version</code>确认docker安装成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:tmp jincheng.sunjc$ docker version</span><br><span class=\"line\">Client: Docker Engine - Community</span><br><span class=\"line\"> Version:           19.03.4</span><br><span class=\"line\"> API version:       1.40</span><br><span class=\"line\"> Go version:        go1.12.10</span><br><span class=\"line\"> Git commit:        9013bf5</span><br><span class=\"line\"> Built:             Thu Oct 17 23:44:48 2019</span><br><span class=\"line\"> OS/Arch:           darwin/amd64</span><br><span class=\"line\"> Experimental:      false</span><br><span class=\"line\"></span><br><span class=\"line\">Server: Docker Engine - Community</span><br><span class=\"line\"> Engine:</span><br><span class=\"line\">  Version:          19.03.4</span><br><span class=\"line\">  API version:      1.40 (minimum version 1.12)</span><br><span class=\"line\">  Go version:       go1.12.10</span><br><span class=\"line\">  Git commit:       9013bf5</span><br><span class=\"line\">  Built:            Thu Oct 17 23:50:38 2019</span><br><span class=\"line\">  OS/Arch:          linux/amd64</span><br><span class=\"line\">  Experimental:     false</span><br><span class=\"line\"> containerd:</span><br><span class=\"line\">  Version:          v1.2.10</span><br><span class=\"line\">  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339</span><br><span class=\"line\"> runc:</span><br><span class=\"line\">  Version:          1.0.0-rc8+dev</span><br><span class=\"line\">  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657</span><br><span class=\"line\"> docker-init:</span><br><span class=\"line\">  Version:          0.18.0</span><br><span class=\"line\">  GitCommit:        fec3683</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>启动容器<br>我们启动一个Python 3.7版本的容器如果是第一次启动可能需要较长时间来拉取镜像：<br><code>docker run -it Python:3.7 /bin/bash</code>, 如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:libexec jincheng.sunjc$  docker run -it Python:3.7 /bin/bash</span><br><span class=\"line\">Unable to find image &apos;Python:3.7&apos; locally</span><br><span class=\"line\">3.7: Pulling from library/Python</span><br><span class=\"line\">8f0fdd3eaac0: Pull complete </span><br><span class=\"line\">d918eaefd9de: Pull complete </span><br><span class=\"line\">43bf3e3107f5: Pull complete </span><br><span class=\"line\">27622921edb2: Pull complete </span><br><span class=\"line\">dcfa0aa1ae2c: Pull complete </span><br><span class=\"line\">bf6840af9e70: Pull complete </span><br><span class=\"line\">167665d59281: Pull complete </span><br><span class=\"line\">ffc544588c7f: Pull complete </span><br><span class=\"line\">4ebe99df65fe: Pull complete </span><br><span class=\"line\">Digest: sha256:40d615d7617f0f3b54614fd228d41a891949b988ae2b452c0aaac5bee924888d</span><br><span class=\"line\">Status: Downloaded newer image for Python:3.7</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>容器中安装virtualenv<br>我们在刚才启动的容器中安装virtualenv， <code>pip install virtualenv</code>,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# pip install virtualenv</span><br><span class=\"line\">Collecting virtualenv</span><br><span class=\"line\">  Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB)</span><br><span class=\"line\">     |████████████████████████████████| 3.4MB 2.0MB/s </span><br><span class=\"line\">Installing collected packages: virtualenv</span><br><span class=\"line\">Successfully installed virtualenv-16.7.9</span><br><span class=\"line\">root@1b48d2b526ae:/#</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建Python环境<br>以always copy方式建立一个全新的Python环境，名字随意，以venv为例，<code>virtualenv --always-copy venv</code>, 如下：</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# virtualenv --always-copy venv</span><br><span class=\"line\">Using base prefix &apos;/usr/local&apos;</span><br><span class=\"line\">New Python executable in /venv/bin/Python</span><br><span class=\"line\">Installing setuptools, pip, wheel...</span><br><span class=\"line\">done.</span><br><span class=\"line\">root@1b48d2b526ae:/#</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>安装Apache Beam<br>在新的Python环境中安装apache-beam 2.15.0，<code>venv/bin/pip install apache-beam==2.15.0</code>,如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# venv/bin/pip install apache-beam==2.15.0</span><br><span class=\"line\">Collecting apache-beam==2.15.0</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>查看docker中的Python环境<br>用<code>exit</code>命令退出容器，用<code>docker ps -a</code>找到docker容器的id，用于拷贝文件,如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@1b48d2b526ae:/# exit</span><br><span class=\"line\">exit</span><br><span class=\"line\">jincheng:libexec jincheng.sunjc$ docker ps -a</span><br><span class=\"line\">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES</span><br><span class=\"line\">1b48d2b526ae        Python:3.7          &quot;/bin/bash&quot;         7 minutes ago       Exited (0) 8 seconds ago                       elated_visvesvaraya</span><br></pre></td></tr></table></figure>\n\n<p>由于刚刚结束，一般来说是列表中的第一条，可以根据容器的镜像名Python:3.7来分辨。我们记下最左边的容器ID。如上是<code>1b48d2b526ae</code>。</p>\n<ul>\n<li>打包Python环境<br>从将容器中的Python环境拷贝出来，我们切换到<code>flink/build-target</code>录下，拷贝 <code>docker cp 1b48d2b526ae:/venv ./</code>并打包<code>zip -r venv.zip venv</code>。<br>最终<code>flink/build-target</code>录下生成<code>venv.zip</code>。</li>\n</ul>\n<h2 id=\"部署作业\"><a href=\"#部署作业\" class=\"headerlink\" title=\"部署作业\"></a>部署作业</h2><p>终于到部署作业的环节了:), flink on yarn支持两种模式，per-job和session。per-job模式在提交job时会为每个job单独起一个flink集群，session模式先在yarn上起一个flink集群，之后提交job都提交到这个flink集群。</p>\n<h3 id=\"Pre-Job-模式部署作业\"><a href=\"#Pre-Job-模式部署作业\" class=\"headerlink\" title=\"Pre-Job 模式部署作业\"></a>Pre-Job 模式部署作业</h3><p>执行以下命令，以Pre-Job模式部署PyFlink作业：<br><code>bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</code>,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</span><br><span class=\"line\">2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">2020-01-02 13:04:52,889 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</span><br><span class=\"line\">2020-01-02 13:04:55,945 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class=\"line\">2020-01-02 13:04:56,049 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class=\"line\">2020-01-02 13:05:01,153 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.</span><br><span class=\"line\">2020-01-02 13:05:01,177 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class=\"line\">2020-01-02 13:05:01,294 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system</span><br><span class=\"line\">2020-01-02 13:05:02,600 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0004</span><br><span class=\"line\">2020-01-02 13:05:02,971 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0004</span><br><span class=\"line\">2020-01-02 13:05:02,972 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated</span><br><span class=\"line\">2020-01-02 13:05:02,975 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED</span><br><span class=\"line\">2020-01-02 13:05:23,138 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.</span><br><span class=\"line\">2020-01-02 13:05:23,140 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:61616 of application &apos;application_1577936885434_0004&apos;.</span><br><span class=\"line\">Job has been submitted with JobID a41d82194a500809fd715da8f29894a0</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID a41d82194a500809fd715da8f29894a0 has finished.</span><br><span class=\"line\">Job Runtime: 35576 ms</span><br></pre></td></tr></table></figure>\n\n<p>上面信息已经显示运行完成，在Web界面可以看到作业状态：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417223687.jpg\" alt></p>\n<p>我们再检验一下计算结果<code>cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result</code>：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417681405.jpg\" alt></p>\n<p>到这里，我们以Pre-Job的方式成功部署了PyFlink的作业！相比提交到本地Standalone集群，多了三个参数，我们简单说明如下：</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-m yarn-cluster</td>\n<td>以Per-Job模式部署到yarn集群</td>\n</tr>\n<tr>\n<td>-pyarch venv.zip</td>\n<td>将当前目录下的venv.zip上传到yarn集群</td>\n</tr>\n<tr>\n<td>-pyexec venv.zip/venv/bin/Python</td>\n<td>指定venv.zip中的Python解释器来执行Python UDF，路径需要和zip包内部结构一致。</td>\n</tr>\n</tbody></table>\n<h3 id=\"Session-模式部署作业\"><a href=\"#Session-模式部署作业\" class=\"headerlink\" title=\"Session 模式部署作业\"></a>Session 模式部署作业</h3><p>以Session模式部署作业也非常简单，我们实际操作一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/yarn-session.sh </span><br><span class=\"line\">2020-01-02 13:58:53,049 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.memory.process.size, 1024m</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class=\"line\">2020-01-02 13:58:53,050 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class=\"line\">2020-01-02 13:58:53,051 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.execution.failover-strategy, region</span><br><span class=\"line\">2020-01-02 13:58:53,413 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class=\"line\">2020-01-02 13:58:53,476 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to jincheng.sunjc (auth:SIMPLE)</span><br><span class=\"line\">2020-01-02 13:58:53,509 INFO  org.apache.flink.runtime.security.modules.JaasModule          - Jaas file will be created as /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/jaas-3848984206030141476.conf.</span><br><span class=\"line\">2020-01-02 13:58:53,521 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.</span><br><span class=\"line\">2020-01-02 13:58:53,562 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class=\"line\">2020-01-02 13:58:58,803 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.</span><br><span class=\"line\">2020-01-02 13:58:58,824 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class=\"line\">2020-01-02 13:59:03,975 WARN  org.apache.flink.yarn.YarnClusterDescriptor                   - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system</span><br><span class=\"line\">2020-01-02 13:59:04,779 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1577936885434_0005</span><br><span class=\"line\">2020-01-02 13:59:04,799 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1577936885434_0005</span><br><span class=\"line\">2020-01-02 13:59:04,799 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated</span><br><span class=\"line\">2020-01-02 13:59:04,801 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED</span><br><span class=\"line\">2020-01-02 13:59:24,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.</span><br><span class=\"line\">2020-01-02 13:59:24,713 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.</span><br><span class=\"line\">JobManager Web Interface: http://localhost:62247</span><br></pre></td></tr></table></figure>\n\n<p>执行成功后不会返回，但会启动一个JoBManager Web，地址如上<code>http://localhost:62247</code>，可复制到浏览器查看:<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779449909178.jpg\" alt></p>\n<p>我们可以修改conf/flink-conf.yaml中的配置参数。如果要更改某些内容，请参考<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>。接下来我们提交作业，首先按组合键Ctrl+Z将yarn-session.sh进程切换到后台，并执行bg指令让其在后台继续执行, 然后执行以下命令，即可向Session模式的flink集群提交job <code>bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ bin/flink run -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py</span><br><span class=\"line\"></span><br><span class=\"line\">2020-01-02 14:10:48,285 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.</span><br><span class=\"line\">Job has been submitted with JobID bea33b7aa07c0f62153ab5f6e134b6bf</span><br><span class=\"line\">Program execution finished</span><br><span class=\"line\">Job with JobID bea33b7aa07c0f62153ab5f6e134b6bf has finished.</span><br><span class=\"line\">Job Runtime: 34405 ms</span><br></pre></td></tr></table></figure>\n\n<p>如果在打印<code>finished</code>之前查看之前的web页面，我们会发现Session集群会有一个正确运行的作业，如下：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779454827330.jpg\" alt><br>如果已经运行完成，那么我们应该会看到状态也变成结束：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779456315383.jpg\" alt></p>\n<p>相比per job模式提交，少了”-m”参数。因为之前已经启动了yarn-session.sh，所以flink默认会向yarn-session.sh启动的集群上提交job。执行完毕后，别忘了关闭yarn-session.sh（session 模式）：先将yarn-session.sh调到前台，执行<code>fg</code>,然后在再按Ctrl+C结束进程或者执行<code>stop</code>，结束时yarn上的集群也会被关闭。</p>\n<h1 id=\"Docker-模式部署\"><a href=\"#Docker-模式部署\" class=\"headerlink\" title=\"Docker 模式部署\"></a>Docker 模式部署</h1><p>我们还可以将Flink Python job打包成docker镜像，然后使用docker-compose或者Kubernetes部署执行，由于现在的docker镜像打包工具并没有完美支持运行Python UDF，因此我们需要往里面添加一些额外的文件。首先是一个仅包含PythonDriver类的jar包. 我们在<code>build-target</code>目录下执行如下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:build-target jincheng.sunjc$ mkdir temp</span><br><span class=\"line\">jincheng:build-target jincheng.sunjc$ cd temp</span><br><span class=\"line\">jincheng:temp jincheng.sunjc$ unzip ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar org/apache/flink/client/Python/PythonDriver.class</span><br><span class=\"line\">Archive:  ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar</span><br><span class=\"line\">  inflating: org/apache/flink/client/Python/PythonDriver.class</span><br></pre></td></tr></table></figure>\n\n<p>解压之后，我们在进行压缩打包：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:temp jincheng.sunjc$ zip Python-driver.jar org/apache/flink/client/Python/PythonDriver.class</span><br><span class=\"line\">  adding: org/apache/flink/client/Python/PythonDriver.class (deflated 56%)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779465411786.jpg\" alt><br>我们得到<code>Python-driver.jar</code>。然后下载一个pyArrow的安装文件(我准备了一个大家下载直接使用即可 <a href=\"https://github.com/sunjincheng121/enjoyment.code/blob/master/share_resource/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl\" target=\"_blank\" rel=\"noopener\">pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl</a>。执行以下命令构建Docker镜像，需要作为artifacts引入的文件有作业文件，Python-driver的jar包和pyarrow安装文件,<code>./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist</code>(进入<code>flink/flink-container/docker</code>目录)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ ./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist</span><br><span class=\"line\">Using flink dist: ../../flink-dist/target/flink-*-bin</span><br><span class=\"line\">a .</span><br><span class=\"line\">a ./flink-1.10-SNAPSHOT</span><br><span class=\"line\">a ./flink-1.10-SNAPSHOT/temp</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Removing intermediate container a0558bbcbdd1</span><br><span class=\"line\"> ---&gt; 00ecda6117b7</span><br><span class=\"line\">Successfully built 00ecda6117b7</span><br><span class=\"line\">Successfully tagged flink-job:latest</span><br></pre></td></tr></table></figure>\n\n<p>构建Docker镜像需要较长时间，请耐心等待。构建完毕之后，可以输入docker images命令在镜像列表中找到构建结果<code>docker images</code>：<br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779503938890.jpg\" alt=\"-w765\"><br>然后我们在构建好的镜像基础上安装好Python udf所需依赖，并删除过程中产生的临时文件：</p>\n<ul>\n<li><p>启动docker容器<br><code>docker run -it --user root --entrypoint /bin/bash --name flink-job-container flink-job</code></p>\n</li>\n<li><p>安装一些依赖<br><code>apk add --no-cache g++ Python3-dev musl-dev</code></p>\n</li>\n<li><p>安装PyArrow<br><code>python -m pip3 install /opt/artifacts/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl</code></p>\n</li>\n<li><p>安装Apache Beam<br><code>python -m pip3 install apache-beam==2.15.0</code></p>\n</li>\n<li><p>删除临时文件<br><code>rm -rf /root/.cache/pip</code></p>\n</li>\n</ul>\n<p>执行完如上命令我可以执行<code>exit</code>退出容器了，然后把这个容器提交为新的flink-job镜像<code>docker commit -c &#39;CMD [&quot;--help&quot;]&#39; -c &quot;USER flink&quot; -c &#39;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&#39; flink-job-container flink-job:latest</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ docker commit -c &apos;CMD [&quot;--help&quot;]&apos; -c &quot;USER flink&quot; -c &apos;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&apos; flink-job-container flink-job:latest </span><br><span class=\"line\">sha256:0740a635e2b0342ddf776f33692df263ebf0437d6373f156821f4dd044ad648b</span><br></pre></td></tr></table></figure>\n\n<p>到这里包含Python UDF 作业的Docker镜像就制作好了，这个Docker镜像既可以以docker-compose使用，也可以结合kubernetes中使用。</p>\n<p>我们以使用docker-compose执行为例，mac版docker自带docker-compose，用户可以直接使用，在<code>flink/flink-container/docker</code>目录下，使用以下命令启动作业,<code>FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose up</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jincheng:docker jincheng.sunjc$ FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose up</span><br><span class=\"line\">WARNING: The SAVEPOINT_OPTIONS variable is not set. Defaulting to a blank string.</span><br><span class=\"line\">Recreating docker_job-cluster_1 ... done</span><br><span class=\"line\">Starting docker_taskmanager_1   ... done</span><br><span class=\"line\">Attaching to docker_taskmanager_1, docker_job-cluster_1</span><br><span class=\"line\">taskmanager_1  | Starting the task-manager</span><br><span class=\"line\">job-cluster_1  | Starting the job-cluster</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">job-cluster_1  | 2020-01-02 08:35:03,796 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Terminating cluster entrypoint process StandaloneJobClusterEntryPoint with exit code 0.</span><br><span class=\"line\">docker_job-cluster_1 exited with code 0</span><br></pre></td></tr></table></figure>\n\n<p>在log中出现“docker_job-cluster_1 exited with code 0”表示job已执行成功，JobManager已经退出。TaskManager还需要较长的时间等待超时后才会退出，我们可以直接按快捷键Ctrl+C提前退出。</p>\n<p>查看执行结果，可以从TaskManager的容器中将结果文件拷贝出来查看,执行 <code>docker cp docker_taskmanager_1:/tmp/result ./; cat result</code><br><img src=\"/2020/01/02/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779542617667.jpg\" alt></p>\n<p>Okay, 到这里本篇要与大家分享的内容已经接近尾声了，如果你期间也很顺利的成功了，可以 Cheers 了:)</p>\n<h1 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h1><p>本篇核心向大家分享了如何以多种方式部署PyFlink作业。期望在PyFlink1.10发布之后，大家能有一个顺利快速体验的快感！在开篇说道部分，为大家分享了老子倡导大家的 “致虚极，守静笃。万物并作，吾以观其复”的大道，同时也给大家带来了2020的祝福，祝福大家 “2020 安！”。</p>\n"}],"PostAsset":[{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/77C24B94-7F26-4F72-BCA3-D0F20A791772.png","slug":"77C24B94-7F26-4F72-BCA3-D0F20A791772.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/40FE5816-DA56-4222-ADDE-1B023EE71BCF.png","slug":"40FE5816-DA56-4222-ADDE-1B023EE71BCF.png","post":"ck0emm5f00001s94hhkr2qea5","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png","slug":"F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png","post":"ck0emm5f00001s94hhkr2qea5","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/051876D7-5A89-4134-8ED0-E6939FC3F412.png","slug":"051876D7-5A89-4134-8ED0-E6939FC3F412.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/26FE2626-FDA4-4474-9B25-5505FEF5FB37.png","slug":"26FE2626-FDA4-4474-9B25-5505FEF5FB37.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png","slug":"4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png","slug":"989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png","slug":"82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/9741456E-BDBB-4609-A014-85FEF5D351AB.png","slug":"9741456E-BDBB-4609-A014-85FEF5D351AB.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/146851FF-D705-4B2D-8C3B-77528BB8DAB5.png","slug":"146851FF-D705-4B2D-8C3B-77528BB8DAB5.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/266A687D-8E31-4266-B52F-200B75244DDB.png","slug":"266A687D-8E31-4266-B52F-200B75244DDB.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png","slug":"5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png","slug":"BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/1AF4C656-6ABA-4F69-A771-E6EC623B382D.png","slug":"1AF4C656-6ABA-4F69-A771-E6EC623B382D.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/9B952F03-53BF-43DF-8051-A2FAC84A600C.png","slug":"9B952F03-53BF-43DF-8051-A2FAC84A600C.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/0B58FCBF-4D98-41F9-98A3-13B31F939916.png","slug":"0B58FCBF-4D98-41F9-98A3-13B31F939916.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png","slug":"5A3AE975-B828-45D4-BD83-591A1F6A1EFF.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/8308BB64-2AB2-480C-9B2E-4AADD515F183.png","slug":"8308BB64-2AB2-480C-9B2E-4AADD515F183.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/237CED69-52AB-4206-BF02-F99183C7186E.png","slug":"237CED69-52AB-4206-BF02-F99183C7186E.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png","slug":"5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/95F7D755-E087-4808-83F7-2A20CA5C685F.png","slug":"95F7D755-E087-4808-83F7-2A20CA5C685F.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png","slug":"93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A94E2FAD-F04A-4691-8C34-3C697FF81476.png","slug":"A94E2FAD-F04A-4691-8C34-3C697FF81476.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/DE913257-7FD0-4983-A511-F8FE11584F9C.png","slug":"DE913257-7FD0-4983-A511-F8FE11584F9C.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/452ada49a233d68c6e614e05e61ba158aa180f83.png","slug":"452ada49a233d68c6e614e05e61ba158aa180f83.png","post":"ck0emm5fg000fs94hsga2rtfu","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/D9E4DDB2-3FF6-471E-9AEC-6754FB9A88F2.png","slug":"D9E4DDB2-3FF6-471E-9AEC-6754FB9A88F2.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png","slug":"1DB881D1-4605-4AA3-ABCB-34BC8E765D79.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/9C73FBBB-80AD-45DA-91E5-21436A7811C2.png","slug":"9C73FBBB-80AD-45DA-91E5-21436A7811C2.png","post":"ck0emm5gj001ks94h7ulryqho","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/C15AA404-D132-4D6C-BA35-B103CA53AE61.png","slug":"C15AA404-D132-4D6C-BA35-B103CA53AE61.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/678508E9-D79F-438A-B236-17A942D31F73.png","slug":"678508E9-D79F-438A-B236-17A942D31F73.png","post":"ck0emm5f00001s94hhkr2qea5","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/A3A08123-0490-41BA-9E07-95B4557A19F5.png","slug":"A3A08123-0490-41BA-9E07-95B4557A19F5.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/096879FE-B4A4-4802-8749-9784162D19D0.png","slug":"096879FE-B4A4-4802-8749-9784162D19D0.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png","slug":"1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/70173B48-E4B8-466C-9846-AD68FEB6F9B2.png","slug":"70173B48-E4B8-466C-9846-AD68FEB6F9B2.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png","slug":"AA7B9029-6AD2-4D4A-A6FE-E48964DA6D76.png","post":"ck0emm5fg000fs94hsga2rtfu","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png","slug":"D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/3794F522-3458-4386-A101-9458FD2D8DBF.png","slug":"3794F522-3458-4386-A101-9458FD2D8DBF.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/comment.png","slug":"comment.png","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/comment.png","slug":"comment.png","post":"ck0emm5fy0011s94hjuxah9ps","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/6DFCA46F-DEB5-4C5C-8540-655D5A4F2423.png","slug":"6DFCA46F-DEB5-4C5C-8540-655D5A4F2423.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/77394C3C-D401-4DE6-A005-D4ADFFBFB892.png","slug":"77394C3C-D401-4DE6-A005-D4ADFFBFB892.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/99DE5B9F-777D-4BDC-BA36-6E39E2B82F79.png","slug":"99DE5B9F-777D-4BDC-BA36-6E39E2B82F79.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/E371FA28-BAAE-448F-A930-E795CBF09B0E.png","slug":"E371FA28-BAAE-448F-A930-E795CBF09B0E.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/comment.png","slug":"comment.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/6F22FA6A-2892-4137-BA10-6D643C60DBEE.png","slug":"6F22FA6A-2892-4137-BA10-6D643C60DBEE.png","post":"ck0emm5gj001ks94h7ulryqho","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/comment.png","slug":"comment.png","post":"ck0emm5gj001ks94h7ulryqho","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/3C082C16-792A-4824-8112-9FD3C87768B5.png","slug":"3C082C16-792A-4824-8112-9FD3C87768B5.png","post":"ck0emm5fo000qs94hfz1hpgj7","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 视频系列 - 2019.08.06直播(德国柏林)/comment.png","slug":"comment.png","post":"ck0emm5fo000qs94hfz1hpgj7","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/02C1D4A6-7290-408E-832A-1FBDCDED75E5.png","slug":"02C1D4A6-7290-408E-832A-1FBDCDED75E5.png","post":"ck0emm5fy0011s94hjuxah9ps","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Kafka/7A1AA068-AA3C-4439-898D-5DE507CC24C6.png","slug":"7A1AA068-AA3C-4439-898D-5DE507CC24C6.png","post":"ck0emm5fy0011s94hjuxah9ps","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业/52A99F20-9648-4AB7-BA20-AA890797A9F0.png","slug":"52A99F20-9648-4AB7-BA20-AA890797A9F0.png","post":"ck0emm5gj001ks94h7ulryqho","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/66FB2539-A89C-4AE8-8639-F62F54B18E78.png","slug":"66FB2539-A89C-4AE8-8639-F62F54B18E78.png","post":"ck0emm5f00001s94hhkr2qea5","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Fault Tolerance/B0D184CA-95DE-40C7-AA80-C4FE44134D16.png","slug":"B0D184CA-95DE-40C7-AA80-C4FE44134D16.png","post":"ck0emm5f00001s94hhkr2qea5","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/b26733d7d193bbea8d594f098bcd98cbea926a88.png","slug":"b26733d7d193bbea8d594f098bcd98cbea926a88.png","post":"ck0emm5fg000fs94hsga2rtfu","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png","slug":"e7ef320a074b23ff8f3996373660bb1cf2f9e87c.png","post":"ck0emm5fg000fs94hsga2rtfu","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Time Interval JOIN/eef2a485ae6f7de29d5965457a1ae0da89055a58.png","slug":"eef2a485ae6f7de29d5965457a1ae0da89055a58.png","post":"ck0emm5fg000fs94hsga2rtfu","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png","slug":"1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/7230573C-621C-4792-BB1C-7EB96A61DBEE.png","slug":"7230573C-621C-4792-BB1C-7EB96A61DBEE.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - State/AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png","slug":"AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png","post":"ck0emm5fi000ks94hctfz1eri","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png","slug":"7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/BC6D8C93-A068-4799-B2CC-6A75AD43549C.png","slug":"BC6D8C93-A068-4799-B2CC-6A75AD43549C.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/E5D7CBF0-A452-4907-8FCF-F97896C6993A.png","slug":"E5D7CBF0-A452-4907-8FCF-F97896C6993A.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 -  Watermark/FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png","slug":"FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png","post":"ck0emm5fb0009s94h3odv0vds","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png","slug":"646B6616-4AA7-4A04-AAE8-A6D1EE006535.png","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/9AB5B4ED-4880-4C8E-A80C-42F562CF1194.png","slug":"9AB5B4ED-4880-4C8E-A80C-42F562CF1194.png","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/B26FF58C-1E74-42DC-B75A-963F898C24B7.png","slug":"B26FF58C-1E74-42DC-B75A-963F898C24B7.png","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/BC393F56-E637-471B-B5B4-8A73B9A1D30A.jpg","slug":"BC393F56-E637-471B-B5B4-8A73B9A1D30A.jpg","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/BDDAD217-10C4-4896-91D9-BAC1B8B1EA20.jpg","slug":"BDDAD217-10C4-4896-91D9-BAC1B8B1EA20.jpg","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)/hehua.png","slug":"hehua.png","post":"ck0emm5g50019s94hk012tecz","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/070B3285-C790-4D6F-BB9B-6981756255CA.jpg","slug":"070B3285-C790-4D6F-BB9B-6981756255CA.jpg","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png","slug":"6669AE2D-CBB7-49AB-A339-4E5ACCA99813.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/B31690BA-A8E2-4C22-A948-8735728542B0.png","slug":"B31690BA-A8E2-4C22-A948-8735728542B0.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/B83130D4-9E1F-4BBC-88FA-2286951D18E5.png","slug":"B83130D4-9E1F-4BBC-88FA-2286951D18E5.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/C3DCF0D9-7C1B-4DB7-88E0-657A6B8D81EC.jpg","slug":"C3DCF0D9-7C1B-4DB7-88E0-657A6B8D81EC.jpg","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png","slug":"DF105B35-22A0-4981-AABB-3C03EDF4AFCA.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python API 时代的产物/comment.png","slug":"comment.png","post":"ck0emm5ge001fs94hv6z984rl","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/12D4B3D0-D367-40B3-B729-95095807CA46.png","slug":"12D4B3D0-D367-40B3-B729-95095807CA46.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/2052C4F1-2EE4-41D1-8A8D-165A40827462.png","slug":"2052C4F1-2EE4-41D1-8A8D-165A40827462.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png","slug":"299C46FC-E45C-4CCE-AC60-4EFDA2E101E9.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/380A4B6E-0F4F-4031-8ED3-286DAA7B0343.jpg","slug":"380A4B6E-0F4F-4031-8ED3-286DAA7B0343.jpg","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/41690DE4-8705-4305-BF74-AC47F8C870B3.png","slug":"41690DE4-8705-4305-BF74-AC47F8C870B3.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png","slug":"DC38424C-75A9-4DE9-9485-32F5F3D0F55F.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/EF81F039-86CC-4799-8DCF-F3B0144B76EC.png","slug":"EF81F039-86CC-4799-8DCF-F3B0144B76EC.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png","slug":"F736DF56-0C25-4DD9-84D8-200F5DC4EC38.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 流表对偶(duality)性/FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png","slug":"FF4F58BD-6C4F-4065-A0B5-EC66B3C31BD0.png","post":"ck0emm5fr000ts94h4vqwxoql","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png","slug":"181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/3AE44275-DFE0-45D4-8259-EB03928B784B.png","slug":"3AE44275-DFE0-45D4-8259-EB03928B784B.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/45343A7B-866B-4A70-A3A5-944C989E35BD.png","slug":"45343A7B-866B-4A70-A3A5-944C989E35BD.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/49829548-F09C-47E6-A245-02B7A92749CA.png","slug":"49829548-F09C-47E6-A245-02B7A92749CA.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/64F21679-7168-47B0-A0B2-31C876FF3830.png","slug":"64F21679-7168-47B0-A0B2-31C876FF3830.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png","slug":"82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png","slug":"C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png","slug":"D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN 算子/D9E93F8F-D39A-40BD-A78D-8E3404225B86.png","slug":"D9E93F8F-D39A-40BD-A78D-8E3404225B86.png","post":"ck0emm5fa0008s94haoo37o4i","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png","slug":"270E89C8-DB14-4AD7-9EF7-EA5518FC394D.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/34546866-F865-4660-8177-A64C5A14C002.png","slug":"34546866-F865-4660-8177-A64C5A14C002.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png","slug":"4ED03355-1070-4A51-8FD1-DB049AB7FEFE.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png","slug":"6EB7A4C3-B19E-43E0-8E88-6CD2D95936AB.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 说道系列- Python Table API 开发环境搭建/B0C5D286-6107-40D0-8F76-9C18DA4650D0.png","slug":"B0C5D286-6107-40D0-8F76-9C18DA4650D0.png","post":"ck0emm5gr001rs94hlfdma729","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png","slug":"1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/1ED29F10-86D2-40CD-B9FD-ADF32D4247CD.png","slug":"1ED29F10-86D2-40CD-B9FD-ADF32D4247CD.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png","slug":"28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/32183E30-E096-46DE-A855-932377D70BFE.png","slug":"32183E30-E096-46DE-A855-932377D70BFE.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png","slug":"3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/37259B89-DD8F-4349-A378-000AAA886479.png","slug":"37259B89-DD8F-4349-A378-000AAA886479.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/42B8EBC9-CD5B-498C-B191-F4241C91483F.png","slug":"42B8EBC9-CD5B-498C-B191-F4241C91483F.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/52B538BB-F615-4B04-AAC5-52293EAFC919.png","slug":"52B538BB-F615-4B04-AAC5-52293EAFC919.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/5DA82C35-6E03-43AD-B695-35C85D85D641.png","slug":"5DA82C35-6E03-43AD-B695-35C85D85D641.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png","slug":"645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/90BC2538-6A12-4F5F-AE42-B87BA5C12871.png","slug":"90BC2538-6A12-4F5F-AE42-B87BA5C12871.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png","slug":"C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png","slug":"C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png","slug":"D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png","slug":"DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/E3709EBB-89AE-44EA-98C7-06A2794597D9.png","slug":"E3709EBB-89AE-44EA-98C7-06A2794597D9.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - JOIN LATERAL/ED960882-39B0-40FD-9400-60D0D66F366E.png","slug":"ED960882-39B0-40FD-9400-60D0D66F366E.png","post":"ck0emm5f50003s94hsw6mkmgp","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/05A64357-27DF-4714-BDDB-B390B7A0814A.png","slug":"05A64357-27DF-4714-BDDB-B390B7A0814A.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/0A4F6AF5-68D5-4E26-BFF3-8E0AEA2B829C.png","slug":"0A4F6AF5-68D5-4E26-BFF3-8E0AEA2B829C.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/16080D31-EB87-48F9-A32E-A992CC6DCEAC.png","slug":"16080D31-EB87-48F9-A32E-A992CC6DCEAC.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/1757B41A-113E-4115-9476-EB9972D296AF.png","slug":"1757B41A-113E-4115-9476-EB9972D296AF.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/1D8D2D78-5454-4919-A7C3-48B2338BDC78.png","slug":"1D8D2D78-5454-4919-A7C3-48B2338BDC78.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/391D7615-63C5-4A18-85BA-540730B6B5E2.png","slug":"391D7615-63C5-4A18-85BA-540730B6B5E2.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/480B5CC6-5363-40FF-92EE-2B242494EBD7.jpg","slug":"480B5CC6-5363-40FF-92EE-2B242494EBD7.jpg","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/59761691-219B-47D5-8DC8-FC1D9C3E949E.png","slug":"59761691-219B-47D5-8DC8-FC1D9C3E949E.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/9F6085AD-4792-4A30-9503-82DF82540304.png","slug":"9F6085AD-4792-4A30-9503-82DF82540304.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/AE17D459-66B1-4946-A962-BFC3B9438E79.png","slug":"AE17D459-66B1-4946-A962-BFC3B9438E79.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/C1407477-934D-40DA-B0F0-78C48B96BEA7.png","slug":"C1407477-934D-40DA-B0F0-78C48B96BEA7.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png","slug":"CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/E18BAB16-4094-48B3-92E5-D45EA18A62C1.png","slug":"E18BAB16-4094-48B3-92E5-D45EA18A62C1.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - SQL 概览/E3CA71CA-8B44-4802-8165-53684A152503.png","slug":"E3CA71CA-8B44-4802-8165-53684A152503.png","post":"ck0emm5f90007s94h6qk1iaa4","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png","slug":"07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/172A63B1-C30D-43EA-B04E-B9BE8706838B.png","slug":"172A63B1-C30D-43EA-B04E-B9BE8706838B.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png","slug":"274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png","slug":"364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png","slug":"3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/44FDFE22-3B21-4514-8E91-E339F9188E4E.png","slug":"44FDFE22-3B21-4514-8E91-E339F9188E4E.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/6EAC4329-C402-4497-BAE2-7E7B778E613B.png","slug":"6EAC4329-C402-4497-BAE2-7E7B778E613B.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/75AC8012-59C7-4105-AE66-4453384E762B.png","slug":"75AC8012-59C7-4105-AE66-4453384E762B.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A05E658F-C511-45E2-870F-5E3227A94570.png","slug":"A05E658F-C511-45E2-870F-5E3227A94570.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png","slug":"A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png","slug":"BF868DDE-D746-4529-878D-91F1A0D7486F.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/CFC8CEDA-8049-459F-B331-15CE8454330F.png","slug":"CFC8CEDA-8049-459F-B331-15CE8454330F.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/D05B8A19-6D97-4809-8723-B4B95FE075A7.png","slug":"D05B8A19-6D97-4809-8723-B4B95FE075A7.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/E512586F-5E27-4EDF-8087-1526D21D5792.png","slug":"E512586F-5E27-4EDF-8087-1526D21D5792.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - Temporal Table JOIN/FAB41F2D-25A8-4166-9583-D857FE9A19F1.png","slug":"FAB41F2D-25A8-4166-9583-D857FE9A19F1.png","post":"ck0emm5fl000ns94hhnc4x3rq","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png","slug":"0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/1401BEB3-37C3-4203-B48E-AABF6B08DB65.png","slug":"1401BEB3-37C3-4203-B48E-AABF6B08DB65.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/166F2FE8-93FB-4966-87B7-F6C98DFF4915.png","slug":"166F2FE8-93FB-4966-87B7-F6C98DFF4915.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/17FB1F6A-C813-43D4-AD6D-31FC03B84533.png","slug":"17FB1F6A-C813-43D4-AD6D-31FC03B84533.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png","slug":"3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/4134FE05-67E4-4625-824D-BBD7B444BFE6.png","slug":"4134FE05-67E4-4625-824D-BBD7B444BFE6.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/43A59ED3-C317-4B5D-80F3-CC332933B93B.png","slug":"43A59ED3-C317-4B5D-80F3-CC332933B93B.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png","slug":"46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/78D31D91-3693-44C8-A7E4-71D6798A78AE.png","slug":"78D31D91-3693-44C8-A7E4-71D6798A78AE.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png","slug":"A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png","slug":"A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B2EEEC59-21F9-41E1-A306-8338A7A72A64.png","slug":"B2EEEC59-21F9-41E1-A306-8338A7A72A64.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B5F9DD64-3574-480A-A409-BEE96561091B.png","slug":"B5F9DD64-3574-480A-A409-BEE96561091B.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BE66E18F-DE01-42EC-9336-5F6172B22636.png","slug":"BE66E18F-DE01-42EC-9336-5F6172B22636.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/E095C802-8CCF-4922-837C-6AAE54A335D8.png","slug":"E095C802-8CCF-4922-837C-6AAE54A335D8.png","post":"ck0emm5fu000ys94hhr6r8oxe","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/013D77B3-0D57-433F-B8D7-476D8CD3384E.png","slug":"013D77B3-0D57-433F-B8D7-476D8CD3384E.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png","slug":"0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/27BED9E7-C89F-4109-BB21-6917378E1634.png","slug":"27BED9E7-C89F-4109-BB21-6917378E1634.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/40302667-3CD6-4EF5-9904-1834343B3E4D.png","slug":"40302667-3CD6-4EF5-9904-1834343B3E4D.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/428A6C6A-8070-4D19-A380-122AAF34EDFF.png","slug":"428A6C6A-8070-4D19-A380-122AAF34EDFF.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/4C991819-3CA8-4E8F-B5EF-11B6AA2EB5B8.png","slug":"4C991819-3CA8-4E8F-B5EF-11B6AA2EB5B8.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/7DD374B0-1901-424C-8683-2AE890B8A5FA.png","slug":"7DD374B0-1901-424C-8683-2AE890B8A5FA.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/800B7916-C9BF-4422-ADC9-2C5D90E30339.png","slug":"800B7916-C9BF-4422-ADC9-2C5D90E30339.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/83147F8B-064E-43AB-ACD5-B6243689462B.png","slug":"83147F8B-064E-43AB-ACD5-B6243689462B.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/8711EDAA-E214-46DA-AB5A-BB63FCC55950.png","slug":"8711EDAA-E214-46DA-AB5A-BB63FCC55950.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png","slug":"B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/B995477B-C4CC-4A04-BE20-09C9100725E0.png","slug":"B995477B-C4CC-4A04-BE20-09C9100725E0.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/BF1A3338-5B92-4898-BC81-A4A6626D4208.png","slug":"BF1A3338-5B92-4898-BC81-A4A6626D4208.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/CF9B03E9-DD03-48DC-B0F1-2C0CEC1CEAFA.png","slug":"CF9B03E9-DD03-48DC-B0F1-2C0CEC1CEAFA.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png","slug":"D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png","slug":"F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180919_180.png","slug":"Snip20180919_180.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180919_181.png","slug":"Snip20180919_181.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_4.png","slug":"Snip20180921_4.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_6.png","slug":"Snip20180921_6.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180921_7.png","slug":"Snip20180921_7.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_17.png","slug":"Snip20180922_17.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_18.png","slug":"Snip20180922_18.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache Flink 漫谈系列 - 概述/Snip20180922_22.png","slug":"Snip20180922_22.png","post":"ck0emm5g9001cs94hltig9zj6","modified":0,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777810183663.jpg","slug":"15777810183663.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777816500228.jpg","slug":"15777816500228.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777853568978.jpg","slug":"15777853568978.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777907875791.jpg","slug":"15777907875791.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373437959.jpg","slug":"15778373437959.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779454827330.jpg","slug":"15779454827330.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779449909178.jpg","slug":"15779449909178.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777878055973.jpg","slug":"15777878055973.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777972228895.jpg","slug":"15777972228895.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417223687.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779417223687.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15776988009185.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15776988009185.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777536894368.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777536894368.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777784863901.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777784863901.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777817105172.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777817105172.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821194059.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777821194059.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777821361064.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777821361064.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777913540792.jpg","slug":"15777913540792.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15777916683830.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15777916683830.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778077301840.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15778077301840.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778372533417.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15778372533417.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778373195478.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15778373195478.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15778389671396.jpg","slug":"15778389671396.jpg","post":"ck4wllksc00005u4hj1k6sj5v","modified":1,"renderable":0},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779417681405.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779417681405.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779456315383.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779456315383.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779465411786.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779465411786.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779466238977.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779466238977.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779503938890.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779503938890.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779542617667.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779542617667.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Apache-Flink-说道系列-PyFlink-作业的多种部署模式/15779569378844.jpg","post":"ck4wllksc00005u4hj1k6sj5v","slug":"15779569378844.jpg","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ck0emm5fa0008s94haoo37o4i","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fh000gs94hrf47ucvr"},{"post_id":"ck0emm5f00001s94hhkr2qea5","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fi000ls94hy5j1j454"},{"post_id":"ck0emm5fb0009s94h3odv0vds","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fl000os94hvwk6nrqa"},{"post_id":"ck0emm5ff000ds94h6weiamkx","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fo000rs94h2t6y9jwu"},{"post_id":"ck0emm5f50003s94hsw6mkmgp","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fr000us94hoa8679ng"},{"post_id":"ck0emm5fg000fs94hsga2rtfu","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fv000zs94hera98muh"},{"post_id":"ck0emm5fi000ks94hctfz1eri","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5fz0012s94hxhgm9gv2"},{"post_id":"ck0emm5f90007s94h6qk1iaa4","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5g20015s94h40tj6031"},{"post_id":"ck0emm5fl000ns94hhnc4x3rq","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5g6001as94hfrdnj9qv"},{"post_id":"ck0emm5fr000ts94h4vqwxoql","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5g9001ds94hbf5qn9qw"},{"post_id":"ck0emm5fu000ys94hhr6r8oxe","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5gf001is94hvqqhh18b"},{"post_id":"ck0emm5fo000qs94hfz1hpgj7","category_id":"ck0emm5fu000vs94h7jd1s0k5","_id":"ck0emm5gj001ls94h76it5rlm"},{"post_id":"ck0emm5g9001cs94hltig9zj6","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5go001ps94h0n9lk34c"},{"post_id":"ck0emm5fy0011s94hjuxah9ps","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5gr001ts94hjef5jvl3"},{"post_id":"ck0emm5ge001fs94hv6z984rl","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5gw001vs94he7gyofnc"},{"post_id":"ck0emm5gj001ks94h7ulryqho","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5h0001ys94h2644ylre"},{"post_id":"ck0emm5g20014s94heloekcqm","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5h30020s94habvrjcyr"},{"post_id":"ck0emm5gn001ns94hb7dg7xpp","category_id":"ck0emm5f60004s94hyfa6xl78","_id":"ck0emm5h80023s94hdhfkqgrg"},{"post_id":"ck0emm5gr001rs94hlfdma729","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5ha0025s94h14b44f61"},{"post_id":"ck0emm5g50019s94hk012tecz","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck0emm5hd0027s94hv69qcodb"},{"post_id":"ck4wllksc00005u4hj1k6sj5v","category_id":"ck0emm5g50016s94h3c7o59r1","_id":"ck4wllktm00025u4h318ivlwg"}],"PostTag":[{"post_id":"ck0emm5fa0008s94haoo37o4i","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fe000cs94hivky0rq0"},{"post_id":"ck0emm5f00001s94hhkr2qea5","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fg000es94hxgz0rbqr"},{"post_id":"ck0emm5fb0009s94h3odv0vds","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fi000js94hcveizngq"},{"post_id":"ck0emm5ff000ds94h6weiamkx","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fk000ms94hpmmcijja"},{"post_id":"ck0emm5f50003s94hsw6mkmgp","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fn000ps94hoxmocwcz"},{"post_id":"ck0emm5fg000fs94hsga2rtfu","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fq000ss94hmalh95pt"},{"post_id":"ck0emm5fi000ks94hctfz1eri","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fu000xs94hefmji1m3"},{"post_id":"ck0emm5f90007s94h6qk1iaa4","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5fx0010s94hv7s2d47y"},{"post_id":"ck0emm5fl000ns94hhnc4x3rq","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5g10013s94hlcbxtuj0"},{"post_id":"ck0emm5fr000ts94h4vqwxoql","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5g50018s94h1uys15dx"},{"post_id":"ck0emm5fu000ys94hhr6r8oxe","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5g9001bs94h2z9a10oj"},{"post_id":"ck0emm5fo000qs94hfz1hpgj7","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5ge001es94hqad4f3wu"},{"post_id":"ck0emm5fo000qs94hfz1hpgj7","tag_id":"ck0emm5fu000ws94h5nevl5b4","_id":"ck0emm5gi001js94h28oimvvf"},{"post_id":"ck0emm5g9001cs94hltig9zj6","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5gm001ms94hjns1rv79"},{"post_id":"ck0emm5fy0011s94hjuxah9ps","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5gr001ss94h0iw5amiz"},{"post_id":"ck0emm5fy0011s94hjuxah9ps","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5gw001us94hqltbefw7"},{"post_id":"ck0emm5fy0011s94hjuxah9ps","tag_id":"ck0emm5gf001hs94hxmyu0pp4","_id":"ck0emm5gz001xs94hf743eo5i"},{"post_id":"ck0emm5gn001ns94hb7dg7xpp","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5h3001zs94hcsbvhvaq"},{"post_id":"ck0emm5g20014s94heloekcqm","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5h80022s94h3nz007n6"},{"post_id":"ck0emm5g20014s94heloekcqm","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5ha0024s94hogyk6oqa"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5hj002bs94hx9p6vhfw"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5gw001ws94hyx0474sq","_id":"ck0emm5hj002cs94hd4igbxel"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5h50021s94hfzz60iar","_id":"ck0emm5hk002es94h3jcppyki"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5hk002fs94h9pjbrboy"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5hi0028s94hzc47h2ic","_id":"ck0emm5hl002hs94hi8pl11gv"},{"post_id":"ck0emm5g50019s94hk012tecz","tag_id":"ck0emm5hi0029s94hbyfl0zng","_id":"ck0emm5hl002is94h6rm5iqz7"},{"post_id":"ck0emm5ge001fs94hv6z984rl","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5hl002js94hd9t31chb"},{"post_id":"ck0emm5ge001fs94hv6z984rl","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5hl002ks94h05iwolyv"},{"post_id":"ck0emm5ge001fs94hv6z984rl","tag_id":"ck0emm5hj002as94hk5j392un","_id":"ck0emm5hl002ls94h05xtf3s2"},{"post_id":"ck0emm5gj001ks94h7ulryqho","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5hl002ms94h2l67xmn1"},{"post_id":"ck0emm5gj001ks94h7ulryqho","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5hl002ns94hkwmriwqt"},{"post_id":"ck0emm5gj001ks94h7ulryqho","tag_id":"ck0emm5hk002ds94hhgwqrv6d","_id":"ck0emm5hl002os94hhlo57gdf"},{"post_id":"ck0emm5gr001rs94hlfdma729","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck0emm5hl002ps94he20t95fy"},{"post_id":"ck0emm5gr001rs94hlfdma729","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck0emm5hl002qs94h2o95tdqn"},{"post_id":"ck0emm5gr001rs94hlfdma729","tag_id":"ck0emm5hk002gs94hxk1c1r12","_id":"ck0emm5hl002rs94h38caa7gi"},{"post_id":"ck4wllksc00005u4hj1k6sj5v","tag_id":"ck0emm5f80005s94h5jtmmzyx","_id":"ck4wllktq00035u4hfd3rfkop"},{"post_id":"ck4wllksc00005u4hj1k6sj5v","tag_id":"ck0emm5g50017s94heskk8obu","_id":"ck4wllkts00045u4hzl0h5qum"},{"post_id":"ck4wllksc00005u4hj1k6sj5v","tag_id":"ck4wllkti00015u4hkou9eq66","_id":"ck4wllktw00055u4hkh11nmo4"}],"Tag":[{"name":"Flink","_id":"ck0emm5f80005s94h5jtmmzyx"},{"name":"视频，Python","_id":"ck0emm5fu000ws94h5nevl5b4"},{"name":"道德经","_id":"ck0emm5g50017s94heskk8obu"},{"name":"圣人为腹不为目","_id":"ck0emm5gf001hs94hxmyu0pp4"},{"name":"Python","_id":"ck0emm5gw001ws94hyx0474sq"},{"name":"Connector","_id":"ck0emm5h50021s94hfzz60iar"},{"name":"孰能浊以静之徐清","_id":"ck0emm5hi0028s94hzc47h2ic"},{"name":"孰能安以动之徐生","_id":"ck0emm5hi0029s94hbyfl0zng"},{"name":"上善若水","_id":"ck0emm5hj002as94hk5j392un"},{"name":"持而盈之-不如其已","_id":"ck0emm5hk002ds94hhgwqrv6d"},{"name":"曲则全","_id":"ck0emm5hk002gs94hxk1c1r12"},{"name":"致虚极守静笃","_id":"ck4wllkti00015u4hkou9eq66"}]}}