<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/me.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/me.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/me.png?v=7.2.0">


  <link rel="mask-icon" href="/images/me.png?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Apache Flink">
<meta name="keywords" content="Apache Flink">
<meta property="og:type" content="website">
<meta property="og:title" content="Pretty Cool Enjoyment">
<meta property="og:url" content="http://enjoyment.cool/index.html">
<meta property="og:site_name" content="Pretty Cool Enjoyment">
<meta property="og:description" content="Apache Flink">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pretty Cool Enjoyment">
<meta name="twitter:description" content="Apache Flink">





  
  
  <link rel="canonical" href="http://enjoyment.cool/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Pretty Cool Enjoyment</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Pretty Cool Enjoyment</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/sunjincheng121" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink SQL 概览/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink SQL 概览/" class="post-title-link" itemprop="url">Apache Flink SQL 概览</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:08:03" itemprop="dateModified" datetime="2019-07-07T20:08:03+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink SQL 概览/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink SQL 概览/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Apache-Flink-SQL-概览"><a href="#Apache-Flink-SQL-概览" class="headerlink" title="Apache Flink SQL 概览"></a>Apache Flink SQL 概览</h1><p>本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。</p>
<h1 id="Apache-Flink-SQL-Job的组成"><a href="#Apache-Flink-SQL-Job的组成" class="headerlink" title="Apache Flink SQL Job的组成"></a>Apache Flink SQL Job的组成</h1><p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：<br><img src="/2019/01/01/Apache Flink SQL 概览/E18BAB16-4094-48B3-92E5-D45EA18A62C1.png" alt><br>如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p>
<ul>
<li><p>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</p>
</li>
<li><p>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</p>
</li>
<li><p>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</p>
</li>
</ul>
<h1 id="Apache-Flink-SQL-核心算子"><a href="#Apache-Flink-SQL-核心算子" class="headerlink" title="Apache Flink SQL 核心算子"></a>Apache Flink SQL 核心算子</h1><p>SQL是Structured Quevy Language的缩写，最初是由美国计算机科学家<a href="https://en.wikipedia.org/wiki/Donald_D._Chamberlin#/media/File:Don_Chamberlin.jpg" target="_blank" rel="noopener">Donald D. Chamberlin</a>和Raymond F. Boyce在20世纪70年代早期从 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6359709" target="_blank" rel="noopener">Early History of SQL</a> 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great):</p>
<p><img src="/2019/01/01/Apache Flink SQL 概览/59761691-219B-47D5-8DC8-FC1D9C3E949E.png" alt></p>
<p>在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的”数据库语言SQL”语言定义。Apache Flink SQL 核心算子的语义设计也参考了<a href="http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt" target="_blank" rel="noopener">1992</a> 、<a href="http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf" target="_blank" rel="noopener">2011</a>等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。</p>
<h2 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h2><p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:<br><img src="/2019/01/01/Apache Flink SQL 概览/391D7615-63C5-4A18-85BA-540730B6B5E2.png" alt><br>对应的SQL语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColC FROME tab ;</span><br></pre></td></tr></table></figure>

<h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：<br><img src="/2019/01/01/Apache Flink SQL 概览/9F6085AD-4792-4A30-9503-82DF82540304.png" alt><br>对应的SQL语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM tab WHERE ColA &lt;&gt; &apos;a2&apos; ;</span><br></pre></td></tr></table></figure>

<h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/E3CA71CA-8B44-4802-8165-53684A152503.png" alt><br>对应的SQL语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ;</span><br></pre></td></tr></table></figure>

<h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：<br><img src="/2019/01/01/Apache Flink SQL 概览/1D8D2D78-5454-4919-A7C3-48B2338BDC78.png" alt><br>对应的SQL语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM T1 UNION ALL SELECT * FROM T2</span><br></pre></td></tr></table></figure>

<h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/CD7B2F3B-5802-417F-AD05-BC4BBF4E9B69.png" alt><br>对应的SQL语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM T1 UNION SELECT * FROM T2</span><br></pre></td></tr></table></figure>

<h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>
<ul>
<li>JOIN - INNER JOIN</li>
<li>LEFT JOIN - LEFT OUTER JOIN</li>
<li>RIGHT JOIN - RIGHT OUTER JOIN  </li>
<li>FULL JOIN - FULL OUTER JOIN</li>
</ul>
<p>JOIN与关系代数的Join语义相同，具体如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/16080D31-EB87-48F9-A32E-A992CC6DCEAC.png" alt></p>
<p>对应的SQL语句如下(INNER JOIN)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure>

<p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/05A64357-27DF-4714-BDDB-B390B7A0814A.png" alt><br>对应的SQL语句如下(INNER JOIN)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong> </p>
<ul>
<li><p>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</p>
</li>
<li><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>
</li>
</ul>
<h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>
<h3 id="OverWindow"><a href="#OverWindow" class="headerlink" title="OverWindow"></a>OverWindow</h3><p>OVER Window 目前支持由如下三个元素组合的8中类型：</p>
<ul>
<li>时间 - Processing Time 和 EventTime</li>
<li>数据集 - Bounded 和 UnBounded</li>
<li>划分方式 - ROWS 和 RANGE<br>我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义</li>
</ul>
<h4 id="Bounded-ROWS-Over-Window"><a href="#Bounded-ROWS-Over-Window" class="headerlink" title="Bounded ROWS Over Window"></a>Bounded ROWS Over Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>
<h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    agg1(col1) OVER(</span><br><span class="line">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class="line">     ORDER BY timeCol</span><br><span class="line">     ROWS </span><br><span class="line">     BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class="line">... </span><br><span class="line">FROM Tab1</span><br></pre></td></tr></table></figure>

<ul>
<li>value_expression - 进行分区的字表达式；</li>
<li>timeCol - 用于元素排序的时间字段；</li>
<li>rowCount - 是定义根据当前行开始向前追溯几行元素；</li>
</ul>
<h5 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h5><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:<br><img src="/2019/01/01/Apache Flink SQL 概览/051876D7-5A89-4134-8ED0-E6939FC3F412.png" alt><br>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window.</p>
<h4 id="Bounded-RANGE-Over-Window"><a href="#Bounded-RANGE-Over-Window" class="headerlink" title="Bounded RANGE Over Window"></a>Bounded RANGE Over Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口；</p>
<h5 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h5><p>Bounded RANGE OVER Window的语法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    agg1(col1) OVER(</span><br><span class="line">     [PARTITION BY (value_expression1,..., value_expressionN)] </span><br><span class="line">     ORDER BY timeCol</span><br><span class="line">     RANGE </span><br><span class="line">     BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, </span><br><span class="line">... </span><br><span class="line">FROM Tab1</span><br></pre></td></tr></table></figure>

<ul>
<li>value_expression - 进行分区的字表达式；</li>
<li>timeCol - 用于元素排序的时间字段；</li>
<li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；<h5 id="语义-1"><a href="#语义-1" class="headerlink" title="语义"></a>语义</h5>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：<br><img src="/2019/01/01/Apache Flink SQL 概览/95F7D755-E087-4808-83F7-2A20CA5C685F.png" alt></li>
</ul>
<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window.</p>
<h3 id="GroupWindow"><a href="#GroupWindow" class="headerlink" title="GroupWindow"></a>GroupWindow</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>
<ul>
<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>
<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>
<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加；</li>
</ul>
<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>
<p>GroupWindow的语法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    agg1(col1),</span><br><span class="line">     ... </span><br><span class="line">    aggN(colN)</span><br><span class="line">FROM Tab1</span><br><span class="line">GROUP BY [WINDOW(definition)], [gk]</span><br></pre></td></tr></table></figure>

<ul>
<li>[WINDOW(definition)] - 在具体窗口语义介绍中介绍。</li>
</ul>
<h4 id="Tumble-Window"><a href="#Tumble-Window" class="headerlink" title="Tumble Window"></a>Tumble Window</h4><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/26FE2626-FDA4-4474-9B25-5505FEF5FB37.png" alt><br>假设我们要写一个2分钟大小的Tumble，示例SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT gk, COUNT(*) AS pv </span><br><span class="line">  FROM tab </span><br><span class="line">    GROUP BY TUMBLE(rowtime, INTERVAL &apos;2&apos; MINUTE), gk</span><br></pre></td></tr></table></figure>

<h4 id="Hop-Window"><a href="#Hop-Window" class="headerlink" title="Hop Window"></a>Hop Window</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/AE17D459-66B1-4946-A962-BFC3B9438E79.png" alt><br>假设我们要写一个每5分钟统计近10分钟的页面访问量(PV).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT gk, COUNT(*) AS pv </span><br><span class="line">  FROM tab </span><br><span class="line">    GROUP BY HOP(rowtime, INTERVAL &apos;5&apos; MINUTE, INTERVAL &apos;10&apos; MINUTE), gk</span><br></pre></td></tr></table></figure>

<h4 id="Session-Window"><a href="#Session-Window" class="headerlink" title="Session Window"></a>Session Window</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下：<br><img src="/2019/01/01/Apache Flink SQL 概览/5636227F-29BE-4A3D-A1CD-A0F4EFB35E75.png" alt></p>
<p>假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT gk, COUNT(*) AS pv  </span><br><span class="line">  FROM pageAccessSession_tab</span><br><span class="line">    GROUP BY SESSION(rowtime, INTERVAL &apos;3&apos; MINUTE), gk</span><br></pre></td></tr></table></figure>

<p><strong>说明:</strong> 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数：</p>
<ul>
<li>TUMBLE_START/TUMBLE_END</li>
<li>HOP_START/HOP_END</li>
<li>SESSION_START/SESSION_END</li>
</ul>
<p>这些辅助函数如何使用，请参考如下完整示例的使用方式。</p>
<h1 id="完整的-SQL-Job-案例"><a href="#完整的-SQL-Job-案例" class="headerlink" title="完整的 SQL Job 案例"></a>完整的 SQL Job 案例</h1><p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p>
<table>
<thead>
<tr>
<th>region</th>
<th>userId</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>U0010</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1001</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U2032</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1100</td>
<td>2017-11-11 10:11:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 12:10:00</td>
</tr>
</tbody></table>
<h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>
<h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class="line">  extends SourceFunction[T] &#123;</span><br><span class="line">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  override def cancel(): Unit = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class="line"></span><br><span class="line">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class="line">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class="line">  val rowType = new RowTypeInfo(</span><br><span class="line">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class="line">  val data = Seq(</span><br><span class="line">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class="line">    Right(1510365660000L),</span><br><span class="line">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class="line">    Right(1510365660000L),</span><br><span class="line">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class="line">    Right(1510366200000L),</span><br><span class="line">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class="line">    Right(1510366260000L),</span><br><span class="line">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class="line">    Right(1510373400000L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class="line">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class="line">      &quot;accessTime&quot;,</span><br><span class="line">      new ExistingField(&quot;accessTime&quot;),</span><br><span class="line">      PreserveWatermarks.INSTANCE))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class="line">    execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class="line"></span><br><span class="line">  override def getTableSchema: TableSchema = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class="line">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class="line">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class="line">    if (tempFile.exists()) &#123;</span><br><span class="line">      tempFile.delete()</span><br><span class="line">    &#125;</span><br><span class="line">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // Streaming 环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    // 设置EventTime</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    //方便我们查出输出数据</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    val sourceTableName = &quot;mySource&quot;</span><br><span class="line">    // 创建自定义source数据结构</span><br><span class="line">    val tableSource = new MyTableSource</span><br><span class="line"></span><br><span class="line">    val sinkTableName = &quot;csvSink&quot;</span><br><span class="line">    // 创建CSV sink 数据结构</span><br><span class="line">    val tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    // 注册source</span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    // 注册sink</span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    val sql =</span><br><span class="line">      &quot;SELECT  &quot; +</span><br><span class="line">      &quot;  region, &quot; +</span><br><span class="line">      &quot;  TUMBLE_START(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winStart,&quot; +</span><br><span class="line">      &quot;  TUMBLE_END(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winEnd, COUNT(region) AS pv &quot; +</span><br><span class="line">      &quot; FROM mySource &quot; +</span><br><span class="line">      &quot; GROUP BY TUMBLE(accessTime, INTERVAL &apos;2&apos; MINUTE), region&quot;</span><br><span class="line"></span><br><span class="line">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure>

<p>Cat 方式查看计算结果，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class="line">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class="line">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class="line">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure>

<p>表格化如上结果：<br>| region |winStart  | winEnd | pv|<br>| — | — | — | — |<br>| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1<br>| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2<br>| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1<br>| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 -  Watermark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 -  Watermark</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 19:49:12" itemprop="dateModified" datetime="2019-07-07T19:49:12+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 -  Watermark/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实际问题（乱序）"><a href="#实际问题（乱序）" class="headerlink" title="实际问题（乱序）"></a>实际问题（乱序）</h1><p>在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/7325C35E-CEE0-4DD0-97BB-BD0190B4F915.png" alt><br>那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？</p>
<h1 id="Apache-Flink的时间类型"><a href="#Apache-Flink的时间类型" class="headerlink" title="Apache Flink的时间类型"></a>Apache Flink的时间类型</h1><p>开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/E5D7CBF0-A452-4907-8FCF-F97896C6993A.png" alt></p>
<ul>
<li><p>ProcessingTime<br>是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。</p>
</li>
<li><p>IngestionTime<br>IngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。</p>
</li>
<li><p>EventTime<br>EventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。</p>
</li>
</ul>
<p>开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。</p>
<h1 id="什么是Watermark"><a href="#什么是Watermark" class="headerlink" title="什么是Watermark"></a>什么是Watermark</h1><p>Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示:<br><img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/BC6D8C93-A068-4799-B2CC-6A75AD43549C.png" alt></p>
<h1 id="Watermark的产生方式"><a href="#Watermark的产生方式" class="headerlink" title="Watermark的产生方式"></a>Watermark的产生方式</h1><p>目前Apache Flink 有两种生产Watermark的方式，如下：</p>
<ul>
<li>Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。<br>在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。</li>
<li>Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。</li>
</ul>
<p>所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。</p>
<h1 id="Watermark的接口定义"><a href="#Watermark的接口定义" class="headerlink" title="Watermark的接口定义"></a>Watermark的接口定义</h1><p>对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下：</p>
<ul>
<li><p>Periodic Watermarks - AssignerWithPeriodicWatermarks</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the current watermark. This method is periodically called by the</span><br><span class="line"> * system to retrieve the current watermark. The method may return &#123;@code null&#125; to</span><br><span class="line"> * indicate that no new Watermark is available.</span><br><span class="line"> *</span><br><span class="line"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class="line"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class="line"> * ascending watermarks). If the current watermark is still</span><br><span class="line"> * identical to the previous one, no progress in EventTime has happened since</span><br><span class="line"> * the previous call to this method. If a null value is returned, or theTimestamp</span><br><span class="line"> * of the returned watermark is smaller than that of the last emitted one, then no</span><br><span class="line"> * new watermark will be generated.</span><br><span class="line"> *</span><br><span class="line"> * &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated</span><br><span class="line"> * depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.</span><br><span class="line"> *</span><br><span class="line"> * @see org.Apache.flink.streaming.api.watermark.Watermark</span><br><span class="line"> * @see ExecutionConfig#getAutoWatermarkInterval()</span><br><span class="line"> *</span><br><span class="line"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class="line"> */</span><br><span class="line"> @Nullable</span><br><span class="line"> Watermark getCurrentWatermark();</span><br></pre></td></tr></table></figure>
</li>
<li><p>Punctuated Watermarks - AssignerWithPunctuatedWatermarks </p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Asks this implementation if it wants to emit a watermark. This method is called right after</span><br><span class="line"> * the &#123;@link #extractTimestamp(Object, long)&#125; method.</span><br><span class="line"> *</span><br><span class="line"> * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp</span><br><span class="line"> * is larger than that of the previously emitted watermark (to preserve the contract of</span><br><span class="line"> * ascending watermarks). If a null value is returned, or theTimestamp of the returned</span><br><span class="line"> * watermark is smaller than that of the last emitted one, then no new watermark will</span><br><span class="line"> * be generated.</span><br><span class="line"> *</span><br><span class="line"> * &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of</span><br><span class="line"> * &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.</span><br><span class="line"> *</span><br><span class="line"> * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.</span><br><span class="line"> */</span><br><span class="line"> @Nullable</span><br><span class="line">Watermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Assigns aTimestamp to an element, in milliseconds since the Epoch.</span><br><span class="line"> *</span><br><span class="line"> * &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element.</span><br><span class="line"> * That previousTimestamp may have been assigned from a previous assigner,</span><br><span class="line"> * by ingestionTime. If the element did not carry aTimestamp before, this value is</span><br><span class="line"> * &#123;@code Long.MIN_VALUE&#125;.</span><br><span class="line"> *</span><br><span class="line"> * @param element The element that theTimestamp is wil be assigned to.</span><br><span class="line"> * @param previousElementTimestamp The previous internalTimestamp of the element,</span><br><span class="line"> *                                 or a negative value, if noTimestamp has been assigned, yet.</span><br><span class="line"> * @return The newTimestamp.</span><br><span class="line"> */</span><br><span class="line">long extractTimestamp(T element, long previousElementTimestamp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。</p>
<h1 id="Watermark解决如上问题"><a href="#Watermark解决如上问题" class="headerlink" title="Watermark解决如上问题"></a>Watermark解决如上问题</h1><p>从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。</p>
<p>回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 </p>
<ul>
<li>当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：<img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/FDC7338B-1CA2-4AE4-B4F4-8C0EDE434F71.png" alt><br>上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：</li>
</ul>
<p>CREATE TABLE source(<br>  …,<br>  Event_timeTimeStamp,<br>  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0)<br>) with (<br>  …<br>);</p>
<ul>
<li>如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下：<img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/C15AA404-D132-4D6C-BA35-B103CA53AE61.png" alt></li>
</ul>
<p>上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：<br>CREATE TABLE source(<br>  …,<br>  Event_timeTimeStamp,<br>  WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000)<br>) with (<br>  …<br>);<br>上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s.</p>
<h1 id="多流的Watermark处理"><a href="#多流的Watermark处理" class="headerlink" title="多流的Watermark处理"></a>多流的Watermark处理</h1><p>在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/237CED69-52AB-4206-BF02-F99183C7186E.png" alt></p>
<p>Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图:</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 -  Watermark/77C24B94-7F26-4F72-BCA3-D0F20A791772.png" alt></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - Fault Tolerance</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:33:13" itemprop="dateModified" datetime="2019-07-07T20:33:13+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。</p>
<h1 id="什么是Fault-Tolerance"><a href="#什么是Fault-Tolerance" class="headerlink" title="什么是Fault Tolerance"></a>什么是Fault Tolerance</h1><p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。</p>
<h1 id="传统数据库Fault-Tolerance"><a href="#传统数据库Fault-Tolerance" class="headerlink" title="传统数据库Fault Tolerance"></a>传统数据库Fault Tolerance</h1><p>我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下：</p>
<ul>
<li>Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别…</li>
<li>Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别…</li>
<li>Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别…</li>
<li>Serializable  - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</li>
</ul>
<p>主备复制，事物控制都是传统数据库容错的机制。</p>
<h1 id="流计算Fault-Tolerance的挑战"><a href="#流计算Fault-Tolerance的挑战" class="headerlink" title="流计算Fault Tolerance的挑战"></a>流计算Fault Tolerance的挑战</h1><p>流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。</p>
<h1 id="Apache-Flink的Fault-Tolerance-机制"><a href="#Apache-Flink的Fault-Tolerance-机制" class="headerlink" title="Apache Flink的Fault Tolerance 机制"></a>Apache Flink的Fault Tolerance 机制</h1><p>Apache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 <a href="https://arxiv.org/pdf/1506.08603.pdf" target="_blank" rel="noopener">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="noopener">Determining-Global-States-of-a-Distributed-System Paper</a>，该Paper描述了在分布式系统如何解决全局状态一致性问题。</p>
<p>在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。</p>
<h1 id="Checkpointing-的算法逻辑"><a href="#Checkpointing-的算法逻辑" class="headerlink" title="Checkpointing 的算法逻辑"></a>Checkpointing 的算法逻辑</h1><p>上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/40FE5816-DA56-4222-ADDE-1B023EE71BCF.png" alt></p>
<p>上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点：</p>
<ul>
<li>barrier 由source节点发出；</li>
<li>barrier会将流上event切分到不同的checkpoint中；</li>
<li>汇聚到当前节点的多流的barrier要对齐；</li>
<li>barrier对齐之后会进行Checkpointing，生成snapshot；</li>
<li>完成snapshot之后向下游发出barrier，继续直到Sink节点；</li>
</ul>
<p>这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/66FB2539-A89C-4AE8-8639-F62F54B18E78.png" alt></p>
<p>生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。</p>
<h1 id="Checkpointing-的控制"><a href="#Checkpointing-的控制" class="headerlink" title="Checkpointing 的控制"></a>Checkpointing 的控制</h1><p>上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义）</p>
<ul>
<li>checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式；</li>
<li>checkpointInterval - 检查点时间间隔，单位是毫秒。</li>
<li>checkpointTimeout - 检查点超时时间，单位毫秒。</li>
</ul>
<p>在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下：</p>
<ul>
<li>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。</li>
<li>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。</li>
</ul>
<h1 id="Apache-Flink-如何做到Exactly-once"><a href="#Apache-Flink-如何做到Exactly-once" class="headerlink" title="Apache Flink 如何做到Exactly-once"></a>Apache Flink 如何做到Exactly-once</h1><p>通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？</p>
<h2 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h2><ul>
<li>At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据）</li>
<li>Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复）</li>
</ul>
<p>从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。</p>
<p>我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/B0D184CA-95DE-40C7-AA80-C4FE44134D16.png" alt></p>
<p>上图示意了某个节点进行Checkpointing的过程：</p>
<ul>
<li>当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段；</li>
<li>在进行对齐期间早到的input的数据会被缓存到buffer中；</li>
<li>当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化；</li>
<li>当完Checkpointing时候将barrier广播给下游Operator；</li>
</ul>
<p>多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。</p>
<p>在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public interface CheckpointBarrierHandler &#123;</span><br><span class="line">     ...</span><br><span class="line">     //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent</span><br><span class="line">     BufferOrEvent getNextNonBlocked() throws Exception;</span><br><span class="line">     ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下: </p>
<ul>
<li>Exactly-Once模式 - BarrierBuffer<br>  BarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。</li>
</ul>
<p>BarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。</p>
<ul>
<li>At-Least-Once模式 - BarrierTracker<br>BarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。</li>
</ul>
<p>BarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。</p>
<p>如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。</p>
<h1 id="完整Job的Checkpointing过程"><a href="#完整Job的Checkpointing过程" class="headerlink" title="完整Job的Checkpointing过程"></a>完整Job的Checkpointing过程</h1><p>在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？</p>
<h2 id="整体Checkpointing流程"><a href="#整体Checkpointing流程" class="headerlink" title="整体Checkpointing流程"></a>整体Checkpointing流程</h2><p><img src="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/678508E9-D79F-438A-B236-17A942D31F73.png" alt></p>
<p>上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。</p>
<h2 id="Incremental-Checkpointing"><a href="#Incremental-Checkpointing" class="headerlink" title="Incremental Checkpointing"></a>Incremental Checkpointing</h2><p>对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Fault Tolerance/F19C3E7E-B8FD-49F2-83C9-FAAD568D478F.png" alt></p>
<p>由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。</p>
<h1 id="端到端exactly-once"><a href="#端到端exactly-once" class="headerlink" title="端到端exactly-once"></a>端到端exactly-once</h1><p>根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下：</p>
<ul>
<li><p>外部Source的容错要求<br>Apache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。</p>
</li>
<li><p>外部Sink的容错要求<br>Apache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>
</li>
</ul>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - JOIN LATERAL</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:16:07" itemprop="dateModified" datetime="2019-07-07T20:16:07+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="聊什么"><a href="#聊什么" class="headerlink" title="聊什么"></a>聊什么</h1><p>上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。如下图所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/1C9522A6-61E8-4A5F-8222-7F335B6E09FF.png" alt></p>
<p>本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。</p>
<h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下：</p>
<ul>
<li>Customers<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Customers (</span><br><span class="line">  customerid char(5) NOT NULL,</span><br><span class="line">  city varchar (10) NOT NULL</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert into Customers values(&apos;C001&apos;,&apos;Beijing&apos;);</span><br><span class="line">insert into Customers values(&apos;C002&apos;,&apos;Beijing&apos;);</span><br><span class="line">insert into Customers values(&apos;C003&apos;,&apos;Beijing&apos;);</span><br><span class="line">insert into Customers values(&apos;C004&apos;,&apos;HangZhou&apos;);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>查看数据：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/3471F756-4FBF-4C3F-A28B-7D8F22CF5D80.png" alt></p>
<ul>
<li>Orders<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Orders(</span><br><span class="line">  orderid char(5) NOT NULL,</span><br><span class="line">  customerid char(5) NULL</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">insert into Orders values(&apos;O001&apos;,&apos;C001&apos;);</span><br><span class="line">insert into Orders values(&apos;O002&apos;,&apos;C001&apos;);</span><br><span class="line">insert into Orders values(&apos;O003&apos;,&apos;C003&apos;);</span><br><span class="line">insert into Orders values(&apos;O004&apos;,&apos;C001&apos;);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>查看数据：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/5DA82C35-6E03-43AD-B695-35C85D85D641.png" alt></p>
<h2 id="问题示例"><a href="#问题示例" class="headerlink" title="问题示例"></a>问题示例</h2><p>假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/E3709EBB-89AE-44EA-98C7-06A2794597D9.png" alt></p>
<h3 id="用INNER-JOIN解决"><a href="#用INNER-JOIN解决" class="headerlink" title="用INNER JOIN解决"></a>用INNER JOIN解决</h3><p>如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    c.customerid, c.city, o.orderid </span><br><span class="line">FROM Customers c JOIN Orders o </span><br><span class="line">	ON o.customerid = c.customerid</span><br></pre></td></tr></table></figure>

<p>查询结果如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/ED960882-39B0-40FD-9400-60D0D66F366E.png" alt></p>
<p>但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。</p>
<h2 id="用-Correlated-subquery解决"><a href="#用-Correlated-subquery解决" class="headerlink" title="用 Correlated subquery解决"></a>用 Correlated subquery解决</h2><p>Correlated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。</p>
<ul>
<li>WHERE Clause<br>用WHERE Clause实现上面的查询需求，SQL如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    c.customerid, c.city</span><br><span class="line">FROM Customers c WHERE c.customerid IN (</span><br><span class="line">    SELECT </span><br><span class="line">      o.customerid, o.orderid</span><br><span class="line">    FROM Orders o</span><br><span class="line">    WHERE o.customerid = c.customerid</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>执行情况：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/28AB4B72-87D6-464A-AA67-CD0A50A8BA36.png" alt></p>
<p>上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对<code>o.orderid</code>进行投影, 上面查询我为什么要加一个<code>o.orderid</code>呢，因为查询需求是需要<code>o.orderid</code>的，去掉<code>o.orderid</code>查询能成功，但是拿到的结果并不是我们想要的，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    c.customerid, c.city</span><br><span class="line">FROM Customers c WHERE c.customerid IN (</span><br><span class="line">    SELECT </span><br><span class="line">      o.customerid</span><br><span class="line">    FROM Orders o</span><br><span class="line">    WHERE o.customerid = c.customerid</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>查询结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/D0B3ADED-7C0D-4D10-BBF2-B6FEA4F5137E.png" alt></p>
<p>可见上面查询结果缺少了<code>o.orderid</code>,不能满足我们的查询需求。</p>
<ul>
<li>FROM Clause<br>用FROM Clause实现上面的查询需求，SQL如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    c.customerid, c.city, o.orderid </span><br><span class="line">FROM Customers c, (</span><br><span class="line">    SELECT </span><br><span class="line">      o.orderid, o.customerid </span><br><span class="line">    FROM Orders o</span><br><span class="line">    WHERE o.customerid = c.customerid</span><br><span class="line">) as o</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>我们会得到如下错误：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/37259B89-DD8F-4349-A378-000AAA886479.png" alt><br>错误信息提示我们无法识别<code>c.customerid</code>。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，<br>那么上面的查询需求除了<code>INNER JOIN</code> 我们还可以如何解决呢？</p>
<h1 id="JOIN-LATERAL"><a href="#JOIN-LATERAL" class="headerlink" title="JOIN LATERAL"></a>JOIN LATERAL</h1><p>我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了<code>LATERAL</code>关键字，也叫做 <code>lateral drive table</code>。</p>
<h2 id="CROSS-APPLY和LATERAL"><a href="#CROSS-APPLY和LATERAL" class="headerlink" title="CROSS APPLY和LATERAL"></a>CROSS APPLY和LATERAL</h2><p>上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 <code>LATERAL</code> 的呢？SQL Server是用自己的方言 <code>CROSS APPLY</code> 来支持的。那么为啥不用ANSI-SQL的<code>LATERAL</code>而用<code>CROSS APPLY</code>呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时<code>LATERAL</code>是SQL2003引入的，而<code>CROSS APPLY</code>是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等<code>LATERAL</code>出来的时候，<code>CROSS APPLY</code>在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了<code>CROSS APPLY</code>，但<code>CROSS APPLY</code>的语义与<code>LATERAL</code>却完全一致，同时后续支持<code>LATERAL</code>的Oracle12和PostgreSQL94同时支持了<code>LATERAL</code>和<code>CROSS APPLY</code>。</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>那么我们回到上面的问题，我们用SQL Server的<code>CROSS APPLY</code>来解决上面问题，SQL如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/42B8EBC9-CD5B-498C-B191-F4241C91483F.png" alt></p>
<p>上面得到的结果完全满足查询需求。</p>
<h1 id="JOIN-LATERAL-与-INNER-JOIN-关系"><a href="#JOIN-LATERAL-与-INNER-JOIN-关系" class="headerlink" title="JOIN LATERAL 与 INNER JOIN 关系"></a>JOIN LATERAL 与 INNER JOIN 关系</h1><p>上面的查询需求并没有体现<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别，我们还是以SQL Server中两个查询执行Plan来观察一下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/A3A08123-0490-41BA-9E07-95B4557A19F5.png" alt></p>
<p>上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个<code>LATERAL</code> 出来呢？</p>
<h2 id="性能方面"><a href="#性能方面" class="headerlink" title="性能方面"></a>性能方面</h2><p>我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。</p>
<ul>
<li>LATERAL 的写法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    c.customerid, c.city, o.orderid </span><br><span class="line">FROM Customers c CROSS APPLY (</span><br><span class="line">    SELECT </span><br><span class="line">     TOP(1) o.orderid, o.customerid </span><br><span class="line">    FROM Orders o </span><br><span class="line">    WHERE o.customerid = c.customerid</span><br><span class="line">	ORDER BY o.customerid, o.orderid</span><br><span class="line">) as o</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>查询结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/C6CC909F-7990-4C8B-AE79-CB06A830C9B7.png" alt><br>我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。</p>
<ul>
<li>JOIN 写法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SELECT  c.customerid, c.city, o.orderid</span><br><span class="line"> FROM    Customers c</span><br><span class="line">  JOIN (</span><br><span class="line">   SELECT </span><br><span class="line">     o2.*, </span><br><span class="line">	 ROW_NUMBER() OVER (</span><br><span class="line">		PARTITION BY customerid </span><br><span class="line">		ORDER BY orderid</span><br><span class="line">	  ) AS rn</span><br><span class="line">   FROM    Orders o2</span><br><span class="line"> ) o</span><br><span class="line">ON c.customerid = o.customerid AND o.rn = 1</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>查询结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/90BC2538-6A12-4F5F-AE42-B87BA5C12871.png" alt></p>
<p>如上我们都完成了查询需求，我们在来看一下执行Plan，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/32183E30-E096-46DE-A855-932377D70BFE.png" alt><br>我们直观发现完成相同功能，使用<code>CROSS APPLY</code>进行查询，执行Plan简单许多。</p>
<h2 id="功能方面"><a href="#功能方面" class="headerlink" title="功能方面"></a>功能方面</h2><p>在功能方面<code>INNER JOIN</code>本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入<code>CROSS APPLY</code>的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">   name, log_backup_time </span><br><span class="line">FROM sys.databases AS s</span><br><span class="line"> CROSS APPLY sys.dm_db_log_stats(s.database_id);</span><br></pre></td></tr></table></figure>

<p>查询结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/C1F61F90-B7F6-4E11-B91C-9B1366C3EED3.png" alt></p>
<h1 id="Apache-Flink对-LATERAL的支持"><a href="#Apache-Flink对-LATERAL的支持" class="headerlink" title="Apache Flink对 LATERAL的支持"></a>Apache Flink对 LATERAL的支持</h1><p>前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持<code>LATERAL</code>的，接下来我们看看Apache Flink对<code>LATERAL</code>的支持情况。</p>
<h2 id="Calcite"><a href="#Calcite" class="headerlink" title="Calcite"></a>Calcite</h2><p>Apache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持<code>LATERAL</code>语法，示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    e.NAME, e.DEPTNO, d.NAME </span><br><span class="line">FROM EMPS e, LATERAL (</span><br><span class="line">    SELECT </span><br><span class="line">    *</span><br><span class="line">    FORM DEPTS d </span><br><span class="line">    WHERE e.DEPTNO=d.DEPTNO</span><br><span class="line"> ) as d;</span><br></pre></td></tr></table></figure>

<p>查询结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/DCC358D8-1CAC-4CA2-965A-B53C1842DA69.png" alt><br>我使用的是Calcite官方自带测试数据。</p>
<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>截止到Flink-1.6.2，Apache Flink 中有两种场景使用<code>LATERAL</code>，如下：</p>
<ul>
<li>UDTF(TVF) - User-defined Table Funciton </li>
<li>Temporal Table - 涉及内容会在后续篇章单独介绍。</li>
</ul>
<p>本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持<code>LATERAL</code>。</p>
<h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><p>UDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下：</p>
<ul>
<li><p>基类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Base class for all user-defined functions such as scalar functions, table functions,</span><br><span class="line">  * or aggregation functions.</span><br><span class="line">  */</span><br><span class="line">abstract class UserDefinedFunction extends Serializable &#123;</span><br><span class="line">  // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍）</span><br><span class="line">  def open(context: FunctionContext): Unit = &#123;&#125;</span><br><span class="line">  def close(): Unit = &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>TableFunction</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Base class for a user-defined table function (UDTF). A user-defined table functions works on</span><br><span class="line">  * zero, one, or multiple scalar values as input and returns multiple rows as output.</span><br><span class="line">  *</span><br><span class="line">  * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation</span><br><span class="line">  * method. An evaluation method must be declared publicly, not static and named &quot;eval&quot;.</span><br><span class="line">  * Evaluation methods can also be overloaded by implementing multiple methods named &quot;eval&quot;.</span><br><span class="line">  *</span><br><span class="line">  * User-defined functions must have a default constructor and must be instantiable during runtime.</span><br><span class="line">  *</span><br><span class="line">  * By default the result type of an evaluation method is determined by Flink&apos;s type extraction</span><br><span class="line">  * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more</span><br><span class="line">  * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type</span><br><span class="line">  * can be manually defined by overriding [[getResultType()]].</span><br><span class="line">  */</span><br><span class="line">abstract class TableFunction[T] extends UserDefinedFunction &#123;</span><br><span class="line"></span><br><span class="line">  // 对于泛型T，如果是基础类型那么Flink框架可以自动识别，</span><br><span class="line">  // 对于用户自定义的复杂对象，需要用户overwrite这个实现。</span><br><span class="line">  def getResultType: TypeInformation[T] = null</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>上面定义的核心是要求用户实现<code>eval</code>方法，我们写一个具体示例。</p>
<ul>
<li>示例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 定义一个简单的UDTF返回类型，对应接口上的 T </span><br><span class="line">case class SimpleUser(name: String, age: Int)</span><br><span class="line">// 继承TableFunction，并实现evale方法</span><br><span class="line">// 核心功能是解析以#分割的字符串</span><br><span class="line">class SplitTVF extends TableFunction[SimpleUser] &#123;</span><br><span class="line">  // make sure input element&apos;s format is &quot;&lt;string&gt;#&lt;int&gt;&quot;</span><br><span class="line">  def eval(user: String): Unit = &#123;</span><br><span class="line">    if (user.contains(&quot;#&quot;)) &#123;</span><br><span class="line">      val splits = user.split(&quot;#&quot;)</span><br><span class="line">      collect(SimpleUser(splits(0), splits(1).toInt))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="示例-完整的ITCase-："><a href="#示例-完整的ITCase-：" class="headerlink" title="示例(完整的ITCase)："></a>示例(完整的ITCase)：</h3><ul>
<li>测试数据<br>我们构造一个只包含一个data字段的用户表，用户表数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>data</th>
</tr>
</thead>
<tbody><tr>
<td>Sunny#8</td>
</tr>
<tr>
<td>Kevin#36</td>
</tr>
<tr>
<td>Panpan#36</td>
</tr>
<tr>
<td>* 查询需求</td>
</tr>
<tr>
<td>查询的需求是将data字段flatten成为name和age两个字段的表，期望得到：</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>name</th>
<th>age</th>
</tr>
</thead>
<tbody><tr>
<td>Sunny</td>
<td>8</td>
</tr>
<tr>
<td>Kevin</td>
<td>36</td>
</tr>
<tr>
<td>Panpan</td>
<td>36</td>
</tr>
</tbody></table>
<ul>
<li>查询示例<br>我们以ITCase方式完成如上查询需求，完整代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">  def testLateralTVF(): Unit = &#123;</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">    env.setStateBackend(getStateBackend)</span><br><span class="line">    StreamITCase.clear</span><br><span class="line"></span><br><span class="line">    val userData = new mutable.MutableList[(String)]</span><br><span class="line">    userData.+=((&quot;Sunny#8&quot;))</span><br><span class="line">    userData.+=((&quot;Kevin#36&quot;))</span><br><span class="line">    userData.+=((&quot;Panpan#36&quot;))</span><br><span class="line"></span><br><span class="line">    val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br><span class="line"></span><br><span class="line">    val users = env.fromCollection(userData).toTable(tEnv, &apos;data)</span><br><span class="line"></span><br><span class="line">    val tvf = new SplitTVF()</span><br><span class="line">    tEnv.registerTable(&quot;userTab&quot;, users)</span><br><span class="line">    tEnv.registerFunction(&quot;splitTVF&quot;, tvf)</span><br><span class="line"></span><br><span class="line">    val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row]</span><br><span class="line">    result.addSink(new StreamITCase.StringSink[Row])</span><br><span class="line">    env.execute()</span><br><span class="line">    StreamITCase.testResults.foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>运行结果：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN LATERAL/645848C4-9ACC-4DFD-AF35-F15EB9B2BE20.png" alt></p>
<p>上面的核心语句是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot;</span><br></pre></td></tr></table></figure>

<p>如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇重点向大家介绍了一种新的<code>JOIN</code>类型 - <code>JOIN LATERAL</code>。并向大家介绍了SQL Server中对<code>LATERAL</code>的支持方式，详细分析了<code>JOIN LATERAL</code>和<code>INNER JOIN</code>的区别与联系，最后切入到Apache Flink中，以<code>UDTF</code>示例说明了Apache Flink中对<code>JOIN LATERAL</code>的支持，后续篇章会介绍Apache Flink中另一种使用<code>LATERAL</code>的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见!</p>
<h1 id="关于点赞和评论"><a href="#关于点赞和评论" class="headerlink" title="关于点赞和评论"></a>关于点赞和评论</h1><p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - JOIN 算子</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:13:49" itemprop="dateModified" datetime="2019-07-07T20:13:49+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="聊什么"><a href="#聊什么" class="headerlink" title="聊什么"></a>聊什么</h1><p>在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！</p>
<h1 id="什么是JOIN"><a href="#什么是JOIN" class="headerlink" title="什么是JOIN"></a>什么是JOIN</h1><p>在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/70173B48-E4B8-466C-9846-AD68FEB6F9B2.png" alt></p>
<h1 id="为啥需要JOIN"><a href="#为啥需要JOIN" class="headerlink" title="为啥需要JOIN"></a>为啥需要JOIN</h1><p>JOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个：</p>
<ul>
<li>产生数据的源头可能不是一个系统；</li>
<li>产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下： <ul>
<li>1NF - 列不可再分；</li>
<li>2NF - 符合1NF，并且非主键属性全部依赖于主键属性；</li>
<li>3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来；</li>
<li>BCNF - 符合3NF，并且主键属性之间无依赖关系。</li>
</ul>
</li>
</ul>
<p>当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量）</p>
<h1 id="JOIN种类"><a href="#JOIN种类" class="headerlink" title="JOIN种类"></a>JOIN种类</h1><p>JOIN 在传统数据库中有如下分类：</p>
<ul>
<li>CROSS JOIN - 交叉连接，计算笛卡儿积；</li>
<li>INNER JOIN - 内连接，返回满足条件的记录；</li>
<li>OUTER JOIN<ul>
<li>LEFT - 返回左表所有行，右表不存在补NULL；</li>
<li>RIGHT - 返回右表所有行，左边不存在补NULL；</li>
<li>FULL -  返回左表和右表的并集，不存在一边补NULL;</li>
</ul>
</li>
<li>SELF JOIN - 自连接，将表查询时候命名不同的别名。</li>
</ul>
<h1 id="JOIN语法"><a href="#JOIN语法" class="headerlink" title="JOIN语法"></a>JOIN语法</h1><p>JOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明：</p>
<ul>
<li><p>SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">  a.colA, </span><br><span class="line">  b.colA</span><br><span class="line">FROM  </span><br><span class="line">  tab1 AS a , tab2 AS b</span><br><span class="line">WHERE a.id = b.id and a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure>
</li>
<li><p>SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">  a.colA, </span><br><span class="line">  b.colA</span><br><span class="line">FROM </span><br><span class="line">  tab1 AS a JOIN tab2 AS b ON a.id = b.id</span><br><span class="line">WHERE </span><br><span class="line">  a.other &amp;gt; b.other</span><br></pre></td></tr></table></figure>

<p>本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition]</span><br></pre></td></tr></table></figure>

<h1 id="语义示例说明"><a href="#语义示例说明" class="headerlink" title="语义示例说明"></a>语义示例说明</h1><p>在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/D9E93F8F-D39A-40BD-A78D-8E3404225B86.png" alt></p>
<h2 id="CROSS-JOIN"><a href="#CROSS-JOIN" class="headerlink" title="CROSS JOIN"></a>CROSS JOIN</h2><p>交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT * FROM student JOIN course;</span><br><span class="line">+------+-------+------+-----+-------+--------+</span><br><span class="line">| no   | name  | sex  | no  | name  | credit |</span><br><span class="line">+------+-------+------+-----+-------+--------+</span><br><span class="line">| S001 | Sunny | M    | C01 | Java  |      2 |</span><br><span class="line">| S002 | Tom   | F    | C01 | Java  |      2 |</span><br><span class="line">| S003 | Kevin | M    | C01 | Java  |      2 |</span><br><span class="line">| S001 | Sunny | M    | C02 | Blink |      3 |</span><br><span class="line">| S002 | Tom   | F    | C02 | Blink |      3 |</span><br><span class="line">| S003 | Kevin | M    | C02 | Blink |      3 |</span><br><span class="line">| S001 | Sunny | M    | C03 | Spark |      3 |</span><br><span class="line">| S002 | Tom   | F    | C03 | Spark |      3 |</span><br><span class="line">| S003 | Kevin | M    | C03 | Spark |      3 |</span><br><span class="line">+------+-------+------+-----+-------+--------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。<br>我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   student.name, course.name, score </span><br><span class="line">    -&amp;gt; FROM student JOIN  score ON student.no = score.s_no </span><br><span class="line">    -&amp;gt;              JOIN course ON score.c_no = course.no;</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">| name  | name  | score |</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">| Sunny | Java  |    80 |</span><br><span class="line">| Sunny | Blink |    98 |</span><br><span class="line">| Sunny | Spark |    76 |</span><br><span class="line">| Kevin | Java  |    78 |</span><br><span class="line">| Kevin | Blink |    88 |</span><br><span class="line">| Kevin | Spark |    68 |</span><br><span class="line">+-------+-------+-------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们:</p>
<ul>
<li><p>第一步 student和course 进行交叉联接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   stu.no, c.no, stu.name, c.name</span><br><span class="line">    -&amp;gt; FROM student stu JOIN course c  笛卡尔积</span><br><span class="line">    -&amp;gt; ORDER BY stu.no; -- 排序只是方便大家查看:)</span><br><span class="line">+------+-----+-------+-------+</span><br><span class="line">| no   | no  | name  | name  |</span><br><span class="line">+------+-----+-------+-------+</span><br><span class="line">| S001 | C03 | Sunny | Spark |</span><br><span class="line">| S001 | C01 | Sunny | Java  |</span><br><span class="line">| S001 | C02 | Sunny | Blink |</span><br><span class="line">| S002 | C03 | Tom   | Spark |</span><br><span class="line">| S002 | C01 | Tom   | Java  |</span><br><span class="line">| S002 | C02 | Tom   | Blink |</span><br><span class="line">| S003 | C02 | Kevin | Blink |</span><br><span class="line">| S003 | C03 | Kevin | Spark |</span><br><span class="line">| S003 | C01 | Kevin | Java  |</span><br><span class="line">+------+-----+-------+-------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步 将交叉联接的结果与score表进行左外联接，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   stu.no, c.no, stu.name, c.name,</span><br><span class="line">    -&amp;gt;    CASE </span><br><span class="line">    -&amp;gt;     WHEN s.score IS NULL THEN 0</span><br><span class="line">    -&amp;gt;     ELSE s.score</span><br><span class="line">    -&amp;gt;   END AS score </span><br><span class="line">    -&amp;gt; FROM student stu JOIN course c  -- 迪卡尔积</span><br><span class="line">    -&amp;gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN</span><br><span class="line">    -&amp;gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)</span><br><span class="line">+------+-----+-------+-------+-------+</span><br><span class="line">| no   | no  | name  | name  | score |</span><br><span class="line">+------+-----+-------+-------+-------+</span><br><span class="line">| S001 | C03 | Sunny | Spark |    76 |</span><br><span class="line">| S001 | C01 | Sunny | Java  |    80 |</span><br><span class="line">| S001 | C02 | Sunny | Blink |    98 |</span><br><span class="line">| S002 | C02 | Tom   | Blink |     0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息</span><br><span class="line">| S002 | C03 | Tom   | Spark |     0 |</span><br><span class="line">| S002 | C01 | Tom   | Java  |     0 |</span><br><span class="line">| S003 | C02 | Kevin | Blink |    88 |</span><br><span class="line">| S003 | C03 | Kevin | Spark |    68 |</span><br><span class="line">| S003 | C01 | Kevin | Java  |    78 |</span><br><span class="line">+------+-----+-------+-------+-------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息）</p>
<h2 id="INNER-JOIN"><a href="#INNER-JOIN" class="headerlink" title="INNER JOIN"></a>INNER JOIN</h2><p>内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class="line">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no </span><br><span class="line">    -&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| no   | name  | score |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| S001 | Sunny |    98 |</span><br><span class="line">| S003 | Kevin |    88 |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面按语义的逻辑是:</p>
<ul>
<li>第一步：先进行student和score的内连接，如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   stu.no, stu.name , s.score</span><br><span class="line">    -&amp;gt; FROM student stu JOIN score s ON  stu.no = s.s_no ;</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| no   | name  | score |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| S001 | Sunny |    80 |</span><br><span class="line">| S001 | Sunny |    98 |</span><br><span class="line">| S001 | Sunny |    76 |</span><br><span class="line">| S003 | Kevin |    78 |</span><br><span class="line">| S003 | Kevin |    88 |</span><br><span class="line">| S003 | Kevin |    68 |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<ul>
<li>第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果：    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-&amp;gt; WHERE s.score &amp;gt; 80;</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| no   | name  | score |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| S001 | Sunny |    98 |</span><br><span class="line">| S003 | Kevin |    88 |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   no, name , score</span><br><span class="line">    -&amp;gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &amp;gt;80) as sc ON no = s_no;</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| no   | name  | score |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| S001 | Sunny |    98 |</span><br><span class="line">| S003 | Kevin |    88 |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是：</p>
<ul>
<li><p>第一步：执行过滤子查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT s_no, score FROM score s WHERE s.score &amp;gt;80;</span><br><span class="line">+------+-------+</span><br><span class="line">| s_no | score |</span><br><span class="line">+------+-------+</span><br><span class="line">| S001 |    98 |</span><br><span class="line">| S003 |    88 |</span><br><span class="line">+------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步：执行内连接    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-&amp;gt; ON no = s_no;</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| no   | name  | score |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">| S001 | Sunny |    98 |</span><br><span class="line">| S003 | Kevin |    88 |</span><br><span class="line">+------+-------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则：</p>
<ul>
<li>filter push down</li>
<li>project push down</li>
</ul>
<p>如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/181D7E83-7E6B-408B-85EC-B77D7CD4AC7D.png" alt></p>
<h2 id="LEFT-OUTER-JOIN"><a href="#LEFT-OUTER-JOIN" class="headerlink" title="LEFT OUTER JOIN"></a>LEFT OUTER JOIN</h2><p>左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class="line">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class="line">    -&amp;gt; WHERE s.score is NULL;</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">| no   | name | c_no | score |</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">| S002 | Tom  | NULL |  NULL |</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面查询的执行逻辑上也是分成两步：</p>
<ul>
<li><p>第一步：左外联接查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class="line">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| no   | name  | c_no | score |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| S001 | Sunny | C01  |    80 |</span><br><span class="line">| S001 | Sunny | C02  |    98 |</span><br><span class="line">| S001 | Sunny | C03  |    76 |</span><br><span class="line">| S002 | Tom   | NULL |  NULL | -- 右表不存在的补NULL</span><br><span class="line">| S003 | Kevin | C01  |    78 |</span><br><span class="line">| S003 | Kevin | C02  |    88 |</span><br><span class="line">| S003 | Kevin | C03  |    68 |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步：过滤查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class="line">    -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no</span><br><span class="line">    -&amp;gt; WHERE s.score is NULL;</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">| no   | name | c_no | score |</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">| S002 | Tom  | NULL |  NULL |</span><br><span class="line">+------+------+------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下：</p>
<ul>
<li><p>第一步：先进行过滤查询(获得一个空表）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT * FROM score s WHERE s.score is NULL;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步： 进行左外链接</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   no, name , s.c_no, s.score</span><br><span class="line">    -&amp;gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| no   | name  | c_no | score |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| S001 | Sunny | NULL |  NULL |</span><br><span class="line">| S002 | Tom   | NULL |  NULL |</span><br><span class="line">| S003 | Kevin | NULL |  NULL |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。</p>
<h2 id="RIGHT-OUTER-JOIN"><a href="#RIGHT-OUTER-JOIN" class="headerlink" title="RIGHT OUTER JOIN"></a>RIGHT OUTER JOIN</h2><p>右外链接语义是返回右表所有行，左边不存在补NULL，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class="line">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| c_no | score | no   | name  |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| C01  |    80 | S001 | Sunny |</span><br><span class="line">| C02  |    98 | S001 | Sunny |</span><br><span class="line">| C03  |    76 | S001 | Sunny |</span><br><span class="line">| NULL |  NULL | S002 | Tom   | -- 左边没有的进行补 NULL</span><br><span class="line">| C01  |    78 | S003 | Kevin |</span><br><span class="line">| C02  |    88 | S003 | Kevin |</span><br><span class="line">| C03  |    68 | S003 | Kevin |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">7 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。</p>
<h2 id="FULL-OUTER-JOIN"><a href="#FULL-OUTER-JOIN" class="headerlink" title="FULL OUTER JOIN"></a>FULL OUTER JOIN</h2><p>全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。</p>
<h2 id="SELF-JOIN"><a href="#SELF-JOIN" class="headerlink" title="SELF JOIN"></a>SELF JOIN</h2><p>上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT * FROM student l JOIN student r where l.no = r.no;</span><br><span class="line">+------+-------+------+------+-------+------+</span><br><span class="line">| no   | name  | sex  | no   | name  | sex  |</span><br><span class="line">+------+-------+------+------+-------+------+</span><br><span class="line">| S001 | Sunny | M    | S001 | Sunny | M    |</span><br><span class="line">| S002 | Tom   | F    | S002 | Tom   | F    |</span><br><span class="line">| S003 | Kevin | M    | S003 | Kevin | M    |</span><br><span class="line">+------+-------+------+------+-------+------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="不等值联接"><a href="#不等值联接" class="headerlink" title="不等值联接"></a>不等值联接</h2><p>这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mysql&amp;gt; SELECT </span><br><span class="line">    -&amp;gt;   s.c_no, s.score, no, name</span><br><span class="line">    -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| c_no | score | no   | name  |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">| C01  |    80 | S001 | Sunny |</span><br><span class="line">| C01  |    80 | S002 | Tom   |</span><br><span class="line">| C01  |    80 | S003 | Kevin |</span><br><span class="line">| C02  |    98 | S001 | Sunny |</span><br><span class="line">| C02  |    98 | S002 | Tom   |</span><br><span class="line">| C02  |    98 | S003 | Kevin |</span><br><span class="line">| C03  |    76 | S001 | Sunny |</span><br><span class="line">| C03  |    76 | S002 | Tom   |</span><br><span class="line">| C03  |    76 | S003 | Kevin |</span><br><span class="line">| C01  |    78 | S001 | Sunny |</span><br><span class="line">| C01  |    78 | S002 | Tom   |</span><br><span class="line">| C01  |    78 | S003 | Kevin |</span><br><span class="line">| C02  |    88 | S001 | Sunny |</span><br><span class="line">| C02  |    88 | S002 | Tom   |</span><br><span class="line">| C02  |    88 | S003 | Kevin |</span><br><span class="line">| C03  |    68 | S001 | Sunny |</span><br><span class="line">| C03  |    68 | S002 | Tom   |</span><br><span class="line">| C03  |    68 | S003 | Kevin |</span><br><span class="line">+------+-------+------+-------+</span><br><span class="line">18 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。</p>
<h1 id="Apache-Flink双流JOIN"><a href="#Apache-Flink双流JOIN" class="headerlink" title="Apache Flink双流JOIN"></a>Apache Flink双流JOIN</h1><table>
<thead>
<tr>
<th></th>
<th>CROSS</th>
<th>INNER</th>
<th>OUTER</th>
<th>SELF</th>
<th>ON</th>
<th>WHERE</th>
</tr>
</thead>
<tbody><tr>
<td>Apache Flink</td>
<td>N</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>必选</td>
<td>可选</td>
</tr>
</tbody></table>
<p>Apache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。</p>
<h2 id="双流JOIN与传统数据库表JOIN的区别"><a href="#双流JOIN与传统数据库表JOIN的区别" class="headerlink" title="双流JOIN与传统数据库表JOIN的区别"></a>双流JOIN与传统数据库表JOIN的区别</h2><p>传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别：</p>
<ul>
<li>左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入；</li>
<li>JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。</li>
<li>查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。</li>
</ul>
<h3 id="数据Shuffle"><a href="#数据Shuffle" class="headerlink" title="数据Shuffle"></a>数据Shuffle</h3><p>分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。</p>
<h3 id="数据的保存"><a href="#数据的保存" class="headerlink" title="数据的保存"></a>数据的保存</h3><p>不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作：</p>
<ul>
<li><p>LeftEvent到来存储到LState，RightEvent到来的时候存储到RState；</p>
</li>
<li><p>LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游；</p>
</li>
<li><p>RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。</p>
</li>
</ul>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/3AE44275-DFE0-45D4-8259-EB03928B784B.png" alt></p>
<h2 id="简单场景介绍实现原理"><a href="#简单场景介绍实现原理" class="headerlink" title="简单场景介绍实现原理"></a>简单场景介绍实现原理</h2><h3 id="INNER-JOIN-实现"><a href="#INNER-JOIN-实现" class="headerlink" title="INNER JOIN 实现"></a>INNER JOIN 实现</h3><p>JOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/1FD75B87-CFBE-46EC-AD1F-91D66A2E11A8.png" alt><br>双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点：</p>
<ul>
<li>INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件；</li>
<li>INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。</li>
</ul>
<h3 id="LEFT-OUTER-JOIN-实现"><a href="#LEFT-OUTER-JOIN-实现" class="headerlink" title="LEFT OUTER JOIN 实现"></a>LEFT OUTER JOIN 实现</h3><p>LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/64F21679-7168-47B0-A0B2-31C876FF3830.png" alt><br>下图也是表达LEFT JOIN的语义，只是展现方式不同：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/096879FE-B4A4-4802-8749-9784162D19D0.png" alt><br>上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点：</p>
<ul>
<li>左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。</li>
<li>在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。</li>
</ul>
<h2 id="RIGHT-OUTER-JOIN-和-FULL-OUTER-JOIN"><a href="#RIGHT-OUTER-JOIN-和-FULL-OUTER-JOIN" class="headerlink" title="RIGHT OUTER JOIN  和 FULL OUTER JOIN"></a>RIGHT OUTER JOIN  和 FULL OUTER JOIN</h2><p>RIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。</p>
<h2 id="复杂场景介绍State结构"><a href="#复杂场景介绍State结构" class="headerlink" title="复杂场景介绍State结构"></a>复杂场景介绍State结构</h2><p>上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/49829548-F09C-47E6-A245-02B7A92749CA.png" alt><br>上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？<br>根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/45343A7B-866B-4A70-A3A5-944C989E35BD.png" alt><br>上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点：</p>
<ul>
<li>记录重复记录（完整记录重复记录或者记录相同记录的个数）</li>
<li>记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数）</li>
<li>记录哪一条事件是第一个可以与左边事件进行JOIN的事件</li>
</ul>
<h3 id="双流JOIN的State数据结构"><a href="#双流JOIN的State数据结构" class="headerlink" title="双流JOIN的State数据结构"></a>双流JOIN的State数据结构</h3><p>在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途：</p>
<h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><ul>
<li>Map&lt;JoinKey, Map&lt;rowData, count&gt;&gt;;<ul>
<li>第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件;</li>
<li>第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数<h4 id="数据结构的利用"><a href="#数据结构的利用" class="headerlink" title="数据结构的利用"></a>数据结构的利用</h4></li>
</ul>
</li>
<li>记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取</li>
<li>正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素</li>
<li>判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回</li>
</ul>
<h1 id="双流JOIN的应用优化"><a href="#双流JOIN的应用优化" class="headerlink" title="双流JOIN的应用优化"></a>双流JOIN的应用优化</h1><h2 id="构造更新流"><a href="#构造更新流" class="headerlink" title="构造更新流"></a>构造更新流</h2><p>我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。</p>
<h2 id="NULL造成的热点"><a href="#NULL造成的热点" class="headerlink" title="NULL造成的热点"></a>NULL造成的热点</h2><p>比如我们有A LEFT JOIN  B ON A.aCol = B.bCol LEFT JOIN  C ON B.cCol = C.cCol 的业务，JOB的DAG如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/82B29941-2A5B-44A2-9D78-3A0E3DBA0F24.png" alt><br>假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？<br>我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/C3A7D171-BB33-4DEF-B755-90A020A1C8B3.png" alt></p>
<h2 id="JOIN-ReOrder"><a href="#JOIN-ReOrder" class="headerlink" title="JOIN ReOrder"></a>JOIN ReOrder</h2><p>对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol  JOIN  C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - JOIN 算子/D6FF6BAA-AF5D-486D-8334-EFA39498B9C0.png" alt></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - State/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - State/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - State</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 19:52:19" itemprop="dateModified" datetime="2019-07-07T19:52:19+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - State/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - State/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: “上一次的计算结果保存在哪里，保存在内存可以吗？”，答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。</p>
<h1 id="什么是State"><a href="#什么是State" class="headerlink" title="什么是State"></a>什么是State</h1><p>这个问题似乎有些”弱智”？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。</p>
<h1 id="为什么需要State"><a href="#为什么需要State" class="headerlink" title="为什么需要State"></a>为什么需要State</h1><p>与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。</p>
<h1 id="State-实现"><a href="#State-实现" class="headerlink" title="State 实现"></a>State 实现</h1><p>Apache Flink内部有四种state的存储实现，具体如下：</p>
<ul>
<li>基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；</li>
<li>基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li>
<li>基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；</li>
<li>还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；<h1 id="State-持久化逻辑"><a href="#State-持久化逻辑" class="headerlink" title="State 持久化逻辑"></a>State 持久化逻辑</h1>Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。<br><img src="/2019/01/01/Apache Flink 漫谈系列 - State/4FD40F45-8684-4C6E-8DE0-EB1CE53B5D2E.png" alt><h1 id="State-分类"><a href="#State-分类" class="headerlink" title="State 分类"></a>State 分类</h1>Apache Flink 内部按照算子和数据分组角度将State划分为如下两类：</li>
<li>KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的；</li>
<li>OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。  </li>
</ul>
<h1 id="State-扩容重新分配"><a href="#State-扩容重新分配" class="headerlink" title="State 扩容重新分配"></a>State 扩容重新分配</h1><p>Apache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。</p>
<p>Apache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。</p>
<p>如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - State/AC6AE4BE-418C-40D9-86C0-A4AD2C3BA4FC.png" alt></p>
<p>在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。</p>
<h2 id="OperatorState对扩容的处理"><a href="#OperatorState对扩容的处理" class="headerlink" title="OperatorState对扩容的处理"></a>OperatorState对扩容的处理</h2><p>我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？<br>state 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public interface ListCheckpointed&amp;lt;T extends Serializable&amp;gt; &#123;</span><br><span class="line">    List&amp;lt;T&amp;gt; snapshotState(long var1, long var3) throws Exception;</span><br><span class="line"></span><br><span class="line">    void restoreState(List&amp;lt;T&amp;gt; var1) throws Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public interface InputSplit extends Serializable &#123;</span><br><span class="line">    int getSplitNumber();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - State/7230573C-621C-4792-BB1C-7EB96A61DBEE.png" alt></p>
<p>如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&amp;lt;Integer&amp;gt; assignedPartitions = new LinkedList&amp;lt;&amp;gt;();</span><br><span class="line">for (int i = 0; i &amp;lt; partitions; i++) &#123;</span><br><span class="line">        if (i % consumerCount == consumerIndex) &#123;</span><br><span class="line">                assignedPartitions.add(i);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - State/93B91967-EF73-42BE-B1D1-2DF19B2D5AB8.png" alt></p>
<p>那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。</p>
<h2 id="KeyedState对扩容的处理"><a href="#KeyedState对扩容的处理" class="headerlink" title="KeyedState对扩容的处理"></a>KeyedState对扩容的处理</h2><p>对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。</p>
<h3 id="什么是Key-Groups"><a href="#什么是Key-Groups" class="headerlink" title="什么是Key-Groups"></a>什么是Key-Groups</h3><p>Key-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class KeyGroupRange implements KeyGroupsList, Serializable &#123;</span><br><span class="line">        ...</span><br><span class="line">        ...</span><br><span class="line">        private final int startKeyGroup;</span><br><span class="line">        private final int endKeyGroup;</span><br><span class="line">        ...</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>KeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。</p>
<h3 id="什么决定Key-Groups的个数"><a href="#什么决定Key-Groups的个数" class="headerlink" title="什么决定Key-Groups的个数"></a>什么决定Key-Groups的个数</h3><p>key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。</p>
<p><code>GroupRange.of(0, maxParallelism)</code>如何决定key属于哪个Key-Group<br>确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public static int assignToKeyGroup(Object key, int maxParallelism) &#123;</span><br><span class="line">      return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) &#123;</span><br><span class="line">      return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int partition(T key, int numPartitions) &#123;</span><br><span class="line">      return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - State/989FDD71-F449-4C28-8D03-5D8C68EE5F4D.png" alt></p>
<p>如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。<br>每个Operator实例如何获取Key-Groups<br> 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public static KeyGroupRange computeKeyGroupRangeForOperatorIndex(</span><br><span class="line">      int maxParallelism,</span><br><span class="line">      int parallelism,</span><br><span class="line">      int operatorIndex) &#123;</span><br><span class="line">        GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex);</span><br><span class="line">        int startGroup = splitRange.getStartGroup();</span><br><span class="line">        int endGroup = splitRange.getEndGroup();</span><br><span class="line">   return new KeyGroupRange(startGroup, endGroup - 1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public GroupRange getSplitRange(int numSplits, int splitIndex) &#123;</span><br><span class="line">        ...</span><br><span class="line">        final int numGroupsPerSplit = getNumGroups() / numSplits;</span><br><span class="line">        final int numFatSplits = getNumGroups() % numSplits;</span><br><span class="line"></span><br><span class="line">        int startGroupForThisSplit;</span><br><span class="line">        int endGroupForThisSplit;</span><br><span class="line">        if (splitIndex &amp;lt; numFatSplits) &#123;</span><br><span class="line">            startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1);</span><br><span class="line">            endGroupForThisSplit =   startGroupForThisSplit + numGroupsPerSplit + 1;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits;</span><br><span class="line">            endGroupForThisSplit =  startGroupForThisSplit + numGroupsPerSplit;</span><br><span class="line">        &#125;</span><br><span class="line">        if (startGroupForThisSplit &amp;gt;= endGroupForThisSplit) &#123;</span><br><span class="line">                return GroupRange.emptyGroupRange();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">                return new GroupRange(startGroupForThisSplit, endGroupForThisSplit);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。<br>假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - State/1C193AC1-CC54-4F1A-A3D2-CB99EFF71878.png" alt><br>如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - Table API 概述</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:11:10" itemprop="dateModified" datetime="2019-07-07T20:11:10+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - Table API 概述/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="什么是Table-API"><a href="#什么是Table-API" class="headerlink" title="什么是Table API"></a>什么是Table API</h1><p>在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象，如下图所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/6C38519A-6AE6-43B9-81E3-F9F61C3061C6.png" alt></p>
<p>Apache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。</p>
<h1 id="Table-API的特点"><a href="#Table-API的特点" class="headerlink" title="Table API的特点"></a>Table API的特点</h1><p>Table API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下：</p>
<ul>
<li>声明式 - 用户只关心做什么，不用关心怎么做；</li>
<li>高性能 - 支持查询优化，可以获取最好的执行性能；</li>
<li>流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行；</li>
<li>标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。</li>
</ul>
<p>当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点：</p>
<ul>
<li>表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等</li>
<li>功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能</li>
<li>编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。</li>
</ul>
<p><strong>说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739" target="_blank" rel="noopener">FLIP-29</a> 中规划的新功能。</strong></p>
<h1 id="HelloWorld"><a href="#HelloWorld" class="headerlink" title="HelloWorld"></a>HelloWorld</h1><p>在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。</p>
<h2 id="Maven-依赖"><a href="#Maven-依赖" class="headerlink" title="Maven 依赖"></a>Maven 依赖</h2><p>在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;table.version&gt;1.7.0&lt;/table.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-table_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">  &lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<h2 id="程序结构"><a href="#程序结构" class="headerlink" title="程序结构"></a>程序结构</h2><p>在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/6BD62B32-E0A8-4CC7-88B5-9218ED10DA53.png" alt></p>
<ol>
<li>外部数据源，比如Kafka, Rabbitmq, CSV 等等；</li>
<li>查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等；</li>
<li>外部结果存储，比如Kafka，Cassandra，CSV等。</li>
</ol>
<p><strong>说明：1和3 在Apache Flink中统称为Connector。</strong></p>
<h2 id="主程序"><a href="#主程序" class="headerlink" title="主程序"></a>主程序</h2><p>我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。<br>根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码：</p>
<ul>
<li>TableSoruce Code - 用于创建数据源的代码</li>
<li>Table API Query - 用于进行word count统计的Table API 查询逻辑</li>
<li>TableSink Code - 用于保存word count计算结果的结果表代码 </li>
</ul>
<h3 id="运行模式选择"><a href="#运行模式选择" class="headerlink" title="运行模式选择"></a>运行模式选择</h3><p>一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// Stream运行环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br></pre></td></tr></table></figure>

<h3 id="构建测试Source"><a href="#构建测试Source" class="headerlink" title="构建测试Source"></a>构建测试Source</h3><p>我们用最简单的构建Source方式进行本次测试，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> // 测试数据</span><br><span class="line"> val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class="line"> // 最简单的获取Source方式</span><br><span class="line">val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br></pre></td></tr></table></figure>

<h3 id="WordCount-统计逻辑"><a href="#WordCount-统计逻辑" class="headerlink" title="WordCount 统计逻辑"></a>WordCount 统计逻辑</h3><p>WordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 单词统计核心逻辑</span><br><span class="line">val result = source</span><br><span class="line">  .groupBy(&apos;word) // 单词分组</span><br><span class="line">  .select(&apos;word, &apos;word.count) // 单词统计</span><br></pre></td></tr></table></figure>

<h3 id="定义Sink"><a href="#定义Sink" class="headerlink" title="定义Sink"></a>定义Sink</h3><p>将WordCount的统计结果写入Sink中，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 自定义Sink</span><br><span class="line">    val sink = new RetractSink // 自定义Sink（下面有完整代码）</span><br><span class="line">    // 计算结果写入sink</span><br><span class="line">    result.toRetractStream[(String, Long)].addSink(sink)</span><br></pre></td></tr></table></figure>

<h3 id="完整的HelloWord代码"><a href="#完整的HelloWord代码" class="headerlink" title="完整的HelloWord代码"></a>完整的HelloWord代码</h3><p>为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.scala._</span><br><span class="line">import org.apache.flink.configuration.Configuration</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.table.api.TableEnvironment</span><br><span class="line">import org.apache.flink.table.api.scala._</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable</span><br><span class="line"></span><br><span class="line">object HelloWord &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // 测试数据</span><br><span class="line">    val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;)</span><br><span class="line"></span><br><span class="line">    // Stream运行环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">    // 最简单的获取Source方式</span><br><span class="line">    val source = env.fromCollection(data).toTable(tEnv, &apos;word)</span><br><span class="line"></span><br><span class="line">    // 单词统计核心逻辑</span><br><span class="line">    val result = source</span><br><span class="line">      .groupBy(&apos;word) // 单词分组</span><br><span class="line">      .select(&apos;word, &apos;word.count) // 单词统计</span><br><span class="line"></span><br><span class="line">    // 自定义Sink</span><br><span class="line">    val sink = new RetractSink</span><br><span class="line">    // 计算结果写入sink</span><br><span class="line">    result.toRetractStream[(String, Long)].addSink(sink)</span><br><span class="line"></span><br><span class="line">    env.execute</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class RetractSink extends RichSinkFunction[(Boolean, (String, Long))] &#123;</span><br><span class="line">  private var resultSet: mutable.Set[(String, Long)] = _</span><br><span class="line"></span><br><span class="line">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class="line">    // 初始化内存存储结构</span><br><span class="line">    resultSet = new mutable.HashSet[(String, Long)]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = &#123;</span><br><span class="line">    if (v._1) &#123;</span><br><span class="line">      // 计算数据</span><br><span class="line">      resultSet.add(v._2)</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">      // 撤回数据</span><br><span class="line">      resultSet.remove(v._2)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def close(): Unit = &#123;</span><br><span class="line">    // 打印写入sink的结果数据</span><br><span class="line">    resultSet.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/E56742A6-429A-4B32-BCCD-D1F37ACB32F6.png" alt></p>
<p>虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码:<br><code>source.groupBy(&#39;word).select(&#39;word, &#39;word.count)</code><br>所以Table API开发技术任务非常的简洁高效。</p>
<h1 id="Table-API-算子"><a href="#Table-API-算子" class="headerlink" title="Table API 算子"></a>Table API 算子</h1><p>虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。</p>
<h2 id="示例数据及测试类"><a href="#示例数据及测试类" class="headerlink" title="示例数据及测试类"></a>示例数据及测试类</h2><h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><ul>
<li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
</tbody></table>
<ul>
<li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>o_id</th>
<th>c_id</th>
<th>o_time</th>
<th>o_desc</th>
</tr>
</thead>
<tbody><tr>
<td>o_oo1</td>
<td>c_002</td>
<td>2018-11-05 10:01:01</td>
<td>iphone</td>
</tr>
<tr>
<td>o_002</td>
<td>c_001</td>
<td>2018-11-05 10:01:55</td>
<td>ipad</td>
</tr>
<tr>
<td>o_003</td>
<td>c_001</td>
<td>2018-11-05 10:03:44</td>
<td>flink book</td>
</tr>
</tbody></table>
<ul>
<li>Item_tab<br>商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li>
</ul>
<table>
<thead>
<tr>
<th align="left">itemID</th>
<th align="left">itemType</th>
<th align="left">onSellTime</th>
<th align="left">price</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ITEM001</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:01:00</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">ITEM002</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:02:00</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">ITEM003</td>
<td align="left">Electronic</td>
<td align="left"><em><strong>2017-11-11 10:03:00</strong></em></td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">ITEM004</td>
<td align="left">Electronic</td>
<td align="left"><em><strong>2017-11-11 10:03:00</strong></em></td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM005</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:05:00</td>
<td align="left">40</td>
</tr>
<tr>
<td align="left">ITEM006</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:06:00</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">ITEM007</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:07:00</td>
<td align="left">70</td>
</tr>
<tr>
<td align="left">ITEM008</td>
<td align="left">Clothes</td>
<td align="left">2017-11-11 10:08:00</td>
<td align="left">20</td>
</tr>
</tbody></table>
<ul>
<li>PageAccess_tab<br>页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>region</th>
<th>userId</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>U0010</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1001</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U2032</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1100</td>
<td>2017-11-11 10:11:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 12:10:00</td>
</tr>
</tbody></table>
<ul>
<li>PageAccessCount_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>region</th>
<th>userCount</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>100</td>
<td>2017.11.11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>86</td>
<td>2017.11.11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>210</td>
<td>2017.11.11 10:06:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>33</td>
<td>2017.11.11 10:10:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>129</td>
<td>2017.11.11 12:10:00</td>
</tr>
</tbody></table>
<ul>
<li>PageAccessSession_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li>
</ul>
<table>
<thead>
<tr>
<th>region</th>
<th>userId</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0012</td>
<td>2017-11-11 10:02:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0013</td>
<td>2017-11-11 10:03:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0015</td>
<td>2017-11-11 10:05:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U0110</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U2010</td>
<td>2017-11-11 10:11:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0410</td>
<td>2017-11-11 12:16:00</td>
</tr>
</tbody></table>
<h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><p>我们创建一个<code>TableAPIOverviewITCase.scala</code> 用于接下来介绍Flink Table API算子的功能体验。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.scala._</span><br><span class="line">import org.apache.flink.runtime.state.StateBackend</span><br><span class="line">import org.apache.flink.runtime.state.memory.MemoryStateBackend</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.RichSinkFunction</span><br><span class="line">import org.apache.flink.streaming.api.functions.source.SourceFunction</span><br><span class="line">import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.watermark.Watermark</span><br><span class="line">import org.apache.flink.table.api.&#123;Table, TableEnvironment&#125;</span><br><span class="line">import org.apache.flink.table.api.scala._</span><br><span class="line">import org.apache.flink.types.Row</span><br><span class="line">import org.junit.rules.TemporaryFolder</span><br><span class="line">import org.junit.&#123;Rule, Test&#125;</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">class Table APIOverviewITCase &#123;</span><br><span class="line"></span><br><span class="line">  // 客户表数据</span><br><span class="line">  val customer_data = new mutable.MutableList[(String, String, String)]</span><br><span class="line">  customer_data.+=((&quot;c_001&quot;, &quot;Kevin&quot;, &quot;from JinLin&quot;))</span><br><span class="line">  customer_data.+=((&quot;c_002&quot;, &quot;Sunny&quot;, &quot;from JinLin&quot;))</span><br><span class="line">  customer_data.+=((&quot;c_003&quot;, &quot;JinCheng&quot;, &quot;from HeBei&quot;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  // 订单表数据</span><br><span class="line">  val order_data = new mutable.MutableList[(String, String, String, String)]</span><br><span class="line">  order_data.+=((&quot;o_001&quot;, &quot;c_002&quot;, &quot;2018-11-05 10:01:01&quot;, &quot;iphone&quot;))</span><br><span class="line">  order_data.+=((&quot;o_002&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:01:55&quot;, &quot;ipad&quot;))</span><br><span class="line">  order_data.+=((&quot;o_003&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:03:44&quot;, &quot;flink book&quot;))</span><br><span class="line"></span><br><span class="line">  // 商品销售表数据</span><br><span class="line">  val item_data = Seq(</span><br><span class="line">    Left((1510365660000L, (1510365660000L, 20, &quot;ITEM001&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510365720000L, (1510365720000L, 50, &quot;ITEM002&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510365720000L)),</span><br><span class="line">    Left((1510365780000L, (1510365780000L, 30, &quot;ITEM003&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Left((1510365780000L, (1510365780000L, 60, &quot;ITEM004&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510365780000L)),</span><br><span class="line">    Left((1510365900000L, (1510365900000L, 40, &quot;ITEM005&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510365900000L)),</span><br><span class="line">    Left((1510365960000L, (1510365960000L, 20, &quot;ITEM006&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510365960000L)),</span><br><span class="line">    Left((1510366020000L, (1510366020000L, 70, &quot;ITEM007&quot;, &quot;Electronic&quot;))),</span><br><span class="line">    Right((1510366020000L)),</span><br><span class="line">    Left((1510366080000L, (1510366080000L, 20, &quot;ITEM008&quot;, &quot;Clothes&quot;))),</span><br><span class="line">    Right((151036608000L)))</span><br><span class="line"></span><br><span class="line">  // 页面访问表数据</span><br><span class="line">  val pageAccess_data = Seq(</span><br><span class="line">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0010&quot;))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, &quot;U1001&quot;))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2032&quot;))),</span><br><span class="line">    Right((1510366200000L)),</span><br><span class="line">    Left((1510366260000L, (1510366260000L, &quot;BeiJing&quot;, &quot;U1100&quot;))),</span><br><span class="line">    Right((1510366260000L)),</span><br><span class="line">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class="line">    Right((1510373400000L)))</span><br><span class="line"></span><br><span class="line">  // 页面访问量表数据2</span><br><span class="line">  val pageAccessCount_data = Seq(</span><br><span class="line">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, 100))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, 86))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510365960000L, (1510365960000L, &quot;BeiJing&quot;, 210))),</span><br><span class="line">    Right((1510366200000L)),</span><br><span class="line">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, 33))),</span><br><span class="line">    Right((1510366200000L)),</span><br><span class="line">    Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, 129))),</span><br><span class="line">    Right((1510373400000L)))</span><br><span class="line"></span><br><span class="line">  // 页面访问表数据3</span><br><span class="line">  val pageAccessSession_data = Seq(</span><br><span class="line">    Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class="line">    Right((1510365660000L)),</span><br><span class="line">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0012&quot;))),</span><br><span class="line">    Right((1510365720000L)),</span><br><span class="line">    Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0013&quot;))),</span><br><span class="line">    Right((1510365720000L)),</span><br><span class="line">    Left((1510365900000L, (1510365900000L, &quot;ShangHai&quot;, &quot;U0015&quot;))),</span><br><span class="line">    Right((1510365900000L)),</span><br><span class="line">    Left((1510366200000L, (1510366200000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class="line">    Right((1510366200000L)),</span><br><span class="line">    Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2010&quot;))),</span><br><span class="line">    Right((1510366200000L)),</span><br><span class="line">    Left((1510366260000L, (1510366260000L, &quot;ShangHai&quot;, &quot;U0011&quot;))),</span><br><span class="line">    Right((1510366260000L)),</span><br><span class="line">    Left((1510373760000L, (1510373760000L, &quot;ShangHai&quot;, &quot;U0410&quot;))),</span><br><span class="line">    Right((1510373760000L)))</span><br><span class="line"></span><br><span class="line">  val _tempFolder = new TemporaryFolder</span><br><span class="line"></span><br><span class="line">  // Streaming 环境</span><br><span class="line">  val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">  val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">  env.setParallelism(1)</span><br><span class="line">  env.setStateBackend(getStateBackend)</span><br><span class="line"></span><br><span class="line">  def getProcTimeTables(): (Table, Table) = &#123;</span><br><span class="line">    // 将order_tab, customer_tab 注册到catalog</span><br><span class="line">    val customer = env.fromCollection(customer_data).toTable(tEnv).as(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br><span class="line">    val order = env.fromCollection(order_data).toTable(tEnv).as(&apos;o_id, &apos;c_id, &apos;o_time, &apos;o_desc)</span><br><span class="line">    (customer, order)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def getEventTimeTables():  (Table, Table, Table, Table) = &#123;</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    // 将item_tab, pageAccess_tab 注册到catalog</span><br><span class="line">    val item =</span><br><span class="line">      env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data))</span><br><span class="line">      .toTable(tEnv, &apos;onSellTime, &apos;price, &apos;itemID, &apos;itemType, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">    val pageAccess =</span><br><span class="line">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data))</span><br><span class="line">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">    val pageAccessCount =</span><br><span class="line">      env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data))</span><br><span class="line">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;accessCount, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">    val pageAccessSession =</span><br><span class="line">      env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data))</span><br><span class="line">      .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">    (item, pageAccess, pageAccessCount, pageAccessSession)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  @Rule</span><br><span class="line">  def tempFolder: TemporaryFolder = _tempFolder</span><br><span class="line"></span><br><span class="line">  def getStateBackend: StateBackend = &#123;</span><br><span class="line">    new MemoryStateBackend()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def procTimePrint(result: Table): Unit = &#123;</span><br><span class="line">    val sink = new RetractingSink</span><br><span class="line">    result.toRetractStream[Row].addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def rowTimePrint(result: Table): Unit = &#123;</span><br><span class="line">    val sink = new RetractingSink</span><br><span class="line">    result.toRetractStream[Row].addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Test</span><br><span class="line">  def testProc(): Unit = &#123;</span><br><span class="line">    val (customer, order) = getProcTimeTables()</span><br><span class="line">    val result = ...// 测试的查询逻辑</span><br><span class="line">    procTimePrint(result)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Test</span><br><span class="line">  def testEvent(): Unit = &#123;</span><br><span class="line">    val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables()</span><br><span class="line">    val result = ...// 测试的查询逻辑</span><br><span class="line">    procTimePrint(result)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义Sink</span><br><span class="line">final class RetractingSink extends RichSinkFunction[(Boolean, Row)] &#123;</span><br><span class="line">  var retractedResults: ArrayBuffer[String] = null</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class="line">    super.open(parameters)</span><br><span class="line">    retractedResults = mutable.ArrayBuffer.empty[String]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def invoke(v: (Boolean, Row)) &#123;</span><br><span class="line">    retractedResults.synchronized &#123;</span><br><span class="line">      val value = v._2.toString</span><br><span class="line">      if (v._1) &#123;</span><br><span class="line">        retractedResults += value</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        val idx = retractedResults.indexOf(value)</span><br><span class="line">        if (idx &gt;= 0) &#123;</span><br><span class="line">          retractedResults.remove(idx)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          throw new RuntimeException(&quot;Tried to retract a value that wasn&apos;t added first. &quot; +</span><br><span class="line">                                       &quot;This is probably an incorrectly implemented test. &quot; +</span><br><span class="line">                                       &quot;Try to set the parallelism of the sink to 1.&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def close(): Unit = &#123;</span><br><span class="line">    super.close()</span><br><span class="line">    retractedResults.sorted.foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Water mark 生成器</span><br><span class="line">class EventTimeSourceFunction[T](</span><br><span class="line">  dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123;</span><br><span class="line">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def cancel(): Unit = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h2><p>SELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/EEED7340-FE9D-4791-961C-9996D35E148F.png" alt></p>
<h3 id="Table-API-示例"><a href="#Table-API-示例" class="headerlink" title="Table API 示例"></a>Table API 示例</h3><p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val result = customer</span><br><span class="line">  .select(&apos;c_name, concat_ws(&apos;c_name, &quot; come &quot;, &apos;c_desc))</span><br></pre></td></tr></table></figure>

<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_name</th>
<th>desc</th>
</tr>
</thead>
<tbody><tr>
<td>Kevin</td>
<td>Kevin come from JinLin</td>
</tr>
<tr>
<td>Sunny</td>
<td>Sunny come from JinLin</td>
</tr>
<tr>
<td>Jincheng</td>
<td>Jincheng come from HeBei</td>
</tr>
</tbody></table>
<h3 id="特别说明"><a href="#特别说明" class="headerlink" title="特别说明"></a>特别说明</h3><p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下：</p>
<h4 id="Table-API示例"><a href="#Table-API示例" class="headerlink" title="Table API示例"></a>Table API示例</h4><p>在订单表查询所有的客户id，消除重复客户id, 如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = order</span><br><span class="line">  .groupBy(&apos;c_id)</span><br><span class="line">  .select(&apos;c_id)</span><br></pre></td></tr></table></figure>

<h4 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th>c_id</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
</tr>
<tr>
<td>c_002</td>
</tr>
</tbody></table>
<h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><p>WHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/15657C5E-DD6E-4339-824E-6B17C30886C9.png" alt></p>
<h3 id="Table-API-示例-1"><a href="#Table-API-示例-1" class="headerlink" title="Table API 示例"></a>Table API 示例</h3><p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = customer</span><br><span class="line">  .where(&quot;c_id = &apos;c_001&apos; || c_id = &apos;c_003&apos;&quot;)</span><br><span class="line">  .select( &apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure>

<h3 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
</tbody></table>
<h3 id="特别说明-1"><a href="#特别说明-1" class="headerlink" title="特别说明"></a>特别说明</h3><p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>&amp;&amp;</code>， <code>||</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的<code>IN</code>和<code>NOT IN</code>在Table API里面用<code>intersect</code> 和 <code>minus</code>描述(flink-1.7.0版本)。</p>
<h4 id="Intersect-示例"><a href="#Intersect-示例" class="headerlink" title="Intersect 示例"></a>Intersect 示例</h4><p><code>Intersect</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询已经下过订单的客户信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 计算客户id，并去重</span><br><span class="line">val distinct_cids = order</span><br><span class="line">  .groupBy(&apos;c_id) // 去重</span><br><span class="line">  .select(&apos;c_id as &apos;o_c_id)</span><br><span class="line"></span><br><span class="line">val result = customer</span><br><span class="line">  .join(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class="line">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure>

<h4 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
</tr>
</tbody></table>
<h4 id="Minus-示例"><a href="#Minus-示例" class="headerlink" title="Minus 示例"></a>Minus 示例</h4><p><code>Minus</code>只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在<code>customer_tab</code>查询没有下过订单的客户信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> // 查询下过订单的客户id，并去重</span><br><span class="line"> val distinct_cids = order</span><br><span class="line">   .groupBy(&apos;c_id)</span><br><span class="line">   .select(&apos;c_id as &apos;o_c_id)</span><br><span class="line">   </span><br><span class="line">// 查询没有下过订单的客户信息</span><br><span class="line">val result = customer</span><br><span class="line">  .leftOuterJoin(distinct_cids, &apos;c_id === &apos;o_c_id)</span><br><span class="line">  .where(&apos;o_c_id isNull)</span><br><span class="line">  .select(&apos;c_id, &apos;c_name, &apos;c_desc)</span><br></pre></td></tr></table></figure>

<p>说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。</p>
<h4 id="Result-4"><a href="#Result-4" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
</tbody></table>
<h4 id="Intersect-Minus与关系代数"><a href="#Intersect-Minus与关系代数" class="headerlink" title="Intersect/Minus与关系代数"></a>Intersect/Minus与关系代数</h4><p>如上介绍<code>Intersect</code>是关系代数中的Intersection， <code>Minus</code>是关系代数的Difference， 如下图示意：</p>
<ul>
<li>Intersect(Intersection)<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/CAB50A8D-D49B-4CF5-B2A0-89DDBD0780D2.png" alt></li>
<li>Minus(Difference)<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/CA8E1649-4ED1-46E7-9D28-069093AAD2B1.png" alt></li>
</ul>
<h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/D56FE35E-0212-444C-8B0F-C650F1300C3C.png" alt></p>
<h3 id="Table-API-示例-2"><a href="#Table-API-示例-2" class="headerlink" title="Table API 示例"></a>Table API 示例</h3><p>将order_tab信息按<code>c_id</code>分组统计订单数量，简单示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = order</span><br><span class="line">  .groupBy(&apos;c_id)</span><br><span class="line">  .select(&apos;c_id, &apos;o_id.count)</span><br></pre></td></tr></table></figure>

<h3 id="Result-5"><a href="#Result-5" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>o_count</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>2</td>
</tr>
<tr>
<td>c_002</td>
<td>1</td>
</tr>
</tbody></table>
<h3 id="特别说明-2"><a href="#特别说明-2" class="headerlink" title="特别说明"></a>特别说明</h3><p>  在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p>
<h4 id="Table-API-示例-3"><a href="#Table-API-示例-3" class="headerlink" title="Table API 示例"></a>Table API 示例</h4><p>按时间进行分组，查询每分钟的订单数量，如下：</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val result = order</span><br><span class="line">  .select(&apos;o_id, &apos;c_id, &apos;o_time.substring(1, 16) as &apos;o_time_min)</span><br><span class="line">  .groupBy(&apos;o_time_min)</span><br><span class="line">  .select(&apos;o_time_min, &apos;o_id.count)</span><br></pre></td></tr></table></figure></code></pre><h4 id="Result-6"><a href="#Result-6" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th>o_time_min</th>
<th>o_count</th>
</tr>
</thead>
<tbody><tr>
<td>2018-11-05 10:01</td>
<td>2</td>
</tr>
<tr>
<td>2018-11-05 10:03</td>
<td>1</td>
</tr>
</tbody></table>
<p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p>
<h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/3402EE52-F2D5-452E-A0A5-66F0EC14C878.png" alt></p>
<h3 id="Table-API-示例-4"><a href="#Table-API-示例-4" class="headerlink" title="Table API 示例"></a>Table API 示例</h3><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val result = customer.unionAll(customer)</span><br></pre></td></tr></table></figure>

<h3 id="Result-7"><a href="#Result-7" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
<tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
</tbody></table>
<h3 id="特别说明-3"><a href="#特别说明-3" class="headerlink" title="特别说明"></a>特别说明</h3><p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p>
<h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/AE6DCCF9-32A4-4E7A-B7C5-334983354240.png" alt></p>
<h3 id="Table-API-示例-5"><a href="#Table-API-示例-5" class="headerlink" title="Table API 示例"></a>Table API 示例</h3><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val result = customer.union(customer)</span><br></pre></td></tr></table></figure>

<p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p>
<h3 id="Result-8"><a href="#Result-8" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
</tr>
</tbody></table>
<h3 id="特别说明-4"><a href="#特别说明-4" class="headerlink" title="特别说明"></a>特别说明</h3><p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p>
<h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p>
<ul>
<li>JOIN - INNER JOIN</li>
<li>LEFT JOIN - LEFT OUTER JOIN</li>
<li>RIGHT JOIN - RIGHT OUTER JOIN  </li>
<li>FULL JOIN - FULL OUTER JOIN<br>JOIN与关系代数的Join语义相同，具体如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/97E2A5D0-1D20-4C64-BC13-BBF7439ABA59.png" alt></li>
</ul>
<h3 id="Table-API-示例-JOIN"><a href="#Table-API-示例-JOIN" class="headerlink" title="Table API 示例 (JOIN)"></a>Table API 示例 (JOIN)</h3><p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val result = customer</span><br><span class="line">  .join(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure>

<h3 id="Result-9"><a href="#Result-9" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
<th>o_id</th>
<th>o_c_id</th>
<th>o_time</th>
<th>o_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
<td>o_002</td>
<td>c_001</td>
<td>2018-11-05 10:01:55</td>
<td>ipad</td>
</tr>
<tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
<td>o_003</td>
<td>c_001</td>
<td>2018-11-05 10:03:44</td>
<td>flink book</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
<td>o_oo1</td>
<td>c_002</td>
<td>2018-11-05 10:01:01</td>
<td>iphone</td>
</tr>
</tbody></table>
<h3 id="Table-API-示例-LEFT-JOIN"><a href="#Table-API-示例-LEFT-JOIN" class="headerlink" title="Table API 示例 (LEFT JOIN)"></a>Table API 示例 (LEFT JOIN)</h3><p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/AFEC8C8D-2B39-44EF-9B4F-65D31FE52070.png" alt><br>对应的SQL语句如下(LEFT JOIN)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ;</span><br></pre></td></tr></table></figure>

<ul>
<li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li>
</ul>
<p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val result = customer</span><br><span class="line">  .leftOuterJoin(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id)</span><br></pre></td></tr></table></figure>

<h3 id="Result-10"><a href="#Result-10" class="headerlink" title="Result"></a>Result</h3><table>
<thead>
<tr>
<th>c_id</th>
<th>c_name</th>
<th>c_desc</th>
<th>o_id</th>
<th>c_id</th>
<th>o_time</th>
<th>o_desc</th>
</tr>
</thead>
<tbody><tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
<td>o_002</td>
<td>c_001</td>
<td>2018-11-05 10:01:55</td>
<td>ipad</td>
</tr>
<tr>
<td>c_001</td>
<td>Kevin</td>
<td>from JinLin</td>
<td>o_003</td>
<td>c_001</td>
<td>2018-11-05 10:03:44</td>
<td>flink book</td>
</tr>
<tr>
<td>c_002</td>
<td>Sunny</td>
<td>from JinLin</td>
<td>o_oo1</td>
<td>c_002</td>
<td>2018-11-05 10:01:01</td>
<td>iphone</td>
</tr>
<tr>
<td>c_003</td>
<td>JinCheng</td>
<td>from HeBei</td>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
</tr>
</tbody></table>
<h3 id="特别说明-5"><a href="#特别说明-5" class="headerlink" title="特别说明"></a>特别说明</h3><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p>
<h2 id="Time-Interval-JOIN"><a href="#Time-Interval-JOIN" class="headerlink" title="Time-Interval JOIN"></a>Time-Interval JOIN</h2><p>Time-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">val result = left</span><br><span class="line">  .join(right)</span><br><span class="line">  // 定义Time Interval</span><br><span class="line">  .where(&apos;a === &apos;d &amp;&amp; &apos;c &gt;= &apos;f - 5.seconds &amp;&amp; &apos;c &lt; &apos;f + 6.seconds)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<h2 id="Lateral-JOIN"><a href="#Lateral-JOIN" class="headerlink" title="Lateral JOIN"></a>Lateral JOIN</h2><p>Apache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">val udtf = new UDTF</span><br><span class="line">val result = source.join(udtf(&apos;c) as (&apos;d, &apos;e))</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="Temporal-Table-JOIN"><a href="#Temporal-Table-JOIN" class="headerlink" title="Temporal Table JOIN"></a>Temporal Table JOIN</h2><p>Temporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">val rates = tEnv.scan(&quot;versonedTable&quot;).createTemporalTableFunction(&apos;rowtime, &apos;r_currency)</span><br><span class="line">val result = left.join(rates(&apos;o_rowtime), &apos;r_currency === &apos;o_currency)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>
<h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p>
<ul>
<li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li>
<li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li>
</ul>
<h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>
<h5 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h5><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/FC821998-4A3E-4F10-9351-C7B3060E7349.png" alt><br>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p>
<h5 id="Table-API-示例-6"><a href="#Table-API-示例-6" class="headerlink" title="Table API 示例"></a>Table API 示例</h5><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = item</span><br><span class="line">  .window(Over partitionBy &apos;itemType orderBy &apos;rowtime preceding 2.rows following CURRENT_ROW as &apos;w)</span><br><span class="line">  .select(&apos;itemID, &apos;itemType, &apos;onSellTime, &apos;price, &apos;price.max over &apos;w as &apos;maxPrice)</span><br></pre></td></tr></table></figure>

<h4 id="Result-11"><a href="#Result-11" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th align="left">itemID</th>
<th align="left">itemType</th>
<th align="left">onSellTime</th>
<th align="left">price</th>
<th align="left">maxPrice</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ITEM001</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:01:00</td>
<td align="left">20</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">ITEM002</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:02:00</td>
<td align="left">50</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">ITEM003</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:03:00</td>
<td align="left">30</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">ITEM004</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:03:00</td>
<td align="left">60</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM005</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:05:00</td>
<td align="left">40</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM006</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:06:00</td>
<td align="left">20</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM007</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:07:00</td>
<td align="left">70</td>
<td align="left">70</td>
</tr>
<tr>
<td align="left">ITEM008</td>
<td align="left">Clothes</td>
<td align="left">2017-11-11 10:08:00</td>
<td align="left">20</td>
<td align="left">20</td>
</tr>
</tbody></table>
<h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p>
<h5 id="语义-1"><a href="#语义-1" class="headerlink" title="语义"></a>语义</h5><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/79DC098C-1171-4C0E-A735-C4F63B6010FD.png" alt></p>
<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p>
<h5 id="Tabel-API-示例"><a href="#Tabel-API-示例" class="headerlink" title="Tabel API 示例"></a>Tabel API 示例</h5><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = item</span><br><span class="line">  .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w)</span><br><span class="line">  .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice)</span><br></pre></td></tr></table></figure>

<h5 id="Result（Bounded-RANGE-OVER-Window）"><a href="#Result（Bounded-RANGE-OVER-Window）" class="headerlink" title="Result（Bounded RANGE OVER Window）"></a>Result（Bounded RANGE OVER Window）</h5><table>
<thead>
<tr>
<th align="left">itemID</th>
<th align="left">itemType</th>
<th align="left">onSellTime</th>
<th align="left">price</th>
<th align="left">maxPrice</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ITEM001</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:01:00</td>
<td align="left">20</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">ITEM002</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:02:00</td>
<td align="left">50</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">ITEM003</td>
<td align="left">Electronic</td>
<td align="left"><em><strong>2017-11-11 10:03:00</strong></em></td>
<td align="left">30</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM004</td>
<td align="left">Electronic</td>
<td align="left"><em><strong>2017-11-11 10:03:00</strong></em></td>
<td align="left">60</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM005</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:05:00</td>
<td align="left">40</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">ITEM006</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:06:00</td>
<td align="left">20</td>
<td align="left">40</td>
</tr>
<tr>
<td align="left">ITEM007</td>
<td align="left">Electronic</td>
<td align="left">2017-11-11 10:07:00</td>
<td align="left">70</td>
<td align="left">70</td>
</tr>
<tr>
<td align="left">ITEM008</td>
<td align="left">Clothes</td>
<td align="left">2017-11-11 10:08:00</td>
<td align="left">20</td>
<td align="left">20</td>
</tr>
</tbody></table>
<h4 id="特别说明-6"><a href="#特别说明-6" class="headerlink" title="特别说明"></a>特别说明</h4><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p>
<ul>
<li>GROUP BY - <code>tab.groupBy(&#39;d).select(d, MAX(c))</code></li>
<li>OVER Window = <code>tab.window(Over.. as &#39;w).select(&#39;a, &#39;b, &#39;c, &#39;d, c.max over &#39;w)</code><br>如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li>
</ul>
<h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>
<ul>
<li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>
<li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>
<li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li>
</ul>
<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>
<h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><h5 id="语义-2"><a href="#语义-2" class="headerlink" title="语义"></a>语义</h5><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/C2A514D5-61F1-4464-95ED-17EB2A030EC2.png" alt></p>
<h5 id="Table-API-示例-7"><a href="#Table-API-示例-7" class="headerlink" title="Table API 示例"></a>Table API 示例</h5><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val result = pageAccess</span><br><span class="line">  .window(Tumble over 2.minute on &apos;rowtime as &apos;w)</span><br><span class="line">  .groupBy(&apos;w, &apos;region)</span><br><span class="line">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure>

<h5 id="Result-12"><a href="#Result-12" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>region</th>
<th>winStart</th>
<th>winEnd</th>
<th>pv</th>
</tr>
</thead>
<tbody><tr>
<td>BeiJing</td>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:02:00.0</td>
<td>1</td>
</tr>
<tr>
<td>BeiJing</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:12:00.0</td>
<td>2</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:02:00.0</td>
<td>1</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 04:10:00.0</td>
<td>2017-11-11 04:12:00.0</td>
<td>1</td>
</tr>
</tbody></table>
<h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p>
<h5 id="语义-3"><a href="#语义-3" class="headerlink" title="语义"></a>语义</h5><p>Hop 滑动窗口语义如下所示：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/3B56ADE1-9D3F-4AE2-86EE-A70C2B924D4D.png" alt></p>
<h5 id="Table-API-示例-8"><a href="#Table-API-示例-8" class="headerlink" title="Table API 示例"></a>Table API 示例</h5><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val result = pageAccessCount</span><br><span class="line">  .window(Slide over 10.minute every 5.minute on &apos;rowtime as &apos;w)</span><br><span class="line">  .groupBy(&apos;w)</span><br><span class="line">  .select(&apos;w.start, &apos;w.end, &apos;accessCount.sum as &apos;accessCount)</span><br></pre></td></tr></table></figure>

<h5 id="Result-13"><a href="#Result-13" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>winStart</th>
<th>winEnd</th>
<th>accessCount</th>
</tr>
</thead>
<tbody><tr>
<td>2017-11-11 01:55:00.0</td>
<td>2017-11-11 02:05:00.0</td>
<td>186</td>
</tr>
<tr>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:10:00.0</td>
<td>396</td>
</tr>
<tr>
<td>2017-11-11 02:05:00.0</td>
<td>2017-11-11 02:15:00.0</td>
<td>243</td>
</tr>
<tr>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:20:00.0</td>
<td>33</td>
</tr>
<tr>
<td>2017-11-11 04:05:00.0</td>
<td>2017-11-11 04:15:00.0</td>
<td>129</td>
</tr>
<tr>
<td>2017-11-11 04:10:00.0</td>
<td>2017-11-11 04:20:00.0</td>
<td>129</td>
</tr>
</tbody></table>
<h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p>
<h5 id="语义-4"><a href="#语义-4" class="headerlink" title="语义"></a>语义</h5><p>Session 会话窗口语义如下所示：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Table API 概述/D1197B0B-2DCF-434E-A7BB-A250187079F0.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val result = pageAccessSession</span><br><span class="line">  .window(Session withGap 3.minute on &apos;rowtime as &apos;w)</span><br><span class="line">  .groupBy(&apos;w, &apos;region)</span><br><span class="line">  .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br></pre></td></tr></table></figure>

<h5 id="Result-14"><a href="#Result-14" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>region</th>
<th>winStart</th>
<th>winEnd</th>
<th>pv</th>
</tr>
</thead>
<tbody><tr>
<td>BeiJing</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:13:00.0</td>
<td>1</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:01:00.0</td>
<td>2017-11-11 02:08:00.0</td>
<td>4</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:14:00.0</td>
<td>2</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 04:16:00.0</td>
<td>2017-11-11 04:19:00.0</td>
<td>1</td>
</tr>
</tbody></table>
<h4 id="嵌套Window"><a href="#嵌套Window" class="headerlink" title="嵌套Window"></a>嵌套Window</h4><p>在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用<code>&#39;w.rowtime</code>来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> ... </span><br><span class="line"> val result = pageAccess</span><br><span class="line">   .window(Tumble over 2.minute on &apos;rowtime as &apos;w1)</span><br><span class="line">   .groupBy(&apos;w1)</span><br><span class="line">   .select(&apos;w1.rowtime as &apos;rowtime, &apos;col1.count as &apos;cnt)</span><br><span class="line">   .window(Session withGap 3.minute on &apos;rowtime as &apos;w2)</span><br><span class="line">   .groupBy(&apos;w2)</span><br><span class="line">   .select(&apos;cnt.sum)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h1 id="Source-amp-Sink"><a href="#Source-amp-Sink" class="headerlink" title="Source&amp;Sink"></a>Source&amp;Sink</h1><p>上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下：</p>
<table>
<thead>
<tr>
<th>region</th>
<th>userId</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>U0010</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1001</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U2032</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1100</td>
<td>2017-11-11 10:11:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 12:10:00</td>
</tr>
</tbody></table>
<h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p>
<h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) </span><br><span class="line">  extends SourceFunction[T] &#123;</span><br><span class="line">  override def run(ctx: SourceContext[T]): Unit = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">      case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">      case Right(w) =&gt; ctx.emitWatermark(new Watermark(w))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  override def cancel(): Unit = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123;</span><br><span class="line"></span><br><span class="line">  val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;)</span><br><span class="line">  val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING))</span><br><span class="line">  val rowType = new RowTypeInfo(</span><br><span class="line">    Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  // 页面访问表数据 rows with timestamps and watermarks</span><br><span class="line">  val data = Seq(</span><br><span class="line">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)),</span><br><span class="line">    Right(1510365660000L),</span><br><span class="line">    Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)),</span><br><span class="line">    Right(1510365660000L),</span><br><span class="line">    Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)),</span><br><span class="line">    Right(1510366200000L),</span><br><span class="line">    Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)),</span><br><span class="line">    Right(1510366260000L),</span><br><span class="line">    Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)),</span><br><span class="line">    Right(1510373400000L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123;</span><br><span class="line">    Collections.singletonList(new RowtimeAttributeDescriptor(</span><br><span class="line">      &quot;accessTime&quot;,</span><br><span class="line">      new ExistingField(&quot;accessTime&quot;),</span><br><span class="line">      PreserveWatermarks.INSTANCE))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123;</span><br><span class="line">     execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getReturnType: TypeInformation[Row] = rowType</span><br><span class="line"></span><br><span class="line">  override def getTableSchema: TableSchema = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class="line">    // 打印sink的文件路径，方便我们查看运行结果</span><br><span class="line">    println(&quot;Sink path : &quot; + tempFile)</span><br><span class="line">    if (tempFile.exists()) &#123;</span><br><span class="line">      tempFile.delete()</span><br><span class="line">    &#125;</span><br><span class="line">    new CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // Streaming 环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    // 设置EventTime</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    //方便我们查出输出数据</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    val sourceTableName = &quot;mySource&quot;</span><br><span class="line">    // 创建自定义source数据结构</span><br><span class="line">    val tableSource = new MyTableSource</span><br><span class="line"></span><br><span class="line">    val sinkTableName = &quot;csvSink&quot;</span><br><span class="line">    // 创建CSV sink 数据结构</span><br><span class="line">    val tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    // 注册source</span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    // 注册sink</span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    val result = tEnv.scan(sourceTableName)</span><br><span class="line">      .window(Tumble over 2.minute on &apos;accessTime as &apos;w)</span><br><span class="line">      .groupBy(&apos;w, &apos;region)</span><br><span class="line">      .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv)</span><br><span class="line"></span><br><span class="line">    result.insertInto(sinkTableName)</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure>

<p>Cat 方式查看计算结果，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class="line">BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1</span><br><span class="line">BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2</span><br><span class="line">ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1</span><br></pre></td></tr></table></figure>

<p>表格化如上结果：<br>| region |winStart  | winEnd | pv|<br>| — | — | — | — |<br>| BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1<br>| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2<br>| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1<br>| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1</p>
<p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。</p>
<h1 id="关于点赞和评论"><a href="#关于点赞和评论" class="headerlink" title="关于点赞和评论"></a>关于点赞和评论</h1><p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - Temporal Table JOIN</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:30:01" itemprop="dateModified" datetime="2019-07-07T20:30:01+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="什么是Temporal-Table"><a href="#什么是Temporal-Table" class="headerlink" title="什么是Temporal Table"></a>什么是Temporal Table</h1><p>在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。<br>在<a href="https://sigmodrecord.org/publications/sigmodRecord/1209/pdfs/07.industry.kulkarni.pdf" target="_blank" rel="noopener">ANSI-SQL 2011</a> 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/D05B8A19-6D97-4809-8723-B4B95FE075A7.png" alt><br>上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。</p>
<h1 id="ANSI-SQL-2011-Temporal-Table示例"><a href="#ANSI-SQL-2011-Temporal-Table示例" class="headerlink" title="ANSI-SQL 2011 Temporal Table示例"></a>ANSI-SQL 2011 Temporal Table示例</h1><p>我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：</p>
<ul>
<li><p>DDL 示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Emp</span><br><span class="line">ENo INTEGER,</span><br><span class="line">Sys_Start TIMESTAMP(12) GENERATED</span><br><span class="line">ALWAYS AS ROW Start,</span><br><span class="line">Sys_end TIMESTAMP(12) GENERATED</span><br><span class="line">ALWAYS AS ROW END,</span><br><span class="line">EName VARCHAR(30),</span><br><span class="line">PERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)</span><br><span class="line">) WITH SYSTEM VERSIONING</span><br></pre></td></tr></table></figure>
</li>
<li><p>DML 示例</p>
</li>
</ul>
<ol>
<li>INSERT<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO Emp (ENo, EName) VALUES (22217, &apos;Joe&apos;)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png" alt><br><strong>说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。</strong></p>
<ol start="2">
<li>UPDATE</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE Emp SET EName = &apos;Tom&apos; WHERE ENo = 22217</span><br></pre></td></tr></table></figure>

<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/364BB800-5D85-411A-A4E5-A9E4FAD4A8D2.png" alt></p>
<p><strong>说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值”Joe”的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值”Tom”生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。</strong></p>
<ol start="3">
<li>DELETE (假设执行DELETE之前的表内容如下)<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/BF868DDE-D746-4529-878D-91F1A0D7486F.png" alt><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE FROM Emp WHERE ENo = 22217</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/172A63B1-C30D-43EA-B04E-B9BE8706838B.png" alt></p>
<p><strong>说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。</strong></p>
<ol start="4">
<li>SELECT<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT ENo,EName,Sys_Start,Sys_End FROM Emp </span><br><span class="line">FOR SYSTEM_TIME AS OF TIMESTAMP &apos;2011-01-02 00:00:00&apos;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。</strong></p>
<h1 id="SQLServer-Temporal-Table-示例"><a href="#SQLServer-Temporal-Table-示例" class="headerlink" title="SQLServer Temporal Table 示例"></a>SQLServer Temporal Table 示例</h1><h2 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Department</span><br><span class="line">(</span><br><span class="line">DeptID int NOT NULL PRIMARY KEY CLUSTERED</span><br><span class="line">, DeptName varchar(50) NOT NULL</span><br><span class="line">, ManagerID INT NULL</span><br><span class="line">, ParentDeptID int NULL</span><br><span class="line">, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL</span><br><span class="line">, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL</span><br><span class="line">, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)</span><br><span class="line">)</span><br><span class="line">WITH (SYSTEM_VERSIONING = ON);</span><br></pre></td></tr></table></figure>

<p>执行上面的语句，在数据库会创建当前表和历史表，如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/6EAC4329-C402-4497-BAE2-7E7B778E613B.png" alt><br>Department 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：<code>SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。</code></p>
<h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><ul>
<li>INSERT - 插入列不包含SysStartTime和SysEndTime列<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])</span><br><span class="line">VALUES(10, &apos;Marketing&apos;, 101, 1);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>执行之后我们分别查询当前表和历史表，如下图：<img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/3D6C354F-BC2C-40E4-95AE-7303B47E3BDE.png" alt><br>我们第一条INSERT语句数据值的有效时间是操作那一刻<code>2018-06-06 05:50:20.7913985</code> 到永远 <code>9999-12-31 23:59:59.9999999</code>，但这时刻历史表还没有任何信息。我们接下来进行更新操作。</p>
<ul>
<li>UPDATE<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：<img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/75AC8012-59C7-4105-AE66-4453384E762B.png" alt><br>注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10</span><br></pre></td></tr></table></figure>

<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/274B8B4C-28DD-4B34-9AFE-DDACDBB05621.png" alt><br>到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。</p>
<ul>
<li>SELECT <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]</span><br><span class="line">FROM [dbo].[Department]</span><br><span class="line">FOR SYSTEM_TIME AS OF &apos;2018-06-06 05:50:21.0000000&apos; ;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/A94E2FAD-F04A-4691-8C34-3C697FF81476.png" alt><br>SELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 <code>SysStartTime &lt;= &#39;2018-06-06 05:50:21.0000000&#39; and SysEndTime &gt; &#39;2018-06-06 05:50:21.0000000&#39;</code> 。</p>
<h1 id="Apache-Flink-Temporal-Table"><a href="#Apache-Flink-Temporal-Table" class="headerlink" title="Apache Flink Temporal Table"></a>Apache Flink Temporal Table</h1><p>我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用<code>FOR SYSTEM_TIME AS OF</code>的语法，目前Apache Flink中使用 <code>LATERAL TABLE(TemporalTableFunction)</code>的语法。这一点后续需要推动社区进行改进。</p>
<h2 id="为啥需要-Temporal-Table"><a href="#为啥需要-Temporal-Table" class="headerlink" title="为啥需要 Temporal Table"></a>为啥需要 Temporal Table</h2><p>我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下：</p>
<table>
<thead>
<tr>
<th>rowtime(ts)</th>
<th>currency(pk)</th>
<th>rate</th>
</tr>
</thead>
<tbody><tr>
<td>09:00:00</td>
<td>US Dollar</td>
<td>102</td>
</tr>
<tr>
<td>09:00:00</td>
<td>Euro</td>
<td>114</td>
</tr>
<tr>
<td>09:00:00</td>
<td>Yen</td>
<td>1</td>
</tr>
<tr>
<td>10:45:00</td>
<td>Euro</td>
<td>116</td>
</tr>
<tr>
<td>11:15:00</td>
<td>Euro</td>
<td>119</td>
</tr>
<tr>
<td>11:49:00</td>
<td>Pounds</td>
<td>108</td>
</tr>
</tbody></table>
<p>RatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。</p>
<p>假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM RatesHistory AS r</span><br><span class="line">WHERE r.rowtime = (</span><br><span class="line">  SELECT MAX(rowtime)</span><br><span class="line">  FROM RatesHistory AS r2</span><br><span class="line">  WHERE r2.currency = r.currency</span><br><span class="line">  AND r2.rowtime &lt;= &apos;10:58&apos;);</span><br></pre></td></tr></table></figure>

<p>相应Flink代码如下：</p>
<ul>
<li><p>定义数据源-genRatesHistorySource </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class="line">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class="line">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class="line">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class="line">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class="line">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class="line">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line"> writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.STRING,Types.STRING,Types.STRING</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;,&quot;,</span><br><span class="line">      rowDelim = &quot;$&quot;,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  def writeToTempFile(</span><br><span class="line">    contents: String,</span><br><span class="line">    filePrefix: String,</span><br><span class="line">    fileSuffix: String,</span><br><span class="line">    charset: String = &quot;UTF-8&quot;): String = &#123;</span><br><span class="line">    val tempFile = File.createTempFile(filePrefix, fileSuffix)</span><br><span class="line">    val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset)</span><br><span class="line">    tmpWriter.write(contents)</span><br><span class="line">    tmpWriter.close()</span><br><span class="line">    tempFile.getAbsolutePath</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>主程序代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // Streaming 环境</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    //方便我们查出输出数据</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    val sourceTableName = &quot;RatesHistory&quot;</span><br><span class="line">    // 创建CSV source数据结构</span><br><span class="line">    val tableSource = CsvTableSourceUtils.genRatesHistorySource</span><br><span class="line">    // 注册source</span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line"></span><br><span class="line">    // 注册retract sink</span><br><span class="line">    val sinkTableName = &quot;retractSink&quot;</span><br><span class="line">    val fieldNames = Array(&quot;rowtime&quot;, &quot;currency&quot;, &quot;rate&quot;)</span><br><span class="line">    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTableSink(</span><br><span class="line">      sinkTableName,</span><br><span class="line">      fieldNames,</span><br><span class="line">      fieldTypes,</span><br><span class="line">      new MemoryRetractSink)</span><br><span class="line"></span><br><span class="line">    val SQL =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |SELECT *</span><br><span class="line">        |FROM RatesHistory AS r</span><br><span class="line">        |WHERE r.rowtime = (</span><br><span class="line">        |  SELECT MAX(rowtime)</span><br><span class="line">        |  FROM RatesHistory AS r2</span><br><span class="line">        |  WHERE r2.currency = r.currency</span><br><span class="line">        |  AND r2.rowtime &lt;= &apos;10:58:00&apos;  )</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    // 执行查询</span><br><span class="line">    val result = tEnv.SQLQuery(SQL)</span><br><span class="line"></span><br><span class="line">    // 将结果插入sink</span><br><span class="line">    result.insertInto(sinkTableName)</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行结果如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/82C7A5DC-7E84-4965-9A0A-4DD12615B24E.png" alt><br>结果表格化一下：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>rowtime(ts)</th>
<th>currency(pk)</th>
<th>rate</th>
</tr>
</thead>
<tbody><tr>
<td>09:00:00</td>
<td>US Dollar</td>
<td>102</td>
</tr>
<tr>
<td>09:00:00</td>
<td>Yen</td>
<td>1</td>
</tr>
<tr>
<td>10:45:00</td>
<td>Euro</td>
<td>116</td>
</tr>
</tbody></table>
<p>Temporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append  Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。</p>
<p>在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。</p>
<h2 id="如何定义Temporal-Table"><a href="#如何定义Temporal-Table" class="headerlink" title="如何定义Temporal Table"></a>如何定义Temporal Table</h2><p>在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。</p>
<ul>
<li><p>内部TemporalTableFunction定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class TemporalTableFunction private(</span><br><span class="line">    @transient private val underlyingHistoryTable: Table,</span><br><span class="line">    // 时间属性，相当于版本信息</span><br><span class="line">    private val timeAttribute: Expression,</span><br><span class="line">    // 主键定义</span><br><span class="line">    private val primaryKey: String,</span><br><span class="line">    private val resultType: RowTypeInfo)</span><br><span class="line">  extends TableFunction[Row] &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>用户创建TemporalTableFunction方式<br>在<code>Table</code>中添加了<code>createTemporalTableFunction</code>方法，该方法需要传入时间属性和主键，接口定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// Creates TemporalTableFunction backed up by this table as a history table.</span><br><span class="line"></span><br><span class="line">def createTemporalTableFunction(</span><br><span class="line">      timeAttribute: Expression,</span><br><span class="line">      primaryKey: Expression): TemporalTableFunction = &#123;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val tab = ...</span><br><span class="line">val temporalTableFunction = tab.createTemporalTableFunction(&apos;time, &apos;pk)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="案例代码"><a href="#案例代码" class="headerlink" title="案例代码"></a>案例代码</h2><ul>
<li>需求描述<br>假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。</li>
<li>Orders 数据</li>
</ul>
<table>
<thead>
<tr>
<th>amount</th>
<th>currency</th>
<th>order_time</th>
</tr>
</thead>
<tbody><tr>
<td>2</td>
<td>Euro</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>US Dollar</td>
<td>3</td>
</tr>
<tr>
<td>50</td>
<td>Yen</td>
<td>4</td>
</tr>
<tr>
<td>3</td>
<td>Euro</td>
<td>5</td>
</tr>
</tbody></table>
<ul>
<li>Rates 数据</li>
</ul>
<table>
<thead>
<tr>
<th>currency</th>
<th>rate</th>
<th>rate_time</th>
</tr>
</thead>
<tbody><tr>
<td>US Dollar</td>
<td>102</td>
<td>1</td>
</tr>
<tr>
<td>Euro</td>
<td>114</td>
<td>1</td>
</tr>
<tr>
<td>Yen</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Euro</td>
<td>116</td>
<td>5</td>
</tr>
<tr>
<td>Euro</td>
<td>117</td>
<td>7</td>
</tr>
</tbody></table>
<ul>
<li><p>统计需求对应的SQL</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT o.currency, o.amount, r.rate</span><br><span class="line">  o.amount * r.rate AS yen_amount</span><br><span class="line">FROM</span><br><span class="line">  Orders AS o,</span><br><span class="line">  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class="line">WHERE r.currency = o.currency</span><br></pre></td></tr></table></figure>
</li>
<li><p>预期结果</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>currency</th>
<th>amount</th>
<th>rate</th>
<th>yen_amount</th>
</tr>
</thead>
<tbody><tr>
<td>US Dollar</td>
<td>1</td>
<td>102</td>
<td>102</td>
</tr>
<tr>
<td>Yen</td>
<td>50</td>
<td>1</td>
<td>50</td>
</tr>
<tr>
<td>Euro</td>
<td>2</td>
<td>114</td>
<td>228</td>
</tr>
<tr>
<td>Euro</td>
<td>3</td>
<td>116</td>
<td>348</td>
</tr>
</tbody></table>
<h3 id="Without-connnector-实现代码"><a href="#Without-connnector-实现代码" class="headerlink" title="Without connnector 实现代码"></a>Without connnector 实现代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">object TemporalTableJoinTest &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line">// 设置时间类型是 event-time  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    // 构造订单数据</span><br><span class="line">    val ordersData = new mutable.MutableList[(Long, String, Timestamp)]</span><br><span class="line">    ordersData.+=((2L, &quot;Euro&quot;, new Timestamp(2L)))</span><br><span class="line">    ordersData.+=((1L, &quot;US Dollar&quot;, new Timestamp(3L)))</span><br><span class="line">    ordersData.+=((50L, &quot;Yen&quot;, new Timestamp(4L)))</span><br><span class="line">    ordersData.+=((3L, &quot;Euro&quot;, new Timestamp(5L)))</span><br><span class="line"></span><br><span class="line">    //构造汇率数据</span><br><span class="line">    val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)]</span><br><span class="line">    ratesHistoryData.+=((&quot;US Dollar&quot;, 102L, new Timestamp(1L)))</span><br><span class="line">    ratesHistoryData.+=((&quot;Euro&quot;, 114L, new Timestamp(1L)))</span><br><span class="line">    ratesHistoryData.+=((&quot;Yen&quot;, 1L, new Timestamp(1L)))</span><br><span class="line">    ratesHistoryData.+=((&quot;Euro&quot;, 116L, new Timestamp(5L)))</span><br><span class="line">    ratesHistoryData.+=((&quot;Euro&quot;, 119L, new Timestamp(7L)))</span><br><span class="line"></span><br><span class="line">// 进行订单表 event-time 的提取</span><br><span class="line">    val orders = env</span><br><span class="line">      .fromCollection(ordersData)</span><br><span class="line">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]())</span><br><span class="line">      .toTable(tEnv, &apos;amount, &apos;currency, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">// 进行汇率表 event-time 的提取</span><br><span class="line">    val ratesHistory = env</span><br><span class="line">      .fromCollection(ratesHistoryData)</span><br><span class="line">      .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]())</span><br><span class="line">      .toTable(tEnv, &apos;currency, &apos;rate, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">// 注册订单表和汇率表</span><br><span class="line">    tEnv.registerTable(&quot;Orders&quot;, orders)</span><br><span class="line">    tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory)</span><br><span class="line">    val tab = tEnv.scan(&quot;RatesHistory&quot;);</span><br><span class="line">// 创建TemporalTableFunction</span><br><span class="line">    val temporalTableFunction = tab.createTemporalTableFunction(&apos;rowtime, &apos;currency)</span><br><span class="line">//注册TemporalTableFunction</span><br><span class="line">tEnv.registerFunction(&quot;Rates&quot;,temporalTableFunction)</span><br><span class="line"></span><br><span class="line">    val SQLQuery =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |SELECT o.currency, o.amount, r.rate,</span><br><span class="line">        |  o.amount * r.rate AS yen_amount</span><br><span class="line">        |FROM</span><br><span class="line">        |  Orders AS o,</span><br><span class="line">        |  LATERAL TABLE (Rates(o.rowtime)) AS r</span><br><span class="line">        |WHERE r.currency = o.currency</span><br><span class="line">        |&quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.SQLQuery(SQLQuery))</span><br><span class="line"></span><br><span class="line">    val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row]</span><br><span class="line">    // 打印查询结果</span><br><span class="line">    result.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的<code>TimeCharacteristic.EventTime</code> 模式，需要调用<code>assignTimestampsAndWatermarks</code>方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) -  Watermark》。 在本示例中提取EventTime的完整代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import java.SQL.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"></span><br><span class="line">class OrderTimestampExtractor[T1, T2]</span><br><span class="line">  extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123;</span><br><span class="line">  override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123;</span><br><span class="line">    element._3.getTime</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查看运行结果:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/44FDFE22-3B21-4514-8E91-E339F9188E4E.png" alt></p>
<h3 id="With-CSVConnector-实现代码"><a href="#With-CSVConnector-实现代码" class="headerlink" title="With CSVConnector 实现代码"></a>With CSVConnector 实现代码</h3><p>在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。</p>
<ul>
<li><p>genEventRatesHistorySource</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;ts#currency#rate&quot;,</span><br><span class="line">      &quot;1#US Dollar#102&quot;,</span><br><span class="line">      &quot;1#Euro#114&quot;,</span><br><span class="line">      &quot;1#Yen#1&quot;,</span><br><span class="line">      &quot;3#Euro#116&quot;,</span><br><span class="line">      &quot;5#Euro#119&quot;,</span><br><span class="line">      &quot;7#Pounds#108&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.LONG,Types.STRING,Types.LONG</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;#&quot;,</span><br><span class="line">      rowDelim = CommonUtils.line,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>genRatesOrderSource</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;ts#currency#amount&quot;,</span><br><span class="line">      &quot;2#Euro#10&quot;,</span><br><span class="line">      &quot;4#Euro#10&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.LONG,Types.STRING,Types.LONG</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;#&quot;,</span><br><span class="line">      rowDelim = CommonUtils.line,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>主程序代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"> * or more contributor license agreements.  See the NOTICE file</span><br><span class="line"> * distributed with this work for additional information</span><br><span class="line"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"> * to you under the Apache License, Version 2.0 (the</span><br><span class="line"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"> * with the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">package org.apache.flink.book.connectors</span><br><span class="line"></span><br><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.typeinfo.&#123;TypeInformation, Types&#125;</span><br><span class="line">import org.apache.flink.book.utils.&#123;CommonUtils, FileUtils&#125;</span><br><span class="line">import org.apache.flink.table.sinks.&#123;CsvTableSink, TableSink&#125;</span><br><span class="line">import org.apache.flink.table.sources.CsvTableSource</span><br><span class="line">import org.apache.flink.types.Row</span><br><span class="line"></span><br><span class="line">object CsvTableSourceUtils &#123;</span><br><span class="line"></span><br><span class="line">  def genWordCountSource: CsvTableSource = &#123;</span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;words&quot;,</span><br><span class="line">      &quot;Hello Flink&quot;,</span><br><span class="line">      &quot;Hi, Apache Flink&quot;,</span><br><span class="line">      &quot;Apache FlinkBook&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;words&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.STRING</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;#&quot;,</span><br><span class="line">      rowDelim = &quot;$&quot;,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def genRatesHistorySource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;rowtime ,currency   ,rate&quot;,</span><br><span class="line">    &quot;09:00:00   ,US Dollar  , 102&quot;,</span><br><span class="line">    &quot;09:00:00   ,Euro       , 114&quot;,</span><br><span class="line">    &quot;09:00:00  ,Yen        ,   1&quot;,</span><br><span class="line">    &quot;10:45:00   ,Euro       , 116&quot;,</span><br><span class="line">    &quot;11:15:00   ,Euro       , 119&quot;,</span><br><span class="line">    &quot;11:49:00   ,Pounds     , 108&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.STRING,Types.STRING,Types.STRING</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;,&quot;,</span><br><span class="line">      rowDelim = &quot;$&quot;,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def genEventRatesHistorySource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;ts#currency#rate&quot;,</span><br><span class="line">      &quot;1#US Dollar#102&quot;,</span><br><span class="line">      &quot;1#Euro#114&quot;,</span><br><span class="line">      &quot;1#Yen#1&quot;,</span><br><span class="line">      &quot;3#Euro#116&quot;,</span><br><span class="line">      &quot;5#Euro#119&quot;,</span><br><span class="line">      &quot;7#Pounds#108&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.LONG,Types.STRING,Types.LONG</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;#&quot;,</span><br><span class="line">      rowDelim = CommonUtils.line,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def genRatesOrderSource: CsvTableSource = &#123;</span><br><span class="line"></span><br><span class="line">    val csvRecords = Seq(</span><br><span class="line">      &quot;ts#currency#amount&quot;,</span><br><span class="line">      &quot;2#Euro#10&quot;,</span><br><span class="line">      &quot;4#Euro#10&quot;</span><br><span class="line">    )</span><br><span class="line">    // 测试数据写入临时文件</span><br><span class="line">    val tempFilePath =</span><br><span class="line">      FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;)</span><br><span class="line"></span><br><span class="line">    // 创建Source connector</span><br><span class="line">    new CsvTableSource(</span><br><span class="line">      tempFilePath,</span><br><span class="line">      Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;),</span><br><span class="line">      Array(</span><br><span class="line">        Types.LONG,Types.STRING,Types.LONG</span><br><span class="line">      ),</span><br><span class="line">      fieldDelim = &quot;#&quot;,</span><br><span class="line">      rowDelim = CommonUtils.line,</span><br><span class="line">      ignoreFirstLine = true,</span><br><span class="line">      ignoreComments = &quot;%&quot;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * Example:</span><br><span class="line">    * genCsvSink(</span><br><span class="line">    *   Array[String](&quot;word&quot;, &quot;count&quot;),</span><br><span class="line">    *   Array[TypeInformation[_] ](Types.STRING, Types.LONG))</span><br><span class="line">    */</span><br><span class="line">  def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;)</span><br><span class="line">    if (tempFile.exists()) &#123;</span><br><span class="line">      tempFile.delete()</span><br><span class="line">    &#125;</span><br><span class="line">    new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>运行结果如下 ：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/A05E658F-C511-45E2-870F-5E3227A94570.png" alt></p>
<h2 id="内部实现原理"><a href="#内部实现原理" class="headerlink" title="内部实现原理"></a>内部实现原理</h2><p>我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示:</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/07A663B1-7CCF-4A80-9F07-2C1E87A9C1B6.png" alt></p>
<h1 id="Temporal-Table-JOIN-vs-双流JOIN-vs-Lateral-JOIN"><a href="#Temporal-Table-JOIN-vs-双流JOIN-vs-Lateral-JOIN" class="headerlink" title="Temporal Table JOIN vs 双流JOIN vs Lateral JOIN"></a>Temporal Table JOIN vs 双流JOIN vs Lateral JOIN</h1><p>在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？</p>
<ul>
<li>双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。 </li>
<li>LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。</li>
<li>Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table  或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点<br>单流驱动，Temporal Table 是被动查询。</li>
</ul>
<h2 id="Temporal-Table-JOIN-vs-LATERAL-JOIN"><a href="#Temporal-Table-JOIN-vs-LATERAL-JOIN" class="headerlink" title="Temporal Table JOIN vs  LATERAL JOIN"></a>Temporal Table JOIN vs  LATERAL JOIN</h2><p>从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。</p>
<h2 id="Temporal-Table-JOIN-vs-双流-JOIN"><a href="#Temporal-Table-JOIN-vs-双流-JOIN" class="headerlink" title="Temporal Table JOIN vs 双流 JOIN"></a>Temporal Table JOIN vs 双流 JOIN</h2><p>Temporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。</p>
<h1 id="Temporal-Table-JOIN改进"><a href="#Temporal-Table-JOIN改进" class="headerlink" title="Temporal Table JOIN改进"></a>Temporal Table JOIN改进</h1><p>个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的<code>FOR SYSTEM_TIME AS OF</code>标准语法。改进后的处理逻辑示意图:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - Temporal Table JOIN/A936792A-9239-4D86-BA0C-4EE85DFDDA7D.png" alt><br>其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。</p>
<h1 id="关于点赞和评论"><a href="#关于点赞和评论" class="headerlink" title="关于点赞和评论"></a>关于点赞和评论</h1><p>本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - 持续查询(Continuous Queries)</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 20:00:16" itemprop="dateModified" datetime="2019-07-07T20:00:16+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。</p>
<h1 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h1><p>在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/266A687D-8E31-4266-B52F-200B75244DDB.png" alt></p>
<p>如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。</p>
<p>那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？</p>
<h1 id="静态查询"><a href="#静态查询" class="headerlink" title="静态查询"></a>静态查询</h1><p>传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from flink_tab;</span><br><span class="line">+----+------+--------+</span><br><span class="line">| id | user | clicks |</span><br><span class="line">+----+------+--------+</span><br><span class="line">|  1 | Mary |      1 |</span><br><span class="line">+----+------+--------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。</p>
<h1 id="持续查询"><a href="#持续查询" class="headerlink" title="持续查询"></a>持续查询</h1><p>什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着…</p>
<h1 id="静态-持续查询特点"><a href="#静态-持续查询特点" class="headerlink" title="静态/持续查询特点"></a>静态/持续查询特点</h1><p>静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：<br>| 查询类型 | 计算次数 |计算结果  |<br>| — | — | — |<br>| 静态查询 | 1 | 最终结果 |<br>| 持续查询 | 无限 | 不断更新 |</p>
<h1 id="静态-持续查询关系"><a href="#静态-持续查询关系" class="headerlink" title="静态/持续查询关系"></a>静态/持续查询关系</h1><p>接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Bob&apos;, 1);</span><br><span class="line">Query OK, 1 row affected (0.08 sec)</span><br><span class="line"></span><br><span class="line">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Mary&apos;;</span><br><span class="line">Query OK, 1 row affected (0.06 sec)</span><br></pre></td></tr></table></figure>

<p>这时候我们再进行查询 <code>select * from flink_tab</code> ，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MySQL&gt; select * from flink_tab;</span><br><span class="line">+----+------+--------+</span><br><span class="line">| id | user | clicks |</span><br><span class="line">+----+------+--------+</span><br><span class="line">|  1 | Mary |      2 |</span><br><span class="line">|  2 | Bob  |      1 |</span><br><span class="line">+----+------+--------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>那么我们看见，相同的查询SQL(<code>select * from flink_tab</code>)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/1401BEB3-37C3-4203-B48E-AABF6B08DB65.png" alt></p>
<p>假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Llz&apos;, 1);</span><br><span class="line">Query OK, 1 row affected (0.08 sec)</span><br><span class="line"></span><br><span class="line">MySQL&gt; update flink_tab set clicks=2 where user=&apos;Bob&apos;;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL&gt; update flink_tab set clicks=3 where user=&apos;Mary&apos;;</span><br><span class="line">Query OK, 1 row affected (0.05 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br></pre></td></tr></table></figure>

<p>到现在我们不难想象，上面图内容的核心要点如下：</p>
<ul>
<li>时间 </li>
<li>表数据变化</li>
<li>触发计算 </li>
<li>计算结果更新</li>
</ul>
<p>接下来我们利用传统数据库现有的机制模拟一下持续查询…</p>
<h2 id="无PK的-Append-only-场景"><a href="#无PK的-Append-only-场景" class="headerlink" title="无PK的 Append only 场景"></a>无PK的 Append only 场景</h2><p>接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下：</p>
<table>
<thead>
<tr>
<th>timestamp</th>
<th>user</th>
<th>clicks</th>
</tr>
</thead>
<tbody><tr>
<td>1525099013</td>
<td>Mary</td>
<td>1</td>
</tr>
<tr>
<td>1525099026</td>
<td>Bob</td>
<td>1</td>
</tr>
<tr>
<td>1525099035</td>
<td>Mary</td>
<td>2</td>
</tr>
<tr>
<td>1525099047</td>
<td>Llz</td>
<td>1</td>
</tr>
<tr>
<td>1525099056</td>
<td>Bob</td>
<td>2</td>
</tr>
<tr>
<td>1525099065</td>
<td>Mary</td>
<td>3</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中</span><br><span class="line"> DELIMITER ;;</span><br><span class="line">create trigger flink_tab_ts_trigger_insert after insert</span><br><span class="line">on flink_tab_ts for each row</span><br><span class="line">  begin</span><br><span class="line">       select ts, user, clicks from flink_tab_ts into OUTFILE &apos;/Users/jincheng.sunjc/testdir/atas/trigger.sql&apos;;</span><br><span class="line">  end ;;</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<p>上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">| Variable_name            | Value |</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">| require_secure_transport | OFF   |</span><br><span class="line">| secure_file_priv         | NULL  |</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=’’打开写文件限制；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MySQL&gt; show variables like &apos;%secure%&apos;;</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">| Variable_name            | Value |</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">| require_secure_transport | OFF   |</span><br><span class="line">| secure_file_priv         |       |</span><br><span class="line">+--------------------------+-------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>下面我们对flink_tab_ts进行INSERT操作：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/166F2FE8-93FB-4966-87B7-F6C98DFF4915.png" alt></p>
<p>我们再来看看6次trigger 查询计算的结果：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/146851FF-D705-4B2D-8C3B-77528BB8DAB5.png" alt></p>
<p>大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。</p>
<h2 id="有PK的Update场景"><a href="#有PK的Update场景" class="headerlink" title="有PK的Update场景"></a>有PK的Update场景</h2><p>我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">drop table flink_tab;</span><br><span class="line">create table flink_tab(</span><br><span class="line">    user VARCHAR(100) NOT NULL,</span><br><span class="line">    clicks INT NOT NULL,</span><br><span class="line">    PRIMARY KEY (user)</span><br><span class="line"> );</span><br><span class="line"></span><br><span class="line"> DELIMITER ;;</span><br><span class="line">create trigger flink_tab_trigger_insert after insert</span><br><span class="line">on flink_tab for each row</span><br><span class="line">  begin</span><br><span class="line">       select user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class="line">  end ;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">DELIMITER ;;</span><br><span class="line">create trigger flink_tab_trigger_ after update</span><br><span class="line">on flink_tab for each row</span><br><span class="line">  begin</span><br><span class="line">        select ts, user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;;</span><br><span class="line">  end ;;</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<p> 同样我做如下6次DML操作，Trigger 6次查询计算：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/46EA19BC-C44F-4D3A-860F-DE343E1B04EB.png" alt></p>
<p>在来看看这次的结果与append only 有什么不同？</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/E095C802-8CCF-4922-837C-6AAE54A335D8.png" alt></p>
<p>我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)!</p>
<h2 id="关系定义"><a href="#关系定义" class="headerlink" title="关系定义"></a>关系定义</h2><p>上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(<code>select * from flink_tab_ts</code>) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。</p>
<p>那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点),  在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。</p>
<p><strong>特别说明：</strong> 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录!</p>
<h1 id="Apache-Flink-如何做到持续查询"><a href="#Apache-Flink-如何做到持续查询" class="headerlink" title="Apache Flink 如何做到持续查询"></a>Apache Flink 如何做到持续查询</h1><h2 id="动态表上面持续查询"><a href="#动态表上面持续查询" class="headerlink" title="动态表上面持续查询"></a>动态表上面持续查询</h2><p>在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/43A59ED3-C317-4B5D-80F3-CC332933B93B.png" alt></p>
<h2 id="增量计算"><a href="#增量计算" class="headerlink" title="增量计算"></a>增量计算</h2><p>我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/3446A4F4-A131-4AE5-A0C9-B117A65B9E53.png" alt></p>
<p>一个简单的计数和求和查询SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 求订单总数和所有订单的总金额</span><br><span class="line">select count(id) as cnt，sum(amount)as sumAmount from order_tab;</span><br></pre></td></tr></table></figure>

<p>这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BC8FD47E-F124-4E2B-AE3A-4B99E5C13EF8.png" alt></p>
<p>如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result(n) = calculation(result(n-1), n)。</span><br></pre></td></tr></table></figure>

<h3 id="无PK的Append-Only-场景"><a href="#无PK的Append-Only-场景" class="headerlink" title="无PK的Append Only 场景"></a>无PK的Append Only 场景</h3><p>在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/78D31D91-3693-44C8-A7E4-71D6798A78AE.png" alt></p>
<h3 id="有PK的Update-场景"><a href="#有PK的Update-场景" class="headerlink" title="有PK的Update 场景"></a>有PK的Update 场景</h3><p>现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B2EEEC59-21F9-41E1-A306-8338A7A72A64.png" alt></p>
<p>查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> CREATE TABLE order_tab(</span><br><span class="line">   id BIGINT,</span><br><span class="line">   region VARCHAR</span><br><span class="line"> ) </span><br><span class="line"></span><br><span class="line">CREATE TABLE region_count_sink(</span><br><span class="line">   order_cnt BIGINT, </span><br><span class="line">   region_cnt BIGINT,</span><br><span class="line">   PRIMARY KEY(order_cnt) -- 主键</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line">-- 按地区分组计算每个地区的订单数量</span><br><span class="line">CREATE VIEW order_count_view AS</span><br><span class="line">    SELECT</span><br><span class="line">        region, count(id) AS order_cnt</span><br><span class="line">    FROM  order_tab </span><br><span class="line">    GROUP BY region;</span><br><span class="line"></span><br><span class="line">-- 按订单数量分组统计具有相同订单数量的地区数量</span><br><span class="line">INSERT INTO region_count_sink </span><br><span class="line">    SELECT </span><br><span class="line">        order_cnt,</span><br><span class="line">        count(region) as region_cnt</span><br><span class="line">    FROM order_count_view </span><br><span class="line">    GROUP BY order_cnt;</span><br></pre></td></tr></table></figure>

<p>上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，<a href="https://data.aliyun.com/product/sc" target="_blank" rel="noopener">了解更多</a>)：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A415CDF6-5FE3-442B-AFDA-70CB9CEC37A9.png" alt></p>
<p>上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。</p>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><ul>
<li><p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code><br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/17FB1F6A-C813-43D4-AD6D-31FC03B84533.png" alt></p>
</li>
<li><p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code><br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/A6CFC228-420F-4B1E-9F3D-1A286D604F2F.png" alt></p>
</li>
</ul>
<p>按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？</p>
<p>其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？</p>
<h3 id="正确处理"><a href="#正确处理" class="headerlink" title="正确处理"></a>正确处理</h3><ul>
<li><p>第一层分析：<code>SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;</code><br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/0B59286E-D2BD-4DBC-9FEA-DF390FC1CC9B.png" alt></p>
</li>
<li><p>第二层分析：<code>SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt;</code><br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/5BB27EDB-098E-4CC0-B9D6-7514EFE7DE3F.png" alt></p>
</li>
</ul>
<p>上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def accumulate(acc: CountAccumulator): Unit = &#123;</span><br><span class="line">    acc.f0 += 1L // acc.f0 存储记数</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def retract(acc: CountAccumulator, value: Any): Unit = &#123;</span><br><span class="line">    if (value != null) &#123;</span><br><span class="line">      acc.f0 -= 1L //acc.f0 存储记数</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Apache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。</p>
<h1 id="Apache-Flink-Connector-类型"><a href="#Apache-Flink-Connector-类型" class="headerlink" title="Apache Flink Connector 类型"></a>Apache Flink Connector 类型</h1><p>本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？</p>
<h2 id="Apache-Flink-Source"><a href="#Apache-Flink-Source" class="headerlink" title="Apache Flink Source"></a>Apache Flink Source</h2><p>现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">SQL：</span><br><span class="line"></span><br><span class="line">CREATE TABLE inventory_tab(</span><br><span class="line">   product_id VARCHAR,</span><br><span class="line">   product_count BIGINT</span><br><span class="line">); </span><br><span class="line"></span><br><span class="line">CREATE TABLE sales_tab(</span><br><span class="line">   product_id VARCHAR,</span><br><span class="line">   sales_count BIGINT</span><br><span class="line">  ) ;</span><br><span class="line"></span><br><span class="line">CREATE TABLE join_sink(</span><br><span class="line">   product_id VARCHAR, </span><br><span class="line">   product_count BIGINT,</span><br><span class="line">   sales_count BIGINT,</span><br><span class="line">   PRIMARY KEY(product_id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE VIEW join_view AS</span><br><span class="line">    SELECT</span><br><span class="line">        l.product_id, </span><br><span class="line">        l.product_count,</span><br><span class="line">        r.sales_count</span><br><span class="line">    FROM inventory_tab l </span><br><span class="line">        JOIN  sales_tab r </span><br><span class="line">        ON l.product_id = r.product_id;</span><br><span class="line"></span><br><span class="line">INSERT INTO join_sink </span><br><span class="line">  SELECT </span><br><span class="line">      product_id, </span><br><span class="line">      product_count,</span><br><span class="line">      sales_count</span><br><span class="line">  FROM join_view ;</span><br></pre></td></tr></table></figure>


<p>代码结构图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/BE66E18F-DE01-42EC-9336-5F6172B22636.png" alt></p>
<p>实现示意图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/B5F9DD64-3574-480A-A409-BEE96561091B.png" alt></p>
<p>上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。</p>
<p>那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">SQL:</span><br><span class="line"></span><br><span class="line">CREATE TABLE inventory_tab(</span><br><span class="line">   product_id VARCHAR,</span><br><span class="line">   product_count BIGINT</span><br><span class="line">  ) </span><br><span class="line"></span><br><span class="line"> CREATE TABLE sales_tab(</span><br><span class="line">   product_id VARCHAR,</span><br><span class="line">   sales_count BIGINT</span><br><span class="line">  )</span><br><span class="line">CREATE VIEW inventory_view AS</span><br><span class="line">    SELECT </span><br><span class="line">    product_id,</span><br><span class="line">    LAST_VALUE(product_count) AS product_count</span><br><span class="line">    FROM inventory_tab</span><br><span class="line">    GROUP BY product_id;</span><br><span class="line"></span><br><span class="line">CREATE VIEW sales_view AS</span><br><span class="line">    SELECT </span><br><span class="line">    product_id,</span><br><span class="line">    LAST_VALUE(sales_count) AS sales_count</span><br><span class="line">    FROM sales_tab</span><br><span class="line">    GROUP BY product_id;</span><br><span class="line"></span><br><span class="line">CREATE TABLE join_sink(</span><br><span class="line">   product_id VARCHAR, </span><br><span class="line">   product_count BIGINT,</span><br><span class="line">   sales_count BIGINT,</span><br><span class="line">   PRIMARY KEY(product_id)</span><br><span class="line">)WITH (</span><br><span class="line">    type = &apos;print&apos;</span><br><span class="line">) ;</span><br><span class="line"></span><br><span class="line">CREATE VIEW join_view AS</span><br><span class="line">    SELECT</span><br><span class="line">        l.product_id, </span><br><span class="line">        l.product_count,</span><br><span class="line">        r.sales_count</span><br><span class="line">    FROM inventory_view l </span><br><span class="line">        JOIN  sales_view r </span><br><span class="line">        ON l.product_id = r.product_id;</span><br><span class="line"></span><br><span class="line"> INSERT INTO join_sink </span><br><span class="line">  SELECT </span><br><span class="line">      product_id, </span><br><span class="line">      product_count,</span><br><span class="line">      sales_count</span><br><span class="line">  FROM join_view ;</span><br></pre></td></tr></table></figure>

<p>代码结构：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/4134FE05-67E4-4625-824D-BBD7B444BFE6.png" alt></p>
<p>示意图:<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 持续查询(Continuous Queries)/D5D7D6E8-83A1-4290-BE79-974C9913DAE5.png" alt></p>
<p>如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。</p>
<p><strong>说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。</strong></p>
<h2 id="Apache-Flink-Sink"><a href="#Apache-Flink-Sink" class="headerlink" title="Apache Flink Sink"></a>Apache Flink Sink</h2><p>在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型:</p>
<ul>
<li>Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句；</li>
<li>Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新；</li>
<li>Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令；</li>
</ul>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://enjoyment.cool/2019/01/01/Apache Flink 漫谈系列 - 概述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jincheng Sun">
      <meta itemprop="description" content="Apache Flink">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pretty Cool Enjoyment">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/01/Apache Flink 漫谈系列 - 概述/" class="post-title-link" itemprop="url">Apache Flink 漫谈系列 - 概述</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-01 18:18:18" itemprop="dateCreated datePublished" datetime="2019-01-01T18:18:18+08:00">2019-01-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-07 19:36:57" itemprop="dateModified" datetime="2019-07-07T19:36:57+08:00">2019-07-07</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/archives/Apache-Flink-漫谈/" itemprop="url" rel="index"><span itemprop="name">Apache Flink 漫谈</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/01/01/Apache Flink 漫谈系列 - 概述/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/01/Apache Flink 漫谈系列 - 概述/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Apache-Flink-的命脉"><a href="#Apache-Flink-的命脉" class="headerlink" title="Apache Flink 的命脉"></a>Apache Flink 的命脉</h1><p>“<strong>命脉</strong>“ 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以”<strong>批是流的特例</strong>“的认知进行系统设计的。</p>
<h1 id="唯快不破"><a href="#唯快不破" class="headerlink" title="唯快不破"></a>唯快不破</h1><p>我们经常听说 “天下武功，唯快不破”，大概意思是说 “任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破”。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是”<strong>快</strong>“,也就是 “<strong>低延时</strong>“。</p>
<p>就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 “<strong>低延时</strong>“ 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 “<strong>快</strong>“呢？根本原因是Apache Flink 设计之初就认为 “<strong>批是流的特例</strong>“，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。</p>
<p>那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是”<strong>流是批的特例</strong>“ 和 “<strong>批是流的特例</strong>“ 两种不同认知产物。</p>
<h2 id="Micro-Batching-模式"><a href="#Micro-Batching-模式" class="headerlink" title="Micro Batching 模式"></a>Micro Batching 模式</h2><p>Micro-Batching 计算模式认为 “<strong>流是批的特例</strong>“， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/0FF418DC-20FF-4D9B-A2C7-681F2954A07F.png" alt></p>
<p>很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。</p>
<h2 id="Native-Streaming-模式"><a href="#Native-Streaming-模式" class="headerlink" title="Native Streaming 模式"></a>Native Streaming 模式</h2><p>Native Streaming 计算模式认为 “”<strong>批是流的特</strong>“, 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180921_7.png" alt></p>
<p>很明显Native Streaming模式占据了流计算领域 “<strong>低延时</strong>“ 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现<br>Native Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。</p>
<h1 id="丰富的部署模式"><a href="#丰富的部署模式" class="headerlink" title="丰富的部署模式"></a>丰富的部署模式</h1><p>Apache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。</p>
<h2 id="Local-模式"><a href="#Local-模式" class="headerlink" title="Local 模式"></a>Local 模式</h2><p>该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html" target="_blank" rel="noopener">参考</a></p>
<h2 id="Cluster模式"><a href="#Cluster模式" class="headerlink" title="Cluster模式"></a>Cluster模式</h2><p>该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/cluster_setup.html" target="_blank" rel="noopener">参考</a> YARN Cluster <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">参考</a><br>这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180921_4.png" alt></p>
<p>其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N&gt;=1)个JM候选,进而解决SPOF问题，示意如下：<br>    <img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180921_6.png" alt></p>
<p>在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。</p>
<h2 id="Cloud-模式"><a href="#Cloud-模式" class="headerlink" title="Cloud 模式"></a>Cloud 模式</h2><p>该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/gce_setup.html" target="_blank" rel="noopener">参考</a>，Amazon的EC2 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html" target="_blank" rel="noopener">参考</a>，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。</p>
<h1 id="完善的容错机制"><a href="#完善的容错机制" class="headerlink" title="完善的容错机制"></a>完善的容错机制</h1><h2 id="什么是容错"><a href="#什么是容错" class="headerlink" title="什么是容错"></a>什么是容错</h2><p>容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。</p>
<h2 id="容错的处理模式"><a href="#容错的处理模式" class="headerlink" title="容错的处理模式"></a>容错的处理模式</h2><p>在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够”运行”外，还要求能”正确运行”,也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制：</p>
<ul>
<li><strong>At Most Once</strong>：最多消费一次，这种处理机制会存在数据丢失的可能。</li>
<li><strong>At Least Once</strong>：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。</li>
<li><strong>Exactly Once</strong>：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。</li>
</ul>
<h2 id="Apache-Flink的容错机制"><a href="#Apache-Flink的容错机制" class="headerlink" title="Apache Flink的容错机制"></a>Apache Flink的容错机制</h2><p>Apache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图:</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180922_17.png" alt></p>
<p>目前Apache Flink 支持两种数据容错机制：</p>
<ul>
<li><strong>At Least Once</strong></li>
<li><strong>Exactly Once</strong></li>
</ul>
<p>其中 <strong>Exactly Once</strong> 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 <strong>End-to-End</strong> 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景：</p>
<ul>
<li><p><strong>场景一</strong>：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180922_18.png" alt></p>
</li>
<li><p><strong>场景二</strong>：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/BF1A3338-5B92-4898-BC81-A4A6626D4208.png" alt></p>
</li>
<li><p><strong>场景三</strong>：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/7DD374B0-1901-424C-8683-2AE890B8A5FA.png" alt></p>
</li>
</ul>
<h3 id="系统内部容错"><a href="#系统内部容错" class="headerlink" title="系统内部容错"></a>系统内部容错</h3><p>Apache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 <a href="https://arxiv.org/pdf/1506.08603.pdf" target="_blank" rel="noopener">Lightweight Asynchronous Snapshots for Distributed Dataflows</a> 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="noopener">Determining-Global-States-of-a-Distributed-System Paper</a>。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了<strong>At Least Once</strong> 和 <strong>Exactly Once</strong> 两种容错处理模式。</p>
<p>Apache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/Snip20180922_22.png" alt></p>
<p>这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。</p>
<h3 id="外部Source容错"><a href="#外部Source容错" class="headerlink" title="外部Source容错"></a>外部Source容错</h3><p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。</p>
<h3 id="外部Sink容错"><a href="#外部Sink容错" class="headerlink" title="外部Sink容错"></a>外部Sink容错</h3><p>Apache Flink 要做到 <strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 <strong>exactly once</strong>的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了<strong>End-to-End</strong> 的 <strong>Exactly Once</strong> 语义(重复写入就变成了<strong>At Least Once</strong>了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。</p>
<h1 id="流批统一的计算引擎"><a href="#流批统一的计算引擎" class="headerlink" title="流批统一的计算引擎"></a>流批统一的计算引擎</h1><p>批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？</p>
<h2 id="统一的数据传输层"><a href="#统一的数据传输层" class="headerlink" title="统一的数据传输层"></a>统一的数据传输层</h2><p>开篇我们就介绍Apache Flink 的 “<strong>命脉</strong>“是以”<strong>批是流的特例</strong>“为导向来进行引擎的设计的，系统设计成为 “<strong>Native Streaming</strong>“的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。</p>
<p>Apache Flink 在网络传输层面有两种数据传输模式：</p>
<ul>
<li>PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。</li>
<li>BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。</li>
</ul>
<p>对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。</p>
<h2 id="统一任务调度层"><a href="#统一任务调度层" class="headerlink" title="统一任务调度层"></a>统一任务调度层</h2><p>Apache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。</p>
<h2 id="统一的用户API层"><a href="#统一的用户API层" class="headerlink" title="统一的用户API层"></a>统一的用户API层</h2><p>Apache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。</p>
<h2 id="求同存异"><a href="#求同存异" class="headerlink" title="求同存异"></a>求同存异</h2><p>Apache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。</p>
<h1 id="Apache-Flink-架构"><a href="#Apache-Flink-架构" class="headerlink" title="Apache Flink 架构"></a>Apache Flink 架构</h1><h2 id="组件栈"><a href="#组件栈" class="headerlink" title="组件栈"></a>组件栈</h2><p>我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/B995477B-C4CC-4A04-BE20-09C9100725E0.png" alt><br>TableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？</p>
<h2 id="TableAPI-amp-SQL到DataStrem-amp-DataSet的架构"><a href="#TableAPI-amp-SQL到DataStrem-amp-DataSet的架构" class="headerlink" title="TableAPI&amp;SQL到DataStrem&amp;DataSet的架构"></a>TableAPI&amp;SQL到DataStrem&amp;DataSet的架构</h2><p>TableAPI&amp;SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/B121F914-23FE-4E9A-9887-E9D3F9E55AD5.png" alt></p>
<p>对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。</p>
<h2 id="ANSI-SQL的支持"><a href="#ANSI-SQL的支持" class="headerlink" title="ANSI-SQL的支持"></a>ANSI-SQL的支持</h2><p>Apache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下：</p>
<p><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/800B7916-C9BF-4422-ADC9-2C5D90E30339.png" alt></p>
<ul>
<li>Declarative - 用户只需要表达我想要什么，不用关心如何计算。</li>
<li>Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。</li>
<li>Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。</li>
<li>Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。</li>
<li>Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。</li>
</ul>
<h2 id="无限扩展的优化机制"><a href="#无限扩展的优化机制" class="headerlink" title="无限扩展的优化机制"></a>无限扩展的优化机制</h2><p>Apache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner：</p>
<ul>
<li>HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。</li>
<li>VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。</li>
</ul>
<p>Flink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。</p>
<h1 id="丰富的类库和算子"><a href="#丰富的类库和算子" class="headerlink" title="丰富的类库和算子"></a>丰富的类库和算子</h1><p>Apache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。</p>
<h2 id="类库"><a href="#类库" class="headerlink" title="类库"></a>类库</h2><ul>
<li>CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。</li>
<li>ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。</li>
<li>GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。</li>
</ul>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><p>Apache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。</p>
<h3 id="多流操作"><a href="#多流操作" class="headerlink" title="多流操作"></a>多流操作</h3><ul>
<li><p>UNION - 将多个<strong>字段类型一致</strong>数据流合并为一个数据流，如下示意：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/F1FF4D6C-8497-415C-A8B6-09B7818F0D13.png" alt></p>
</li>
<li><p>JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/013D77B3-0D57-433F-B8D7-476D8CD3384E.png" alt></p>
</li>
</ul>
<p>如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。</p>
<h3 id="单流操作"><a href="#单流操作" class="headerlink" title="单流操作"></a>单流操作</h3><p>将多流变成单流之后，我们按数据输入输出的不同归类如下：<br>| 类型 | 输入 |  输出|Table/SQL算子|DataStream/DataSet算子|<br>| — | — | — | — |— |<br>| Scalar Function | 1 | 1 | Built-in &amp; UDF, |Map|<br>| Table Function| 1 | N(N&gt;=0) | Built-in &amp; UDTF |FlatMap|<br>| Aggregate Function| N(N&gt;=0) |1  |  Built-in &amp; UDAF|Reduce|</p>
<p>如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。</p>
<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>Apache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下：<br><img src="/2019/01/01/Apache Flink 漫谈系列 - 概述/D9D623B5-A2DD-4A84-AD1B-84DCBECE62CB.png" alt></p>
<p>这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本篇概要的介绍了”<strong>批是流的特例</strong>“这一设计观点是Apache Flink的”<strong>命脉</strong>“，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的”低延迟”需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。</p>
<p>本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！</p>
<p>最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Jincheng Sun">
            
              <p class="site-author-name" itemprop="name">Jincheng Sun</p>
              <div class="site-description motion-element" itemprop="description">Apache Flink</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/sunjincheng121/enjoyment" title="GitHub &rarr; https://github.com/sunjincheng121/enjoyment" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/sunjincheng121" title="Twitter &rarr; https://twitter.com/sunjincheng121" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jincheng Sun</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://http-enjoyment-cool.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
