<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关注公众号更精彩]]></title>
    <url>%2F2020%2F05%2F27%2F%E5%85%B3%E6%B3%A8%E5%85%AC%E4%BC%97%E5%8F%B7%E6%9B%B4%E7%B2%BE%E5%BD%A9%2F</url>
    <content type="text"><![CDATA[博客 vs 订阅号这个似乎不用比，订阅号更方便！ 我的订阅号 目前我在公众号新推出了《Apache Flink 知其然，知其所以然》的系列视频课程。《Apache Flink 知其然，知其所以然》课程，在内容上会先对Flink整体架构和所适用的场景做一个基础介绍，让你对Flink有一个整体的认识！然后对核心概念进行详细介绍，让你深入了解流计算中一些核心术语的含义，然后对Flink 各个层面的API，如 SQL/Table&amp;DataStreamAPI/PythonAPI 进行详细的介绍,以及底层的实现原理进行剖析和具体场景的最佳实践分析，让你对Flink所提供的功能做到 知其然，知其所以然。整体课程的三个部分定位如下： Flink知其然 - 注重Flink的使用，面向初级人群； Flink知其所以然 - 注重原理分析和生产经验分享，面向高级进阶人群； Flink开发者 - 重点为大家分享作者在社区的开发经验，意在辅助大家更好的参与社区开发； 本课程将会让一线开发能具备使用Apache Flink进行线上业务快速开发的能力，帮助架构师或者团队Team leader和Tech lead对实际工作中构建流计算平台做更好的技术选型决策和规划。 感谢感谢所有关注我博客的朋友，从现在开始我所有的文章/视频分享都会在我的订阅号进行分享，请大家移步到我的订阅号，我们持续学习，持续成长！]]></content>
      <categories>
        <category>公众号</category>
      </categories>
      <tags>
        <tag>公众号</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进ASF系列 - 如何成为一个合格的ASF贡献者(Contributor)]]></title>
    <url>%2F2020%2F04%2F16%2F%E8%B5%B0%E8%BF%9BASF%E7%B3%BB%E5%88%97-%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%90%88%E6%A0%BC%E7%9A%84ASF%E8%B4%A1%E7%8C%AE%E8%80%85-Contributor%2F</url>
    <content type="text"><![CDATA[阿里土话ASF是一个开源组织，他有自身的文化，阿里是一个要活好102年的公司，其文化底蕴非凡！分享ASF之前总想或多或少的和大家分享一些阿里的味道！ 给世界带来微小而美好的改变 把幸运种子种到别人身上去，你才会有幸运 Never， Never， Never Give Up（永不放弃） 没错，阿里人看到上面的三句话会倍感亲切，因为上面三句都是 “阿里土话”。虽然是阿里土话，但我认为其有放诸四海而皆准的魅力！这些话，同样适用于激发和指导你成为合格的开源贡献者！不经意间看了一眼窗外，心里开心默笑，其实阿里每个角落都充满着阿里气息： 缘其实每个人参与社区贡献的机缘不一样，但无外乎两种： 偶发事件 - 由于参与某项工作，无选择的就参与了某个开源项目。（我就是这种，很自然的就跟着 蒋晓伟 老师踏入了Apache Flink之旅） 因爱而求 - 每个码农都有一颗想让自己的的代码被应用全球的梦想追求！这梦想的实现要依托有开源的力量，ASF完全可以为你营造最好的实现梦想的环境～ 给个理由也许目前的你既没有工作的需要，也没有对开源产生任何爱意❤️。但了解一件事情总没有坏处，了解参与开源的利好，也许明天你就踏上了开源之旅！ 顺势而为 - 如果你是一个码农，那么参与开源是一种must to have的事情，目前开源领域形式大好，各大公司纷纷拥抱开源，比如：Google，Alibaba，Hortonworks, Tencent, Facebook等等。参与开源无疑是扩大了生存的空间。 业界身份证 - 参与开源就是在一点一滴的描绘你的业界身份证，你的开源贡献可以公示全球，由一行代码，一句文档的贡献，到成为某个项目的管理者（PMC成员），到成为 ASF Member 甚至 成为董事会成员，这些就像你曾经为之努力的学士学位，硕士学位，博士学位一样为世人所认可！不夸张的说，某些情况甚至比学位证书还实用！ 无国界导师 - 参与开源还有一个特别特别特别重大的利好，就是你可以在你所关注的领域寻找到最好的导师，所谓最好，不是牛，最好是最适合！所谓“三人行，必有我师”，社区交流最初你会感觉和业界大牛无法沟通，因为他的一句话，需要分解成十句才能懵懵懂，甚至不懂！这个不是大牛原因，也不是你的原因，是大牛对你水平的了解不足导致，即使大牛了解你的水平，也很难让大牛将一句话分解成十句甚至百句话讨论你关注的问题，不是大牛不愿意，更多是我们不好意思:)。所以说，要“门当户对”找到 社区比你水平略高，同时你也能给人家一些反哺的贡献者长时间交流。目前ASF有7600+的Committer，就算是某一个项目，也应该有几十个Committer和数以百计的贡献者，总会遇到可以和你一起进步的小伙伴！虽不曾谋面，却已熟若亲朋！ 全球性分享 - 目前ASF项目有140多个领域，参与者覆盖230个国家。如果你小有成就，想将你的知识分享给更多的人，想利用的开源知识帮助更多的人，那么无疑ASF为你提供了这样的平台！你又何尝不能成为上面“无国界导师”中的一员的导师呢！ 最佳实践 - 不知 - 了解 - 有方案 - 最佳方案，开源是一个智慧的集结地，一个功能可能有几十种实现方式，这并不是一两个人或者某一个小团队能考虑到(ALL)的，然而在开源一个问题，你在考虑的同时，可能在世界的某个角落，某些角落还有其他人或团队在考虑解决方案，当大家共享方案的时候，智慧的火花将会产生！ 领导力塑造 - ASF有公司和项目的治理方式，不论你目前是否一个（技术）管理者，参与开源贡献你都有机会挖掘你自身的领导力。 TL，可以是 Team Lead，也可以是 Tech lead，作为一个码农，往往对 Tech lead 更加情有独钟！在开源贡献一向遵循 “正确的就是拥护的，正确的就是坚持的”！如果你一贯的在社区发出自己独到的见解（管理&amp;技术），那么势必会塑造你独特的领导力，社区领导力是内心驱动的影响力，所有的拥护者都是无外界压力，无情感偏见，发自内心的崇拜赞许！如果你能达到这样的成就，将胜过你职业的晋升，当然这个也会促进你的职业晋升！ 给世界带来微小而美好的改变毋以善小而不为，小善举大美好！有的时候一个文档优化，甚至一个typo的贡献都会让成千上万的人获益。更有意思的是很多开源贡献者都是从文档贡献开始的 ：）一个很有意思的统计，28%的开源贡献来自偶然的文档改进。 Casual contributions are far from being trivial. After a manual inspection of a sample of casual contributions, we found that although 28.64% of them are related to grammar and typo fixes. 认真生活，快乐工作 - 参与开源不仅仅是工作或业余爱好的演练台，也是生活的一部分，他会让你在快乐工作的同时寻找到“臭味相投”的挚友！通过参与Apache Flink项目我也交到了几位德国好朋友，甚至有些好朋友还成为了我的儿子的好朋友！：）看看下图有没有你和我共同的朋友？世界很小～？ 给个原则没有规矩，不成方圆，如果上面的理由足够打动你，那么我再给你一些参与开源的原则： 给世界带来微小而美好的改变始终坚信滴水成河的道理，社区贡献在于积累，贵在坚持，不因善小而不为，任何贡献都会使得社区变得更加美好！ 公开沟通 - 参与开源的一个重要的原则就是公开沟通，任何问题不论大小，都要在可以被追溯，可以被任何感兴趣的人查阅的方式进行讨论。比如：邮件列表，JIRA上，PR中等。切记不要单独私信讨论，公开讨论有助于更多的人参与，而且还确保了在讨论过程中一些无意识的错误可以很容易被发现和纠正。 保持尊重 - 社区的任何贡献都是以自愿为原则的，不能强迫任何人做事情，也不要无礼貌的敦促任何人做事情的进度（除非特殊情况）。更不能因为意见不通就进行人身攻击，不要以为这好笑，其实是真是发生过的！尊重是相互的，你给予我春风，我自送你一缕芳香！大家努力营造开心和谐的社区氛围。 简明扼要 - 我们可以大胆的在社区提问，追问！但切记在提问之前将自己的问题反复思考，这是对自己负责也是对其他社区贡献者的尊重！因为你的一次问题描述可能将被数百人阅读。写一封简明扼要的邮件意味着人们可以尽可能有效地理解你的意图。如果需要详细说明，请考虑添加摘要。也就是，你的问题描述要简明扼要（这个和能力有关，尽自己最好就行），尽量写清楚上下文，你在什么情况下，遇到了怎样的问题，如何问题再现等等，你的描述越简明扼要，越清晰完整，越容易被人取得别人帮助！ 前进一小步，文明一大步 ：）这可不是 WC 用语，而是确确实实的利他原则。阿里巴巴国际化战役中有一个要求，就是参与国际化建设的阿里人员，到哪个国家，就必须用当地的语言与当地客户沟通。这体现了足够的尊重，体现了足够的服务前的准备和付出！我们在开源社区问问题也是一样的，不能遇到问题，不加思索的就向社区提问，在提问之前要先进行各种尝试，各种资料的查阅，社区已有问题的查阅，同时带上自己的观点去提问，让想帮助你的社区人员看到你在这个问题上的努力。 把幸运种子种到别人身上去，你才会有幸运我们不仅仅是问题的提出者，慢慢我们自己也变成问题的解决者，由社区求助者变成社区救助员！努力寻求自己反哺开源社区的机会，为他人付出也是为自己积累！所谓 “为自己，照亮他人！”。假如你相信来世今生的话，这里有个故事分享给给你： 从前有两个要投胎转世的人，在投胎之前有机会选择投胎之后是做“一生向别人索取的人”还是做“一生施惠于他人的人”，选择了“一生向别人索取的人”的那个，投胎做了 乞丐，而选择了“一生施惠于他人的人”的那个做了富翁！ 所以做社区也一样，努力做施惠于别人的人，就是在社区德高望重，具有领导力的人！：） Never， Never， Never Give Up（永不放弃）理想总是美好的，现实总是骨感的，参与社区的人都具备热情，耐心的品质，但社区的问题太多，需要得到帮助的人也太多，相同问题千人千面，意见不一致也是司空见惯的，切记不要以为提的问题都会很快有人帮忙，你提交的PR很快有人Review。每个大的社区贡献都需要如下图所示的投票决定，也许你的贡献被接受，也许你的贡献被拒绝，但请不要太在意一次的贡献成功与否，无轮遇到什么困难，挫折，都要 Never， Never， Never Give Up（永不放弃），因为马总说过 “今天很残酷，明天更残酷，后天很美好，但是绝大部分人是死在明天晚上，看不到后天的太阳”。 只有坚持到最后的人，才能享受到和煦的阳光！ 如何开始最常见的参与Apache贡献的方式是选择一个你感兴趣的项目，因为爱好才是最好的原动力！我曾经用一句话描述过ASF：“ASF是一个与阿里巴巴同龄（成立于1999年)，有完整的组织(董事会)架构管理，以软件（140个领域）技术全球（覆盖230个国家）共享为使命的公益组织”，里面提及 ASF有140个技术领域总有一个你感兴趣的！ ASF 项目目前分为两大类： 孵化器项目 - 是正在孵化的项目，也就是，在成为ASF 顶级项目之前，需要在ASF进行孵化，当从孵化器毕业之后就会成为Apache顶级项目。参与孵化器项目的好处是你能对项目有更早的参与，有多细节变化的了解，也很容易得到该项目的重视：），目前ASF所有孵化器项目列表请这里查阅！ Apache 顶级项目 - 这是已经从孵化器毕业的Apache 顶级项目，顶级项目的运作一般已经完全符合Apache Way。直接参与顶级项目的好处是能开始就接触很规范的社区贡献方式和更高的质量代码，有更多的学习资料和更多的参与者。目前Apache 顶级项目列表，可以查阅这里！ 一旦选择参与某个项目，不论在什么情况下，你都要听从自己的直觉，做你认为更好或者不同的事情。永远都不忘初心，坚持自己所坚持的～～，也永远牢记上面的原则，其中你会发现“给世界带来微小而美好的改变”非常受用。假如，你在查看文档时候，发现了某个链接的错误或者typo错误。假如，你在使用产品的过程中发现了问题，请不要坐视不理，径直绕开，或者向社区提出问题，等待其他人来修复，因为这正是你贡献社区的好机会，解决这些你能看到的问题，因为，在解决这个问题的同时，也许会有新的问题被你发现～～ 进而你就入道啦：） 准备工作目前ASF开源项目都是在github上面托管的。所以正式参与ASF开源贡献之前你要做一些准备工作： 创建一个github账号点击创建，为了演示，我创建了一个“pyflink”账号 ：） Fork 你要参与的项目以Apache Flink为例,如下： 点击 “Fork” 之后，会在你的github账号下出现一个flink项目，如下： Clone 代码到本地做代码贡献之前需要Clone你刚才fork的Flink代码到你本地，以备提交第一个社区贡献PR！ 阅读项目贡献说明一般具体项目会有介绍如何参与该项目的贡献，以Apache Flink为例 就有关于如果参与Flink社区贡献的说明, 比如： Apache Flink is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Flink. There are several ways to interact with the community and to contribute to Flink including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving the website, or testing release candidates. 详情查阅这里 订阅邮件列表社区问题大多会在具体项目的社区邮件列表里面进行讨论，所以邮件列表是了解社区动态最重要的输入，以Apache Flink 为例，需要订阅 开发邮件列表和用户邮件列表，如下： user-subscribe@flink.apache.org / user-zh-subscribe@flink.apache.org dev-subscribe@flink.apache.org 首先，点击上面的link，会引导你给 xxx-subscribe@flink.apache.org 发送邮件。然后你会收到官方确认邮件。最后你回复确认邮件之后还会收到一封欢迎邮件，也就意味这你订阅成功了！注意上面每个邮件列表都需要单独发起订阅。OK, 到这一步你已经完成了为Apache Flink做贡献的准备工作了：），接下来就要寻找做贡献的机会了！ 创建issue或者解决issue目前大多数ASF项目的问题采用JIRA管理（当然也有例外），我们以Apache Flink为例，当用户发现的问题可以在这里查阅。 如果你发现有你感兴趣的issue，不要犹豫，直接在JIRA下放留言，你想帮忙解决这个问题，并share你解决问题的方法，这样社区会有Committer来与你沟通了！以一个之前我向Flink提交的issue为例 FLINK-13471： 当发现有人创建了issue，但还没有分配给任何人，你就可以尝试帮助解决这个问题，在完成开发后提交PR。 当然，如果你发现了问题，如果是你确认的小问题，可以直接创建新的issue, 如果你对这个问题并不确定，可以在开发邮件列表里面就像邮件讨论。当确认问题之后，再创建issue. 提交PR不论你是修复文档还是贡献代码，都建议在你刚才fork的项目中创建一个用于提交PR的分支，以我上面的为例，我会本地创建一个名为LINK-13473-PR的分支，当完成开发之后，将分支push到自己的仓库，就可以创建PR了，如下： 点击“Compare &amp; pull request”，进行PR创建，如下： 创建PR，有几个值得注意的点： 确保你的分支和官方git的master分支没有冲突，也就是如图显示“ Able to merge.”。 要对PR所要解决的问题，在Title里面简明的体现出来，比如“ [FLINK-13471][table] Add FlatAggregate support to stream Table API(blink planner) ” 明确了 JIRA号FLINK-13471, 模块table 和PR的内容是Add FlatAggregate support to stream Table API(blink planner)。 同时在详情里面要清楚的描述你改动的点，不同项目有不同的要求，但总体上保持上面提到的 前进一小步，文明一大步的原则，你写的越清楚，Review的人越容易理解你的改动，你的PR越容易得到有效的反馈。 最后，点击“create pull request”完成PR的创建！不过，这还没有完成社区贡献，还需要等待社区其他贡献者的Review。 正常情况下，除非是typo的贡献，一般有代码逻辑的PR都会或多或少的得到reviewer的改进反馈，这时候就是学习交流的好机会啦：） 你可以尽可能的发表你的看法，解释你的设计，当然也要充分理解反馈的内容，最后根据沟通达成的内容进行PR的更新！ 最后。。。最后。。。最后 达到了社区代码质量的要求，Committer会帮助你进行代码的Merge，这样你就完成了社区第一份贡献喽！！ 开始1-100之旅常识性观念是0-1很难，因为那是创新，那是新领域的探索，那是酝酿了很久之后的第一步！但是参与ASF开源贡献，恰恰是0-1很容易，1-100才是一个持久战。需要上面提到的 “Never， Never， Never Give Up（永不放弃）”， 因为我真的看到了很多社区贡献者在一个社区贡献了一段时间之后，如果没有拿到自己想要的结果，比如成为Committer，就会永远的在这个项目贡献里面消失了，Give Up 了！这不是危言耸听，这是真是的现实！所以在ASF开源贡献的道理上，的确有很多人被 马老师的话所命中：“今天很残酷，明天更残酷，后天很美好，但是绝大部分人是死在明天晚上，看不到后天的太阳”。所以，你…准备好了吗？：） 但行善事，莫问前程不论做人，做事还是社区贡献，很多道理都是相通的，在下面的ASF金字塔中，我们从 贡献者 到 董事会成员的路是漫长的，如果你天天想着什么时候成Committer，什么时候成为PMC成员，什么时候成为ASF Member，什么时候能够当选董事会成员，我确信，在ASF开源贡献中，你将无法做到 “快乐工作，认真生活”！过急的目标驱动会增加你的烦恼，相反，登山而不思山顶 攀登，将会迎来一路的惊喜！所以在参与开源的开始，我最后的建议就是：“但行善事，莫问前程”！加油⛽️ 为你打气我相信在ASF开源贡献之旅，你会有很多次要放弃的念头，你会遇到很多怀疑自己的时刻，你会时不时的怀疑社区管理者是否有问题？总之，如果你想把他当作一生的乐趣，在你没有找到乐趣之前，你一定需要下面的在文章开头已经提及的三句话： 给世界带来微小而美好的改变 把幸运种子种到别人身上去，你才会有幸运 Never， Never， Never Give Up（永不放弃） 你参与社区的目的是为了尽自己微薄之力，来让ASF开源社区更美好！你参与社区的信念是为其他人播撒幸福幸运的种子，你并没有在乎得到什么回报，你相信“因果”！你参与社区的坚守是永不放弃，因为只要我在前行，必将抵达彼岸！永不放弃要深刻你脑海！ 我很喜欢上面这三句阿里土话，我们共勉把！ 诚挚邀请我目前在负责Apache Flink的PyFlink建设，诚挚邀请想参与ASF社区贡献的你，以PyFlink作为你的开源之旅的首站！期待在Apache Flink社区PyFlink的建设中，遇见你～～ 小结本篇为大家介绍了参与开源的利好，原则，以及介绍为自己的第一个社区贡献需要做怎样的准备。最后诚挚邀请想参与开源建设的朋友首站加入Apache Flink 的PyFlink建设 关于作者 查阅更多内容]]></content>
      <categories>
        <category>走进ASF系列</category>
      </categories>
      <tags>
        <tag>ASF，走进ASF系列</tag>
        <tag>如何成为一个合格的ASF贡献者(Contributor)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进ASF系列 - Apache 董事会是如何选举的？]]></title>
    <url>%2F2020%2F04%2F10%2F%E8%B5%B0%E8%BF%9BASF%E7%B3%BB%E5%88%97-Apache-%E8%91%A3%E4%BA%8B%E4%BC%9A%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%89%E4%B8%BE%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[最简单的即是最困难的阿里的新六脉中有一脉叫做“因为信任，所以简单”。在我们的生活工作中最宝贵的是信任，同时最脆弱的也是信任。你复杂，世界便复杂；你简单，世界也简单。ASF组织也是一样，提倡公开，提倡民主，提倡信任。在上一篇的《走进ASF系列 - 初识ASF组织架构及治理》中提到ASF采用选举和任命的方式进行公司治理，选举是所有ASF成员共同参与，以公平公正公开的态度来进行董事会成员的选举和新ASF成员的提名。会议中全球700多位成员，不论你来自哪个国家，哪个民族在ASF组织中都一视同仁，也许未曾谋面，但却彼此信任，信任每一成员所抛出的观点，信任每一成员的提名，信任每一个成员的投出的一票。因为每个成员明白，自己的每一言，每一票都是对自己人格的勾勒。 作为一个即将迎来9周年的阿里人，我也借此和大家分享一下我对“因为信任，所以简单”的理解。 阿里新六脉是阿里人价值观的衡量标准，是阿里人个人的行为准则，是阿里人思考问题，与世为人的百宝箱。新六脉是自我修炼的宝典，但不是约束其他人的工具。我们在做事情时候不要用新六脉去衡量其他同学是否做到了，而是拿新六脉来衡量自己是否做到了，我们就以“因为信任，所以简单”为例，这里的”信任”我更愿意理解成当你做事情，做决定的时候是否“信任”自己的选择是对的，是否“信任”自己的做的事情是正确的，是否“信任”自己的方向是否着眼于未来的！如果你对自己的思考，选择，行动都是那么的”坚信“，那么你在执行的时候，不论你遇到怎样的问题你的决定都是“简单”的。如同路对了，永远不惧怕遥远，因为你已经在路上。同样在阿里巴巴将 Blink 进行开源和收购Flink母公司的路上也是异常艰辛的，但我们坚信这是对的，所以我们的脚步是坚定的，遇到困难时候的选择是简单的，那就是 披荆斩棘，为开源，为更多的人，更多的企业能够享受到Blink/Flink的美好而努力。 ASF 金字塔ASF金字塔描述了不同角色的群体特征，越上面人群越小，但对ASF肩负的责任越大，越下面人群越大，但对ASF的责任越小，参与越容易！ 今天我们将为大家介绍ASF如何发展金字塔最顶端的管理人员！ ASF年度成员大会程序我对ASF的一句话定义是：”ASF是一个与阿里巴巴同龄（成立于1999年)，有完整的组织(董事会)架构管理，以软件（140个领域）技术全球（覆盖230个国家）共享为使命的公益组织！” ASF为了更好的服务全球，让ASF组织更好的运转，需要每年进行ASF董事会成员和ASF成员的选举和任命。从原则上讲，任何人都可以成为Apache 董事会成员，当然前提条件是为这个团队做贡献，成为起成员。 ASF至少每13个月举行一次年度成员大会，选举新的董事会，并对新的候选人进行投票。ASF年度会议由成员志愿者按照以下基本程序进行。 [说明] 本篇参考[1]内容进行编写，部分措辞和语言组织略有调整，但保证内容含义的严格一致性。ASF虽然是一个非盈利组织，但是ASF是以公司化的方式进行运作的，有股东，董事会，还有执行机构，也有一些其他的雇员。本文中所提到的ASF年度大会算的是股东大会。 目录 听众 会议机制-IRC沟通 成员候选人投票 如何计算成员投票 怎样决定如何对成员候选人进行投票 董事会投票 如何计算董事会投票 怎样决定如何对候选进行上投票 听众本文档是ASF召开正式成员会议的概述。如果您是ASF的成员，请参阅本年度会议的 Private README.txt 以获取有相关方面的最新详细信息： /repos/private/foundation/Meetings/20200331/README.txt 会议机制-利用IRC沟通由于我们的成员来自全球，因此成员会议在三天的时间内通过freenode的IRC举行。会议的前半部分（通常是星期二）在IRC实时聊天中举行，邀请所有成员参加。就像任何大型会议一样，轮值主席会负责根据预定的会议日程（agenda）和与会人员轮流在线交互。在会议的上半场中，我们审查来自各执行官的关于过去一年的基金会状况的报告。在上场年会无法出席的成员可以提交代理申请，以便可以将他们标记为出席。 在会议上半场结束时，议程列出了下一届董事会的候选人，以及基金会的任何新候选人。候选人宣布后，会议将进行约46小时的休会。 在休会期间，我们的Apache STeVe投票软件会向所有合格成员发送安全的私人电子邮件投票。投票通过电子邮件开放40多个小时，使世界各地的成员都可以方便地进行投票。所有选举计数和跟踪工作均由Apache STeVe和几位Member志愿者选举监督员执行。使用您的Apache ID登录后，在Web界面进行投票。 在会议恢复之前（通常在星期四），投票将关闭几个小时，以使选举监督人员可以相互核对他们的票数是否一致。当会议在IRC上恢复时，主席宣布候选人名单的结果，并在会议上宣布董事会选举。会议的下半场通常要短得多，如果成员已经参加了上半场，则无需参加下半场。 请不要等到最后一分钟进行投票：由于在会议召开前10天列出了所有董事会成员和新成员候选人，因此您有足够的时间事先研究您的选择。同时，投票过程是有监督的，监督人员来自组织的志愿者。 会议结束后，将立即成立并宣布新董事会的任期，并由提名他们的现有成员向幸运的新成员候选人发出私人邀请。请注意，我们不会公开新当选成员（Member）的姓名，因为可能（很少）有些人不接受成为新成员的邀请。 成员候选人投票如何计算成员投票要选出新成员，根据我们的章程4.1的规定，他/她必须获得在提名中进行投票的赞成票多于否决票。所有投票跟踪和记录都由Apache STeVe处理，由我们的票监督人员进行监控。投票的运行和审核过程均由ASF成员私下完成。 对新成员候选人的投票是保密的；由于投票是针对个人的，因此投票监控器确投票结果后，便不会共享投票结果。 怎样决定如何对成员候选人进行投票这完全由每个ASF成员来决定。新成员候选人由现有成员提名，候选人发布说明为什么他们认为候选人会成为好的成员。提名通常需要几秒钟的时间，其中许多还包括有关候选人当选原因的个人事迹。 由于新候选人是参与Apache项目的人员，因此许多人通过搜索邮件列表来查看候选人过去如是何参与我们的社区贡献的。使用PonyMail的存档非常容易-ASF成员可以在其中查看所有邮件列表，甚至是私人邮件列表。 许多成员都希望从现有成员那里获得强有力的提名理由，解释说明提名人为什么会成为好的候选人。在投票之前花几秒钟仔细阅读一下成员提名文件十分重要。 董事会投票如何计算董事会投票额ASF使用一次可转让投票（STV）来选举每年董事会的所有9个席位。每个候选人都单独参加；没有候选人名单。只有ASF成员才能提名人选参加董事会选举；所有候选人都已经是ASF成员。 STV旨在帮助小规模的相关选区选举董事会成员。这种计票设计有助于使选民展示自己的实际愿望，避免过于灵活。请继续阅读有关如何使用STV的讨论，包括有关志愿者投票监控员实际上如何收集选票的详细信息。 要记住的最重要的事情是：请按照您的偏好顺序进行投票！尽一切努力使您的＃1偏好进入董事会；＃1选票比其余选票更重要。如果您按字母顺序投票（某些人似乎对过去的选票进行了投票），那么您会发出强烈的信号，表示您希望董事会以Awful先生和Beastly女士之类的名字出现-可能不是您想要的。我们的Apache STeVe工具将分配给候选人的字母随机化，以尝试解决这个问题。 选举结果使用Seek的Meek方法计算。技术细节可以查阅Apache STeVe项目代码，当然这是Apache项目。 STV计票循环进行。每当董事会候选人获得足够的选票以当选时，该循环就会列出名字。投票首先要分配给该选票上指示的第一候选人。随着投票的进行，投票将重新分配。当确定某候选人不会当选时候，他们的选票将重新分配。当候选人当选时，他或她只带了足够的选票就足以使他当选；他们的其他选票是根据该选票上排名较低的排序发送的。 该YouTube视频提供一个轻松活泼的介绍：Politics in the Animal Kingdom: Single Transferable Vote: or a shorter description of how second, third, etc. place votes are allocated. Wikipedia has a general overview of Single Transferable Voting. 怎样决定如何对董事会候选人进行投票显然，这取决于每个ASF成员的决定。实际上，所有成员以及董事会的所有候选人都是以个人身份行事，这意味着我们的公司治理永远不会受到其他公司或其雇主的影响。董事会的这种独立性是多年来ASF成功的关键因素。 STV票数排名；您所投票的第一人比第二人更可能获得投票（依此类推）。 如果您确实不希望某个人成为董事会成员，请完全省略他们，而不是将他们放在最后。 尽管有9个空位，但您可以根据需要给任何人投票-即使第9位之后的投票也可能最终有重大意义。请注意，您不希望当选的人根本不会出现在您的列表中。 例如，如果在选举中有89位成员投票，则候选人只需10张第一票就可以赢得董事会九个席位之一。这有助于确保首位偏好的小选区能够获得董事会席位。 并提醒：您的投票顺序很重要！提交前请仔细检查您的选票，您可以根据需要多次投票给董事会。仅使用最后一次投票。因此，如果您犯了一个错误或改变了主意，只需再次投票。 致努力成为ASF董事会成员和ASF成员的开源贡献者随着国内对各大企业对开源的不断投入，目前有大量的国人在积极的参与开源建设，如果你目前正在无私的进行着开源贡献，如果你也恰巧是在参与ASF开源建设，那么请在默默奉献的同时，了解ASF的治理策略，了解ASF的“晋升”之路。但这里强调一下，“晋升”是为了更大的责任，“晋升”是为了更好的服务!!! 小结“因为信任，所以简单” 不仅仅是阿里的行为准则，也是进行开源贡献的为人之本。从小白到ASF高官是一个辛苦的过程，但最值得回忆的是路上的风景。踏上山顶才知道自己的渺小，从Contributor变成了Committer之后，才知道Committer不仅仅意味这你有了代码提交的权限，更意味着你对所参与的开源项目和Contributor的代码贡献有着不可推卸的责任！我们一路“晋升”，一路证明着自己，但证明的不仅仅是自己的能力，更是证明着自己敢于承担更大责任的勇气！期待在ASF看见优秀的你～ 任何问题欢迎留言，也欢迎线下探讨～～ 参考链接[1] https://www.apache.org/foundation/governance/meetings 查阅更多内容]]></content>
      <categories>
        <category>走进ASF系列</category>
      </categories>
      <tags>
        <tag>ASF，走进ASF系列</tag>
        <tag>Apache 董事会是如何选举的</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进ASF系列 - 初识ASF组织架构及治理]]></title>
    <url>%2F2020%2F04%2F06%2F%E8%B5%B0%E8%BF%9BASF%E7%B3%BB%E5%88%97-%E7%A7%92%E6%87%82ASF%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[费曼技巧（The Feynman Technique）费曼技巧是一种 “以教为学” 的学习方式，源于诺贝尔物理奖获得者，理查德·费曼(Richard Feynman)。这个学习方法的秘诀是尝试用最简单的语言说清楚你要表达的内容，哪怕是一个深奥的科学理论，你也能用简单的表达方式让5岁的孩子或者60岁的老人很容易的理解和熟记。由于我也是在上周（2020.04.03)刚刚收到的成为ASF Member的邀请，所以《走进ASF系列》文章与其说是和大家分享，不如说是和大家一起学习！ ASF 是什么？我想“ ASF 是什么？”是一个最简单的问题，也是一个最难回答的问题。简单是因为你可以以任何一个角度去描述，就像盲人摸象。而困难是因为你很难用一句话就完整全面的定义“ ASF 是什么？”。所以面对这样的问题我的考虑是，先描述表象，再探究其细节。就像我们回答宝马是什么，我们会说“别摸我”，奥迪是什么？我们会说“四个圈”。那么 ASF是什么？我想说，ASF是下面这个漂亮的羽毛 ；） 了解Logo也许是欣赏一个新事物最好的开始。那么下一秒你将有一连串的问题浮现出来？她多大了？干什么的？怎么运作的？和我有什么关系？接下来我们慢慢道来… 与阿里同龄 Apache软件基金会（ASF）是在美国注册的503©3非营利的公共慈善组织，与1999年成立。那么成立的最终目的是什么呢？ASF最核心的目的如下： 通过提供硬件、通讯工具以及业务基础架构，为开放式协作开发软件项目提供基础。 建立独立法人实体，任何公司和个人可以向其进行捐赠，并能够确保这些捐赠将用于公共利益。 为个人志愿者提供避免针对基金会项目的法律诉讼的手段。 保护应用于其软件产品的“Apache”品牌免受其他组织的滥用。 所以ASF是有着美好愿景和伟大使命的公益性组织。ASF和阿里巴巴一样都成立于1999年，而且在利他方面可谓英雄所见略同，阿里巴巴以“让天下没有难做的生意”为使命， ASF则致力于开源软件生态的营造，让软件技术能够在全球共享，这是ASF无比耀眼的魅力所在！ 逐本溯源ASF的成立并非凭空出世，在ASF的前身其实是叫“Apache 小组”，这个小组当时在维护由NCSA编写的HTTPD Web服务器。从1995到1999年，由“Apache小组”所领导开发的 Apache HTTPD web服务成为了市场的领导者，一直持续到现在，目前仍占据了市场65%的份额。当年使用该服务的人越来越多，但是最初的开发人员对该项目失去了兴趣，并转移到其他项目上，从而使用户失去了支持。于是一个叫Brian Behlendorf的大牛在自己的计算机上创建了一个邮件列表，供这些用户协作以修复，维护和改进该软件。从1995年2月不到12位软件工程师通过电子邮件共享代码补丁开始，到后面的有了Apache网站，网站上又托管了很多关联项目（例如mod_ perl项目，PHP项目，Java Apache项目），这样的情形面临的问题越来越多，比如，潜在的个人被法律攻击等，进而越来越需要一个更加协调和结构化的组织来解决这些问题。于是到1999年6月ASF便注册成立了！：） 成长的苦恼随着软件产品的不断成熟，用户越来越多，反馈的问题和软件增强的建议也越来越多，同时参与进行代码修补的人也越来越多。问题也随即而来，最初的几位创始人根本没有办法处理所有的问题，Review所有的代码补丁，Merge所以的提交，那么怎么办呢？当时Apache成员认为如果某位开发者，已经“赢得”了成为社区的一员的资格时，他们就授予此开发者对代码仓库的直接访问权限，也就是目前我们说的Committer，这样不断的增加团队的力量，也提高了团队开发项目的能力，进而更有效地维护和发展了团队。当然这只是冰山一角，ASF的治理当然不仅仅是Committer的发展了，下面章节我们进行系统的介绍。 ASF治理目前ASF拥有超过140多个跨技术范围的项目，涉及350多个具体项目，7000+的代码贡献者，参与人员覆盖了全球230多个国家，为保障其健康的发展必须有一套简单高效的组织架构和治理手段。那么ASF的治理将涉及到ASF组织本身的治理和ASF对Apache网站所托管的项目的治理。 公司的治理总的来看，ASF的治理相当简单：由成员(Member)来选举董事会；董事会任命各种管理人员并创建PMCs；PMCs定期向董事会报告，大多数其他管理人员向总裁然汇报，然后总裁向董事会报告。 组织架构ASF公司治理首先要有一个完善的组织架构，和分工明确的角色职责。如下图所示： 上面的组织架构整体围绕的“董事会”进行角色构建，董事会有主席(Chair)和副主席(Vice Char)。 公司设有总裁，副总裁，秘书，财务总监，法律顾问等职位。每种角色各肩其职。作为初识ASF的文章，我们简单了解如下宏观的内容即可： ASF 由董事会来管理，董事会由成员构成。 现有成员(Member)会定期提名和选举新成员，每年提名并选举9名董事加入董事会。 董事会任命业务官员，并将特定政策或业务领域的责任分配给每位官员。例如，法律事务委员会副总裁负责为ASF和所有Apache项目设置法律政策，并与公司顾问进行合作。 董事会任命执行官，包括总裁，秘书，财务总监等，他们负责ASF的特定领域运营。 大多数官员每月直接向总裁报告，而总裁则每月向董事会报告ASF的整体运营状况。 基础设施副总裁向总裁汇报，并对基础设施团队的运营进行宏观监督。在团队中，带薪的基础设施 管理者角色有助于管理日常操作，带薪人员确定工作优先级，并维护所有Apache项目中使用的服务。 那么这些角色是如何产生的呢？ASF还需要一套选举任命的机制来保证整个组织的健康运行。 选举和任命董事会负责创建并更新项目管理委员会（ PMCs ）。董事会只是批准孵化器的合理构建请求（毕业成为 TLPs ）或来自 PMCs 自身（添加或删除 PMC 成员）的请求。在每种情况下，向董事会建议进行的变更都已经由相关的 PMC 完成了投票。如下图所示： 董事会(Board)董事会由9名懂事组成，负责管理和监督ASF的各种事物。包括人员，资产（资金，知识产权，商标和支持项目所需要的基础设施）以及为项目分配资源。 项目管理委员会（PMC）项目管理委员会由董事会决议设立，负责具体一个项目的事物管理。每个PMC由至少一名ASF官员组成，并指定一名成员为主席。每个PMC中的主席就是ASF在各个项目中的眼睛和耳朵，对ASF至关重要。当然PMC负责发展各自新的PMC成员和发展项目的Committer。 各种VPASF有这种VP，他们大多数官员每月直接向总裁报告，而总裁则每月向董事会报告ASF的整体运营状况。 也许这些内容对于刚刚了解ASF或者刚刚加入ASF开源建设的同学简单了解一下即可，因为似乎距离还很“遥远”。（加引号原因是，我认为是 思想认为 的遥远，不是真的那么远）。接下来我们看看和具体项目贡献者密切相关的内容 - ASF项目治理。 项目的治理每个项目都由项目的PMC进行独立管理，PMC以Apache的方式，遵循由所有官员为所有项目设置的一些核心原则。如下图所示： 上图我以Apache Flink项目为例勾勒了项目治理的概要图。每个PMC都由ASF董事会进行管理，每个PMC都会负责制定自己项目的技术方向。 PMC每季度直接向董事会报告一次。董事会对PMC进行监督，以确保PMC健康发展，顺利的管理社区，确保PMC遵守Apache的原则。项目的技术方向由PMC指定，董事会不对项目提供技术治理。 每个PMC的主席都是该项目的副总裁，因此也是ASF的官员。主席的主要职责是确保项目报告全面，并提交到董事会。 PMC对软件版本发布进行投票。PMC进行适当的发布治理，进而确保了所有的源代码版本发布都是ASF官方行为。 PMCs为项目提名并选举新提交者(Committer)。PMCs还负责提名并投票新的PMC成员，然后PMCs向董事会汇报建议的变化。 Apache孵化器(Incubator)是一种特殊的（I）PMC：它的工作是指导新建立的Podling社区，以帮助他们学习Apache Way。在每个Podling对他们的软件发布进行投票后，IPMC成员也对该Podling的发布过程进行监督和投票，以确保Podling逐渐成长为一个合格的Apache项目。 根据政策，只有个人可以充当会员，提交者(Committer)或PMC成员(Member)或管理人员。这是ASF和Apache项目保持其独立性的一种方式。 同时每个项目都有不同的参与角色，比如：用户（Users)，贡献者(Contributors)，提交者(Committers)，新的PMC Members等等。不同角色的职责如下： 用户(Users) - 就是使用项目润君的的大众，他们以错误报告和功能建议的形式向开发人员提供反馈进而为项目做贡献。 贡献者(Contributors) - 就是开发人员，以写代码或写文档的形式为项目做贡献。开发人员可以多种方式共享社区，比如参加者邮件列表讨论、提交代码补丁、提交文档等等。 提交者(Committers) - 提交者是一批特殊的贡献者，他们是拥有代码仓库写操作权限的开发者。 PMC 成员(Member) - 项目管理委员会（PMC）成员，是由在项目的开发中表现突出的提交者（Committers）选举出来的，他们拥有写入代码仓库的权限、拥有社区相关事务的投票权、以及有权提名新的PMC成员和Committer的权利和职责。 ASF 董事会现状今年ASF年度成员会议, 在3.31～4.2期间举行， ASF 董事会选情相当激烈，有16为ASF Member参与竞选。竞选结果是去年的九席董事换了七位，二位董事 Craig Russell 和 Shane Curcuru 获得了连任。 目前同时会成员如下： Shane Curcuru (连任董事) Bertrand Delacretaz（原董事） Roy Fielding（原董事） Niclas Hedhman（新任董事） Justin Mclean（新任董事） Craig Russell (连任董事) Sam Ruby（原董事） Patricia Shanahan （新任董事） Sander Striker （原董事） 2020 新晋ASF Member上面的竞选和任命章节已经介绍过，ASF一年一度的成员会议中，不仅仅会选举9名董事成员，也会提名和投票新的ASF Member 。 （此处省略。。。等待官宣） 恭喜他们（当然也祝福一下我自己的幸运）！据最新统计（2020.04.06) ASF Member 全球有765名，其中在中国的有20+人。 ASF 一张图对于初识的用户来说，上面的内容也许太多了，记住一个Logo容易，记住一张图似乎比上面的文字要简单许多：） ASF金字塔描述了不同角色的群体特征，越上面人群越小，但对ASF肩负的责任越大，越下面人群越大，但对ASF的责任越小，参与越容易！ 记住 “ASF是什么？” 从 羽毛 Logo开始，参与ASF贡献从 用户 开始。 ASF 一句话图可以描述表象，文字可以表述内涵，开篇的羽毛代表了ASF，但怎么描述这个漂亮的羽毛呢？“ASF是一个与阿里巴巴同龄（成立于1999年)，有完整的组织(董事会)架构管理，以软件（140个领域）技术全球（覆盖230个国家）共享为使命的公益组织”。 小结本篇是《走进ASF系列》的第一篇，我努力尝试在我自己身上体验“费曼技巧”，从概要介绍了什么ASF，进而推进介绍ASF组织架构，ASF公司治理和项目治理，意在让 想要 或 刚刚 参与ASF贡献的同学对ASF有一个初步认知。最后介绍了目前ASF董事会成员和2020年新晋的中国ASF Member。同时，为了方便大家记忆，以一张图，一句话的方式收尾本篇内容。 ** 同时《走进ASF系列》与我之前的《Apache Flink 漫谈系列》不同，讨论ASF更多的是非技术的分享，容易变成无重点，无内容，无收获的乏味分享，这是我最不愿意看见的现象。所以肯请大家在本系列的初期多给我一些或好或坏的评论建议，以便我们共同成长！谢谢大家～ ** 参考[1] http://www.apache.org/foundation/governance/orgchart]]></content>
      <categories>
        <category>走进ASF系列</category>
      </categories>
      <tags>
        <tag>ASF，走进ASF系列</tag>
        <tag>初识ASF组织架构及治理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进ASF系列 - 序]]></title>
    <url>%2F2020%2F04%2F06%2F%E8%B5%B0%E8%BF%9BASF%E7%B3%BB%E5%88%97-%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[走进ASF系列 - 序特别说明《走进ASF系列》分享的内容和观点不代表任何组织，仅代表我个人对Apache软件基金会（Apache Software Foundation，ASF）的认知！ Who我2011年加入阿里，经历过若干组织架构调整，做过行为日志OPlog，阿里郎，云转码，文档转换等若干产品，在2016年10月份由于团队需要有幸接触到Blink的开发，开始了解Apache Flink社区，由初期的参与社区开发到后来逐渐主导具体模块的开发，到负责Apache Flink Python API(PyFlink) 的建设。 目前是 ASF Member, PMC member of @ApacheFlink and a Committer for @ApacheFlink, @ApacheBeam, @ApacheIoTDB 。我的开源之旅如下： Why为啥要写《走进ASF系列》文章？核心考虑是发现国内大多数人都听说过ASF，同时我们国家也是世界上使用开源最多的国家之一，如下图展示了在ASF 2019年年度报告中披露的ASF软件在全球各国家的下载量排名： 中国已然成为Apache软件基金会项目下载量最大的地区。但是真正了解并参与ASF贡献，并且成为ASF Member的人非常稀少。这个现象并非我们没有能力参与ASF贡献，更不是没有能力成为ASF Member，而是对ASF了解的渠道比较少，同时或多或少的还可能有一点语言上障碍的原因，国内对ASF的参与和世界其他西方国家相比来说，参与的人数非常稀少！分享一些具体数字给大家: ASF 拥有7000+代码贡献者，然而为 ASF 项目提交贡献的中国工程师仅千人规模，不足七分之一。 ASF 项目约350个，然而由中国发起的 ASF 项目仅19个，已成为顶级项目的比例更是不足5%。 ASF 孵化器拥有导师200多个，然而活跃的中国导师不超过5位。 ASF 每年在美国、欧洲等地举办 ApacheCon ，然而迄今为止，ASF 尚未在中国举办过一次 ApacheCon。 最新统计(2020.04.05) ASF Member 全球有765名，华人有40名，其中在中国的有22人。 作为全球最大Apache开源消费国， ASF在国内有广泛的用户基础，近年来国内个大互联网公司也逐渐参与了Apache 开源建设，我们很有必要思考如何让中国的工程师有更多的有机会了解ASF，有更多的机会参与ASF贡献，有更多的机会成为Apache 开源项目的 Committer，PMC ，甚至是 ASF Member。 所以《走进ASF系列》文章就是将我对ASF的认识和参与ASF贡献的经验与大家进行分享，大力推动ASF在中国的发展，竭尽全力让国人在ASF开源领域占有一席之地～～ What《走进ASF系列》文章会从ASF 组织架构，ASF公司和ASF项目治理方案入手，宏观了解ASF如何运作。然后分多篇ATA进行各个细节的介绍，比如，如何参与Apache 具体项目做贡献，如何成为某个项目的Committer，如何成为某个项目的PMC成员，如何选择多个Apache 项目进行多领域贡献，如何成为ASF Member等等进行分享，各篇之间可能没有过多的关联，组织形式较为松散，望大家多见谅！ When《走进ASF系列》文章分享要多久完成？这个我目前没有明确的时间计划，除非有同学强烈要求，一般情况分享时间会与工作闲忙有很大关系，至于什么时候终止分享，我想应该会持续分享！ 共同进步本系列文章可能由于个人疏忽或者知识和表达能力有限，有些用词或技术点描述可能有偏差，欢迎钉钉，电话，留言等各种反馈，不断修正，共同为后来者打造优质的博客。 关于点赞和评论本系列文章难免有很多缺陷和不足，欢迎大家给予反馈和建议，同时，也真诚希望读者对有收获的篇章给予 点赞 鼓励，您的点赞 和 评论是我持续输出的原动力～～]]></content>
      <categories>
        <category>走进ASF系列</category>
      </categories>
      <tags>
        <tag>ASF，走进ASF系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 扫雷系列 - PyFlink如何解决多JAR包依赖问题]]></title>
    <url>%2F2020%2F03%2F31%2FApache-Flink-%E6%89%AB%E9%9B%B7%E7%B3%BB%E5%88%97-PyFlink%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9AJAR%E5%8C%85%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[《Apache Flink 扫雷系列》简介本篇是《Apache Flink 扫雷系列》的第一篇，所以简单介绍一下这一系列的特点，本系列所定义的”雷”是指由于目前Apache Flink目前的设计问题导致的用户非便利性问题的临时解决办法。那么为什么明知道有设计问题还不进行设计重构，避免这些”雷”的存在呢？其实社区的发展和我们各个公司内部产品发展一样，都有一些客观因素导致实际存在的问题无法及时得到解决，比如，社区的Release或者内部产品发布的的周期问题，在没有新的Release之前的一些对用户非友好的问题就需要有一些“非正规”的解决方式，或者说是临时解决方案，这种方案的特点就是，能解决问题，但不是通用性解决手段，只能民间流传，不能官方宣扬。所以《Apache Flink 扫雷系列》就是为大家提供能够解决大家现实问题，但是可能不是最佳实践，大家在这系列中可以有更大的反哺社区的机会：） 开篇说”雷”本篇的”雷”是目前针对Apache Flink 1.10集以前版本中，在利用CLI提交作业时候只能提交一个JAR的功能问题解决，也就是针对命令参数-j,--jarfile &lt;jarfile&gt; Flink program JAR file. 的问题。目前 -j只允许用户提供一个JAR包，这在很多场景是不太合理的，不用说用户自己的JAR包，就单说Flink用户使用的Connector的JAR，在一个作业里面就可以能使用多个不同的Connector类型，比如在《PyFlink 场景案例 - PyFlink实现CDN日志实时分析》为例，就用了Kafka,MySql,CSV等多种功能JAR包的依赖。这个普遍的问题就是要用手工方式将这三个JARs合并成一个，然后提交作业时候用-j选项上传到集群。 扫雷难度面对合并多个JAR包，也许Java用户还好（虽然不便利，但应该都会操作），但对于Python用户，在没有涉及过Java开发的情况下，可能要花费一些时间来完成JARs的合并，甚至有可能有种无从下手的感觉。所以本篇主要针对的是不了解Java的Flink Python用户。 案例选取为了大家能够实际的体验实际效果，我们选取一个具体的案例来说明如果进行多JARs的合并。我们就选取我在2020年3月17日直播中所说的《PyFlink 场景案例 - PyFlink实现CDN日志实时分析》来进行说明。 案例回顾《PyFlink 场景案例 - PyFlink实现CDN日志实时分析》核心是针对灌入Kafka的CDN日志数据经过PyFlink进行按地区的下载量，下载速度的统计，最终将统计数据写入到MySql中。同时放入到Kafka的数据格式是CSV(&#39;format.type&#39; = &#39;csv&#39;)。所以我们依赖的JARs如下： flink-sql-connector-kafka_2.11-1.10.0.jar flink-jdbc_2.11-1.10.0.jar flink-csv-1.10.0-sql-jar.jar mysql-connector-java-8.0.19.jar 我们可以用如下命令下载： 1234$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.11/1.10.0/flink-sql-connector-kafka_2.11-1.10.0.jar$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-jdbc_2.11/1.10.0/flink-jdbc_2.11-1.10.0.jar$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-csv/1.10.0/flink-csv-1.10.0-sql-jar.jar$ curl -O https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.19/mysql-connector-java-8.0.19.jar 我们将如上4个JARs下载到某个目录，我这里下载到本机的temp目录： “雷”存在的场景说明为啥在博客《PyFlink 场景案例 - PyFlink实现CDN日志实时分析》并没有提到要合并JARs的问题？ 是的，这个“雷”的存在是有一定的条件的： 作业提交的集群环境没有预先安装你所有需要的JARs（大部分情况都是不会安装的） 上面条件是必须成立，才会存在扫雷的问题。那么在博客中我在集群环境预安装了说需要的JARs，也就是博客中提到的下载JARs到集群lib目录PYFLINK_LIB=python -c &quot;import pyflink;import os;print(os.path.dirname(os.path.abspath(pyflink.__file__))+&#39;/lib&#39;)&quot;)的操作。 合并JARs的注意点合并JARs的一个很重要的点是涉及到了JAR包的Service Provider机制,详细规范详见。这是让Python人员是很难注意到的合并重点。JAR包的Service Provider机制会允许在JAR包的META-INF/services目录下保存Service Provider的配置文件。简单说就是他为开发者提供了一种扩展机制，在开发阶段只是定义接口，然后在包含实现的JAR包进行实现配置，就可以调用到实际接口的实现类。关于JAR包META-INF目录结构简单说明如下： META-INF - 目录中的下列文件和目录获得Java 2平台的认可与解释，用来配置应用程序、扩展程序、类加载器和服务： MANIFEST.MF - 清单文件，用来定义与扩展和数据包相关的数据。 INDEX.LIST - 这个文件由JAR工具的新“-i”选项生成，其中包含在一个应用程序或扩展中定义的数据包的地址信息。它是JarIndex的一部分，被类加载器用来加速类加载过程。 x.SF - JAR文件的签名文件。x代表基础文件名。 x.DSA - 这个签名块文件与同名基础签名文件有关。此文件存储对应签名文件的数字签名。 services - 这个目录存储所有服务提供程序配置文件。 注意：provider配置文件必须是以UTF-8编码。 合并操作解压JARs1$ mkdir jobjar csv jdbc kafka mysql 其中jobjar存放最终我们打包的JAR内容， csv jdbc kafka mysql存放对应的JAR所解压的内容。具体命令如下： 1234$ unzip flink-csv-1.10.0-sql-jar.jar -d csv/$ unzip flink-sql-connector-kafka_2.11-1.10.0.jar -d kafka/$ unzip flink-jdbc_2.11-1.10.0.jar -d jdbc/$ unzip mysql-connector-java-8.0.19.jar -d mysql 解压之后我们会在刚才的目录得到如下文件内容：我们核心要处理的是class文件夹和 META-INF/services文件夹，如图csv和kafka的JAR解压之后的内容。其中，Class文件夹可以直接拷贝。但是services要进行同名的合并，比如上用于Flink的Connector的服务发现配置org.apache.flink.table.factories.TableFactory是需要将文件内容进行合并的。 合并JARs首先我们创建META-INF和META-INF/services目录,目录结构如下： 12345jincheng:jobjar jincheng.sunjc$ tree -L 2.└── META-INF └── services2 directories, 0 files class文件合并将csv jdbc kafka mysql的class直接copy到jobjar目录，如下： 123456789101112$ cp -rf ../csv/org .$ cp -rf ../jdbc/org .$ cp -rf ../kafka/org .$ cp -rf ../mysql/com .$ tree -L 2 . ├── META-INF │ └── services ├── com │ └── mysql └── org └── apache 详细的目录结构如下： services合并Service Provider是JAR的一个标准，不仅仅Flink的Connector使用了Service Provider机制，同时Kafka使用了配置的服务发现。所以我们要将所有的services里面的内容按文件名进行合并。以csv和kafka为例： 在CSV里面的META-INF/services里面只有一个和Flink的connector相关的配置，内容如下： 在Kafka里面的META-INF/services里面有Flink的connector相关的配置和Kafka内部使用的配置，内容如下： 所以我们需要将Kafka相关的直接copy到jobjar/META-INF/services/目录，然后将csv和Kafka关于org.apache.flink.table.factories.TableFactory的配置进行内容合并。合并的内容如下： 1234567# Licensed to the Apache Software Foundation (ASF) under......# limitations under the License.org.apache.flink.formats.csv.CsvRowFormatFactoryorg.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactory 我们最终将4个JARs的services配置进行合并之后的最终代码如下： 12345678# Licensed to the Apache Software Foundation (ASF) under......# limitations under the License.org.apache.flink.formats.csv.CsvRowFormatFactoryorg.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryorg.apache.flink.api.java.io.jdbc.JDBCTableSourceSinkFactory 大家可以尝试使用的命令如下： 1234$ cat ../csv/META-INF/services/org.apache.flink.table.factories.TableFactory | grep ^[^#] &gt;&gt; META-INF/services/org.apache.flink.table.factories.TableFactory$ cat ../kafka/META-INF/services/org.apache.flink.table.factories.TableFactory | grep ^[^#] &gt;&gt; META-INF/services/org.apache.flink.table.factories.TableFactory$ cat ../kafka/META-INF/services/org.apache.flink.kafka.shaded.org.apache.kafka.common.config.provider.ConfigProvider | grep ^[^#] &gt;&gt; META-INF/services/org.apache.flink.kafka.shaded.org.apache.kafka.common.config.provider.ConfigProvider$ cat ../jdbc/META-INF/services/org.apache.flink.table.factories.TableFactory | grep ^[^#] &gt;&gt; META-INF/services/org.apache.flink.table.factories.TableFactory 创建JAR这一步骤没有特别强调的内容，直接用用zip或者jar命令进行打包就好了。 1$ jincheng:jobjar jincheng.sunjc$ jar -cf myjob.jar META-INF com org 我最终产生的JAR可以在这里下载，用于对比你自己打包的是否和我的一样：） OK，到这里我们就完成了多JARs的合并工作。我们可以尝试应用CLI进行提交命令了。 CLI提交作业 启动集群（我修改了flink-conf，将端口更改到4000了） 1234/usr/local/lib/python3.7/site-packages/pyflink/bin/start-cluster.sh localStarting cluster.Starting standalonesession daemon on host jincheng.local.Starting taskexecutor daemon on host jincheng.local. 提交作业当没有添加-j选项时候，提交作业如下：1$PYFLINK_LIB/../bin/flink run -m localhost:4000 -py cdn_demo.py 报错如下： 提供正确的-j参数，将我们打包的JAR提交到集群的情况，如下： 1$PYFLINK_LIB/../bin/flink run -j ~/temp/jobjar/myjob.jar -m localhost:4000 -py cdn_demo.py 同时Web控制台可以查看提交的作业： 小结本篇核心介绍了PyFlink的用户如何解决多JARs依赖作业提交问题，也许这不是最Nice的解决方法，但至少是你解决多JARs依赖作业提交的方法之一，祝你 “扫雷” 顺利，也期望如果你有更好的解决办法，留言或者邮件与我分享哦：）！]]></content>
      <categories>
        <category>Apache Flink 扫雷系列</category>
      </categories>
      <tags>
        <tag>PyFlink</tag>
        <tag>扫雷</tag>
        <tag>多JAR包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyFlink 场景案例 - PyFlink实现CDN日志实时分析]]></title>
    <url>%2F2020%2F03%2F27%2FPyFlink%20%E5%9C%BA%E6%99%AF%E6%A1%88%E4%BE%8B-PyFlink%E5%AE%9E%E7%8E%B0CDN%E6%97%A5%E5%BF%97%E5%AE%9E%E6%97%B6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[放空自己三十辐共一毂，当其无，有车之用。埏埴以为器，当其无，有器之用。凿户牖以为室，当其无，有室之用。— 深悟 “空” 之道。 CDN 日志实时分析综述CDN将源站资源缓存至遍布全球的加速节点上，当终端用户请求获取该资源时，无需回源，系统自动调用离终端用户最近的CDN节点上已缓存的资源，那么如何进行实时日志分析呢？ 架构CDN日志的解析一般有一个通用的架构模式，就是首先要将各个边缘节点的日志数据进行采集，一般会采集到消息队列，然后将消息队列和实时计算集群进行集成进行实时的日志分析，最后将分析的结果写到存储系统里面。那么我今天的案例将架构实例化，消息队列采用Kafka，实时计算采用Flink，最终将数据存储到MySql中。如下图所示： 需求说明阿里云实际的CDN日志数据结构如下（可能会不断丰富字段信息）： 为了介绍方便，我们将实际的统计需求进行简化，示例将从CDN访问日志中，根据IP解析出其所属的地区，统计指标： 按地区统计资源访问量 按地区统计资源下载总量 按地区统计资源平均下载速度 CDN实时日志分析UDF定义这里我们需要定义一个 ip_to_province()的UDF，输入是ip地址，输出是地区名字字符串。UDF的输入类型是一个字符串，输出类型也是一个字符串。同时我们会用到地理区域查询服务（http://whois.pconline.com.cn/ipJson.jsp?ip=27.184.139.25)，大家在自己的生产环境要替换为可靠的地域查询服务。 123456789101112131415161718192021222324252627282930313233343536import reimport jsonfrom pyflink.table import DataTypesfrom pyflink.table.udf import udffrom urllib.parse import quote_plusfrom urllib.request import urlopen@udf(input_types=[DataTypes.STRING()], result_type=DataTypes.STRING())def ip_to_province(ip): &quot;&quot;&quot; format: &#123; &apos;ip&apos;: &apos;27.184.139.25&apos;, &apos;pro&apos;: &apos;河北省&apos;, &apos;proCode&apos;: &apos;130000&apos;, &apos;city&apos;: &apos;石家庄市&apos;, &apos;cityCode&apos;: &apos;130100&apos;, &apos;region&apos;: &apos;灵寿县&apos;, &apos;regionCode&apos;: &apos;130126&apos;, &apos;addr&apos;: &apos;河北省石家庄市灵寿县 电信&apos;, &apos;regionNames&apos;: &apos;&apos;, &apos;err&apos;: &apos;&apos; &#125; &quot;&quot;&quot; try: urlobj = urlopen( \ &apos;http://whois.pconline.com.cn/ipJson.jsp?ip=%s&apos; % quote_plus(ip)) data = str(urlobj.read(), &quot;gbk&quot;) pos = re.search(&quot;&#123;[^&#123;&#125;]+\&#125;&quot;, data).span() geo_data = json.loads(data[pos[0]:pos[1]]) if geo_data[&apos;pro&apos;]: return geo_data[&apos;pro&apos;] else: return geo_data[&apos;err&apos;] except: return &quot;UnKnow&quot; 数据读取和结果写入定义按照通用的作业结构，需要定义Source connector来读取Kafka数据，定义Sink connector来将计算结果存储到MySQL。最后是编写统计逻辑。在这特别说明一下，在PyFlink中也支持SQL DDL的编写，我们用一个简单的DDL描述，就完成了Source Connector的开发。其中connector.type填写kafka。SinkConnector也一样，用一行DDL描述即可，其中connector.type填写jdbc。 Kafka 数据源读取DDL定义数据统计需求我们只选取核心的字段，比如：uuid，表示唯一的日志标示，client_ip表示访问来源，request_time表示资源下载耗时，response_size表示资源数据大小。Kafka数据字段进行简化如下： DDL定义如下： 1234567891011121314151617kafka_source_ddl = &quot;&quot;&quot;CREATE TABLE cdn_access_log ( uuid VARCHAR, client_ip VARCHAR, request_time BIGINT, response_size BIGINT, uri VARCHAR) WITH ( &apos;connector.type&apos; = &apos;kafka&apos;, &apos;connector.version&apos; = &apos;universal&apos;, &apos;connector.topic&apos; = &apos;access_log&apos;, &apos;connector.properties.zookeeper.connect&apos; = &apos;localhost:2181&apos;, &apos;connector.properties.bootstrap.servers&apos; = &apos;localhost:9092&apos;, &apos;format.type&apos; = &apos;csv&apos;, &apos;format.ignore-parse-errors&apos; = &apos;true&apos;)&quot;&quot;&quot; MySql 数据写入DDL定义其中我们发现我们需求是按地区分组，但是原始日志里面并没有地区的字段信息，所以我们需要定义一个Python UDF 更具 client_ip 来查询对应的地区。所以我们对应的MySqL统计结果表如下：DDL定义如下： 123456789101112131415mysql_sink_ddl = &quot;&quot;&quot;CREATE TABLE cdn_access_statistic ( province VARCHAR, access_count BIGINT, total_download BIGINT, download_speed DOUBLE) WITH ( &apos;connector.type&apos; = &apos;jdbc&apos;, &apos;connector.url&apos; = &apos;jdbc:mysql://localhost:3306/Flink&apos;, &apos;connector.table&apos; = &apos;access_statistic&apos;, &apos;connector.username&apos; = &apos;root&apos;, &apos;connector.password&apos; = &apos;123456&apos;, &apos;connector.write.flush.interval&apos; = &apos;1s&apos;)&quot;&quot;&quot; 核心统计逻辑我们首先要将client_ip转换为地区名字，然后在做数据统计，如下核心统计逻辑： 12345678910111213# 核心的统计逻辑t_env.from_path(&quot;cdn_access_log&quot;)\ .select(&quot;uuid, &quot; &quot;ip_to_province(client_ip) as province, &quot; # IP 转换为地区名称 &quot;response_size, request_time&quot;)\ .group_by(&quot;province&quot;)\ .select( # 计算访问量 &quot;province, count(uuid) as access_count, &quot; # 计算下载总量 &quot;sum(response_size) as total_download, &quot; # 计算下载速度 &quot;sum(response_size) * 1.0 / sum(request_time) as download_speed&quot;) \ .insert_into(&quot;cdn_access_statistic&quot;) 完整的作业代码我们整体看一遍完整代码结构，首先是核心依赖的导入，然后是我们需要创建一个ENV，并设置采用的planner（目前Flink支持Flink和blink两套planner）建议大家采用 blink planner。 接下来将我们刚才描述的kafka和mysql的ddl进行表的注册。再将Python UDF进行注册，这里特别提醒一点，UDF所依赖的其他文件也可以在API里面进行制定，这样在job提交时候会一起提交到集群。然后是核心的统计逻辑，最后调用executre提交作业。这样一个实际的CDN日志实时分析的作业就开发完成了。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import osfrom pyFlink.datastream import StreamExecutionEnvironmentfrom pyflink.table import StreamTableEnvironment, EnvironmentSettingsfrom enjoyment.cdn.cdn_udf import ip_to_provincefrom enjoyment.cdn.cdn_connector_ddl import kafka_source_ddl, mysql_sink_ddl# 创建Table Environment， 并选择使用的Plannerenv = StreamExecutionEnvironment.get_execution_environment()t_env = StreamTableEnvironment.create( env, environment_settings=EnvironmentSettings.new_instance().use_blink_planner().build())# 创建Kafka数据源表t_env.sql_update(kafka_source_ddl)# 创建MySql结果表t_env.sql_update(mysql_sink_ddl)# 注册IP转换地区名称的UDFt_env.register_function(&quot;ip_to_province&quot;, ip_to_province)# 添加依赖的Python文件t_env.add_Python_file( os.path.dirname(os.path.abspath(__file__)) + &quot;/enjoyment/cdn/cdn_udf.py&quot;)t_env.add_Python_file(os.path.dirname( os.path.abspath(__file__)) + &quot;/enjoyment/cdn/cdn_connector_ddl.py&quot;)# 核心的统计逻辑t_env.from_path(&quot;cdn_access_log&quot;)\ .select(&quot;uuid, &quot; &quot;ip_to_province(client_ip) as province, &quot; # IP 转换为地区名称 &quot;response_size, request_time&quot;)\ .group_by(&quot;province&quot;)\ .select( # 计算访问量 &quot;province, count(uuid) as access_count, &quot; # 计算下载总量 &quot;sum(response_size) as total_download, &quot; # 计算下载速度 &quot;sum(response_size) * 1.0 / sum(request_time) as download_speed&quot;) \ .insert_into(&quot;cdn_access_statistic&quot;)# 执行作业t_env.execute(&quot;pyFlink_parse_cdn_log&quot;) 环境搭建（MacOS）安装MySQL1234567891011121314$ brew install mysqlUpdating Homebrew...==&gt; Auto-updated Homebrew!......MySQL is configured to only allow connections from localhost by defaultTo connect run: mysql -urootTo have launchd start mysql now and restart at login: brew services start mysqlOr, if you don&apos;t want/need a background service you can just run: mysql.server start 如果没有安装brew，执行以下指令安装: 1$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装完成后，执行安全配置，将root用户的密码设置为JxXX&amp;&amp;l2j2#: 1$ mysql_secure_installation 启动mysql服务： 1234567$ brew services start mysql==&gt; Tapping homebrew/servicesCloning into &apos;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-services&apos;.........==&gt; Successfully started `mysql` (label: homebrew.mxcl.mysql) 登录mysql并创建flink数据库和cdn_access_statistic表，并确认开放端口号为默认的3306： 123456789101112$ mysql -uroot -pEnter password: JxXX&amp;&amp;l2j2#mysql&gt; use flink;Database changedmysql&gt; CREATE TABLE cdn_access_statistic( -&gt; province VARCHAR(255) PRIMARY KEY, -&gt; access_count BIGINT, -&gt; total_download BIGINT, -&gt; download_speed DOUBLE -&gt; ) ENGINE=INNODB CHARSET=utf8mb4;Query OK, 0 rows affected (0.01 sec)exit 这样mysql的环境就准备好了。 安装Kafka在Mac系统安装Kafka也非常方便，如下： 123456789101112131415brew install kafkaUpdating Homebrew...==&gt; Installing dependencies for kafka: zookeeper......==&gt; Summary/usr/local/Cellar/zookeeper/3.5.7: 394 files, 11.3MB==&gt; Installing kafka......==&gt; kafkaTo have launchd start kafka now and restart at login: brew services start kafkaOr, if you don&apos;t want/need a background service you can just run: zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties 安装完成后执行以下指令启动zookeeper和kafka： 12345$ cd /usr/local/opt/kafka/libexec/bin/$ ./zookeeper-server-start.sh ../config/zookeeper.properties &amp;$ (回车)$ ./kafka-server-start.sh ../config/server.properties &amp;$ (回车) 然后在kafka中创建名为cdn_access_log的topic: 123$ kafka-topics --create --replication-factor 1 --partitions 1 --topic cdn_access_log --zookeeper localhost:2181...Created topic cdn_access_log. 这样kafka的环境也准备好了. 安装PyFlink确保使用Python3.5+的版本，检验如下： 12$ python --versionPython 3.7.6 说过pip install安装PyFlink 1234python -m pip install apache-flink==1.10.0......Successfully installed apache-beam-2.15.0 dill-0.2.9 pyarrow-0.14.1 检查我们是否成功安装了PyFlink和依赖的Beam，如下： 123$ python -m pip list |grep apache-apache-beam 2.15.0 apache-flink 1.10.0 如上显示说明已经完成安装。 Copy使用的Connector的JAR包Flink默认是没有打包connector的，所以我们需要下载各个connector所需的jar包并放入PyFlink的lib目录。首先拿到PyFlink的lib目录的路径： 123$ PYFLINK_LIB=`python -c &quot;import pyflink;import os;print(os.path.dirname(os.path.abspath(pyflink.__file__))+&apos;/lib&apos;)&quot;`$ echo $PYFLINK_LIB/usr/local/lib/python3.7/site-packages/pyflink/lib 然后想需要的jar包下载到lib目录中去： 12345$ cd $PYFLINK_LIB$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.11/1.10.0/flink-sql-connector-kafka_2.11-1.10.0.jar$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-jdbc_2.11/1.10.0/flink-jdbc_2.11-1.10.0.jar$ curl -O https://repo1.maven.org/maven2/org/apache/flink/flink-csv/1.10.0/flink-csv-1.10.0-sql-jar.jar$ curl -O https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.19/mysql-connector-java-8.0.19.jar 最终lib的JARs如下： 1234567891011121314$ ls -latotal 310448drwxr-xr-x 12 jincheng.sunjc admin 384 3 27 15:48 .drwxr-xr-x 28 jincheng.sunjc admin 896 3 24 11:44 ..-rw-r--r-- 1 jincheng.sunjc admin 36695 3 27 15:48 flink-csv-1.10.0-sql-jar.jar-rw-r--r-- 1 jincheng.sunjc admin 110055308 2 27 13:33 flink-dist_2.11-1.10.0.jar-rw-r--r-- 1 jincheng.sunjc admin 89695 3 27 15:48 flink-jdbc_2.11-1.10.0.jar-rw-r--r-- 1 jincheng.sunjc admin 2885638 3 27 15:49 flink-sql-connector-kafka_2.11-1.10.0.jar-rw-r--r-- 1 jincheng.sunjc admin 22520058 2 27 13:33 flink-table-blink_2.11-1.10.0.jar-rw-r--r-- 1 jincheng.sunjc admin 19301237 2 27 13:33 flink-table_2.11-1.10.0.jar-rw-r--r-- 1 jincheng.sunjc admin 489884 2 27 13:33 log4j-1.2.17.jar-rw-r--r-- 1 jincheng.sunjc admin 2356711 3 27 15:48 mysql-connector-java-8.0.19.jar-rw-r--r-- 1 jincheng.sunjc admin 20275 12 5 17:01 pyflink-demo-connector-0.1.jar-rw-r--r-- 1 jincheng.sunjc admin 9931 2 27 13:33 slf4j-log4j12-1.7.15.jar 这样所有的环境就准备好了。 本地运行作业 启动本地集群1$PYFLINK_LIB/../bin/start-cluster.sh local 启动成功后可以打开web界面（默认8081端口，我更改为4000） 提交作业示例涉及到如下三个py文件： cdn_demo.py cdn_connector_ddl.py cdn_udf.py 我们在作业文件所在目录进行执行： 12$ $PYFLINK_LIB/../bin/flink run -m localhost:4000 -py cdn_demo.pyJob has been submitted with JobID e71de2db44fd6cfb9bae93bd04ee2ec9 查看控制台如下： 目前我们已经成功的将作业启动起来了，接下来我们向Kafka里面进行数据的写入。 Mock 数据我们对CDN数据进行Mock，并进行统计可视化，工具代码请下载 进行源码编译 12$ python setup.py sdist$ sudo python -m pip install dist/* 运行工具 123456789$ start_dashboard.py --------------------------------------environment config--------------------------------------Target kafka port: localhost:9092Target kafka topic: cdn_access_logTarget mysql://localhost:3306/flinkTarget mysql table: cdn_access_statisticListen at: http://localhost:64731----------------------------------------------------------------------------------------------To change above environment config, edit this file: /usr/local/bin/start_dashboard.py 打开可视化界面： （根据命令行提示的 Listen at：后面的地址） 小结本篇是场景案例系列，开篇分享了老子教导我们要学会放空自己，空杯才能注水。全篇围绕阿里云CDN日志实时分析需求进行展开，描述了任何利用PyFlink解决实际业务问题。最后又为大家提供了示例的数据的Mock工具和可视化统计页面，希望对大家有所帮助。]]></content>
      <categories>
        <category>PyFlink 场景案例</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>PyFlink</tag>
        <tag>场景案例</tag>
        <tag>CDN日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 视频系列 - 必修课！一小时吃透PyFlink]]></title>
    <url>%2F2020%2F03%2F16%2FApache-Flink-%E8%A7%86%E9%A2%91%E7%B3%BB%E5%88%97-%E5%BF%85%E4%BF%AE%E8%AF%BE%EF%BC%81%E4%B8%80%E5%B0%8F%E6%97%B6%E5%90%83%E9%80%8FPyFlink%2F</url>
    <content type="text"><![CDATA[序本文整理于 Apache Flink PMC/Apache Beam Committer 孙金城（金竹）的《PyFlink 架构、应用案例及未来规划》直播分享。详见：http://dwz.date/QFx 图1 开场大家晚上好，很开心以直播的方式与大家进行第二次PyFlink的分享。这里可能有些朋友是第一次看我直播，所以还是先简单的自我介绍一下。我叫孙金城，来自阿里巴巴，淘宝花名金竹，平时喜欢写一些和Flink相关的技术博客。早在2016我就被Apache Flink的纯流式架构所吸引，目前是Apache Flink的PMC 成员。近年来各大互联网公司的大数据部门都有要建设统一用户接口，透明切换底层大数据计算引擎的平台需求，这一点上Apache Beam做的非常优秀，经过不断的贡献，目前我也是Apache Beam的Committer。随着5G的到来，IoT领域似乎要迎来一个崭新的春天，IoT与流计算的融合势在必行，目前我也是Apache IoTDB的Committer。作为一个开源爱好者，除了对各个开源项目进行贡献，我也喜欢参加ASF组织的各种活动，目前也是ASF Beijing的Member。那么在自我介绍的同时也与大家分享了我对开源项目选择性参与和对知识获取的思考。 图2 分享目标我希望通过今天的分享让大家了解到Python用户如何使用Flink现有的所有功能，同时和大家介绍Python生态如何利用Flink具备分布式的数据处理能力。那么以怎样的方式让大家了解到这些呢？我将以4个方面进行今天的分享，第一部分 主要介绍PyFlink的深层含义，并探讨存在的意义。第二部分，将细致分析PyFlink现有架构和架构背后的思考。 第三部分 将介绍PyFlink能应用在怎样的业务场景中，并结合CDN的业务场景让大家对PyFlink的业务开发有很好的体感认知。最后一部分，我将与大家分享PyFlink未来的重点功能规划和承载的使命愿景。我认为任何事物长久的，可持续发展，都需要有自身灵魂所在，都需要树立正确的使命愿景，就像阿里巴巴以让天下没有难做的生意为使命，以活102年，并到2036年，服务20亿消费者，创造1亿就业机会和帮助1000万中小企业为愿景一样，定义和寻找PyFlink的使命愿景对PyFlink的持续发展至关重要，我们在最后大家一起讨论。 图3 PyFlink 是啥？必要性？ 图4 Flink on Python and Python on Flink那么PyFlink是什么？这个问题也许会让人感觉问题的答案太明显了，那就是Flink + Python，也就是Flink on Python。那么到底Flink on Python 意味着这什么呢？那么一个非常容易想到的方面就是能够让Python用户享受到Flink的所有功能。其实不仅如此，PyFlink的存在还有另外一个非常重要的意义就是，Python on Flink，我们可以将Python丰富的生态计算能力运行在Flink框架之上，这将极大的推动Python生态的发展。其实，如果你在仔细深究一下，你会发现这个结合并非偶然。 图5 Python生态和大数据生态Pythoh生态与大数据生态有密不可分的关系，我们先看看大家都在用Python解决什么实际问题？通过一份用户调查我们发现，大多数Python用户正在解决 ”数据分析“，”机器学习“的问题，那么这些问题场景在大数据领域也有很好的解决方案。那么Python生态和大数据生态结合，抛开扩大大数据产品的受众用户之外，对Python生态一个特别重要到意义就是 单机到分布式的能力增强，我想，这也是大数据时代海量数据分析对Python生态的强需求。 图6 Why Flink and Why Python好了，Python生态和大数据的结合是时代的要求，那么Flink为啥选择Python生态作为多语言支持的切入点，而不是Go或者R呢？作为用户的你，为啥选择PyFlink而不是PySpark或者PyHive呢？首先我们说说选择Flink的理由： 第一，最主要的是架构优势，Flink是纯流架构的流批统一的计算引擎； 第二，从ASF的客观统计看，Flink是2019年度最活跃的开源项目，这意味着Flink鲜活的生命力； 第三，Flink不仅仅是开源项目而且也经历过无数次，各个大数据公司的生产环境的历练，值得信赖！ 那么我再来看看Flink在选择多语言支持时候，为啥选择了Python而不是其他语言呢？我们还是看一下数据统计，如下：Python语言流行程度仅次于Java和C，其实我们发现自18年开始Python的发展非常迅速，并且还在持续。那么Java/Scala是Flink的默认语言，所以选择Python来进行Flink多语言的支持似乎很合理。这些权威的统计信息，大家可以在我提供的链接进行查看更详细的信息。 图7 目前看PyFlink的产生是时代的必然，但仅仅想清楚PyFlink存在的意义还远远不够，因为我们最终的目标是让Flink和Python用户受益，真真正正的解决实际的现实问题。所以，我们还需要继续深入，一起探究PyFlink该如何落地？ 图8 PyFlink 架构，为什么？任何事情在想清楚之后，还要做明白，要将PyFlink落地，首要解决的是分析清楚要达成的核心目标和要达成目标解决的核心问题。那么PyFlink的核心目标到底是什么呢？ 图9 PyFlink的核心目标我们在前面的分析过程中已经提到过，这里我们再具体化一下，PyFlink的核心目标就是： 第一，将Flink能力输出到Python用户，进而可以让Python用户使用所有的Flink能力。 第二，将Python生态现有的分析计算功能运行到Flink上，进而增强Python生态对大数据问题的解决能力。 围绕这2个核心的目标，我们再来分析，要达成这样的目标，需要解决的核心问题是什么？ 图10 Flink功能Python化为了PyFlink落地，我们需要在Flink上开发一套和现有Java一样的Python的引擎吗？答案是 NO，这在Flink1.8之前已经尝试过。我们做设计有一个很好的原则就是追求以最小的代价完成既定的目标，所以最好的方式是仅仅提供一层Python API 复用现有的计算引擎。那么对于Flink而言我们要提供怎样的Python API呢？那就是我们熟知的： High-level的TableAPI/SQL； 有状态的DataStream API； 好，我们现在的思考越来越切近Flink内部了，接踵而来的问题就是，我们如何提供一套Python的Table API和DataStream API呢？核心要解决的问题是什么呢？ 图11 Flink功能Python化的核心问题核心问题显而易见是Python VM和Java VM的握手，他们之间要建立通讯，这是Flink多语言支持的核心问题。好，面对核心问题我们要进行技术选型. Here we go… 图12 Flink功能Python化的VM通讯技术选型就当前的Java VM和Python VM通讯的问题而言，目前比较显著的解决方案有Apache Beam，一个著名的多语言多引擎支持项目，另外一个专门解决Java VM和Python VM通讯问的的Py4J。我们从不同视角进行分析对比， 首先，Py4J和Beam对比，就好像有穿山功能的穿山甲和一个力量强大的大象，要穿越一到墙，我们可以打个洞，也可以推到整面墙。所以在当前VM通讯的场景，Beam显得有些复杂。因为Beam在通用性上做了很多的努力，在极端情况会丧失一定程度的灵活性。 图13 从另一个视角来看，Flink本身有交互式编程的需求，比如FLIP-36，同时还要在多语言支持的同时，确保各种语言的接口设计语义一致性，这些在Beam现有的架构下很难满足。所以在这样一种思考下，我们选择Py4J作为Java VM和Python VM之间通讯的桥梁。 图14 Flink功能Python化的技术架构其实如果我们解决了Python VM和Java VM通讯的问题，本质上是在努力达成我们第一个目标，就是将现有Flink功能输出给Python用户，也就是我们Flink1.9所完成的工作，接下来我们看看Flink1.9 PyFlink API 的架构，如下：我们利用Py4J解决通讯问题，在PythonVM启动一个Gateway，并且Java VM启动一个Gateway Server用于接受Python 的请求，同时在Python API里面提供和Java API一样的对象，比如 TableENV， Table，等等。这样Python在写Python API的时候本质是在调用Java API。当然，在Flink 1.9中还解决了作业部署问题，我们可以用Python命令，Python shell和CLI等多种方式进行作业提交。 图15 那么基于这样的架构又怎样的优势呢？第一个就是简单，并确保Python API语义和Java API的一致性，第二点，Python 作业可以达到和Java一样的极致性能，那么Java的性能怎样呢？我想大家已经熟知，在去年双11 Flink Java API已经具备了每秒25.51亿次的数据处理的能力。 图16 Python生态分布化OK，在完成了现有Flink功能向Python用户的输出之后，接下来我们继续探讨，如何将Python生态功能引入Flink中，进而将Python 功能分布式化。如何达成？通常我们可以有如下2种做法： 1. 选择有代表性的Python类库，将其API增加到PyFlink中，这种方式是一个漫长的过程，因为Python的生态库太多了，但无论如何，我们在引入这些APIs之前，首要解决的问题是，解决Python的执行问题。我们结合现有Flink Table API的现状和现有Python类库的特点，我们可以对现有所有的Python类库功能视为 用户自定义函数（UDF），集成到Flink中。这样我们就找到了集成Python生态到Flink中的手段是将其视为UDF，也就是我们Flink1.10中的工作。那么集成的核心问题是什么？没错，刚才说过，是Python UDF的执行问题。好，我们针对这个核心问题进行技术选型吧，Here we go… 图17 Python生态分布化的UDF执行技术选型解决Python UDF执行问题可不仅仅是VM之间通讯的问题了，它涉及到Python执行环境的管理，业务数据在Java和Python之间的解析，Flink State Backend能力向Python的输出，Python UDF执行的监控等等，是一个非常复杂的问题。面对这样复杂的问题，前面我们介绍过Apache Beam，支持多引擎多语言，无所不能的大象可以出场了，我们来看一下Beam是怎么解决Python UDF执行问题的 ：）Beam为了解决多语言和多引擎支持问题高度抽象了一个叫 Portability Framework 的架构，如下图，Beam目前可以支持Java/Go/Python等多种语言，其中图下方 Beam Fu Runners 和 Execution之间就解决了 引擎和UDF执行环境的问题。其核心是对利用Protobuf进行数据结构抽象，利用gRPC协议进行通讯，同时封装了核心的gRPC 服务。所以这时候Beam更像是一只萤火虫，照亮了PyFlink解决UDF执行问题之路。：）（多说一嘴，萤火虫已经成为了Aapche Beam的吉祥物）。我们接下来看看Beam到底提供了哪些gRPC服务。 图18 如图 Runner部分是Java的算子执行，SDK Worker部分是Python的执行环境，Beam已经抽象Control/Data/State/Logging等服务。并这些服务已经在Beam的Flink runner上稳定高效的运行了很久了。所以在PyFlink UDF执行上面我们可以站在巨人的肩膀上了：），这里我们发现Apache Beam 在API层面和在UDF的执行层面都有解决方案，而PyFlink在API层面采用了Py4J解决VM通讯问题，在UDF执行需求上采用了Beam的Protability Framework解决UDF执行环境问题。这也表明了PyFlink在技术选型上严格遵循以最小的代价达成既定目标的原则，在技术选型上永远会选择最合适的，最符合PyFlink长期发展的技术架构。（BTW，与Beam的合作过程中，我也向Beam社区提交了20+的优化patch）。 图19 Python生态分布化的UDF技术架构OK，我们再整体看一下 PyFlink UDF的整体架构。在UDF的架构中我们我既要考虑Java VM和Python VM的通讯问题，又要考虑在编译阶段和在运行阶段的不同需求。图中我们以绿色表示Java VM的行为，蓝色表示Python VM的行为。首先我们看看编译阶段，也就是local的设计，在local的设计是纯API的mapping调用，我们仍然要过Py4J来解决通讯问题，也就是如图Python每执行一个API就会同步的调用Java所对应的API。对UDF的支持上，需要添加UDF注册的API，register_function，但仅仅是注册还不够，用户在自定义Python UDF的时候往往会依赖一些三方库，所以我们还需要增加添加依赖的方法，那就是一系列的add方法，比如add_Python_file()。再编写Python作业的同时，Java API也会同时被调用在提交作业之前，Java端会构建. 图20 JobGraph。然后通过CLI等多种方式将作业提交到集群进行运行。我们再来看看运行时Python和Java的不同分工情况，首先在Java端与普通Java作业一样，JobMaster将作业分配给TaskManger，TaskManager会执行一个个Task，task里面就涉及到了Java和Python的算子执行。在Python UDF的算子中我们会设计各种gRPC服务来完成Java VM和Python VM的各种通讯，比如 DataService 完成业务数据通讯，StateService完成Python UDF对Java Statebackend的调用，当然还有Logging和Metrics等其他服务。这些服务都是基于Beam的Fn API来构建的，最终在Python的Worker里面运行用户的UDF，运行结束之后再利用对应的gRPC服务将结果返回给Java端的PythonUDF算子。当然Python的worker不仅仅是Process模式，可以是Docker模式甚至是External的服务集群。这种扩展机制，为后面PyFlink与Python生态的其他框架集成打下了坚实的基础，在后面我们介绍PyFlink大图的时候，我们会介绍这一点：）。好，这就是PyFlink 在1.10中引入Python UDF支持的架构。那么这样的架构有怎样的优势呢？ 首先，Beam是一个成熟的多语言支持框架，基于Beam进行架构我们后面可以很容易进行其他语言的支持扩展。 同时Beam对State的服务抽象也方便PyFlink增加对Stateful UDF的支持。还有一个方面是方便维护，同一套框架由Apache Beam和Apache Flink两个非常活跃的社区共同维护和优化 … 图21 PyFlink 场景，怎么用？好了解了这么多关于PyFlink的架构和架构背后的思考，我们还是以一个具体场景案例，来增加一些对PyFlink的体感吧！ 图22 PyFlink 适用的场景好，在具体的案例之前我们先简单分享一些PyFlink所能适用的业务场景。首先PyFlink既然是Python+Flink，那其适用场景也可以从java和Python两方面去分析，第一个Java所适用的场景PyFlink都适用： 第一个，事件驱动型，比如：刷单，监控等； 第二个，数据分析型的，比如：库存，双11大屏等； 第三个，数据管道，也就是ETL场景，比如一些日志的解析等； 第四个，机器学习，比如个性推荐等； 第五个，科学计算类； 这些都可以尝试使用PyFlink来完成。除此之外还有Python生态特有的场景，比如科学计算等。那么这么多的应用场景，PyFlink到底有哪些可用的API呢？ 图23 PyFlink的安装使用具体的API开发之前，首先要安装PyFlink，目前PyFlink支持pip install 进行安装，这里特别提醒一下具体命令是：pip install apache-Flink。 图24 PyFlink的APIs目前PyFlink API完全与Java Table API对齐，各种关系操作都支持，同时对window也有很好的支持，并且这里稍微提一下就是PyFlink里面有些易用性API比SQL还要强大，比如：这些对columns进行操作的APIs。除了这些APIs，PyFlink还提供多种定义Python UDF的方式。 图25 PyFlink的UDF定义首先，可以扩展ScalarFunction，这种方式可以提供更多的辅助功能，比如添加Metrics。除此之外Python语言所支持的任何方式的方法定义，在PyFlink UDF中都是支持的，比如：Lambda Function，Named Function和CallableFunction等。当定义完方法后，我们用PyFlink所提供的Decorators进行打标，并描述input和output的数据类型就可以了。当然后面版本我们也可以根据Python语言的type hint特性再进一步简化，进行类型推导。为了直观，我们看一个具体的UDF定义的例子： 图26 Python UDF定义示例我们定义两个数相加的例子，首先导入必须的类，然后是刚才我们提到的几种定义方式。这个简单直接，我们闲话少叙，开始看看实际的案例吧：） 图27 PyFlink案例-阿里云CDN实时日志分析我们这里以一个阿里云CDN实时日志分析的例子来介绍如何用PyFlink解决实际的业务问题。CDN我们都很熟悉，为了进行资源的下载加速。那么CDN日志的解析一般有一个通用的架构模式，就是首先要将各个边缘节点的日志数据进行采集，一般会采集到消息队列，然后将消息队列和实时计算集群进行集成进行实时的日志分析，最后将分析的结果写到存储系统里面。那么我今天的案例将架构实例化，消息队列采用Kafka，实时计算采用Flink，最终将数据存储到MySql中。 图28 阿里云CDN实时日志分析需求说明我们在来看看业务统计的需求，为了介绍方便，我们将实际的统计需求进行简化，示例中只进行按地区分组，进行资源访问量，下载量和下载速度的统计。数据格式我们只选取核心的字段，比如：uuid，表示唯一的日志标示，client_ip表示访问来源，request_time表示资源下载耗时，response_size表示资源数据大小。其中我们发现我们需求是按地区分组，但是原始日志里面并没有地区的字段信息，所以我们需要定义一个Python UDF 根据 client_ip 来查询对应的地区。好，我们首先看如何定义这个UDF。 图29 阿里云CDN实时日志分析UDF定义这里我们用了刚才提到的named function的方式定义一个 ip_to_province()的UDF，输入是ip地址，输出是地区名字字符串。我们这里描述了输入类型是一个字符串，输出类型也是一个字符串。当然这里面的查询服务仅供演示，大家在自己的生产环境要替换为可靠的地域查询服务。 图30 123456789101112131415161718192021222324252627282930313233343536import reimport jsonfrom pyFlink.table import DataTypesfrom pyFlink.table.udf import udffrom urllib.parse import quote_plusfrom urllib.request import urlopen@udf(input_types=[DataTypes.STRING()], result_type=DataTypes.STRING())def ip_to_province(ip): &quot;&quot;&quot; format: &#123; &apos;ip&apos;: &apos;27.184.139.25&apos;, &apos;pro&apos;: &apos;河北省&apos;, &apos;proCode&apos;: &apos;130000&apos;, &apos;city&apos;: &apos;石家庄市&apos;, &apos;cityCode&apos;: &apos;130100&apos;, &apos;region&apos;: &apos;灵寿县&apos;, &apos;regionCode&apos;: &apos;130126&apos;, &apos;addr&apos;: &apos;河北省石家庄市灵寿县 电信&apos;, &apos;regionNames&apos;: &apos;&apos;, &apos;err&apos;: &apos;&apos; &#125; &quot;&quot;&quot; try: urlobj = urlopen( \ &apos;http://whois.pconline.com.cn/ipJson.jsp?ip=%s&apos; % quote_plus(ip)) data = str(urlobj.read(), &quot;gbk&quot;) pos = re.search(&quot;&#123;[^&#123;&#125;]+\&#125;&quot;, data).span() geo_data = json.loads(data[pos[0]:pos[1]]) if geo_data[&apos;pro&apos;]: return geo_data[&apos;pro&apos;] else: return geo_data[&apos;err&apos;] except: return &quot;UnKnow&quot; 阿里云CDN实时日志分析Connector定义我们完成了需求分析和UDF的定义，我们开始进行作业的开发了，按照通用的作业结构，需要定义Source connector来读取Kafka数据，定义Sink connector来将计算结果存储到MySQL。最后是编写统计逻辑。在这特别说明一下，在PyFlink中也支持SQL DDL的编写，我们用一个简单的DDL描述，就完成了Source Connector的开发。其中connector.type填写kafka。SinkConnector也一样，用一行DDL描述即可，其中connector.type填写jdbc。描述connector的逻辑非常简单，我们在看看核心统计逻辑是否也一样简单：） 图31 12345678910111213141516171819202122232425262728293031323334kafka_source_ddl = &quot;&quot;&quot;CREATE TABLE cdn_access_log ( uuid VARCHAR, client_ip VARCHAR, request_time BIGINT, response_size BIGINT, uri VARCHAR) WITH ( &apos;connector.type&apos; = &apos;kafka&apos;, &apos;connector.version&apos; = &apos;universal&apos;, &apos;connector.topic&apos; = &apos;access_log&apos;, &apos;connector.properties.zookeeper.connect&apos; = &apos;localhost:2181&apos;, &apos;connector.properties.bootstrap.servers&apos; = &apos;localhost:9092&apos;, &apos;format.type&apos; = &apos;csv&apos;, &apos;format.ignore-parse-errors&apos; = &apos;true&apos;)&quot;&quot;&quot;mysql_sink_ddl = &quot;&quot;&quot;CREATE TABLE cdn_access_statistic ( province VARCHAR, access_count BIGINT, total_download BIGINT, download_speed DOUBLE) WITH ( &apos;connector.type&apos; = &apos;jdbc&apos;, &apos;connector.url&apos; = &apos;jdbc:mysql://localhost:3306/Flink&apos;, &apos;connector.table&apos; = &apos;access_statistic&apos;, &apos;connector.username&apos; = &apos;root&apos;, &apos;connector.password&apos; = &apos;root&apos;, &apos;connector.write.flush.interval&apos; = &apos;1s&apos;)&quot;&quot;&quot; 阿里云CDN实时日志分析核心统计逻辑首先从数据源读取数据，然后需要先将clien_ip 利用我们刚才定义的ip_to_province(ip)转换为具体的地区。之后，在进行按地区分组，统计访问量，下载量和资源下载速度。最后将统计结果存储到结果表中。这个统计逻辑中，我们不仅使用了Python UDF，而且还使用了Flink 内置的 Java AGG函数，sum和count。 图32 12345678910111213# 核心的统计逻辑t_env.from_path(&quot;cdn_access_log&quot;)\ .select(&quot;uuid, &quot; &quot;ip_to_province(client_ip) as province, &quot; # IP 转换为地区名称 &quot;response_size, request_time&quot;)\ .group_by(&quot;province&quot;)\ .select( # 计算访问量 &quot;province, count(uuid) as access_count, &quot; # 计算下载总量 &quot;sum(response_size) as total_download, &quot; # 计算下载速度 &quot;sum(response_size) * 1.0 / sum(request_time) as download_speed&quot;) \ .insert_into(&quot;cdn_access_statistic&quot;) 阿里云CDN实时日志分析完整代码我们在整体看一遍完整代码，首先是核心依赖的导入，然后是我们需要创建一个ENV，并设置采用的planner（目前Flink支持Flink和blink两套planner）建议大家采用 blink planner。 接下来将我们刚才描述的kafka和mysql的ddl进行表的注册。再将Python UDF进行注册，这里特别提醒一点，UDF所依赖的其他文件也可以在API里面进行制定，这样在job提交时候会一起提交到集群。然后是核心的统计逻辑，最后调用executre提交作业。这样一个实际的CDN日志实时分析的作业就开发完成了。我们再看一下实际的统计效果。 图33 12345678910111213141516171819202122232425262728293031323334353637383940414243import osfrom pyFlink.datastream import StreamExecutionEnvironmentfrom pyFlink.table import StreamTableEnvironment, EnvironmentSettingsfrom enjoyment.cdn.cdn_udf import ip_to_provincefrom enjoyment.cdn.cdn_connector_ddl import kafka_source_ddl, mysql_sink_ddl# 创建Table Environment， 并选择使用的Plannerenv = StreamExecutionEnvironment.get_execution_environment()t_env = StreamTableEnvironment.create( env, environment_settings=EnvironmentSettings.new_instance().use_blink_planner().build())# 创建Kafka数据源表t_env.sql_update(kafka_source_ddl)# 创建MySql结果表t_env.sql_update(mysql_sink_ddl)# 注册IP转换地区名称的UDFt_env.register_function(&quot;ip_to_province&quot;, ip_to_province)# 添加依赖的Python文件t_env.add_Python_file( os.path.dirname(os.path.abspath(__file__)) + &quot;/enjoyment/cdn/cdn_udf.py&quot;)t_env.add_Python_file(os.path.dirname( os.path.abspath(__file__)) + &quot;/enjoyment/cdn/cdn_connector_ddl.py&quot;)# 核心的统计逻辑t_env.from_path(&quot;cdn_access_log&quot;)\ .select(&quot;uuid, &quot; &quot;ip_to_province(client_ip) as province, &quot; # IP 转换为地区名称 &quot;response_size, request_time&quot;)\ .group_by(&quot;province&quot;)\ .select( # 计算访问量 &quot;province, count(uuid) as access_count, &quot; # 计算下载总量 &quot;sum(response_size) as total_download, &quot; # 计算下载速度 &quot;sum(response_size) * 1.0 / sum(request_time) as download_speed&quot;) \ .insert_into(&quot;cdn_access_statistic&quot;)# 执行作业t_env.execute(&quot;pyFlink_parse_cdn_log&quot;) 阿里云CDN实时日志分析运行效果我们采用mock的数据向kafka发送CDN日志数据，右边实时的按地区统计资源的访问量，下载量和下载速度。这个示例的mock数据工具，源代码和操作过程，在今天的直播后，会更新到我的博客当中。方便大家在自己的环境中进行体验。 图34 PyFlink 未来，会怎样？总体来看PyFlink的业务开发还是非常简洁的，不用关心底层的实现细节，只需要按照SQL或者Table API的方式描述业务逻辑就行。那么，我们再整体看看PyFlink的未来会怎样呢？ 图35 PyFlink 本心驱动 RoadmapPyFlink的发展始终要以本心驱动，我们要围绕将现有Flink功能输出到Python用户，将Python生态功能集成到Flink当中为目标。PyFlink的Roadmap如图所示：首先解决Python VM和Java VM的通讯问题，然后将现有的Table API功能暴露给Python用户，提供Python Table API，这也就是Flink 1.9中所进行的工作，接下来我们要为将Python功能集成到Flink做准备就是集成Apache Beam，提供Python UDF的执行环境，并增加Python 对其他类库依赖的管理功能，为用户提供User-defined-Funciton的接口定义，支持Python UDF，这就是Flink 1.10所做的工作。为了进一步扩大Python生态的分布式功能，PyFlink将提供Pandas的Series和DataFram的支持，也就是用户可以在PyFlink中直接使用Pandas的UDF。同时为增强用户的易用性，让用户有更多的方式使用PyFlink，后续增加在Sql Client中使用Python UDF。面对Python用户的机器学习问题，增加Python 的 ML pipeline API。监控Python UDF的执行情况对，对实际的生产业务非常关键，所以PyFlink会增加Python UDF的Metric管理. 这些在Flink1.11中将与用户见面。但这些功能只是PyFlink规划的冰山一角，后续我们还要进行性能优化，图计算API，Pandas on Flink的Pandas原生API 等等。。。进而完成不断将Flink现有功能推向Python生态，将Python 生态的强大功能不断集成到Flink当中，进而完成Python生态分布化的初衷。 图36 PyFlink 1.11 预览我们快速的预览一下即将与大家见面的Flink 1.11中的PyFlink的重点内容。 功能好，我们将视角由远方拉近到Flink1.11版本PyFlink的核心功能，PyFlink会围绕着 功能，性能和易用性不断努力，在1.11在功能上会增加Pandas UDF的支持，这样Pandas生态的实用类库功能可以在PyFlink中直接使用，比如累积分布函数，CDF等。 图37 还会增加ML Pipeline API的支持，这样大家可以利用PyFlink完成一些机器学习场景的业务需求，我这里是一个使用pyFlink完成KMeans的示例。 图38 性能在性能上PyFlink也会有更多的投入，我们利用Codegen，CPython，优化序列化和反序列化的方式提高PythonUDF的执行性能，目前我们初步对1.10和1.11进行性能对比来看，1.11将比1.10有近15倍的性能提升。 图39 易用性在用户的易用性上PyFlink会在SQL DDL和SQL Client中增加对Python UDF的支持。让用户有更多的方式选择来使用PyFlink。 图40 PyFlink 大图（使命愿景）今天已经介绍了很多，比如什么是PyFlink，PyFlink的存在的意义，PyFlink API架构，UDF架构，以及架构背后的取舍和现有架构的优势，并介绍了CDN的案例，介绍了PyFlink的Roadmap，预览了Flink 1.11版本中PyFlink的重点，那么接下来还有什么呢？ 那么最后我们再来看看PyFlink的未来会怎样？在以“Flink功能Python化，Python生态分布化”的使命驱动下，PyFlink会有怎样的布局？我们快速分享一下：PyFlink是Apache Flink的一部分，涉及到Runtime层面和API层面。在这两个层面PyFlink会有怎样的发展？Runtime层面，PyFlink会构建解决Java VM和Python VM的通讯问题的gRPC通用服务，比如（Control/Data/State等）在这套框架之上会抽象出Java 的Python UDF算子，Python的执行容器构建，支持多种Python的Execution，比如 Process，Docker和External，尤其值得强调的是External以Socket的方式提供了无限的扩展能力，在后续的Python生态集成上至关重要。API层面，我们会使命驱动，将Flink上所以的API进行Python化，当然这也依托于引入Py4J的VM通讯框架之上，PyFlink会逐渐增加各种API的支持，Python Table API，UDX的接口API，ML Pipeline，DataStream，CEP，Gelly，State，等Flink所具备的Java APIs和Python生态用户的最爱Pandas APIs等。在这些API的基础之上，PyFlink还会不断的进行生态系统的集成，比如 方便用户开发的Notebook的集成，Zeppelin，Jupyter，并与阿里开源的Alink进行集成，目前PyAlink已经完全应用了PyFlink所提供的功能，后面还会和现有的AI系统平台进行集成，比如大家熟知的TensorFlow等等。所以此时我会发现使命驱动的力量会让PyFlink的生命线不断延续…当然这种生命的延续更需要更多的血液融入。这里再次强调一下PyFlink的使命：“Flink能力Python化，Python生态分布化”。目前PyFlink的核心贡献者们正以这样的使命而持续活跃在社区。 图41 PyFlink核心贡献者及问题支持在分享的最后，我想介绍一下目前PyFlink的核心贡献者。他们是： 付典，目前付典是Flink以及另外两个Apache顶级项目的Committer，在PyFlink模块做了巨大的贡献。 黄兴勃，目前专注PyFlink的UDF性能优化，曾经是阿里与安全算法挑战赛的冠军，在AI和中间件性能比赛中也有很好的成绩。 程鹤群，为大家做过多次分享，相信大家还记得他为大家带来的《Flink知识图谱》分享。 钟葳，关注PyFlink的UDF依赖管理和易用性工作，目前已经有很多的代码贡献。 孙金城， 就是我自己，全力投入PyFlink的建设。 大家后续在使用PyFlink的时候，如果有什么问题都可以联系我们中的任何一位寻求支持。 图42 当然遇到通用性问题还是建议大家邮件到Flink的用户列表和中文用户列表，这样能问题共享。当然如果你遇到特别急的个别问题，也非常欢迎您邮件到刚才介绍的小伙伴邮箱，同时，为了问题的积累和有效的分享，更期望大家遇到问题可以在Stackoverflow进行提问题。首先搜索你遇到问题是否已经被解答过，如果没有，请描述清楚，最后提醒大家要为问题打上 PyFlink的tags。这样我们及时订阅回复您问题。 图43 总结也欢迎大家订阅我的博客，我们可以有更多的线下交流。那么在最后我也简单的总结一下今天的分享，今天对深入剖析了PyFlink深层含义；介绍了PyFlink API架构是核心采用Py4J框架进行VM之间的通讯，API的设计上Python API和Java API保持语义的一致；还介绍了Python UDF架构，以集成Apache Beam 的Portability Framework的方式获取高效稳定的Python UDF的支持，并且细致分析了架构背后思考，对技术选型的取舍和现有架构的优势；然后介绍了PyFlink所适用的业务场景，并以阿里云CDN日志实时分析的案例让大家对PyFlink的使用有一定的体感；最后介绍了PyFlink的Roadmap和预览了Flink 1.11版本中PyFlink的重点，预期PyFlink 1.11相对于1.10会有15倍以上的性能提升，最后和大家一起分享了PyFlink的使命，PyFlink的使命是”Flink能力Python化，Python生态分布化”。留在最后的是提供给大家一种更有效的问题求助的方式，大家有什么问题可以随时抛给刚才向大家介绍的PyFlink小伙伴，那么这些小伙伴已经在直播群里了，接下来有什么问题，我们可以一起探讨。：）再次感谢大家参加今天分享，谢谢大家！ 图44]]></content>
      <categories>
        <category>Apache Flink 视频</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>视频，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three Min Series - How to create UDF in PyFlink 1.10]]></title>
    <url>%2F2020%2F02%2F27%2FThree-Min-Series-How-to-create-UDF-in-PyFlink-1-10%2F</url>
    <content type="text"><![CDATA[Motivation/动机Python UDF has been well supported in Apache Flink 1.10. This article takes 3 minutes to show you how to use Python UDF in PyFlink 在Apache Flink 1.10 中已经对Python UDF进行了很好的支持，本篇用3分钟时间向大家介绍如何在PyFlink中使用Python UDF。 How to defined a Python UDF in PyFlink Steps/操作步骤 Check Python Version12jincheng:videos jincheng.sunjc$ python --versionPython 3.7.6 It’s better to use Python 3.5+最好使用Python3.5+ Install PyFlink 1python -m pip install apache-flink Defined Python UDFs 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-from pyflink.table import ScalarFunction, DataTypesfrom pyflink.table.udf import udf# Extend ScalarFunctionclass Add(ScalarFunction): def eval(self, i, j): return i + jadd1 = udf(Add(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())# Named Function@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())def add2(i, j): return i + j# Lambda Functionadd3 = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())# Callable Functionclass CallableAdd(object): def __call__(self, i, j): return i + jadd4 = udf(CallableAdd(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()) Python UDF example 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: utf-8 -*-from pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import StreamTableEnvironment, DataTypesfrom pyflink.table.descriptors import FileSystem, OldCsv, Schemafrom enjoyment.three_minutes.myudfs import add1, add2, add3, add4import tempfilesink_path = tempfile.gettempdir() + &apos;/streaming.csv&apos;# init envenv = StreamExecutionEnvironment.get_execution_environment()env.set_parallelism(1)t_env = StreamTableEnvironment.create(env)# register functiont_env.register_function(&quot;add1&quot;, add1)t_env.register_function(&quot;add2&quot;, add2)t_env.register_function(&quot;add3&quot;, add3)t_env.register_function(&quot;add4&quot;, add4)t = t_env.from_elements([(1, 2, &apos;Welcome&apos;), (2, 3, &apos;To&apos;), (3, 4, &apos;PyFlink&apos;)], [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])t_env.connect(FileSystem().path(sink_path))\ .with_format(OldCsv() .field_delimiter(&apos;,&apos;) .field(&quot;add1&quot;, DataTypes.BIGINT()) .field(&quot;add2&quot;, DataTypes.BIGINT()) .field(&quot;add3&quot;, DataTypes.BIGINT()) .field(&quot;add4&quot;, DataTypes.BIGINT()) .field(&quot;b&quot;, DataTypes.BIGINT()) .field(&quot;c&quot;, DataTypes.STRING()))\ .with_schema(Schema() .field(&quot;add1&quot;, DataTypes.BIGINT()) .field(&quot;add2&quot;, DataTypes.BIGINT()) .field(&quot;add3&quot;, DataTypes.BIGINT()) .field(&quot;add4&quot;, DataTypes.BIGINT()) .field(&quot;b&quot;, DataTypes.BIGINT()) .field(&quot;c&quot;, DataTypes.STRING()))\ .register_table_sink(&quot;pyflink_sink&quot;)t.select(&quot;add1(a, b), add2(a, b), add3(a, b), add4(a, b), b, c&quot;).insert_into(&quot;pyflink_sink&quot;)t_env.execute(&quot;pyflink_udf&quot;) Run and print the result Video presentation/视频演示 视频 Q&amp;AIf you have questions, you can leave a comment or send an email: sunjincheng121@gmail.com 如有疑问，您可以发表评论或发送电子邮件：sunjincheng121@gmail.com]]></content>
      <categories>
        <category>Three Min Series</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyFlink</tag>
        <tag>UDF</tag>
        <tag>PyFlink 1.10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three Min Series - How to using PyFlink Shell]]></title>
    <url>%2F2020%2F02%2F22%2FThree-Min-Series-How-to-using-PyFlink-Shell%2F</url>
    <content type="text"><![CDATA[Three Min Series - How to using PyFlink ShellMotivation/动机Apache Flink 1.10 was just released shortly. PyFlink provides users with the most convenient way to experience-PyFlink Shell. This article takes 3 minutes to tell you how to quickly experience PyFlink. Apache Flink 1.10 刚刚发布不久，PyFlink为用户提供了一种最便捷的体验方式 - PyFlink Shell. 本篇用3分钟时间向大家介绍如何快速体验PyFlink。 How to deploy the PyFlink Job As shown in the figure above, we can deploy PyFlink jobs to MiniCluster / Standalone / Yarn or Cloud clusters. 如上图所示，我们可以将PyFlink作业部署到MiniCluster/Standalone/Yarn或者Cloud集群。 Steps/操作步骤 Check Python Version12jincheng:videos jincheng.sunjc$ python --versionPython 3.7.6 It’s better to use Python 3.5+最好使用Python3.5+ Install PyFlink 1python -m pip install apache-flink Start PyFlink Shell 1pyflink-shell.sh local Python UDF example 1234567891011121314151617181920212223242526272829303132333435363738from PyFlink.datastream import StreamExecutionEnvironmentfrom PyFlink.table import StreamTableEnvironment, DataTypesfrom PyFlink.table.descriptors import Schema, OldCsv, FileSystemfrom PyFlink.table.udf import udfenv = StreamExecutionEnvironment.get_execution_environment()env.set_parallelism(1)t_env = StreamTableEnvironment.create(env)@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())def add(i, j): from mpmath import fadd return int(fadd(1, 2))t_env.set_python_requirements(&quot;/tmp/requirements.txt&quot;, &quot;/tmp/cached_dir&quot;)t_env.register_function(&quot;add&quot;, add)t_env.connect(FileSystem().path(&apos;/tmp/input&apos;)) \ .with_format(OldCsv() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySource&apos;)t_env.connect(FileSystem().path(&apos;/tmp/output&apos;)) \ .with_format(OldCsv() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySink&apos;)t_env.from_path(&apos;mySource&apos;)\ .select(&quot;add(a, b)&quot;) \ .insert_into(&apos;mySink&apos;)t_env.execute(&quot;tutorial_job&quot;) Submit PyFlink Job to Cluster We can submit the job to the standalone cluster with the following command, and see the video demonstration for details. 我们可以用如下命令将作业提交到Standalone集群，详细情况查阅视频演示。 1pyflink-shell.sh remote &lt;hostname&gt; &lt;portnumber&gt; Video presentation/视频演示 视频 Q&amp;AIf you have questions, you can leave a comment or send an email: sunjincheng121@gmail.com 如有疑问，您可以发表评论或发送电子邮件：sunjincheng121@gmail.com]]></content>
      <categories>
        <category>Three Min Series</category>
      </categories>
      <tags>
        <tag>PyFlink</tag>
        <tag>PyFlink 1.10</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep dive how to support Python UDF in Apache Flink 1.10]]></title>
    <url>%2F2020%2F02%2F19%2FDeep-dive-how-to-support-Python-UDF-in-Apache-Flink-1-10%2F</url>
    <content type="text"><![CDATA[Deep dive how to support Python UDF in Apache Flink 1.10Background在Apache Flink 1.9版中，我们引入了PyFlink模块，支持了Python Table API。Python用户可以完成数据转换和数据分析的作业。但是，您可能会发现在PyFlink 1.9中还不支持定义Python UDFs，对于想要扩展系统内置功能的Python用户来说，这可能诸多不便。 在刚刚发布的ApacheFlink 1.10中，PyFlink添加了对Python UDFs的支持。这意味着您可以从现在开始用Python编写UDF并扩展系统的功能。此外，本版本还支持Python UDF环境和依赖管理，因此您可以在UDF中使用第三方库，从而利用Python生态丰富的第三方库资源。 Apache Flink 1.9 introduced the Python Table API, allowing developers and data engineers to write Python Table API jobs for Table Transformations and Analysis, such as Python ETL or Aggregate jobs. However, Python users faced some limitations when it came to support for Python UDFs in Apache Flink 1.9, preventing them from extending the system’s built-in functionality. In Apache Flink1.10, the community further extended the support for Python by adding Python UDFs in PyFlink. Additionally, both the Python UDF Environment and Dependency Management are now supported, allowing users to import third-party libraries in the UDFs, leveraging Python’s rich set of third-party libraries. Architecture of Python UDFs supports in Pyflink在深入了解如何定义和使用Python UDFs之前，我们将解释UDFs在PyFlink中工作的架构和背景，并提供一些有关我们底层实现的细节介绍。 Before diving into how you can define and use Python UDFs, we explain the architecture and background behind how UDFs work in PyFlink and provide some additional context about the implementation of our approach. Beam on FlinkApache Beam是一个统一编程模型框架，实现了可使用任何语言开发可以运行在任何执行引擎上的批处理和流处理作业，这得益于Beam的Portability Framework，如下图所示： Apache Beam is an advanced unified programming model implement batch and streaming data processing jobs that run on any execution engine and any languages.This benefits from the Beam Portability Framework, as shown in the following figure: Figure 1: Portability Framework 上图是Beam的 Portability Framework 的体系结构。 它描述了Beam如何支持多种语言和多种引擎的方式。 关于Flink Runner部分，我们可以说是Beam on Flink。 那么，这与PyFlink支持Python UDF有什么关系呢？ 这将接下来“Flink on Beam” 中介绍。 The above figure is the architecture of the Beam Portability Framework. It describes how the Beam supports multiple languages and multiple runners. Regarding Flink Runner, we can say that it is Beam on Flink. So what does this have to do with Python UDFs supports in PyFlink? This is covered in the next Flink on Beam section. Flink on BeamApache Flink是一个开源项目，因此，它的社区也更多地使用开源。 例如，PyFlink中对Python UDF的支持选择了基于Apache Beam这辆豪华跑车之上进行构建。:) Apache Flink is an open source project, so, its community also more embraces open source. Such as, Python UDFs support in PyFlink chose to build based on Apache Beam(A luxury sports car ). :) Figure 2: Flink on Beam PyFlink对Python UDFs的支持上，Python的运行环境管理以及Python运行环境Python VM和Java运行环境JVM的通讯至关重要。幸运的是，Apache Beam的Portability Framework完美解决了这个问题。所以才有了如下PyFlink on Beam Portability Framework的架构如下： In PyFlink’s support for Python UDFs, the management of the Python runtime environment and the communication between the Python runtime environment Python VM and the Java runtime environment JVM are critical. Fortunately, Apache Beam’s Portability Framework solves this problem perfectly. Therefore, we have the following architecture: PyFlink on Beam Portability Framework. Figure 3: PyFlink on Beam Portability Framework Beam Portability Framework 是一个成熟的多语言支持框架，框架高度抽象了语言之间的通信协议(gRPC),定义了数据的传输格式(Protobuf)，并且根据通用流计算框架所需要的组件，抽象个各种服务，比如, DataService，StateService，MetricsService等。在这样一个成熟的框架下，PyFlink可以快速的构建自己的Python算子，同时重用Apache Beam Portability Framework中现有SDK harness组件，可以支持多种Python运行模式，如：Process，Docker，etc.，这使得PyFlink对Python UDF的支持变得非常容易，在Apache Flink 1.10中的功能也非常的稳定和完整。那么为啥说是Apache Flink和Apache Beam 共同打造呢，是因为我发现目前Apache Beam Portability Framework的框架也存在很多优化的空间，所以我在Beam社区进行了优化讨论，并且在Beam 社区也贡献了30+的优化补丁。 Beam Portability Framework is a mature multi-language support framework. The framework highly abstracts the communication protocol between languages (gRPC), defines the data transmission format (Protobuf), and abstracts each component according to the components required by the general stream computing framework services, such as, DataService, StateService, MetricsService, etc. Under such a mature framework, PyFlink can quickly build its own Python operators, while reusing the existing SDK harness components in the Apache Beam Portability Framework, and can support multiple Python operating modes, such as: Process, Docker, etc., which It makes PyFlink support for Python UDF very easy, and the features in Apache Flink 1.10 are also very stable and complete. So why did it say that Apache Flink and Apache Beam were jointly built? Because there is still a lot of work for optimization in the current Apache Beam Portability Framework framework, so I conducted optimization discussions in the Beam community, and contributed to the Beam community 30+ optimized patches. Communication between JVM and Python VM由于Python UDF无法直接在JVM中运行，因此需要由Apache Flink算子在初始化时启动的Python进程来准备Python执行环境。 Python ENV服务负责启动，管理和终止Python进程。 如下图4所示，Apache Flink算子和Python执行环境之间的通信和涉及多个组件： Since Python UDFs cannot directly run in JVM, they are executed within the Python environment, in a Python Process initiated by the Apache Flink operator upon initialization. The Python environment is responsible for launching, managing and tearing down the Python Process. As illustrated in Figure 4 below, several components are involved in the communication between the Apache Flink operator and the Python execution environment: Figure 4: Communication between JVM and Python VM 环境管理服务: 负责启动和终止Python执行环境。 数据服务: 负责在Apache Flink算子和Python执行环境之间传输输入数据和接收用户UDF的执行结果。 日志服务: 是记录对用户UDF日志输出支持的机制。 它可以将用户UDF产生的日志传输到Apache Flink算子，并与Apache Flink的日志系统集成。 说明: 其中 metrics 服务 计划在 Apache Flink 1.11 进行支持。 Environment Service: Responsible for launching and destroying the Python execution environment. Data Service: Responsible for transferring the input data and the user-defined function execution results between the Apache Flink operator and the Python execution environment. Logging Service: Mechanism for logging support for user defined functions. It allows transferring log entries produced by user defined functions to the Apache Flink operator and integrates with Apache Flink’s own logging system. NOTE: Supporting metrics is currently planned for Apache Flink 1.11. 图5 描述了从Java 算子到Python进程之间初始化和执行UDF的概要流程。 Figure 5 below describes the high-level flow between initializing and executing UDFs from the Java operator to the Python process. Figure 5: High-level flow between Python VM and JVM流程可以概括为如下两部分： 初始化Python执行环境。 Python UDF Runner启动所需的gRPC服务，如数据服务、日志服务等。 Python UDF Runner另起进程并启动Python执行环境。 Python worker向PythonUserDefinedFunctionRunner进行注册。 Python UDF Runner向Python worker发送需要在Python进程中执行的用户定义函数。 Python worker将用户定义的函数转换为Beam 执行算子（注意：目前，PyFlink利用Beam的可移植性框架[1]来执行Python UDF）。 Python worker和Flink Operator之间建立gRPC连接，如数据连接、日志连接等。 处理输入元素。 Python UDF Runner通过gRPC数据服务将输入元素发送给Python worker执行。 Python用户定义函数还可以在执行期间通过gRPC日志服务和metrics服务将日志和metrics收集到Python UDF Runner。 执行结果可以通过gRPC数据服务发送到Python UDF Runner。 The high-level flow can be summarized in two parts: Initialization of the Python execution environment. The Python UDF Runner starts the gRPC services, such as the data service, logging service, etc. The Python UDF Runner launches the Python execution environment in a separate process. The Python worker registers to PythonUserDefinedFunctionRunner. The Python UDF Runner sends the user defined functions to be executed in the Python worker. The Python worker transforms the user defined functions to Beam operations. (Note that we leverage the power of Beam’s Portability Framework to execute the Python UDF.) The Python worker establishes the gRPC connections, such as the data connection, logging connection, etc. Processing of the input elements. The Python UDF Runner sends the input elements via the gRPC data service to the Python worker for execution. The Python use defined function can access the state via gRPC state service during execution. The Python user defined function can also aggregate the logging and metrics to the Python UDF Runner via the gRPC logging service and the metrics service during execution. The execution results are finally sent to the Python UDF Runner via the gRPC data service. How to use PyFlink with UDFs in Apache Flink 1.10本节将介绍用户如何定义UDF，并完整展示了如何安装PyFlink，如何在PyFlink中定义/注册/调用UDF，以及如何执行作业。 This section provides some Python user defined function (UDF) examples, Including how to install PyFlink, how to define/register/invoke UDFs in PyFlink and how to execute the job. Install PyFlink我们需要先安装PyFlink，可以通过PyPI获得，并且可以使用pip install进行便捷安装。 注意: 安装和运行PyFlink需要Python 3.5或更高版本。 Using Python in Apache Apache Flink requires installing PyFlink. PyFlink is available through PyPi and can be easily installed using pip: 1$ python -m pip install apache-Apache Flink Please note that Python 3.5 or higher is required to install and run PyFlink. Define a Python UDF除了扩展基类ScalarFunction之外，定义Python UDF的方法有很多。下面的示例显示了定义Python UDF的不同方法，该函数以BIGINT类型的两列作为输入参数，并返回它们的和作为结果。 There are many ways to define a Python scalar function, besides extending the base class ScalarFunction. The following example shows the different ways of defining a Python scalar function that takes two columns of BIGINT as input parameters and returns the sum of them as the result. Option 1: extending the base class ScalarFunction 12345class Add(ScalarFunction): def eval(self, i, j): return i + jadd = udf(Add(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()) Option 2: Python function 123@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())def add(i, j): return i + j option 3: lambda function 1add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()) option 4: callable function 12345class CallableAdd(object): def __call__(self, i, j): return i + jadd = udf(CallableAdd(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()) option 5: partial function partial_add(i, j, k):1234 return i + j + kadd = udf(functools.partial(partial_add, k=1), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()) Register a Python UDF register the Python function table_env.register_function(&quot;add&quot;, add) Invoke a Python UDF my_table.select(&quot;add(a, b)&quot;) Example Code 下面是一个使用Python UDF的完整示例。Below, you can find a complete example of using Python UDF. PyFlink.datastream import StreamExecutionEnvironment12345678910111213141516171819202122232425262728293031from PyFlink.table import StreamTableEnvironment, DataTypesfrom PyFlink.table.descriptors import Schema, OldCsv, FileSystemfrom PyFlink.table.udf import udfenv = StreamExecutionEnvironment.get_execution_environment()env.set_parallelism(1)t_env = StreamTableEnvironment.create(env)t_env.register_function(&quot;add&quot;, udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT()))t_env.connect(FileSystem().path(&apos;/tmp/input&apos;)) \ .with_format(OldCsv() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySource&apos;)t_env.connect(FileSystem().path(&apos;/tmp/output&apos;)) \ .with_format(OldCsv() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySink&apos;)t_env.from_path(&apos;mySource&apos;)\ .select(&quot;add(a, b)&quot;) \ .insert_into(&apos;mySink&apos;)t_env.execute(&quot;tutorial_job&quot;) Submit the job Firstly, you need to prepare the input data in the “/tmp/input” file. For example, 1$ echo &quot;1,2&quot; &gt; /tmp/input Next, you can run this example on the command line, 1$ python python_udf_sum.py The command builds and runs the Python Table API program in a local mini-cluster. You can also submit the Python Table API program to a remote cluster using different command lines, (see more details here). Finally, you can see the execution result on the command line: 12$ cat /tmp/output3 Python UDF dependency management在许多情况下，您可能希望在Python UDF中导入第三方依赖。下面的示例将指导您如何管理依赖项。 In many cases, you would like to import third-party dependencies in the Python UDF. The example below provides detailed guidance on how to manage such dependencies. 假设您想使用mpmath来执行上述示例中两数的和。Python UDF逻辑可能如下：Suppose you want to use the mpmath to perform the sum of the example above. The Python UDF may look like: 1234@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())def add(i, j): from mpmath import fadd # add third-party dependency return int(fadd(1, 2)) 要使其在不包含依赖项的工作节点上运行，可以使用以下API指定依赖项： To make it available on the worker node that does not contain the dependency, you can specify the dependencies with the following API: 123# echo mpmath==1.1.0 &gt; requirements.txt# pip download -d cached_dir -r requirements.txt --no-binary :all:t_env.set_python_requirements(&quot;/path/of/requirements.txt&quot;, &quot;/path/of/cached_dir&quot;) 用户需要提供一个requirements.txt文件，并且在里面申明使用的第三方依赖。如果无法在群集中安装依赖项（网络问题），则可以使用参数“requirements_cached_dir”，指定包含这些依赖项的安装包的目录，如上面的示例所示。依赖项将上传到群集并脱机安装。 A requirements.txt file that defines the third-party dependencies is used. If the dependencies cannot be accessed in the cluster, then you can specify a directory containing the installation packages of these dependencies by using the parameter “requirements_cached_dir”, as illustrated in the example above. The dependencies will be uploaded to the cluster and installed offline. 下面是一个使用依赖管理的完整示例： Below, we showcase a complete example of using dependency management for Python UDFs in Apache Flink 1.10： 1234567891011121314151617181920212223242526272829303132333435363738from PyFlink.datastream import StreamExecutionEnvironmentfrom PyFlink.table import StreamTableEnvironment, DataTypesfrom PyFlink.table.descriptors import Schema, OldCsv, FileSystemfrom PyFlink.table.udf import udfenv = StreamExecutionEnvironment.get_execution_environment()env.set_parallelism(1)t_env = StreamTableEnvironment.create(env)@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())def add(i, j): from mpmath import fadd return int(fadd(1, 2))t_env.set_python_requirements(&quot;/tmp/requirements.txt&quot;, &quot;/tmp/cached_dir&quot;)t_env.register_function(&quot;add&quot;, add)t_env.connect(FileSystem().path(&apos;/tmp/input&apos;)) \ .with_format(OldCsv() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;a&apos;, DataTypes.BIGINT()) .field(&apos;b&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySource&apos;)t_env.connect(FileSystem().path(&apos;/tmp/output&apos;)) \ .with_format(OldCsv() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;sum&apos;, DataTypes.BIGINT())) \ .create_temporary_table(&apos;mySink&apos;)t_env.from_path(&apos;mySource&apos;)\ .select(&quot;add(a, b)&quot;) \ .insert_into(&apos;mySink&apos;)t_env.execute(&quot;tutorial_job&quot;) Submit the job Firstly, you need to prepare input data in the “/tmp/input” file. For example, echo "1,2" >link12 Secondly, you can prepare the dependency requirements file and the cache dir, 12$ echo &quot;mpmath==1.1.0&quot; &gt; /tmp/requirements.txt$ pip download -d /tmp/cached_dir -r /tmp/requirements.txt --no-binary :all: Next, you can run this example on the command line, 1$ python python_udf_sum.py Finally, you can see the execution result on the command line: 12$ cat /tmp/output3 Quick startPyFlink为大家提供了一种非常方便的开发体验方式 - PyFlink Shell。当成功执行 python -m pip install apache-flink之后，你可以直接以pyflink-shell.sh local来启动一个PyFlink Shell进行开发体验，如下所示： Pyflink provides a very convenient way to development - pyflink shell. After successfully executing python -m pip install apache-flink, you can directly start a pyflink shell with pyflink-shell.sh local as shown below: More不仅仅是简单的ETL场景支持，PyFlink可以完成很多复杂场的业务场景需求，比如我们最熟悉的双11大屏的场景，如下： Not only simple ETL scenario support, PyFlink can fulfill the requirements of many complex business scenario, such as the double 11 large screen scenario that we are most familiar with, as follows: 关于上面示例的更多详细请查阅 这里。More detail about above example can be found here. Conclusion &amp; Upcoming work在本博客中，我们介绍了PyFlink中Python UDF的架构，并给出了如何定义、注册、调用和运行UDF的示例。随着1.10的发布，它将为Python用户提供更多的可能来编写Python作业逻辑。同时，我们一直积极与社区合作，不断改进PyFlink的功能和性能。今后，我们计划在标量和聚合函数中引入对Pandas的支持；通过SQL客户端增加对Python UDF使用的支持，以扩展Python UDF的使用范围；并做更多的性能改进。近期，邮件列表上有一个关于新功能支持的讨论，您可以查看并找到更多详细信息。 In this blog post, we introduced the architecture of Python UDFs in PyFlink and provided some examples on how to define, register and invoke UDFs. Apache Flink 1.10 brings Python support in the framework to new levels, allowing Python users to write even more magic with their preferred language. The community is actively working towards continuously improving the functionality and performance of PyFlink. Future work in upcoming releases will introduce support for Pandas UDFs in scalar and aggregate functions, add support to use Python UDFs through the SQL client to further expand the usage scope of Python UDFs and finally work towards even more performance improvements. To find more information about the upcoming work with Python in Apache Apache Flink you can join the discussion on the Apache Apache Flink mailing and share your suggestions and thoughts with the community. 在社区贡献者的不断努力之下，PyFlink的功能可以如上图一样可以迅速从幼苗变成大树: With the continuous efforts of community contributors, PyFlink can quickly change from seedlings to trees as shown in the follows figure: PyFlink Want YouPyFlink是一个新组件，仍然需要做很多工作。 因此，热诚欢迎每个人加入对PyFlink的贡献，包括提出问题，提交错误报告，提出新功能，加入讨论，贡献代码或文档。。。期望在PyFlink见到你！ PyFlink is a new component and still needs a lot of work. Therefore, we sincerely welcome everyone to join our contribution to PyFlink, including asking questions, submitting error reports, proposing new functions, joining discussions, contributing codes or documents… Hope to see you in PyFlink!]]></content>
      <categories>
        <category>Globalization</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>PyFlink</tag>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three Min Series - How PyFlink does ETL]]></title>
    <url>%2F2020%2F01%2F22%2FThree-Min-Series-How-PyFlink-does-ETL%2F</url>
    <content type="text"><![CDATA[Motivation/动机Apache Flink Python API (PyFlink) is a new feature provided by Flink 1.9. Many users do not understand the business scenarios for PyFlink. This article will introduce how PyFlink is applied to ETL. Apache Flink Python API（PyFlink）是Flink 1.9提供的一项新功能。 许多用户不了解PyFlink的业务场景。 本文将介绍如何将PyFlink应用于ETL。 What’s ETL/什么是ETLETL is a type of data integration that refers to the three steps (extract, transform, load) used to blend data from multiple sources. ETL是一种数据集成类型，它涉及用于混合来自多个源的数据的三个步骤（提取，转换，加载）。 PyFlink is very suitable for ETL, that is, extracting data by defining a Source Connector. After transforming the data through various operators, the data is loaded into another store by defining a Sink connector. PyFlink非常适合ETL，即通过定义Source Connector提取数据。 然后通过各种运算符对数据进行转换之后，通过定义Sink Connector将数据加载到另一个存储中。 User Case/案例Data &amp; Req / 数据和ETL需求The Data info as follows: 1234567891011121314151617181920212223242526BeiJing,7355291HeBei,20813492ShanXi,10654162NeiMeng,8470472LiaoNing,15334912JiLin,9162183HeLongJiang,13192935ShangHai,8893483JiangSu,25635291ZheJiang,20060115AnHui,19322432FuJian,11971873JiangXi,11847841ShanDong,30794664HeNan,26404973HuBei,17253385HuNan,19029894GuangDong,32222752GuangXi,13467663ChongQing,10272559SiChuang,26383458GuiZhou,10745630YunNan,12695396ShanXi,11084516GanSu,7113833XinJiang,6902850 Source Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# -*- coding: utf-8 -*-from pyflink.dataset import ExecutionEnvironmentfrom pyflink.table import DataTypes, BatchTableEnvironmentfrom pyflink.table.descriptors import Schema, OldCsv, FileSystemimport osimport time# Data source file (Suppose our data is stored in a CSV file)source_file = &apos;Population_information.csv&apos;# Result file (Suppose store the data into the CSV file)sink_file = &apos;Population_information_More_Than_5_Million.csv&apos;# Create a Environmentexec_env = ExecutionEnvironment.get_execution_environment()exec_env.set_parallelism(1)t_env = BatchTableEnvironment.create(exec_env)# Delete the result file if the it exists,if os.path.exists(sink_file): os.remove(sink_file)# Create the Source connector by using `connect` method.t_env.connect(FileSystem().path(source_file)) \ .with_format(OldCsv() .field(&apos;Province&apos;, DataTypes.STRING()) .field(&apos;Population&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;Province&apos;, DataTypes.STRING()) .field(&apos;Population&apos;, DataTypes.BIGINT())) \ .register_table_source(&apos;source_tab&apos;)# Create the Sink connector by using `connect` method.t_env.connect(FileSystem().path(sink_file)) \ .with_format(OldCsv() .field_delimiter(&apos;,&apos;) .field(&apos;Province&apos;, DataTypes.STRING()) .field(&apos;Population&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;Province&apos;, DataTypes.STRING()) .field(&apos;Population&apos;, DataTypes.BIGINT())) \ .register_table_sink(&apos;sink_tab&apos;)# Extract provinces with population over 5 milliont_env.from_path(&apos;source_tab&apos;) \ .select(&apos;Province, Population&apos;) \ .where(&apos;Population &gt;= 5000000&apos;) \ .insert_into(&apos;sink_tab&apos;)# Execute the Jobprint(&quot;run: &quot; + str(time.time()))t_env.execute(&quot;ETL_Extract_provinces_with_population_over_5_million&quot;)print(&quot;finish: &quot; + str(time.time())) Video presentation/视频演示 Q&amp;AIf you have questions, you can leave a comment or send an email: sunjincheng121@gmail.com 如有疑问，您可以发表评论或发送电子邮件：sunjincheng121@gmail.com]]></content>
      <categories>
        <category>Three Min Series</category>
      </categories>
      <tags>
        <tag>PyFlink</tag>
        <tag>PyFlink 1.9</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three Min Series - Setting up the dev environment for Pyflink 1.9]]></title>
    <url>%2F2020%2F01%2F20%2FThree-Min-Series-Setting-up-the-dev-environment-for-Pyflink-1-9%2F</url>
    <content type="text"><![CDATA[Motivation/动机The goal of this blog is to help users quickly build a PyFlink dev environment. 本篇目标是帮助用户快速搭建PyFlink的开发环境。 Steps/操作步骤 Check dependencies 123mvn --versionscala -versionpython --version You can set a soft link by: “sudo ln -s /usr/local/Cellar/python/3.7.3/bin/python3 /usr/local/bin/python” for your python3. 12which pip You can install the “pip” by: “curl -O https://bootstrap.pypa.io/get-pip.py; sudo python get-pip.py” for current python. Download Source code 1git clone https://github.com/apache/flink.git Build PyFlink1.9 123git fetch origin release-1.9; git checkout -b release-1.9 origin/release-1.9; mvn clean install -DskipTests -Dfast Install PyFlink 1.9 1cd flink-python; python setup.py sdist; pip install dist/*.tar.gz --user Check installation status 12pip list |grep apache-flink Download PyCharm 1curl -O https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg Install IDE and create new Project NOTE: Ensure the python Interpreter should be points to which is python. Copy WordCount example and run it 1cp ~/foryou/flink/build-target/examples/python/table/batch/word_count.py &#123;to your project dir&#125;. Video presentation/视频演示 Q&amp;AIf you have questions, you can leave a comment or send an email: sunjincheng121@gmail.com 如有疑问，您可以发表评论或发送电子邮件：sunjincheng121@gmail.com]]></content>
      <categories>
        <category>Three Min Series</category>
      </categories>
      <tags>
        <tag>PyFlink</tag>
        <tag>PyFlink 1.9</tag>
        <tag>environment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three Min Series - Run the Example of WordCount in PyFlink 1.9]]></title>
    <url>%2F2020%2F01%2F19%2FThree-Min-Series-Run-the-Example-of-WordCount-in-PyFlink-1-9%2F</url>
    <content type="text"><![CDATA[Motivation/动机PyFlink is a new module recently added to Apache Flink . Many people want to quickly experience how to run PyFlink jobs. This blog will run the WordCount example in PyFlink 1.9 in 3 minutes. PyFlink是最近添加到Apache Flink的新模块。 许多人想快速体验如何运行PyFlink作业。 本文将在3分钟内完成在PyFlink 1.9中运行WordCount示例。 Steps/操作步骤 Check Python Version/检查Python版本12jincheng:videos jincheng.sunjc$ python --versionPython 3.7.6 It’s better to use Python 2.7.6+最好使用Python2.7.6+ Download binary distribution/下载二进制发布包 1curl -O http://mirrors.gigenet.com/apache/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz Start cluster/启动集群 123tar -zxvf flink-1.9.1-bin-scala_2.11.tgzcd flink-1.9.1bin/start-cluster.sh If all goes well, open the console: http://localhost:8081/ Submit WordCount job/提交WordCount作业 123456jincheng:flink-1.9.1 jincheng.sunjc$ ./bin/flink run -py examples/python/table/batch/word_count.pyStarting execution of programResults directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/resultProgram execution finishedJob with JobID 12ffba88d96dc7fd78b514aa40bb4b6d has finished.Job Runtime: 981 ms Check the result 123456789jincheng:flink-1.9.1 jincheng.sunjc$ cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/resultASF,2Apache,2Foundation,1......with,2work,1you,2 Stop cluster/停止集群 123jincheng:flink-1.9.1 jincheng.sunjc$ bin/stop-cluster.sh Stopping taskexecutor daemon (pid: 2718) on host jincheng.local.Stopping standalonesession daemon (pid: 2302) on host jincheng.local. Video presentation/视频演示 Q&amp;AIf you have questions, you can leave a comment or send an email to: sunjincheng121@gmail.com 如有疑问，您可以发表评论或发送电子邮件到：sunjincheng121@gmail.com]]></content>
      <categories>
        <category>Three Min Series</category>
      </categories>
      <tags>
        <tag>PyFlink</tag>
        <tag>PyFlink 1.9</tag>
        <tag>WordCount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列 - PyFlink 作业的多种部署模式]]></title>
    <url>%2F2020%2F01%2F02%2FApache-Flink-%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-PyFlink-%E4%BD%9C%E4%B8%9A%E7%9A%84%E5%A4%9A%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[开篇说道老子说：“致虚极，守静笃。万物并作，吾以观其复”。 字里行间是告诉人们要尽量使心灵达到虚寂的极致，持之以恒的坚守宁静。老子认为“虚”和“静”是心灵的本初的状态，也应该是一种常态，即，是一种没有心机，没有成见，没有外界名利，物质诱惑，而保持的状态。当达到这个状态之后，就可以静观万物的蓬勃发展，了解万物的循环往复的变化，通晓自然之理，体悟自然之道。 “致虚极，守静笃” 并不代表对任何事情无动于衷，而是通过 “致虚极，守静笃” 的方式了解万物变化的根源，就好比，看到新芽不惊，看到落叶不哀，因为我们知道，落叶归根，重归泥土之后，还会再破土而出，开始新的轮回。 在2020年1月1日零点我发了一条朋友圈，对自己的祝福是：”㊗️ 2020 安”。当然这里我也祝福所有看到这篇博客的你: “㊗️ 2020 安”。 祝福大家 “安”，也即祝福大家 能够在新的一年，追求内心的修炼，摒去纷纷复杂的物欲干扰，拂去心灵的灰尘，保持自己的赤子之心！ 道破 “天机”前面一些博客大多介绍PyFlink的功能开发，比如，如何使用各种算子(Join/Window/AGG etc.)，如何使用各种Connector(Kafka, CSV, Socket etc.)，还有一些实际的案例。这些都停留在开发阶段，一旦开发完成，我们就面临激动人心的时刻，那就是 将我们精心设计开发的作业进行部署，那么问题来了，你知道怎样部署PyFlink的作业吗？这篇将为大家全面介绍部署PyFlink作业的各种模式。 组件栈回顾 上面的组件栈除了PyFlink是第一次添加上去，其他部分大家应该非常熟悉了。目前PyFlink基于Java的Table API之上，同时在Runtime层面有Python的算子和执行容器。那么我们聚焦重点，看最底层的 Deploy部分，上图我们分成了三种部署模式，Local/Cluster/Cloud，其中Local模式还有2种不同方式，一是SingleJVM，也即是MiniCluster, 前面博客里面运行示例所使用的就是MiniCluster。二是SingleNode，也就是虽然是集群模式，但是所有角色都在一台机器上。下面我们简单介绍一下上面这几种部署模式的区别： Local-SingleJVM 模式- 该模式大多是开发测试阶段使用的方式，所有角色TM，JM等都在同一个JVM里面。 Local-SingleNode 模式 - 意在所有角色都运行在同一台机器，直白一点就是从运行的架构上看，这种模式虽然是分布式的，但集群节点只有1个，该模式大多是测试和IoT设备上进行部署使用。 Cluster 模式 - 也就是我们经常用于投产的分布式部署方式，上图根据对资源管理的方式不同又分为了多种，如：Standalone 是Flink自身进行资源管理，YARN，顾名思义就是利用资源管理框架Yarn来负责Flink运行资源的分配，还有结合Kubernetes等等。 Cloud 模式- 该部署模式是结合其他云平台进行部署。 接下来我们看看PyFlink的作业可以进行怎样的模式部署？ 环境依赖 JDK 1.8+ (1.8.0_211) Maven 3.x (3.2.5) Scala 2.11+ (2.12.0) Python 3.5+ (3.7.6) Git 2.20+ (2.20.1) PyFlink安装因为目前PyFlink已经部署到PyPI上面了，安装PyFlink只需要使用如下命令： 检查Python版本 12$ python --version# the version printed here must be 3.5+ 安装PyFlink 1$ python -m pip install apache-flink==1.10.0 检查核心包我们核心需要apache-beam和apache-flink，如下命令： 1234567$ pip listPackage Version ----------------------------- ---------alabaster 0.7.12 apache-beam 2.15.0 apache-flink 1.10.0atomicwrites 1.3.0 如上信息证明你我们所需的Python依赖已经okay了，接下来回过头来在看看如何进行业务需求的开发。 PyFlink示例作业接下来我们开发一个简单的PyFlink作业，源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import loggingimport osimport shutilimport sysimport tempfilefrom pyflink.table import BatchTableEnvironment, EnvironmentSettingsfrom pyflink.table.descriptors import FileSystem, OldCsv, Schemafrom pyflink.table.types import DataTypesfrom pyflink.table.udf import udfdef word_count(): environment_settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build() t_env = BatchTableEnvironment.create(environment_settings=environment_settings) # register Results table in table environment tmp_dir = tempfile.gettempdir() result_path = tmp_dir + &apos;/result&apos; if os.path.exists(result_path): try: if os.path.isfile(result_path): os.remove(result_path) else: shutil.rmtree(result_path) except OSError as e: logging.error(&quot;Error removing directory: %s - %s.&quot;, e.filename, e.strerror) logging.info(&quot;Results directory: %s&quot;, result_path) # we should set the Python verison here if `Python` not point t_env.get_config().set_python_executable(&quot;python3&quot;) t_env.connect(FileSystem().path(result_path)) \ .with_format(OldCsv() .field_delimiter(&apos;,&apos;) .field(&quot;city&quot;, DataTypes.STRING()) .field(&quot;sales_volume&quot;, DataTypes.BIGINT()) .field(&quot;sales&quot;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&quot;city&quot;, DataTypes.STRING()) .field(&quot;sales_volume&quot;, DataTypes.BIGINT()) .field(&quot;sales&quot;, DataTypes.BIGINT())) \ .register_table_sink(&quot;Results&quot;) @udf(input_types=DataTypes.STRING(), result_type=DataTypes.ARRAY(DataTypes.STRING())) def split(input_str: str): return input_str.split(&quot;,&quot;) @udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING()) def get(arr, index): return arr[index] t_env.register_function(&quot;split&quot;, split) t_env.register_function(&quot;get&quot;, get) t_env.get_config().get_configuration().set_string(&quot;parallelism.default&quot;, &quot;1&quot;) data = [(&quot;iPhone 11,30,5499,Beijing&quot;, ), (&quot;iPhone 11 Pro,20,8699,Guangzhou&quot;, ), (&quot;MacBook Pro,10,9999,Beijing&quot;, ), (&quot;AirPods Pro,50,1999,Beijing&quot;, ), (&quot;MacBook Pro,10,11499,Shanghai&quot;, ), (&quot;iPhone 11,30,5999,Shanghai&quot;, ), (&quot;iPhone 11 Pro,20,9999,Shenzhen&quot;, ), (&quot;MacBook Pro,10,13899,Hangzhou&quot;, ), (&quot;iPhone 11,10,6799,Beijing&quot;, ), (&quot;MacBook Pro,10,18999,Beijing&quot;, ), (&quot;iPhone 11 Pro,10,11799,Shenzhen&quot;, ), (&quot;MacBook Pro,10,22199,Shanghai&quot;, ), (&quot;AirPods Pro,40,1999,Shanghai&quot;, )] t_env.from_elements(data, [&quot;line&quot;]) \ .select(&quot;split(line) as str_array&quot;) \ .select(&quot;get(str_array, 3) as city, &quot; &quot;get(str_array, 1).cast(LONG) as count, &quot; &quot;get(str_array, 2).cast(LONG) as unit_price&quot;) \ .select(&quot;city, count, count * unit_price as total_price&quot;) \ .group_by(&quot;city&quot;) \ .select(&quot;city, &quot; &quot;sum(count) as sales_volume, &quot; &quot;sum(total_price) as sales&quot;) \ .insert_into(&quot;Results&quot;) t_env.execute(&quot;word_count&quot;)if __name__ == &apos;__main__&apos;: logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=&quot;%(message)s&quot;) word_count() 接下来我们就介绍如何用不同部署模式运行PyFlink作业！ Local-SingleJVM 模式部署该模式多用于开发测试阶段，简单的利用Python pyflink_job.py命令，PyFlink就会默认启动一个Local-SingleJVM的Flink环境来执行作业，如下： 首先确认你Python是3.5+，然后执行上面的PyFlink作业Python deploy_demo.py,结果写入到本地文件，然后cat计算结果，如果出现如图所示的结果，则说明准备工作已经就绪。:),如果有问题，欢迎留言！这里运行时SingleJVM，在运行这个job时候大家可以查看java进程： 我们发现只有一个JVM进程，里面包含了所有Flink所需角色。 Local-SingleNode 模式部署这种模式一般用在单机环境中进行部署，如IoT设备中，我们从0开始进行该模式的部署操作。我们进入到flink/build-target目录，执行如下命令(个人爱好，我把端口改成了8888)： 1234jincheng:build-target jincheng.sunjc$ bin/start-cluster.sh ...Starting cluster.Starting standalonesession daemon on host jincheng.local. 查看一下Flink的进程： 我们发现有TM和JM两个进程，虽然在一台机器(Local）但是也是一个集群的架构。上面信息证明已经启动完成，我们可以查看web界面：http://localhost:8888/（我个人爱好端口是8888，默认是8080）, 如下： 目前集群环境已经准备完成，我们看如果将作业部署到集群中，一条简单的命令，如下： 1bin/flink run -m localhost:8888 -py ~/deploy_demo.py 这里如果你不更改端口可以不添加-m 选项。如果一切顺利，你会得到如下输出： 123456jincheng:build-target jincheng.sunjc$ bin/flink run -m localhost:8888 -py ~/deploy_demo.py Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/resultJob has been submitted with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6Program execution finishedJob with JobID 3ae7fb8fa0d1867daa8d65fd87ed3bc6 has finished.Job Runtime: 5389 ms 其中/var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result 目录是计算结果目录，我们可以产看一下，如下： 123456jincheng:build-target jincheng.sunjc$ cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/resultBeijing,110,622890Guangzhou,20,173980Shanghai,90,596910Shenzhen,30,317970Hangzhou,10,138990 同时我们也可以在WebUI上面进行查看，在完成的job列表中，显示如下：到此，我们完成了在Local模式，其实也是只有一个节点的Standalone模式下完成PyFlink的部署。最后我们为了继续下面的操作，请停止集群： 123jincheng:build-target jincheng.sunjc$ bin/stop-cluster.shStopping taskexecutor daemon (pid: 45714) on host jincheng.local.Stopping standalonesession daemon (pid: 45459) on host jincheng.local. Cluster YARN 模式部署这个模式部署，我们需要一个YARN环境，我们一切从简，以单机部署的方式准备YARN环境，然后再与Flink进行集成。 准备YARN环境 安装Hadoop 我本机是mac系统，所以我偷懒一下直接用brew进行安装： 1234567891011121314jincheng:bin jincheng.sunjc$ brew install HadoopUpdating Homebrew...==&gt; Auto-updated Homebrew!Updated 2 taps (homebrew/core and homebrew/cask).==&gt; Updated FormulaePython ✔ doxygen minio ntopng typescriptcertbot libngspice mitmproxy ooniprobedoitlive minimal-racket ngspice openimageio==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-==&gt; Downloading from http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.2.1/######################################################################## 100.0%🍺 /usr/local/Cellar/Hadoop/3.2.1: 22,397 files, 815.6MB, built in 5 minutes 12 seconds 完成之后，检验一下hadoop版本： 12jincheng:bin jincheng.sunjc$ hadoop versionHadoop 3.2.1 超级顺利，hadoop被安装到了/usr/local/Cellar/hadoop/3.2.1/目录下，brew还是很能提高生产力啊～ 配置免登(SSH) Mac系统自带了ssh，我们可以简单配置一下即可，我们先打开远程登录。 系统偏好设置 -&gt; 共享 中，左边勾选 远程登录 ，右边选择 仅这些用户 （选择所有用户更宽松），并添加当前用户。 12jincheng:bin jincheng.sunjc$ whoamijincheng.sunjc 我当前用户是 jincheng.sunjc。 配置图如下： 然后生产证书，如下操作： 1234567891011121314151617181920ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsaGenerating public/private rsa key pair./Users/jincheng.sunjc/.ssh/id_rsa already exists.Overwrite (y/n)? yYour identification has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.Your public key has been saved in /Users/jincheng.sunjc/.ssh/id_rsa.pub.The key fingerprint is:SHA256:IkjKkOjfMx1fxWlwtQYg8hThph7Xlm9kPutAYFmQR0A jincheng.sunjc@jincheng.localThe key&apos;s randomart image is:+---[RSA 2048]----+| ..EB=.o.. ||.. =.+.+ o .||+ . B. = o ||+o . + o + . ||.o. . .+S. * o || . ..o.= + = || . + o . . = || o o o || .o |+----[SHA256]-----+ 接下来将公钥追加到如下文件，并修改文件权限： 12jincheng.sunjc$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysjincheng.sunjc$ chmod 0600 ~/.ssh/authorized_keys 利用ssh localhost验证，看到 Last login: 字样为ssh成功 123jincheng:~ jincheng.sunjc$ ssh localhostPassword:Last login: Tue Dec 31 18:26:48 2019 from ::1 设置环境变量 设置JAVA_HOME,HADOOP_HOME和HADOOP_CONF_DIR，vi ~/.bashrc: 12345export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Homeexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexecexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop NOTE: 后续操作要确保的terminal环境变量是生效哦， 如果不生效可以执行 source ~/.bashrc。:) 修改配置 1) 修改core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2) 修改hdfs-site.xml12345678910111213&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3) 修改yarn-site.xml配置 YARN 作为资源管理框架： 123456789&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 简单的配置已经完成，我们执行一下简单命令启动环境： 格式化文档系统： 1234567jincheng:libexec jincheng.sunjc$ hadoop namenode -format......2019-12-31 18:58:53,260 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at jincheng.local/127.0.0.1************************************************************/ 启动服务： 我们先启动hdf再启动yarn，如下图： Okay,一切顺利的话，我们会启动namenodes，datanodes，resourcemanager和nodemanagers。我们有几个web界面可以查看，如下： 1). Overview 界面， http://localhost:9870 如下： 2). NodeManager界面， http://localhost:8042，如下： 3). ResourceManager 管理界面 http://localhost:8088/,如下： 目前YARN的环境已经准备完成，我们接下来看如何与Flink进行集成。 Flink集成Hadoop包切换到编译结果目录下flink/build-target,并将haddop的JAR包放到lib目录。在官网下载hadoop包， 12cd lib;curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar &gt; flink-shaded-hadoop-2-uber-2.8.3-7.0.jar 下载后，lib目录下文件如下： 到现在为止我们可以提交PyFlink的作业到由YARN进行资源分配的集群了。但为了确保集群上有正确的Python环境我们最好打包一个Python环境到集群上面。因为大部分情况下我们无法得知yarn集群上的Python版本是否符合我们的要求(Python 3.5+，装有apache-beam 2.15.0)，因此我们需要打包一个符合条件的Python环境，并随job文件提交到yarn集群上。 打包Python环境再次检查一下当前Python的版本是否3.5+，如下： 12jincheng:lib jincheng.sunjc$ PythonPython 3.7.6 (default, Dec 31 2019, 09:48:30) 由于这个Python环境是用于集群的，所以打包时的系统需要和集群一致。如果不一致，比如集群是linux而本机是mac，我们需要在虚拟机或者docker中打包。以下列出两种情况的示范方法，读者根据需求选择一种即可。 本地打包（集群和本机操作系统一致时）如果集群所在机器的操作系统和本地一致（都是mac或者都是linux），直接通过virtualenv打包一个符合条件的Python环境： 安装virtualenv使用python -m pip install virtualenv 进行安装如下： 123456jincheng:tmp jincheng.sunjc$ python -m pip install virtualenv==16.7.9Collecting virtualenv Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB) |████████████████████████████████| 3.4MB 2.0MB/s Installing collected packages: virtualenvSuccessfully installed virtualenv-16.7.9 我本地环境已经成功安装。 创建Python环境用virtualenv以always-copy方式建立一个全新的Python环境，名字随意，以venv为例，virtualenv --always-copy venv: 123456$ virtualenv --always-copy venvUsing base prefix &apos;/usr/local/Cellar/Python/3.7.6/Frameworks/Python.framework/Versions/3.7&apos;New Python executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/Python3.7Also creating executable in /Users/jincheng.sunjc/temp/hadoop/tmp/venv/bin/PythonInstalling setuptools, pip, wheel...done. 接下来要copy原环境的Python标准库到venv中，如下： 1234$ VENV_PYTHON=venv/bin/python$ DST=`$VENV_PYTHON -c &quot;import os;import pip;print(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(pip.__file__)))))&quot;`$ SRC=`$VENV_PYTHON -c &quot;import os;import contextlib;print(os.path.dirname(os.path.abspath(contextlib.__file__)))&quot;`$ if [ &quot;$SRC&quot; != &quot;$DST&quot; ]; then find &quot;$SRC&quot; -maxdepth 1 ! -name &quot;site-packages&quot; ! -name &quot;__pycache__&quot; ! -name &quot;python3.*&quot; -exec cp -r -- &quot;&#123;&#125;&quot; &quot;$&#123;DST&#125;&quot; \; ; fi 在新环境中安装apache-beam 2.15.0使用venv/bin/pip install apache-beam==2.15.0进行安装： 12345jincheng:tmp jincheng.sunjc$ venv/bin/pip install apache-beam==2.15.0Collecting apache-beam==2.15.0......Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7 上面信息已经说明我们成功的在Python环境中安装了apache-beam==2.15.0。接下来我们打包Python环境。 打包Python环境我们将Python打包成zip文件，zip -r venv.zip venv 如下： 12345678zip -r venv.zip venv...... adding: venv/lib/Python3.7/re.py (deflated 68%) adding: venv/lib/Python3.7/struct.py (deflated 46%) adding: venv/lib/Python3.7/sre_parse.py (deflated 80%) adding: venv/lib/Python3.7/abc.py (deflated 72%) adding: venv/lib/Python3.7/_bootlocale.py (deflated 63%) 查看一下zip大小： 12jincheng:tmp jincheng.sunjc$ du -sh venv.zip 81M venv.zip 这个大小实在太大了，核心问题是Beam的包非常大，后面我会持续在Beam社区提出优化建议。我们先忍一下:(。 Docker中打包（比如集群为linux，本机为mac时）我们选择在docker中打包，可以从以下链接下载最新版docker并安装：https://download.docker.com/mac/stable/Docker.dmg安装完毕后重启终端，执行docker version确认docker安装成功： 12345678910111213141516171819202122232425262728jincheng:tmp jincheng.sunjc$ docker versionClient: Docker Engine - Community Version: 19.03.4 API version: 1.40 Go version: go1.12.10 Git commit: 9013bf5 Built: Thu Oct 17 23:44:48 2019 OS/Arch: darwin/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.4 API version: 1.40 (minimum version 1.12) Go version: go1.12.10 Git commit: 9013bf5 Built: Thu Oct 17 23:50:38 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683 启动容器我们启动一个Python 3.7版本的容器如果是第一次启动可能需要较长时间来拉取镜像：docker run -it Python:3.7 /bin/bash, 如下： 1234567891011121314jincheng:libexec jincheng.sunjc$ docker run -it Python:3.7 /bin/bashUnable to find image &apos;Python:3.7&apos; locally3.7: Pulling from library/Python8f0fdd3eaac0: Pull complete d918eaefd9de: Pull complete 43bf3e3107f5: Pull complete 27622921edb2: Pull complete dcfa0aa1ae2c: Pull complete bf6840af9e70: Pull complete 167665d59281: Pull complete ffc544588c7f: Pull complete 4ebe99df65fe: Pull complete Digest: sha256:40d615d7617f0f3b54614fd228d41a891949b988ae2b452c0aaac5bee924888dStatus: Downloaded newer image for Python:3.7 容器中安装virtualenv我们在刚才启动的容器中安装virtualenv， pip install virtualenv,如下： 1234567root@1b48d2b526ae:/# pip install virtualenv==16.7.9Collecting virtualenv Downloading https://files.Pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl (3.4MB) |████████████████████████████████| 3.4MB 2.0MB/s Installing collected packages: virtualenvSuccessfully installed virtualenv-16.7.9root@1b48d2b526ae:/# 创建Python环境以always copy方式建立一个全新的Python环境，名字随意，以venv为例，virtualenv --always-copy venv, 如下： 123456root@1b48d2b526ae:/# virtualenv --always-copy venvUsing base prefix &apos;/usr/local&apos;New Python executable in /venv/bin/PythonInstalling setuptools, pip, wheel...done.root@1b48d2b526ae:/# 接下来要copy原环境的Python标准库到venv中，如下： 1234# VENV_PYTHON=venv/bin/python# DST=`$VENV_PYTHON -c &quot;import os;import pip;print(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(pip.__file__)))))&quot;`# SRC=`$VENV_PYTHON -c &quot;import os;import contextlib;print(os.path.dirname(os.path.abspath(contextlib.__file__)))&quot;`# if [ &quot;$SRC&quot; != &quot;$DST&quot; ]; then find &quot;$SRC&quot; -maxdepth 1 ! -name &quot;site-packages&quot; ! -name &quot;__pycache__&quot; ! -name &quot;python3.*&quot; -exec cp -r -- &quot;&#123;&#125;&quot; &quot;$&#123;DST&#125;&quot; \; ; fi 安装Apache Beam在新的Python环境中安装apache-beam 2.15.0，venv/bin/pip install apache-beam==2.15.0,如下： 12345root@1b48d2b526ae:/# venv/bin/pip install apache-beam==2.15.0Collecting apache-beam==2.15.0......Successfully installed apache-beam-2.15.0 avro-Python3-1.9.1 certifi-2019.11.28 chardet-3.0.4 crcmod-1.7 dill-0.2.9 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.26.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.8 mock-2.0.0 numpy-1.18.0 oauth2client-3.0.0 pbr-5.4.4 protobuf-3.11.2 pyarrow-0.14.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 pydot-1.4.1 pymongo-3.10.0 pyparsing-2.4.6 pytz-2019.3 pyyaml-3.13 requests-2.22.0 rsa-4.0 six-1.13.0 urllib3-1.25.7 查看docker中的Python环境用exit命令退出容器，用docker ps -a找到docker容器的id，用于拷贝文件,如下： 12345root@1b48d2b526ae:/# exitexitjincheng:libexec jincheng.sunjc$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1b48d2b526ae Python:3.7 &quot;/bin/bash&quot; 7 minutes ago Exited (0) 8 seconds ago elated_visvesvaraya 由于刚刚结束，一般来说是列表中的第一条，可以根据容器的镜像名Python:3.7来分辨。我们记下最左边的容器ID。如上是1b48d2b526ae。 打包Python环境从将容器中的Python环境拷贝出来，我们切换到flink/build-target录下，拷贝 docker cp 1b48d2b526ae:/venv ./并打包zip -r venv.zip venv。最终flink/build-target录下生成venv.zip。 部署作业终于到部署作业的环节了:), flink on yarn支持两种模式，per-job和session。per-job模式在提交job时会为每个job单独起一个flink集群，session模式先在yarn上起一个flink集群，之后提交job都提交到这个flink集群。 Pre-Job 模式部署作业执行以下命令，以Pre-Job模式部署PyFlink作业： 如果是Docker模式打包的话，如果宿主环境还没有安装virtualenv，那么需要首先执行如下命令： 12$ pip install virtualenv==16.7.9$ virtualenv --python /usr/local/bin/python3 venv 接下来激活环境并提交作业： 123456789101112131415161718192021$ source venv/bin/activate$ bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py2020-01-02 13:04:52,889 WARN org.apache.flink.yarn.cli.FlinkYarnSessionCli - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.2020-01-02 13:04:52,889 WARN org.apache.flink.yarn.cli.FlinkYarnSessionCli - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.Results directory: /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result2020-01-02 13:04:55,945 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:80322020-01-02 13:04:56,049 INFO org.apache.flink.yarn.YarnClusterDescriptor - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2020-01-02 13:05:01,153 WARN org.apache.flink.yarn.YarnClusterDescriptor - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.2020-01-02 13:05:01,177 INFO org.apache.flink.yarn.YarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;2020-01-02 13:05:01,294 WARN org.apache.flink.yarn.YarnClusterDescriptor - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system2020-01-02 13:05:02,600 INFO org.apache.flink.yarn.YarnClusterDescriptor - Submitting application master application_1577936885434_00042020-01-02 13:05:02,971 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1577936885434_00042020-01-02 13:05:02,972 INFO org.apache.flink.yarn.YarnClusterDescriptor - Waiting for the cluster to be allocated2020-01-02 13:05:02,975 INFO org.apache.flink.yarn.YarnClusterDescriptor - Deploying cluster, current state ACCEPTED2020-01-02 13:05:23,138 INFO org.apache.flink.yarn.YarnClusterDescriptor - YARN application has been deployed successfully.2020-01-02 13:05:23,140 INFO org.apache.flink.yarn.YarnClusterDescriptor - Found Web Interface localhost:61616 of application &apos;application_1577936885434_0004&apos;.Job has been submitted with JobID a41d82194a500809fd715da8f29894a0Program execution finishedJob with JobID a41d82194a500809fd715da8f29894a0 has finished.Job Runtime: 35576 ms 上面信息已经显示运行完成，在Web界面可以看到作业状态： 我们再检验一下计算结果cat /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/result： 到这里，我们以Pre-Job的方式成功部署了PyFlink的作业！相比提交到本地Standalone集群，多了三个参数，我们简单说明如下： 参数 说明 -m yarn-cluster 以Per-Job模式部署到yarn集群 -pyarch venv.zip 将当前目录下的venv.zip上传到yarn集群 -pyexec venv.zip/venv/bin/Python 指定venv.zip中的Python解释器来执行Python UDF，路径需要和zip包内部结构一致。 Session 模式部署作业以Session模式部署作业也非常简单，我们实际操作一下： 1234567891011121314151617181920212223jincheng:build-target jincheng.sunjc$ bin/yarn-session.sh 2020-01-02 13:58:53,049 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.address, localhost2020-01-02 13:58:53,050 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.port, 61232020-01-02 13:58:53,050 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.heap.size, 1024m2020-01-02 13:58:53,050 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.memory.process.size, 1024m2020-01-02 13:58:53,050 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.numberOfTaskSlots, 12020-01-02 13:58:53,050 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: parallelism.default, 12020-01-02 13:58:53,051 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.execution.failover-strategy, region2020-01-02 13:58:53,413 WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2020-01-02 13:58:53,476 INFO org.apache.flink.runtime.security.modules.HadoopModule - Hadoop user set to jincheng.sunjc (auth:SIMPLE)2020-01-02 13:58:53,509 INFO org.apache.flink.runtime.security.modules.JaasModule - Jaas file will be created as /var/folders/fp/s5wvp3md31j6v5gjkvqbkhrm0000gp/T/jaas-3848984206030141476.conf.2020-01-02 13:58:53,521 WARN org.apache.flink.yarn.cli.FlinkYarnSessionCli - The configuration directory (&apos;/Users/jincheng.sunjc/blog/demo_dev/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf&apos;) already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.2020-01-02 13:58:53,562 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:80322020-01-02 13:58:58,803 WARN org.apache.flink.yarn.YarnClusterDescriptor - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.2020-01-02 13:58:58,824 INFO org.apache.flink.yarn.YarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;2020-01-02 13:59:03,975 WARN org.apache.flink.yarn.YarnClusterDescriptor - The file system scheme is &apos;file&apos;. This indicates that the specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.The Flink YARN client needs to store its files in a distributed file system2020-01-02 13:59:04,779 INFO org.apache.flink.yarn.YarnClusterDescriptor - Submitting application master application_1577936885434_00052020-01-02 13:59:04,799 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1577936885434_00052020-01-02 13:59:04,799 INFO org.apache.flink.yarn.YarnClusterDescriptor - Waiting for the cluster to be allocated2020-01-02 13:59:04,801 INFO org.apache.flink.yarn.YarnClusterDescriptor - Deploying cluster, current state ACCEPTED2020-01-02 13:59:24,711 INFO org.apache.flink.yarn.YarnClusterDescriptor - YARN application has been deployed successfully.2020-01-02 13:59:24,713 INFO org.apache.flink.yarn.YarnClusterDescriptor - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.JobManager Web Interface: http://localhost:62247 执行成功后不会返回，但会启动一个JoBManager Web，地址如上http://localhost:62247，可复制到浏览器查看: 我们可以修改conf/flink-conf.yaml中的配置参数。如果要更改某些内容，请参考官方文档。接下来我们提交作业，首先按组合键Ctrl+Z将yarn-session.sh进程切换到后台，并执行bg指令让其在后台继续执行, 然后执行以下命令，即可向Session模式的flink集群提交job bin/flink run -m yarn-cluster -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py： 1234567jincheng:build-target jincheng.sunjc$ bin/flink run -pyarch venv.zip -pyexec venv.zip/venv/bin/Python -py deploy_demo.py2020-01-02 14:10:48,285 INFO org.apache.flink.yarn.YarnClusterDescriptor - Found Web Interface localhost:62247 of application &apos;application_1577936885434_0005&apos;.Job has been submitted with JobID bea33b7aa07c0f62153ab5f6e134b6bfProgram execution finishedJob with JobID bea33b7aa07c0f62153ab5f6e134b6bf has finished.Job Runtime: 34405 ms 如果在打印finished之前查看之前的web页面，我们会发现Session集群会有一个正确运行的作业，如下：如果已经运行完成，那么我们应该会看到状态也变成结束： 相比per job模式提交，少了”-m”参数。因为之前已经启动了yarn-session.sh，所以flink默认会向yarn-session.sh启动的集群上提交job。执行完毕后，别忘了关闭yarn-session.sh（session 模式）：先将yarn-session.sh调到前台，执行fg,然后在再按Ctrl+C结束进程或者执行stop，结束时yarn上的集群也会被关闭。 Docker 模式部署我们还可以将Flink Python job打包成docker镜像，然后使用docker-compose或者Kubernetes部署执行，由于现在的docker镜像打包工具并没有完美支持运行Python UDF，因此我们需要往里面添加一些额外的文件。首先是一个仅包含PythonDriver类的jar包. 我们在build-target目录下执行如下命令： 12345jincheng:build-target jincheng.sunjc$ mkdir tempjincheng:build-target jincheng.sunjc$ cd tempjincheng:temp jincheng.sunjc$ unzip ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar org/apache/flink/client/Python/PythonDriver.classArchive: ../opt/flink-Python_2.11-1.10-SNAPSHOT.jar inflating: org/apache/flink/client/Python/PythonDriver.class 解压之后，我们在进行压缩打包： 12jincheng:temp jincheng.sunjc$ zip Python-driver.jar org/apache/flink/client/Python/PythonDriver.class adding: org/apache/flink/client/Python/PythonDriver.class (deflated 56%) 我们得到Python-driver.jar。然后下载一个pyArrow的安装文件(我准备了一个大家下载直接使用即可 pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl。执行以下命令构建Docker镜像，需要作为artifacts引入的文件有作业文件，Python-driver的jar包和pyarrow安装文件,./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-dist(进入flink/flink-container/docker目录) 1234567891011jincheng:docker jincheng.sunjc$ ./build.sh --job-artifacts ~/deploy_demo.py,Python-driver.jar,pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl --with-Python3 --from-local-distUsing flink dist: ../../flink-dist/target/flink-*-bina .a ./flink-1.10-SNAPSHOTa ./flink-1.10-SNAPSHOT/temp......Removing intermediate container a0558bbcbdd1 ---&gt; 00ecda6117b7Successfully built 00ecda6117b7Successfully tagged flink-job:latest 构建Docker镜像需要较长时间，请耐心等待。构建完毕之后，可以输入docker images命令在镜像列表中找到构建结果docker images：然后我们在构建好的镜像基础上安装好Python udf所需依赖，并删除过程中产生的临时文件： 启动docker容器docker run -it --user root --entrypoint /bin/bash --name flink-job-container flink-job 安装一些依赖apk add --no-cache g++ Python3-dev musl-dev 安装PyArrowpython -m pip3 install /opt/artifacts/pyarrow-0.12.0a0-cp36-cp36m-linux_x86_64.whl 安装Apache Beampython -m pip3 install apache-beam==2.15.0 删除临时文件rm -rf /root/.cache/pip 执行完如上命令我可以执行exit退出容器了，然后把这个容器提交为新的flink-job镜像docker commit -c &#39;CMD [&quot;--help&quot;]&#39; -c &quot;USER flink&quot; -c &#39;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&#39; flink-job-container flink-job:latest： 12jincheng:docker jincheng.sunjc$ docker commit -c &apos;CMD [&quot;--help&quot;]&apos; -c &quot;USER flink&quot; -c &apos;ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]&apos; flink-job-container flink-job:latest sha256:0740a635e2b0342ddf776f33692df263ebf0437d6373f156821f4dd044ad648b 到这里包含Python UDF 作业的Docker镜像就制作好了，这个Docker镜像既可以以docker-compose使用，也可以结合kubernetes中使用。 我们以使用docker-compose执行为例，mac版docker自带docker-compose，用户可以直接使用，在flink/flink-container/docker目录下，使用以下命令启动作业,FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose up: 1234567891011jincheng:docker jincheng.sunjc$ FLINK_JOB=org.apache.flink.client.Python.PythonDriver FLINK_JOB_ARGUMENTS=&quot;-py /opt/artifacts/deploy_demo.py&quot; docker-compose upWARNING: The SAVEPOINT_OPTIONS variable is not set. Defaulting to a blank string.Recreating docker_job-cluster_1 ... doneStarting docker_taskmanager_1 ... doneAttaching to docker_taskmanager_1, docker_job-cluster_1taskmanager_1 | Starting the task-managerjob-cluster_1 | Starting the job-cluster......job-cluster_1 | 2020-01-02 08:35:03,796 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Terminating cluster entrypoint process StandaloneJobClusterEntryPoint with exit code 0.docker_job-cluster_1 exited with code 0 在log中出现“docker_job-cluster_1 exited with code 0”表示job已执行成功，JobManager已经退出。TaskManager还需要较长的时间等待超时后才会退出，我们可以直接按快捷键Ctrl+C提前退出。 查看执行结果，可以从TaskManager的容器中将结果文件拷贝出来查看,执行 docker cp docker_taskmanager_1:/tmp/result ./; cat result Okay, 到这里本篇要与大家分享的内容已经接近尾声了，如果你期间也很顺利的成功了，可以 Cheers 了:) 小结本篇核心向大家分享了如何以多种方式部署PyFlink作业。期望在PyFlink1.10发布之后，大家能有一个顺利快速体验的快感！在开篇说道部分，为大家分享了老子倡导大家的 “致虚极，守静笃。万物并作，吾以观其复”的大道，同时也给大家带来了2020的祝福，祝福大家 “2020 安！”。]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>致虚极守静笃</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列 - 如何在PyFlink 1.10中自定义Python UDF]]></title>
    <url>%2F2019%2F12%2F05%2FApache-Flink-%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%E5%A6%82%E4%BD%95%E5%9C%A8PyFlink-1-10%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89Python-UDF%2F</url>
    <content type="text"><![CDATA[开篇说道老子在《道德经》第七章中说：”天长地久，天地所以能长且久者，以其不自生，故能长生。是以圣人后其身而身先，外其身而身存。以其无私，故能成其私。”。其核心内容简单来说就是 “利他”，即：人为自己照亮别人，外其身而身存！这里我也分享一个真实的事情和一个我不记得在哪里读到的故事： 前些时我回老家发现村里给每个盲人发了一个拐杖，这个拐杖竟然还有手电筒，我第一反应很是诧异：”盲人要手电筒干什么？”, 不过我想聪明的你一定在笑我傻，盲人的手电筒当然是为了照亮行路。没错，盲人打手电筒照亮漆黑夜路，方便其他行人的同时，也保护了自己！ 还有一个故事，那是在战争年代，一个班长带队去打仗，当一个炮弹飞来那一刹那，他刚要卧倒，却看见一个新兵还在傻傻的站着，没有丝毫避让的意识，他立马扑过去，用身体保护着新兵，当一声巨响之后，他刚要训斥新兵为啥不懂得卧倒自保的时候，后头发现，炮弹正将他最初要卧倒的地方炸的很大的坑。 “天长地久，天地所以能长且久者，以其不自生，故能长生”，天地之所以长久是因为他们的从不考虑自身的得失，而是默默无闻的滋养着万物，万物自然割舍不下天地，天地自会伴随生生不息的万物长存。“是以圣人后其身而身先，外其身而身存。以其无私，故能成其私” 品行高尚的人总是先考虑别人的得失，当然人人都感激其德行，自然也就尊为圣人。 我们都是普通人，但我们始终要 相信小的伟大，为别人，也为自己，始终保持“利他”的精神。 这篇博客的意义每篇博客都有其预期的目标，本篇希望再Apache Flink 1.10发布之前提前给大家介绍一下PyFlink的Python UDF的功能，让Apche Fink的Python用户可以轻松的应用PyFlink开发自己的业务需求。 Python UDF的发展趋势我们知道PyFlink是在Apache Flink 1.9版新增的，那么在Apache Flink 1.10中Python UDF功能支持的速度是否能够满足用户的急切需求呢？ 直观的判断，PyFlink Python UDF的功能可以如上图一样可以迅速从幼苗变成大树，为啥有此判断，请继续往下看… Flink on Beam我们都知道有Beam on Flink的场景，就是Beam支持多种Runner，也就是说Beam SDK 编写的Job可以运行在Flink之上。如下图所示： 上面这图是Beam Portability Framework的架构图，他描述了Beam如何支持多语言，如何支持多Runner，单独说Apache Flink的时候我们就可以说是Beam on Flink，那么怎么解释 Flink on Beam呢？ 在Apache Flink 1.10中我们所说的 Flink on Beam更精确的说是PyFlink on Beam Portability Framework。我们看一下简单的架构图，如下：Beam Portability Framework 是一个成熟的多语言支持框架，框架高度抽象了语言之间的通信协议(gRPC),定义了数据的传输格式(Protobuf)，并且根据通用流计算框架所需要的组件，抽象个各种服务，比如DataService，StateService，MetricsService等。在这样一个成熟的框架下，PyFlink可以快速的构建自己的Python算子，同时重用Apache Beam Portability Framework中现有SDK harness组件，可以支持多种Python运行模式，如：Process，Docker，etc.，这使得PyFlink对Python UDF的支持变得非常容易，在Apache Flink 1.10中的功能也非常的稳定和完整。那么为啥说是Apache Flink和Apache Beam 共同打造呢，是因为我发现目前Apache Beam Portability Framework的框架也存在很多优化的空间，所以我在Beam社区进行了优化讨论,详情，并且在Beam 社区也贡献了20+的优化补丁，详情。概要了解了Apache Flink 1.10中Python UDF的架构之后，我们还是切入的代码部分，看看如何开发和使用Python UDF… 如何定义Python UDF在Apache Flink 1.10中我们有多种方式进行UDF的定义，比如： Extend ScalarFunction, e.g.: 123class HashCodeMean(ScalarFunction): def eval(self, i, j): return (hash(i) + hash(j)) / 2 Lambda Function 1lambda i, j: (hash(i) + hash(j)) / 2 Named Function 12def hash_code_mean(i, j): return (hash(i) + hash(j)) / 2 Callable Function 123class CallableHashCodeMean(object): def __call__(self, i, j): return (hash(i) + hash(j)) / 2 我们发现上面定义函数除了第一个扩展ScalaFunction的方式是PyFlink特有的，其他方式都是Python语言本身就支持的，也就是说，在Apache Flink 1.10中PyFlink允许以任何Python语言所支持的方式定义UDF。 如何使用Python UDF那么定义完UDF我们应该怎样使用呢？Apache Flink 1.10中提供了2种Decorators，如下： Decorators - udf(), e.g. : 12udf(lambda i, j: (hash(i) + hash(j)) / 2, [for input types], [for result types]) Decorators - @udf, e.g. : 123@udf(input_types=..., result_type=...) def hash_code_mean(…): return … 然后在使用之前进行注册，如下： 1st_env.register_function(&quot;hash_code&quot;, hash_code_mean) 接下来就可以在Table API/SQL中进行使用了，如下： 1my_table.select(&quot;hash_code_mean(a, b)&quot;).insert_into(&quot;Results&quot;) 目前为止，我们已经完成了Python UDF的定义，声明和注册了。接下来我们还是看一个完整的势力吧：） 案例描述 需求： 假设 苹果 公司要统计该公司产品在双11期间各城市的销售数量和销售金额分布情况。 数据格式每一笔订单是一个字符串，字段用逗号分隔, 例如: 1234ItemName, OrderCount, Price, City-------------------------------------------iPhone 11, 30, 5499, Beijing\niPhone 11 Pro,20,8699,Guangzhou\n 案例分析根据案例的需求和数据结构分析，我们需要对原始字符串进行结构化解析，那么需要一个按“，”号分隔的UDF(split)和一个能够将各个列信息展平的DUF(get)。同时我们需要根据城市进行分组统计。 核心实现UDF定义 Split UDF 1234@udf(input_types=[DataTypes.STRING()], result_type=DataTypes.ARRAY(DataTypes.STRING())) def split(line): return line.split(&quot;,&quot;) Get UDF 123@udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())def get(array, index): return array[index] 注册UDF 注册 Split UDF 1t_env.register_function(&quot;split&quot;, split) 注册 Get UDF 1t_env.register_function(&quot;get&quot;, get) 核心实现逻辑如下代码我们发现核心实现逻辑非常简单，只需要对数据进行解析和对数据进行集合计算： 12345t_env.from_table_source(SocketTableSource(port=9999))\ .alias(&quot;line&quot;)\ .select(&quot;split(line) as str_array&quot;)\ .select(&quot;get(str_array, 3) as city, &quot; &quot;get(str_array, 1).cast(LONG) as count, &quot; &quot;get(str_array, 2).cast(LONG) as unit_price&quot;)\ .select(&quot;city, count, count * unit_price as total_price&quot;)\ .group_by(&quot;city&quot;)\ .select(&quot;city, sum(count) as sales_volume, sum(total_price) as sales&quot;)\ .insert_into(&quot;sink&quot;) t_env.execute(&quot;Sales Statistic&quot;) 上面的代码我们假设是一个Socket的Source，Sink是一个Chart Sink，那么最终运行效果图，如下： 我总是认为在博客中只是文本描述而不能让读者真正的在自己的机器上运行起来的博客，不是好博客，所以接下来我们看看按照我们下面的操作，是否能在你的机器上也运行起来？ :) 环境因为目前PyFlink已经部署到PyPI上面了，安装PyFlink只需要使用如下命令： 检查Python版本 12$ python --version# the version printed here must be 3.5+ 安装PyFlink 1$ python -m pip install apache-flink==1.10.0 检查核心包我们核心需要apache-beam和apache-flink，如下命令： 1234567$ pip listPackage Version ----------------------------- ---------alabaster 0.7.12 apache-beam 2.15.0 apache-flink 1.10.0atomicwrites 1.3.0 如上信息证明你我们所需的Python依赖已经okay了，接下来回过头来在看看如何进行业务需求的开发。 PyFlinlk的Job结构一个完成的PyFlink的Job需要有外部数据源的定义，有业务逻辑的定义和最终计算结果输出的定义。也就是 Source connector， Transformations， Sink connector，接下来我们根据这个三个部分进行介绍来完成我们的需求。 Source Connector我们需要实现一个Socket Connector，首先要实现一个StreamTableSource, 核心代码是实现getDataStream,代码如下： 123456@Override public DataStream&lt;Row&gt; getDataStream(StreamExecutionEnvironment env) &#123; return env.socketTextStream(hostname, port, lineDelimiter, MAX_RETRY) .flatMap(new Spliter(fieldNames.length, fieldDelimiter, appendProctime)) .returns(getReturnType()); &#125; 上面代码利用了StreamExecutionEnvironment中现有socketTextStream方法接收数据，然后将业务订单数据传个一个FlatMapFunction, FlatMapFunction主要实现将数据类型封装为 Row,详细代码查阅 Spliter 同时，我们还需要在Python封装一个SocketTableSource,详情查阅 socket_table_source.py Sink Connector我们预期要得到的一个效果是能够将结果数据进行图形化展示，简单的思路是将数据写到一个本地的文件，然后在写一个HTML页面，使其能够自动更新结果文件，并展示结果。所以我们还需要自定义一个Sink来完成该功能，我们的需求计算结果是会不断的更新的，也就是涉及到Retraction（如果大家不理解这个概念，可以查阅我以前的博客），目前在Flink里面还没有默认支持Retract的Sink，所以我们需要自定义一个RetractSink,比如我们实现一下CsvRetractTableSink. CsvRetractTableSink的核心逻辑是缓冲计算结果，每次更新进行一次全量（这是个纯demo，不能用于生产环境）文件输出。源代码查阅CsvRetractTableSink。 同时我们还需要利用Python进行封装，详见 chart_table_sink.py 在chart_table_sink.py我们封装了一个http server，这样我们可以在浏览器中查阅我们的统计结果。 业务逻辑完成自定义的 Source 和 Sink之后我们终于可以进行业务逻辑的开发了，其实整个过程自定义 Source和Sink是最麻烦的，核心计算逻辑似乎要简单的多。 设置Python版本（很重要）如果你本地环境 python 命令版本是2.x，那么需要对Python版本进行设置，如下： 1t_env.get_config().set_python_executable(&quot;python3&quot;) PyFlink 1.10之后支持Python3.6+版本。 读取数据源PyFlink读取数据源非常简单，如下： 123......t_env.from_table_source(SocketTableSource(port=9999)).alias(&quot;line&quot;) 上面这一行代码定义了监听端口 9999 的数据源，同时结构化Table只有一个名为line的列。 解析原始数据我们需要对上面列进行分析，为了演示 Python UDF，我们在SocketTableSource中并没有对数据进行预处理，所以我们利用上面 UDF定义 一节定义的UDF，来对原始数据进行预处理。 12345.......select(&quot;split(line) as str_array&quot;) .select(&quot;get(str_array, 3) as city, &quot; &quot;get(str_array, 1).cast(LONG) as count, &quot; &quot;get(str_array, 2).cast(LONG) as unit_price&quot;) .select(&quot;city, count, count * unit_price as total_price&quot;) 统计分析核心的统计逻辑是根据 city 进行分组，然后对 销售数量和销售金额进行求和，如下： 12345.......group_by(&quot;city&quot;).select(&quot;city, sum(count) as sales_volume, sum(total_price) as sales&quot;)\ 计算结果输出计算结果写入到我们自定义的Sink中，如下： 123.......insert_into(&quot;sink&quot;) 完整的代码（blog_demo.py） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.demo import ChartConnector, SocketTableSourcefrom pyflink.table import StreamTableEnvironment, EnvironmentSettings, DataTypesfrom pyflink.table.descriptors import Schemafrom pyflink.table.udf import udfenv = StreamExecutionEnvironment.get_execution_environment()t_env = StreamTableEnvironment.create( env, environment_settings=EnvironmentSettings.new_instance().use_blink_planner().build())t_env.connect(ChartConnector())\ .with_schema(Schema() .field(&quot;city&quot;, DataTypes.STRING()) .field(&quot;sales_volume&quot;, DataTypes.BIGINT()) .field(&quot;sales&quot;, DataTypes.BIGINT()))\ .register_table_sink(&quot;sink&quot;)@udf(input_types=[DataTypes.STRING()], result_type=DataTypes.ARRAY(DataTypes.STRING()))def split(line): return line.split(&quot;,&quot;)@udf(input_types=[DataTypes.ARRAY(DataTypes.STRING()), DataTypes.INT()], result_type=DataTypes.STRING())def get(array, index): return array[index]t_env.get_config().set_python_executable(&quot;python3&quot;)t_env.register_function(&quot;split&quot;, split)t_env.register_function(&quot;get&quot;, get)t_env.from_table_source(SocketTableSource(port=6666))\ .alias(&quot;line&quot;)\ .select(&quot;split(line) as str_array&quot;)\ .select(&quot;get(str_array, 3) as city, &quot; &quot;get(str_array, 1).cast(LONG) as count, &quot; &quot;get(str_array, 2).cast(LONG) as unit_price&quot;)\ .select(&quot;city, count, count * unit_price as total_price&quot;)\ .group_by(&quot;city&quot;)\ .select(&quot;city, &quot; &quot;sum(count) as sales_volume, &quot; &quot;sum(total_price) as sales&quot;)\ .insert_into(&quot;sink&quot;)t_env.execute(&quot;Sales Statistic&quot;) 上面代码中大家会发现一个陌生的部分，就是 from pyflink.demo import ChartConnector, SocketTableSource. 其中 pyflink.demo是哪里来的呢？其实就是包含了上面我们介绍的 自定义Source/Sink（Java&amp;Python)。 下面我们来介绍如何增加这个 pyflink.demo模块。 安装 pyflink.demo为了大家方便我把自定义Source/Sink（Java&amp;Python)的源代码放到了 这里 ，大家可以进行如下操作： 下载源码 1git clone https://github.com/sunjincheng121/enjoyment.code.git 编译源码 1cd enjoyment.code/PyUDFDemoConnector/; mvn clean install 构建发布包 1234567python3 setup.py sdist bdist_wheel......adding &apos;pyflink_demo_connector-0.1.dist-info/WHEEL&apos;adding &apos;pyflink_demo_connector-0.1.dist-info/top_level.txt&apos;adding &apos;pyflink_demo_connector-0.1.dist-info/RECORD&apos;removing build/bdist.macosx-10.14-x86_64/wheel 安装Pyflink.demo 123456pip3 install dist/pyflink-demo-connector-0.1.tar.gz......Successfully built pyflink-demo-connectorInstalling collected packages: pyflink-demo-connectorSuccessfully installed pyflink-demo-connector-0.1 出现上面信息证明已经将PyFlink.demo模块成功安装。接下来我们可以运行我们的示例了 :) 运行示例示例的代码在上面下载的源代码里面已经包含了，为了简单，我们利用PyCharm打开enjoyment.code/myPyFlink。 同时在Terminal 启动一个端口： 1nc -l 6666 启动blog_demo,如果一切顺利，启动之后，控制台会输出一个web地址，如下所示：我们打开这个页面，开始是一个空白页面，如下： 我们尝试将下面的数据，一条，一条的发送给Source Connector： 12345678910111213iPhone 11,30,5499,BeijingiPhone 11 Pro,20,8699,GuangzhouMacBook Pro,10,9999,BeijingAirPods Pro,50,1999,BeijingMacBook Pro,10,11499,ShanghaiiPhone 11,30,5999,ShanghaiiPhone 11 Pro,20,9999,ShenzhenMacBook Pro,10,13899,HangzhouiPhone 11,10,6799,BeijingMacBook Pro,10,18999,BeijingiPhone 11 Pro,10,11799,ShenzhenMacBook Pro,10,22199,ShanghaiAirPods Pro,40,1999,Shanghai 当输入第一条订单 iPhone 11,30,5499,Beijing,之后，页面变化如下： 随之订单数据的不断输入，统计图不断变化。一个完整的gif演示如下： 小结本篇从架构到UDF接口定义，在到具体的实例，向大家介绍了在Apache Flink 1.10 发布之后，如何利用PyFlink进行业务开发，其中 用户自定义 Source和Sink部分比较复杂，这也是目前社区需要进行改进的部分(Java/Scala)。真正的核心逻辑部分其实比较简单，为了大家按照本篇进行实战操作有些成就感，所以我增加了自定义Source/Sink和图形化部分。但如果大家想简化实例的实现也可以利用Kafka作为Source和Sink，这样就可以省去自定义的部分，做起来也会简单一些。 同时该篇的 开篇说道 部分重点分享了老子 “利他” 的思想，希望我们大家共勉，做一个有助于他人的人。谢谢大家！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>PyFlink</tag>
        <tag>外其身而身存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上)]]></title>
    <url>%2F2019%2F09%2F09%2FApache%20Flink%20%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%20Python%20API%20%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20Java%20%E8%87%AA%E5%AE%9A%E4%B9%89%20Connector%20(%E4%B8%8A)%2F</url>
    <content type="text"><![CDATA[开篇说道老子说”孰能浊以静之徐清? 孰能安以动之徐生?”,大概是说谁能像浑浊的水流一样停止流动，安静下来慢慢变得澄清？谁可以像草木那样保持长时的静寂，却萌动生机而不息？对于与世为人来说，这句话是指谁能在浊世中保持内心的清净？只有智者可以静看花开花落，就像是莲花，出淤泥而不染。”孰能安以动之徐生” 一句则说在保持安静的同时，还要激发着无穷的生机，顺应自然的同时，默默改变这人世界。老子并不提倡举世皆浊我独清的态度，而是提倡“混兮其若浊”、“和其光，同其尘”，也就是说不要认为自己了不起，每个人的行为都有其道理，我们应该理解他人和接纳他人，和他人一样，表面看起来有点浑浑噩噩，但内心要”静”,要”清”，就像一杯含有沙子的黄河水，只有在安静的状态下，才能慢慢变清，我们内心要从纷繁复杂的”浊”中理清头绪，变得清醒。但静静地弄清楚了还不够，还需要动起来，给外界反馈，发挥自己积极的作用。但切记要安静，即心态要平稳，如果急于求成，慌里慌张地做事情，那就会变成真的”浊”，变得稀里糊涂了。老子建议做一切事都要不急不躁，不“乱”不“浊”，一切要悠然”徐生”，水到渠成。 问题为什么要聊在Python API 中如何使用 Java 自定义 Source/Sink？目前在Apache Flink中集成了一些常用的connector，比如Kafka,elasticsearch6,filesystem等常用的外部系统。但这仍然无法满足需求各异的用户。比如用户用Kafka但是可能用户使用的Kafka版本Flink并没有支持（当然社区也可以添加支持，我们只是说可能情况），还比如用户想使用自建的内部产品，那一定无法集成用户的内部产品到Flink社区，这时候就要用户根据Flink自定义Connector的规范开发自己的Connector了。那么本篇我们就要介绍如果你已经完成了自定义connector，如何在Python Table API中进行使用。 如何自定义Java ConnectorApache Flink Table API/SQL中采用了Service Provider Interfaces (SPI)方式查找系统内置的和用户自定义的Connector。简单说任何Connector都需一个对应的TableFactory的实现，其核心方法是requiredContext,该方法定义对应Connector的唯一特征属性，connector.type，connector.version等。Connector的类型和版本就唯一决定了一个具体的Connector实现。当然任何Connector都要实现具体的TableSource或TableSink。关于自定义Java Connector我们会在一篇完整的Blog进行详细叙述，本篇重点集中在如果已经完成了自定义的Connector，我们如何在Python Table API中使用。 Python Table API中注册Connector我们知道Flink Python Table API中采用Py4j的方式将Python Table API转换为Java Table API，而Java Table API中采用SPI的方式发现和查找Connector，也就是说Pytohn API需要定义一种方式能够告诉Java Table API 用户自定义的Connector信息。对于一个自定义Connector有两个非常重要的信息： CustomConnectorDescriptor - Connector自有的属性定义； CustomFormatDescriptor - Connector的数据格式定义； 所以在Python Table API中我们利用 CustomConnectorDescriptor 和CustomFormatDescriptor来描述用户自定义Connector。 自定义Connector类加载到Classpath用户要使用自定义的Connector，那么第一步是将用户自定义的类要加载到Classpath中，操作方式与Python API中使用Java UDF一致，详见：《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》 Python Table API 描述自定义Connector我们这里以自定义Kafka Source为例，用户编写Python Table API如下： 定义Connector属性我们需要利用CustomConnectorDescriptor来描述Connector属性，如下：1234567custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \ .property(&apos;connector.topic&apos;, &apos;user&apos;) \ .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \ .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \ .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \ .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \ .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;) 上面属性与内置的Kafka 0.11版本一致。这里是示例如何利用CustomConnectorDescriptor描述自定义Connector属性。 定义Connector的数据格式我们需要利用CustomFormatDescriptor来描述Connector数据格式，如下：12345678910111213141516171819202122# the key is &apos;format.json-schema&apos; custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \ .property(&apos;format.json-schema&apos;, &quot;&#123;&quot; &quot; type: &apos;object&apos;,&quot; &quot; properties: &#123;&quot; &quot; a: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; b: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; c: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; time: &#123;&quot; &quot; type: &apos;string&apos;,&quot; &quot; format: &apos;date-time&apos;&quot; &quot; &#125;&quot; &quot; &#125;&quot; &quot;&#125;&quot;) \ .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;) 这个数据格式是沿用了在 《Apache Flink 说道系列- Python API 中如何使用 Kafka》一篇中的示例。 定义Table Source完成Connector属性的描述和Connector数据格式描述之后，我们就可以同使用内置Connector一样定义Table Source了。如下：12345st_env \ .connect(custom_connector) \ .with_format(custom_format) \ .with_schema(...) \ .register_table_source(&quot;source&quot;) 上面with_schema(...)是定义数据流入Flink内部后的Table 数据结构。 定义查询逻辑123456st_env.scan(&quot;source&quot;) .window(Tumble.over(&quot;2.rows&quot;) .on(&quot;proctime&quot;).alias(&quot;w&quot;)) \ .group_by(&quot;w, a&quot;) \ .select(&quot;a, max(b)&quot;) .insert_into(&quot;result&quot;) 查询逻辑是一个简单的每2行作为一个Tumble Window。计算按 a 分组统计 b 的最大值。 完整示例完整的自定义Kafka Source示例: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768if __name__ == &apos;__main__&apos;: custom_connector = CustomConnectorDescriptor(&apos;kafka&apos;, 1, True) \ .property(&apos;connector.topic&apos;, &apos;user&apos;) \ .property(&apos;connector.properties.0.key&apos;, &apos;zookeeper.connect&apos;) \ .property(&apos;connector.properties.0.value&apos;, &apos;localhost:2181&apos;) \ .property(&apos;connector.properties.1.key&apos;, &apos;bootstrap.servers&apos;) \ .property(&apos;connector.properties.1.value&apos;, &apos;localhost:9092&apos;) \ .properties(&#123;&apos;connector.version&apos;: &apos;0.11&apos;, &apos;connector.startup-mode&apos;: &apos;earliest-offset&apos;&#125;) # the key is &apos;format.json-schema&apos; custom_format = CustomFormatDescriptor(&apos;json&apos;, 1) \ .property(&apos;format.json-schema&apos;, &quot;&#123;&quot; &quot; type: &apos;object&apos;,&quot; &quot; properties: &#123;&quot; &quot; a: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; b: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; c: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; time: &#123;&quot; &quot; type: &apos;string&apos;,&quot; &quot; format: &apos;date-time&apos;&quot; &quot; &#125;&quot; &quot; &#125;&quot; &quot;&#125;&quot;) \ .properties(&#123;&apos;format.fail-on-missing-field&apos;: &apos;true&apos;&#125;) s_env = StreamExecutionEnvironment.get_execution_environment() s_env.set_parallelism(1) s_env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime) st_env = StreamTableEnvironment.create(s_env) result_file = &quot;/tmp/custom_kafka_source_demo.csv&quot; if os.path.exists(result_file): os.remove(result_file) st_env \ .connect(custom_connector) \ .with_format( custom_format ) \ .with_schema( # declare the schema of the table Schema() .field(&quot;proctime&quot;, DataTypes.TIMESTAMP()) .proctime() .field(&quot;a&quot;, DataTypes.STRING()) .field(&quot;b&quot;, DataTypes.STRING()) .field(&quot;c&quot;, DataTypes.STRING()) ) \ .in_append_mode() \ .register_table_source(&quot;source&quot;) st_env.register_table_sink(&quot;result&quot;, CsvTableSink([&quot;a&quot;, &quot;b&quot;], [DataTypes.STRING(), DataTypes.STRING()], result_file)) st_env.scan(&quot;source&quot;).window(Tumble.over(&quot;2.rows&quot;).on(&quot;proctime&quot;).alias(&quot;w&quot;)) \ .group_by(&quot;w, a&quot;) \ .select(&quot;a, max(b)&quot;).insert_into(&quot;result&quot;) st_env.execute(&quot;custom kafka source demo&quot;) 详细代码见:https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/udc/CustomKafkaSourceDemo.py 运行CustomKafkaSourceDemo上面示例的运行需要一个Kafka的集群环境，集群信息需要和CustomConnectorDescriptor中描述的一致，比如connector.topic是user， zookeeper.connect, localhost:2181, bootstrap.servers是 localhost:9092。 这些信息与《Apache Flink 说道系列- Python API 中如何使用 Kafka》一篇中的Kafka环境一致，大家可以参考初始化自己的Kafka环境。 准备测试数据运行之前大家要向 user Topic中写入一些测试数据， 1234567891011121314151617if __name__ == &apos;__main__&apos;: topic = &apos;user&apos; topics = list_topics() if topic in topics: delete_topics(topic) create_topic(topic) msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;] for msg in msgs: send_msg(topic, msg) # print test data get_msg(topic) 完整代码可以参考：https://github.com/sunjincheng121/enjoyment.code/blob/master/myPyFlink/enjoyment/kafka/prepare_data.py 运行代码，并查看最终计算结果![338566533211a283d7a715ba32009fd2](Apache Flink 说道系列- Python API 中如何使用 Java 自定义 Connector (上).resources/646B6616-4AA7-4A04-AAE8-A6D1EE006535.png) 待续上面示例是在假设用户已经完成了 Java 自定义Connector的基础之上进行介绍如何利用Python Table API使用自定义Connector，但并没有详细介绍用户如何自定义Java Connector。所以后续我会为大家单独在一篇Blog中介绍Apache Flink 如何自定义Java Connector。官方文档 小结本篇内容篇幅比较短小，具体介绍了Python Table API如何使用自定义Java Connector。其中Kafka环境相关内容需要结合《Apache Flink 说道系列- Python API 中如何使用 Kafka》一篇一起阅读。同时在开篇说道中简单分享了老子关于”孰能浊以静之徐清? 孰能安以动之徐生?”的人生智慧，希望对大家有所启迪！ 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>Python</tag>
        <tag>Connector</tag>
        <tag>孰能浊以静之徐清</tag>
        <tag>孰能安以动之徐生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列- Python API 中如何使用 Kafka]]></title>
    <url>%2F2019%2F08%2F28%2FApache%20Flink%20%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%20Python%20API%20%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20Kafka%2F</url>
    <content type="text"><![CDATA[开篇说道老子说”圣人为腹不为目”，核心意思是说看问题要看本质，不要着眼于外表。交人处事也是一样，不要看其外表和语言要看内心和本质。这里和大家分享一个故事，”九方皋(Gao)相马”。 春秋时期的伯乐是识别千里马的大师，在他上了年纪之后 秦穆公 对他说:”在你的子孙里面给我推荐一个能够识别千里马的人才吧？”，伯乐说:”在我的子孙中没有这样的人才，不过我有个朋友，名叫 九方皋 是个相马的高手，我推荐给您吧”。 于是 秦穆公 召见 九方皋，让他去出巡千里马，过了数月九方皋回来禀告说:”我已经找到千里马了，就在沙丘那个地方”。于是 秦穆公 兴奋的问:”那是一匹什么样子的马啊”？九方皋 回答:”是一匹黄色的母马”。不过 秦穆公 派人把那匹马找来发现是一匹黑色的公马。秦穆公很生气，就派人把伯乐叫来说:”你推荐的相马的人实在糟透了，他连马的颜色和公母都分不清，怎么能认识千里马呢”？ 伯乐听了内心无比佩服的对 秦穆公 说:”这就是他比我还高明的地方，他重视的是马的精神，忽略了马的外表，重视马的内在品质，而忽略了马的颜色和雌雄，九方皋只关注他所关注的重点，而忽略不重要的特征，这样才真的能找到难寻的好马啊”。后来经过检验这的确是一匹天下少有的千里马。 所以”圣人为腹不为目”，注重本质而忽略外在。祝愿大家在日常生活和工作中能看清事情的内在和深层次的所在，而后做出你最正确的决定。 如何创建SourceApache Flink 1.9 有两种方式创建Source： Table Descriptor Table Source Table Descriptor利用Descriptor方式创建Source是比较推荐的做法，以简单的CSV示例如下： 123456789t_env.connect(FileSystem() .path(source_path) .with_format(OldCsv() .field(&quot;word&quot;, DataTypes.STRING())) .with_schema(Schema() .field(&quot;word&quot;, DataTypes.STRING())) .register_table_source(&quot;source&quot;)table = t_env.scan(&quot;source&quot;) Table Source创建TableSource的方式也可以完成Source的创建，如下: 12345678csv_source = CsvTableSource( source_path, [&quot;word&quot;], [DataTypes.STRING()]) t_env.register_table_source(&quot;source&quot;, csv_source)table = t_env.scan(&quot;source&quot;) 如何创建Sink和创建Source一样，Apache Flink 1.9 有两种方式创建Sink： Table Descriptor Table Sink Table Descriptor利用Descriptor方式创建Sink是比较推荐的做法，以简单的CSV示例如下： 123456789t_env.connect(FileSystem() .path(result_path) .with_format(OldCsv() .field(&quot;word&quot;, DataTypes.STRING())) .with_schema(Schema() .field(&quot;word&quot;, DataTypes.STRING())) .register_table_sink(&quot;results&quot;)result_table.insert_into(&quot;results&quot;) Table Sink创建TableSink的方式也可以完成Sink的创建，如下: 123456csv_sink = CsvTableSink( [&quot;word&quot;], [DataTypes.STRING()]) t_env.register_table_sink(&quot;results&quot;, csv_sink)result_table.insert_into(&quot;results&quot;) 关于Format和Schema不论是Source和Sink在利用Table Descriptor进行实例化的时候都是需要设置Format和Schema的，那么什么是Format和Schema呢？ Format - 是描述数据在外部存储系统里面的数据格式。 Schema - 是外部数据加载到Flink系统之后，形成Table的数据结构描述。 Kafka connector JAR加载如《Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业》所描述的问题一样，我们需要显示的将Kafka connector JARs添加到classpath下面。所以在源码构建PyFlink发布包时候，需要在完成源码编译之后，将Kafka相关的JARs复制到 build-target/lib下面。并且需要对应的Format的JARs。以kafka-0.11和Json Format为例： Copy Kafka相关JARs 1cp flink-connectors/flink-sql-connector-kafka-0.11/target/flink-sql-connector-kafka-0.11_*-SNAPSHOT.jar build-target/lib Copy Json Format相关JARs 1cp flink-formats/flink-json/target/flink-json-*-SNAPSHOT-sql-jar.jar build-target/lib 构建PyFlink发布包并安装： 12cd flink-python; python setup.py sdist pip install dist/*.tar.gz --user 环境安装的详细内容可以参考《Apache Flink 说道系列- Python Table API 开发环境搭建》。 安装Kafka环境如果你已经有Kafka的集群环境，可以忽略本步骤，如果没有，为了完成本篇的测试，你可以部署一个简单的Kafka环境。 下载Kafka安装包 1wget https://archive.apache.org/dist/kafka/0.11.0.3/kafka_2.11-0.11.0.3.tgz 解压 1tar zxvf kafka_2.11-0.11.0.3.tgz 启动Zookeeper 12345cd kafka_2.11-0.11.0.3; bin/zookeeper-server-start.sh config/zookeeper.properties最终输出：[2019-08-28 08:47:16,437] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)[2019-08-28 08:47:16,478] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory) 支持删除Topic要支持删除Topic，我们需要修改server.properties配置，将delete.topic.enable=true 打开。 启动Kafka 12345bin/kafka-server-start.sh config/server.properties最终输出：[2019-08-28 08:49:20,280] INFO Kafka commitId : 26ddb9e3197be39a (org.apache.kafka.common.utils.AppInfoParser)[2019-08-28 08:49:20,281] INFO [Kafka Server 0], started (kafka.server.KafkaServer) 如上过程我们完成了简单Kafka的环境搭建。更多细节参考Kafka官方文档 编写一个简单示例这个示例我们从Kafka读取数据，然后创建一个简单的tumble window，最终将计算结果写到CSV文件系统里面。 数据结构我们在Kafka中以Json的数据格式进行存储，假设有如下数据结构： 123456789101112131415161718&#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;a&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;b&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;c&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;time&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date-time&quot; &#125; &#125;&#125; 如上数据结构中time是为了演示event-time window，其他字段没有特殊含义。 数据准备我们按如上数据结构来向Kafka中写入测试数据，我用python操作Kafka，需要安装Kafka的PythonAPI，如下： 1pip install kafka-python 将测试数据写入Kafka： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding: UTF-8 -*-from kafka import KafkaProducerfrom kafka import KafkaAdminClientfrom kafka import KafkaConsumerfrom kafka.admin import NewTopicimport json# 写入消息def send_msg(topic=&apos;test&apos;, msg=None): producer = KafkaProducer(bootstrap_servers=&apos;localhost:9092&apos;, value_serializer=lambda v: json.dumps(v).encode(&apos;utf-8&apos;)) if msg is not None: future = producer.send(topic, msg) future.get()# 读取消息def get_msg(topic=&apos;test&apos;): consumer = KafkaConsumer(topic, auto_offset_reset=&apos;earliest&apos;) for message in consumer: print(message)# 查询所有Topicdef list_topics(): global_consumer = KafkaConsumer(bootstrap_servers=&apos;localhost:9092&apos;) topics = global_consumer.topics() return topics# 创建Topicdef create_topic(topic=&apos;test&apos;): admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;) topics = list_topics() if topic not in topics: topic_obj = NewTopic(topic, 1, 1) admin.create_topics(new_topics=[topic_obj])# 删除Topicdef delete_topics(topic=&apos;test&apos;): admin = KafkaAdminClient(bootstrap_servers=&apos;localhost:9092&apos;) topics = list_topics() if topic in topics: admin.delete_topics(topics=[topic])if __name__ == &apos;__main__&apos;: topic = &apos;mytopic&apos; topics = list_topics() if topic in topics: delete_topics(topic) create_topic(topic) msgs = [&#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 1, &apos;c&apos;: 1, &apos;time&apos;: &apos;2013-01-01T00:14:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 2, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T00:24:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 3, &apos;c&apos;: 3, &apos;time&apos;: &apos;2013-01-01T00:34:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 4, &apos;c&apos;: 4, &apos;time&apos;: &apos;2013-01-01T01:14:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;b&apos;, &apos;b&apos;: 4, &apos;c&apos;: 5, &apos;time&apos;: &apos;2013-01-01T01:24:13Z&apos;&#125;, &#123;&apos;a&apos;: &apos;a&apos;, &apos;b&apos;: 5, &apos;c&apos;: 2, &apos;time&apos;: &apos;2013-01-01T01:34:13Z&apos;&#125;] for msg in msgs: send_msg(topic, msg) # print test data get_msg(topic) 源码:prepare_data.py 运行上面的代码，控制台会输出如下：如上信息证明我们已经完成了数据准备工作。 开发示例创建Kafka的数据源表我们以Table Descriptor的方式进行创建： 1234567891011121314151617181920212223242526272829303132333435363738394041st_env.connect(Kafka() .version(&quot;0.11&quot;) .topic(&quot;user&quot;) .start_from_earliest() .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;) .property( &quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)) .with_format(Json() .fail_on_missing_field(True) .json_schema( &quot;&#123;&quot; &quot; type: &apos;object&apos;,&quot; &quot; properties: &#123;&quot; &quot; a: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; b: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; c: &#123;&quot; &quot; type: &apos;string&apos;&quot; &quot; &#125;,&quot; &quot; time: &#123;&quot; &quot; type: &apos;string&apos;,&quot; &quot; format: &apos;date-time&apos;&quot; &quot; &#125;&quot; &quot; &#125;&quot; &quot;&#125;&quot; ) ) .with_schema(Schema() .field(&quot;rowtime&quot;, DataTypes.TIMESTAMP()) .rowtime(Rowtime() .timestamps_from_field(&quot;time&quot;) .watermarks_periodic_bounded(60000)) .field(&quot;a&quot;, DataTypes.STRING()) .field(&quot;b&quot;, DataTypes.STRING()) .field(&quot;c&quot;, DataTypes.STRING()) ) .in_append_mode() .register_table_source(&quot;source&quot;) 创建CSV的结果表我们以创TableSink实例的方式创建Sink表： 12345678st_env.register_table_sink(&quot;result_tab&quot;, CsvTableSink( [&quot;a&quot;, &quot;b&quot;], [DataTypes.STRING(), DataTypes.STRING()], result_file)) 创1小时的Tumble窗口聚合123456st_env.scan(&quot;source&quot;) .window(Tumble .over(&quot;1.hours&quot;).on(&quot;rowtime&quot;).alias(&quot;w&quot;)) .group_by(&quot;w, a&quot;) .select(&quot;a, max(b)&quot;) .insert_into(&quot;result_tab&quot;) 完整代码 tumble_window.py运行源码，由于我们执行的是Stream作业，作业不会自动停止，我们启动之后，执行如下命令查看运行结果： 1cat /tmp/tumble_time_window_streaming.csv 小结本篇核心是向大家介绍在Flink Python Table API如何读取Kafka数据，并以一个Event-Time的TumbleWindow示例结束本篇介绍。开篇说道部分想建议大家要做到 “圣人为腹不为目”，任何事情都要追求其本质。愿你在本篇中有所收获！ 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>圣人为腹不为目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列- 如何在IDE中运行使用Java UDFs 的Python 作业]]></title>
    <url>%2F2019%2F08%2F11%2FApache%20Flink%20%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%20%E5%A6%82%E4%BD%95%E5%9C%A8IDE%E4%B8%AD%E8%BF%90%E8%A1%8C%E4%BD%BF%E7%94%A8Java%20UDFs%20%E7%9A%84Python%20%E4%BD%9C%E4%B8%9A%2F</url>
    <content type="text"><![CDATA[开篇说道《老子·第九章》：”持而盈之，不如其已；揣而锐之，不可长保。金玉满堂，莫之能守；富贵而骄，自遗其咎。功成身退，天之道也” 在我看来老子说的这些内容，是告诉人们做事不能过于激进，不能贪得无厌，不能因为自己的得势就蛮横无理，任何事情要懂得知足常乐，懂得低调谦让，所谓谦受益满招损，过犹不及，物极必反！ 如果你读了本篇文章能够有意识的今后不与人争长论短，那么将是极大的收获。如果你想听争长论短的坏处请评论区留言！ 问题描述Apache Flink 1.9版本版本中增加了Python Table API的支持，同时大家可以在Python API中使用Java的UDFs，但是这里有个问题在我的直播中提到：目前官方没有提供如何在IDE中运行使用Java UDFs的 Python 作业的方式，原因是没有很好的机制将 Java UDFs的JAR集成到Python的执行环境中。 问题现象如果大家在IDE中执行使用Java UDFs的Python作业会有怎样的现象呢？我还是以我直播中的代码为例: 123456t_env.register_java_function(&quot;len&quot;, &quot;org.apache.flink.udf.UDFLength&quot;) elements = [(word, 1) for word in content.split(&quot; &quot;)] t_env.from_elements(elements, [&quot;word&quot;, &quot;count&quot;]) \ .group_by(&quot;word&quot;) \ .select(&quot;word, len(word), count(1) as count&quot;) \ .insert_into(&quot;Results&quot;) 上面我注册了一个Java UDF org.apache.flink.udf.UDFLength。然后在select中使用len(word),如果我们直接执行上面代码，会有如下错误： 上面提示信息很明显，是找不到org.apache.flink.udf.UDFLength. 问题分析上面的问题大家想必都清楚，就是org.apache.flink.udf.UDFLength没有在Classpath下面，所以只要我们想办法将org.apache.flink.udf.UDFLength所在的JAR添加到classpath就行可以。听了我直播分享的同学应该知道Apache Flink 1.9 Python API的架构，在执行Python的时候本质上是调用Java的API，Apache Flink Python API是如何在运行的时候将Flink Java API的JAR添加到Classpath下面的呢？我们运行python setup.py sdist 之后，在生产的发布文件中，我们可以运行tar -tvf dist/apache-flink-1.9.dev0.tar.gz |grep jar来查看Flink的Java JARs存放位置，如下图： 我们发现只要在deps/lib目录下面的JARs在运行Python时候都是在Classpath下面的。所以我们只要想办法将我们自己定义的UDF放到deps/lib目录下就可以了。那么怎样才能将我们的JAR放到Classpath下面呢？ 解决方案如果你在安装pyflink之前可以将你需要的JAR放到 build-target/lib下面，然后执行python setup.py sdist进行打包和进行安装pip install dist/*.tar.gz这样你的的UDF的JAR也会在Classpath下面。 如果你已经安装好了，你也可以直接将你的JAR包拷贝到pip安装的目录，比如： 我们可以直接将需要的JARs拷贝到你所使用的phython环境的$PY_HOME/site-packages/pyflink/lib/目录下。 未来规划在Apache Flink 1.10版本我们会将这个过程自动化，使用一种比较方便的方式优化用户的使用体验！ 小结本篇有针对性的为大家讲解了如何在IDE中运行使用Java UDF的 Python 作业。并且 开篇说道 部分为大家分享了老子”持而盈之，不如其已”经典语句，并建议大家今后有意识的不要与人争长论短，谨记 谦受益满招损，努力做到谦逊低调，营造欢乐的交际氛围，最后祝福大家在学习Apache Flink Python API的同时，生活工作开心快乐！ 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>持而盈之-不如其已</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 视频系列 - 2019.08.06直播(德国柏林)]]></title>
    <url>%2F2019%2F08%2F08%2FApache%20Flink%20%E8%A7%86%E9%A2%91%E7%B3%BB%E5%88%97%20-%202019.08.06%E7%9B%B4%E6%92%AD(%E5%BE%B7%E5%9B%BD%E6%9F%8F%E6%9E%97)%2F</url>
    <content type="text"><![CDATA[3W What: 演讲主题《Apache Flink Python API 现状及规划》 Where: 德国柏林ververic办公区 When: 2019.08.06，德国时间下午14点，北京时间晚上20点 视频概要通过今天的分享，大家会有如下三方面的收获： 了解到Apache Flink Python API的前世今生，以及未来的规划。 了解到Apache Flink Python API 在1.9中的全新架构和清楚的知道如果搭建Python API的开发环境，了解如何开发Python API，并以各种方式提交Python 作业。 也可以掌握Python API的核心的算子的语义和使用方法 你可以在即将发布的Flink 1.9之后，轻松的进行Python API的开发。 相关资源 视频视频回放 代码https://github.com/sunjincheng121/enjoyment.code/tree/master/training0806 PPThttp://1t.click/A6s 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 视频</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>视频，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列- Python Table API 开发环境搭建]]></title>
    <url>%2F2019%2F07%2F25%2FApache%20Flink%20%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%20Python%20Table%20API%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[开篇说道 - 曲则全“曲则全，枉则直，洼则盈，敝则新，少则得，多则惑” 语出《道德经》第二十二章，整句充分反映了老子的辩证思想，老子用曲则全，枉则直，洼则盈，敝则新，少则得和多则惑来讲述了”道”顺其自然的理论。 所谓的顺其自然，有很多方面，比如前面一篇讲的”上善若水”，水自高而下的流淌也是顺其自然的体现，如果不顺其自然,而是阻止水的自上而下的流淌会发生什么？那就是 灾难，水灾。大家知道在 大禹治水 之前人类是如何处理洪水的？”拦截” 无目的的随处拦截，不是拦截不好，而是只拦截不疏通就违背自然之道了。再想想大禹治水，则是在顺应自然的同时，在疏通水道的同时在恰当的地方进行拦截，使得在不违背水自上而下的自然本性的同时合理影响水流的方向，进而造福人类。 那么本篇的”曲则全，枉则直，洼则盈，敝则新，少则得，多则惑”如何理解呢？直译一下就是：”弯曲才可保全，委屈才可伸展，低洼将可充满，敝旧将可更新，少取反而获得，多取反而迷惑”。其实不难理解，比如:”洼则盈”, “洼”也就是低洼，”盈”就是满，只有低洼的的地势才能赢得水流，也就是只有空的杯子才能装的进水。满的杯子很难再容下其他物品。其实老子无时无刻不在论证为人之道，谦卑将自己放空，才能容纳他人，聚贤聚德。下面我们再以一个简短的故事解释”曲则全”来结束本篇的论道： 春秋时期，齐国君主齐景公非常喜欢捕鸟，当时有个名叫 烛雏 的大臣专门为其捕鸟。但有一次由于 烛雏 的疏忽将捕来的鸟弄飞了。这使得齐景公勃然大怒，要杀了 烛雏。当时 晏子 也附和齐景王对 烛雏 说：你有三大罪状：1. 你的疏忽把鸟放跑了，这是第一罪状，2. 因为鸟飞了使得大王要去杀人，使大王背上了喜欢杀人的坏名声，这是你第二大罪状，3. 这件事情传出去天下人会认为大王把鸟看的比人命还重要，破坏了大王的威望，这是你第三大罪状。所以，大王请将 烛雏 处死吧？可想而知 齐景王并没有完全失去理智，虽然 晏子 没有直接劝阻他不要杀死 烛雏，但他听出了 晏子 的用意，所以自然赦免了 烛雏。 所以真正的智者会顺势而为，见机行事，无法直言的时候就要学会迂回，这也是对老子”曲则全”理论的典型运用！ 概要欲善其事必利其器，要想利用Apache Flink Python Table API进行业务开发，首先要先搭建一个开发环境。本文将细致的叙述如何搭建开发环境，最后在编写一个简单的WordCount示例程序。 依赖环境 JDK 1.8+ (1.8.0_211) Maven 3.x (3.2.5) Scala 2.11+ (2.12.0) Python 2.7+ (2.7.16) Git 2.20+ (2.20.1) 大家如上依赖配置如果有问题，可以在评论区留言。 安装PyFlink我们要想利用Apache Flink Python API 进行业务开发，需要将PyFlink发布包进行安装。目前PyFlink并没有发布到Python仓库，比如著名的PyPI(正在社区讨论，详见)，所以我们需要从源码构建。 源码构建发布包下载flink源代码1git clone https://github.com/apache/flink.git 签出release-1.9分支(1.9版本是Apache Flink Python Table API的第一个版本） 12git fetch origin release-1.9git checkout -b release-1.9 origin/release-1.9 输出如下证明已经签出了release-1.9代码： 12Branch &apos;release-1.9&apos; set up to track remote branch &apos;release-1.9&apos; from &apos;origin&apos;.Switched to a new branch &apos;release-1.9&apos; 构建Java二进制包查看我们在relese-1.9分支并进行源码构建发布包（JAVA） 1mvn clean install -DskipTests -Dfast 上面命令会执行一段时间(时间长短取决于你机器配置),最终输出如下信息，证明已经成功的完成发布包的构建： 12345678910......[INFO] flink-ml-lib ....................................... SUCCESS [ 0.190 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 14:47 min[INFO] Finished at: 2019-07-25T17:38:06+08:00[INFO] Final Memory: 537M/2085M[INFO] ------------------------------------------------------------------------ 在flink-dist/target/下的lib目录和opt目录中flink-python.jar 和 flink-table.jar 都是PyFlink必须依赖的JARs, 在下面构建PyFlink包的时候，会将这些依赖JRAs也包含在里面。 构建PyFlink发布包一般情况我们期望以pip install的方式安装python的类库，我们要想安装PyFlink的类库，也需要构建可用于pip install的发布包。执行如下命令： 1cd flink-python; python setup.py sdist 最终输出如下信息，证明是成功的： 1234567891011......copying pyflink/table/types.py -&gt; apache-flink-1.9.dev0/pyflink/tablecopying pyflink/table/window.py -&gt; apache-flink-1.9.dev0/pyflink/tablecopying pyflink/util/__init__.py -&gt; apache-flink-1.9.dev0/pyflink/utilcopying pyflink/util/exceptions.py -&gt; apache-flink-1.9.dev0/pyflink/utilcopying pyflink/util/utils.py -&gt; apache-flink-1.9.dev0/pyflink/utilWriting apache-flink-1.9.dev0/setup.cfgcreating distCreating tar archiveremoving &apos;apache-flink-1.9.dev0&apos; (and everything under it) 在dist目录的apache-flink-1.9.dev0.tar.gz就是我们可以用于pip install的PyFlink包. 安装PyFlink上面我们构建了PyFlink的发布包，接下来我们利用pip进行安装，如下： 1pip install dist/*.tar.gz 输出如下信息，证明已经安装成功： 123456Building wheels for collected packages: apache-flink Building wheel for apache-flink (setup.py) ... done Stored in directory: /Users/jincheng.sunjc/Library/Caches/pip/wheels/34/b1/23/fca6e31a6de419c9c9d75a6d11df197d538c2ef67e017d79eaSuccessfully built apache-flinkInstalling collected packages: apache-flinkSuccessfully installed apache-flink-1.9.dev0 用pip命令检查是否安装成功： 12345678910111213141516171819202122IT-C02YL16BJHD2:flink-python jincheng.sunjc$ pip listPackage Version --------------- -----------apache-flink 1.9.dev0 configparser 3.7.4 entrypoints 0.3 enum34 1.1.6 flake8 3.7.8 functools32 3.2.3.post2kafka-python 1.4.6 mccabe 0.6.1 pip 19.1.1 py4j 0.10.8.1 pycodestyle 2.5.0 pyflakes 2.1.1 python-dateutil 2.8.0 setuptools 41.0.1 six 1.12.0 tld 0.9.3 typing 3.7.4 virtualenv 16.6.1 wheel 0.33.4 其中 apache-flink 1.9.dev0就是我们刚才安装的版本。 配置IDE开发环境开发Python最好用的集成开发环境应该是PyCharm了，我们接下来介绍如果配置PyCharm环境。 下载安装包从 https://www.jetbrains.com/pycharm/ 选择你需要的版本，如果你需要mac版本，直接打开如下链接下载：https://download.jetbrains.8686c.com/python/pycharm-community-2019.2.dmg 安装配置并创建项目上面下载的应该都是可以直接执行的，一路next之后，就可以对PyCharm进行配置了。我们在启动界面选择创建一个项目，如下：点击 “Create New Project”之后，出现如下界面： 我们要项目取一个名字，比如”myPyFlink”。 选择”Existing interpreter”。 配置”interpreter” 非常重要,这个配置必须和你刚才安装PyFlink的时候(pip install)的python完全一致。 如果没有你需要的python版本，可以点击 4 所示位置进行添加。 如何知道你pip install对应的python，请使用5所示的命令”which is python”进行查看。 如果需要进行4所需要的操作，点击之后界面如下：选择配置”System Interpreter”,并配置”which is python”说输出的路径。点击 “OK”。在顺利完成上面配置之后，我们点击 “Create”，完成项目的创建，如下图：我们可以核实一下，我们依赖的External Libraries是“which is python”说输出的Python。 开发WordCount如果上面的过程一切顺利，那么我们的器已经很锋利了，开始善其事了，新建一个package,名为enjoyment,然后新建一个word_count.py。如下：然后，在弹出的界面写入word_count之后，点击 “OK”。 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-from pyflink.dataset import ExecutionEnvironmentfrom pyflink.table import TableConfig, DataTypes, BatchTableEnvironmentfrom pyflink.table.descriptors import Schema, OldCsv, FileSystemimport os# 数据源文件source_file = &apos;source.csv&apos;#计算结果文件sink_file = &apos;sink.csv&apos;# 创建执行环境exec_env = ExecutionEnvironment.get_execution_environment()# 设置并发为1 方便调试exec_env.set_parallelism(1)t_config = TableConfig()t_env = BatchTableEnvironment.create(exec_env, t_config)# 如果结果文件已经存在,就先删除if os.path.exists(sink_file): os.remove(sink_file)# 创建数据源表t_env.connect(FileSystem().path(source_file)) \ .with_format(OldCsv() .line_delimiter(&apos;,&apos;) .field(&apos;word&apos;, DataTypes.STRING())) \ .with_schema(Schema() .field(&apos;word&apos;, DataTypes.STRING())) \ .register_table_source(&apos;mySource&apos;)# 创建结果表t_env.connect(FileSystem().path(sink_file)) \ .with_format(OldCsv() .field_delimiter(&apos;,&apos;) .field(&apos;word&apos;, DataTypes.STRING()) .field(&apos;count&apos;, DataTypes.BIGINT())) \ .with_schema(Schema() .field(&apos;word&apos;, DataTypes.STRING()) .field(&apos;count&apos;, DataTypes.BIGINT())) \ .register_table_sink(&apos;mySink&apos;)# 非常简单的word_count计算逻辑t_env.scan(&apos;mySource&apos;) \ .group_by(&apos;word&apos;) \ .select(&apos;word, count(1)&apos;) \ .insert_into(&apos;mySink&apos;)# 执行Jobt_env.execute(&quot;wordcount&quot;) 执行并查看运行结果在执行job之前我们需要先创建一个数据源文件,在enjoyment包目录下，创建 source.csv文件，内容如下： 1enjoyment,Apache Flink,cool,Apache Flink 现在我们可以执行，job了： 右键word_count.py, 2. 点击运行’word_count’。运行之后，当控制台出现：”Process finished with exit code 0”，并且在enjoyment目录下出现sink.csv文件，说明我们第一个word_count就成功运行了:)我们双击 “sink.csv”,查看运行结果，如下： 到此我们Python Table API的开发环境搭建完成。 项目源代码为了方便大家快速体验本篇内容，可以下载本篇涉及到的python项目，git地址如下：https://github.com/sunjincheng121/enjoyment.code 小结本篇是为后续Flink Python Table API 的算子介绍做铺垫，介绍如何搭建开发环境。最后以一个最简单，也是最经典的WordCount具体示例来验证开发环境。同时在开篇说道部分简单为大家分享了老子所宣扬的”曲则全”的人生智慧，希望大家在学习技术的同时也能学到为人的智慧！谢谢大家的对本篇博文的查阅！ 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>曲则全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列- Python API 时代的产物]]></title>
    <url>%2F2019%2F07%2F21%2FApache%20Flink%20%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%20Python%20API%20%E6%97%B6%E4%BB%A3%E7%9A%84%E4%BA%A7%E7%89%A9%2F</url>
    <content type="text"><![CDATA[开篇说道-上善若水我们常说”知识就是力量”, “力量”也意味着”能力”，”能力”在没有增加任何限定词语的时候往往是指一个人的综合素质。比如，当阅读本文时候你是想学习Flink Python API如果你很快就能普获道重点，这是学习能力！再如，在我写这篇博文的角度说，如果我写的内容晦涩难懂，那就体现了文字表达能力问题。所以”能力”是包含方方面面的综合素质的体现！那么本篇我们说道的内容就是当你具备了很高的”能力”之后，具体一点就是如果由于你的能力突出做了领导，你将如何处理和你能力相当或者能力不如你的人的关系呢？就此你思考10秒看看你脑子里的答案是什么？ 我想能阅读本篇文章的人都是一心向善，与人友善的好同学，但是在内心很清楚”友善”的原则并且在与人发生矛盾时候妥善解决(大多数谦让他人)的同时自己的内心也能做到愉悦和平静的同学请在本篇评论区留言:) 回答刚才的问题: “如何处理和你能力相当或者能力不如你的人的关系呢？” 我的建议是：”上善若水”，出自老子的《道德经第八章》。老子用水来表达与人为事的自然之道。水至柔，她可以根据与之接触的各种物体的形状改变自己，她以各种形态存在，可以化作甘露滋润万物，可以凝聚江河载船运物！水至刚，她可以滴水穿石、无坚不摧！”刚”是一种能力，”柔”是一种态度，水有能力，但却始终 以一种谦和的低姿态，从高处流往低处！在《道德经》中老子更强调的是自然，任何事情符合自然规律！所谓”无为”并不是”无作为”而是”不违背自然规律”。所以遇到与人发生矛盾的时候，要心怀包容之心，水至柔可以容纳万物之形，人在有能力的时候也要容纳各色之人，容纳”蛮不讲理”之人，因为我们不能”以蛮治蛮”,容纳”污言秽语”之人，因为”身正影自直”，容纳”处处与你为敌”之人，因为”孤掌难鸣”，你不理它，它自消去。容纳”能弱气盛”之人，因为他因”能力不足而容易暴躁”，只要你诚心助之，让他靠自己能力而得到认可，气自然消失，最终也会变得言和语悦！所以任何事情的发生都有气必然的道理，尊重事实，顺其自然，永远修行自身，位高而卑谦，上善若水，与世无争, 无争于世！ 为啥需要 Python API开篇”上善若水”的与人之道是我们正式介绍Python API之前的开胃菜。希望是真的开胃，你还有胃口继续读完全篇。那么Apache Flink 的runtime是用Java编写的，为啥还需要增加对Python的支持呢？也许大家都很清楚，目前很多著名的开源项目都支持Python，比如Beam，spark，kafka等，Flink自然也需要增加对Python的支持，这个角度分析也许很好，但我们想想为啥这些非常火热的项目都纷纷支持Python呢？Python语言有怎样的”魔力”让这些著名的项目都青睐于他？我们看一统计数据： 最流行的编程语言我们看看行业分析公司RedMonk最新的最受欢迎的语言排名数据如下：上图从github和Stack Overflow两个维度进行分析的前十名如下： JavaScript Java Python PHP C++ C# CSS Ruby C TypeScript 我们发现Python排在了第3名，目前也非常流行的R和Go语言排在了15和16名。这个很客观的分享足以证明Python的受众是多么的庞大，任何项目对Python的支持就是在无形的扩大项目的受众用户！ 互联网最火热的领域就目前而言互联网最热的领域应该是大数据计算，以前单机的计算时代已经过去了，为啥单机时代过去了？因为单机处理能力的提高速度远远落后于数据的与日俱增的速度，以几个方面看看为啥目前处于大数据时代，而最炽热的互联网领域是大数据计算？ 为啥是大数据时代 - 数据量与日俱增随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下： 上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是： 1 Byte =8 bit 1 KB = 1,024 Bytes 1 MB = 1,024 KB 1 GB = 1,024 MB 1 TB = 1,024 GB 1 PB = 1,024 TB 1 EB = 1,024 PB 1 ZB = 1,024 EB 1 YB = 1,024 ZB 1 BB = 1,024 YB 1 NB = 1,024 BB 1 DB = 1,024 NB 看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下： 数据价值的产生 - 数据分析我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文： GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。 MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示： BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。 受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下： 但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及在业界家喻户晓！ 数据价值最大化 - 时效性每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目于2010年开始于德国柏林。 AlphaGo - 人工智能在2016谷歌AlphaGo以4:1战胜围棋世界冠军、职业九段棋手李世石之后，人们逐渐用新的眼光审视让深度学习，而且掀起了人工智能的“狂热”。百度百科对人工智能(Artificial Intelligence)，英文缩写为AI的定义是: 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新技术科学。 而机器学习是进行人工智能的一种方法或者工具。机器学习在以Spark，Flink为首的大数据技术平台中具有较高的地位，尤其Spark近些年在ML方面做了巨大的努力，同时PySpark集成了很多优秀的机器学习类库，如典型的Pandas，在这方面远远超过了Flink，所以Flink正面面对自己的不足，在Flink1.9中开启了新的ML接口和新的flink-python模块！ 那么机器学习的重要和Python又有什么关系呢？我们一下统计数据，看什么语言是最流行的机器学习语言？IBM 的数据科学家 Jean-Francois Puget 曾经做过一个有趣的分析。他爬取了著名的求职网站 indeed 上雇主的岗位要求变动趋势，来评估当下市场上最受欢迎的岗位语言。其中当他单独搜索”machine learning”时候，也可以得到一个近似的结果:其结构发现Python是与热的”machine learning”，虽然这是2016年的调查，但是也足以证明Python在”machine learning”方面的地位，同时上面我们提到的redmonk的统计数据也足以证明这一点！ 不仅仅是各种调查，我们也可以从Python的特点和现有的Python生态来说说为什么Python是机器学习的最好语言。 Python 是一种面向对象的解释型程序语言，由荷兰人(Guido van Rossum)于1989年发明，并于1991年发布了第一个版。Python 作为解释型语言，虽然跑得比谁都慢，但Python设计者的哲学是”用一种方法并且只有一种方法来做一件事”。在开发新的Python语法时，如果面临多种选择，Python开发者一般会选择明确的没有或者很少有歧义的语法。简单易学的特点促使了Python有庞大的用户群体，进而很多机器学习的类库也是由Python开发的，比如：NumPy、SciPy和结构化数据操作可以通过Pandas等等。所以Python这种丰富的生态系统为机器学习提供了一定程度的便利性，也必然成为了最受欢迎的机器学习语言！ 小结本篇重点描述了Apache Flink 为啥需要支持Python API。以实际数字说明目前我们处于一个大数据时代，数据的价值要依靠大数据分析，而由于数据的时效性的重要性而催生了著名的Apache Flink流式计算平台。目前在大数据计算时代，AI是赤手可热的发展方向，机器学习是AI的重要手段之一，而恰恰由于语言的特点和生态的优势，Python成为了机器学习最重要的语言，进而表明了Apache Flink 着手发力 Flink API的动机！所谓时势造英雄，Apache Flink Python API是时代的产物，是水顺自然而得之的必然！最后再次强调本篇所提的 “上善如水”的为人之道值得我们以实际生活来慢慢体会！ 关于评论如果你没有看到下面的评论区域，请进行翻墙！]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
        <tag>上善若水</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 说道系列 - 序]]></title>
    <url>%2F2019%2F07%2F13%2FApache-Flink-%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Who本人 孙金城，淘宝花名”金竹”，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。 WhatApache Flink 说”道”系列会分享什么呢？本系列分享的内容会围绕 Apache Flink 1.9 版本内容为核心，每篇博文以点滴知识点为主，以简短的篇幅向大家分享Apache Flink的核心技术。同时每篇文章会选择《道德经》的大道自然的典句向大家分享我对”道”的认知。整个系列致力于集技术知识和哲学思想于一体，学习即生活，生活即学习，学习是生活的一部分，生活本身也是在不断的学习和成长。所谓，“无，名天地之始，有，名万物之母。”。 Why我自己在阅读《Apache Flink 漫谈系列》博文时候，有一个感觉，就是“枯燥”和“累”，该系列的博文篇幅比较长，需要有很强学习欲望的同学才能完整的阅读全篇，虽然每篇的内容应该算是很完整和有一定的知识含量，但是没有为读者提供轻松愉悦的学习环境。就如 马爸爸 所说的要为阿里的同学提供 “认真工作，开心生活” 的工作环境，那么我也挑战一下自己，争取让我的博文能让读者有“开心学习，轻松工作”的感受！因此，我努力开启Apache Flink 说”道”系列的分享！ WhenApache Flink 说”道”系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。]]></content>
      <categories>
        <category>Apache Flink 说道</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>道德经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - SQL 概览]]></title>
    <url>%2F2019%2F04%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20SQL%20%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Apache Flink SQL 概览本篇核心目标是让大家概要了解一个完整的Apache Flink SQL Job的组成部分，以及Apache Flink SQL所提供的核心算子的语义，最后会应用Tumble Window编写一个End-to-End的页面访问的统计示例。 Apache Flink SQL Job的组成我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这个三部分，如下所所示：如上所示，一个完整的Apache Flink SQL Job 由如下三部分： Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。 Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。 Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。 Apache Flink SQL 核心算子SQL是Structured Quevy Language的缩写，最初是由美国计算机科学家Donald D. Chamberlin和Raymond F. Boyce在20世纪70年代早期从 Early History of SQL 中了解关系模型后在IBM开发的。该版本最初称为[SEQUEL: A Structured English Query Language]（结构化英语查询语言），旨在操纵和检索存储在IBM原始准关系数据库管理系统System R中的数据。SEQUEL后来改为SQL，因为“SEQUEL”是英国Hawker Siddeley飞机公司的商标。我们看一款用于特技飞行的英国皇家空军豪客Siddeley Hawk T.1A (Looks great): 在20世纪70年代后期，Oracle公司(当时叫 Relational Software，Inc.)开发了基于SQL的RDBMS，并希望将其出售给美国海军，Central Intelligence代理商和其他美国政府机构。 1979年6月，Oracle 公司为VAX计算机推出了第一个商业化的SQL实现，即Oracle V2。直到1986年，ANSI和ISO标准组正式采用了标准的”数据库语言SQL”语言定义。Apache Flink SQL 核心算子的语义设计也参考了1992 、2011等ANSI-SQL标准。接下来我们将简单为大家介绍Apache Flink SQL 每一个算子的语义。 SELECTSELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某些列, 如下图所示:对应的SQL语句如下： 1SELECT ColA, ColC FROME tab ; WHEREWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语法遵循ANSI-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：对应的SQL语句如下： 1SELECT * FROM tab WHERE ColA &lt;&gt; &apos;a2&apos; ; GROUP BYGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下：对应的SQL语句如下： 1SELECT sex, COUNT(name) AS count FROM tab GROUP BY sex ; UNION ALLUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：对应的SQL语句如下： 1SELECT * FROM T1 UNION ALL SELECT * FROM T2 UNIONUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：对应的SQL语句如下： 1SELECT * FROM T1 UNION SELECT * FROM T2 JOINJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型： JOIN - INNER JOIN LEFT JOIN - LEFT OUTER JOIN RIGHT JOIN - RIGHT OUTER JOIN FULL JOIN - FULL OUTER JOIN JOIN与关系代数的Join语义相同，具体如下： 对应的SQL语句如下(INNER JOIN)： 1SELECT ColA, ColB, T2.ColC, ColE FROM TI JOIN T2 ON T1.ColC = T2.ColC ; LEFT JOIN与INNER JOIN的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补NULL输出，如下：对应的SQL语句如下(INNER JOIN)： 1SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; 说明： 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。 RIGHT JOIN 相当于 LEFT JOIN 左右两个表交互一下位置。FULL JOIN相当于 RIGHT JOIN 和 LEFT JOIN 之后进行UNION ALL操作。 Window在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。 OverWindowOVER Window 目前支持由如下三个元素组合的8中类型： 时间 - Processing Time 和 EventTime 数据集 - Bounded 和 UnBounded 划分方式 - ROWS 和 RANGE我们以的Bounded ROWS 和 Bounded RANGE 两种常用类型，想大家介绍Over Window的语义 Bounded ROWS Over WindowBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。 语法12345678SELECT agg1(col1) OVER( [PARTITION BY (value_expression1,..., value_expressionN)] ORDER BY timeCol ROWS BETWEEN (UNBOUNDED | rowCount) PRECEDING AND CURRENT ROW) AS colName, ... FROM Tab1 value_expression - 进行分区的字表达式； timeCol - 用于元素排序的时间字段； rowCount - 是定义根据当前行开始向前追溯几行元素； 语义我们以3个元素(2 PRECEDING)的窗口为例，如下图:上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window. Bounded RANGE Over WindowBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口； 语法Bounded RANGE OVER Window的语法如下： 12345678SELECT agg1(col1) OVER( [PARTITION BY (value_expression1,..., value_expressionN)] ORDER BY timeCol RANGE BETWEEN (UNBOUNDED | timeInterval) PRECEDING AND CURRENT ROW) AS colName, ... FROM Tab1 value_expression - 进行分区的字表达式； timeCol - 用于元素排序的时间字段； timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；语义我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图： 注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window. GroupWindow根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw: Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加； Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加； Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加； 说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。 GroupWindow的语法如下： 1234567SELECT [gk], agg1(col1), ... aggN(colN)FROM Tab1GROUP BY [WINDOW(definition)], [gk] [WINDOW(definition)] - 在具体窗口语义介绍中介绍。 Tumble WindowTumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：假设我们要写一个2分钟大小的Tumble，示例SQL如下： 123SELECT gk, COUNT(*) AS pv FROM tab GROUP BY TUMBLE(rowtime, INTERVAL &apos;2&apos; MINUTE), gk Hop WindowHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠，具体语义如下：假设我们要写一个每5分钟统计近10分钟的页面访问量(PV). 123SELECT gk, COUNT(*) AS pv FROM tab GROUP BY HOP(rowtime, INTERVAL &apos;5&apos; MINUTE, INTERVAL &apos;10&apos; MINUTE), gk Session WindowSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长,具体语义如下： 假设我们要写一个统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV). 123SELECT gk, COUNT(*) AS pv FROM pageAccessSession_tab GROUP BY SESSION(rowtime, INTERVAL &apos;3&apos; MINUTE), gk 说明: 很多场景用户需要获得Window的开始和结束时间，上面的GroupWindow的SQL示例中没有体现，那么窗口的开始和结束时间应该怎样获取呢? Apache Flink 我们提供了如下辅助函数： TUMBLE_START/TUMBLE_END HOP_START/HOP_END SESSION_START/SESSION_END 这些辅助函数如何使用，请参考如下完整示例的使用方式。 完整的 SQL Job 案例上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下： region userId accessTime ShangHai U0010 2017-11-11 10:01:00 BeiJing U1001 2017-11-11 10:01:00 BeiJing U2032 2017-11-11 10:10:00 BeiJing U1100 2017-11-11 10:11:00 ShangHai U0011 2017-11-11 12:10:00 Source 定义自定义Apache Flink Stream Source需要实现StreamTableSource, StreamTableSource中通过StreamExecutionEnvironment 的addSource方法获取DataStream, 所以我们需要自定义一个 SourceFunction, 并且要支持产生WaterMark，也就是要实现DefinedRowtimeAttributes接口。 Source Function定义支持接收携带EventTime的数据集合，Either的数据结构区分WaterMark和元数据: 12345678910class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123; override def run(ctx: SourceContext[T]): Unit = &#123; dataWithTimestampList.foreach &#123; case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1) case Right(w) =&gt; ctx.emitWatermark(new Watermark(w)) &#125; &#125; override def cancel(): Unit = ???&#125; 定义 StreamTableSource我们自定义的Source要携带我们测试的数据，已经对应WaterMark数据，具体如下: 1234567891011121314151617181920212223242526272829303132333435363738class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123; val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;) val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING)) val rowType = new RowTypeInfo( Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]], fieldNames) // 页面访问表数据 rows with timestamps and watermarks val data = Seq( Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)), Right(1510365660000L), Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)), Right(1510365660000L), Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)), Right(1510366200000L), Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)), Right(1510366260000L), Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)), Right(1510373400000L) ) override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123; Collections.singletonList(new RowtimeAttributeDescriptor( &quot;accessTime&quot;, new ExistingField(&quot;accessTime&quot;), PreserveWatermarks.INSTANCE)) &#125; override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123; execEnv.addSource(new MySourceFunction[Row](data)).setParallelism(1).returns(rowType) &#125; override def getReturnType: TypeInformation[Row] = rowType override def getTableSchema: TableSchema = schema&#125; Sink 定义我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下： 1234567891011def getCsvTableSink: TableSink[Row] = &#123; val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;) // 打印sink的文件路径，方便我们查看运行结果 println(&quot;Sink path : &quot; + tempFile) if (tempFile.exists()) &#123; tempFile.delete() &#125; new CsvTableSink(tempFile.getAbsolutePath).configure( Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;), Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG)) &#125; 构建主程序主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下： 1234567891011121314151617181920212223242526272829303132333435def main(args: Array[String]): Unit = &#123; // Streaming 环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) // 设置EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) //方便我们查出输出数据 env.setParallelism(1) val sourceTableName = &quot;mySource&quot; // 创建自定义source数据结构 val tableSource = new MyTableSource val sinkTableName = &quot;csvSink&quot; // 创建CSV sink 数据结构 val tableSink = getCsvTableSink // 注册source tEnv.registerTableSource(sourceTableName, tableSource) // 注册sink tEnv.registerTableSink(sinkTableName, tableSink) val sql = &quot;SELECT &quot; + &quot; region, &quot; + &quot; TUMBLE_START(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winStart,&quot; + &quot; TUMBLE_END(accessTime, INTERVAL &apos;2&apos; MINUTE) AS winEnd, COUNT(region) AS pv &quot; + &quot; FROM mySource &quot; + &quot; GROUP BY TUMBLE(accessTime, INTERVAL &apos;2&apos; MINUTE), region&quot; tEnv.sqlQuery(sql).insertInto(sinkTableName); env.execute() &#125; 执行并查看运行结果执行主程序后我们会在控制台得到Sink的文件路径，如下： 1Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem Cat 方式查看计算结果，如下： 12345jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911temShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1 表格化如上结果： region winStart winEnd pv BeiJing 2017-11-11 02:00:00.0 2017-11-11 02:02:00.0 1 BeiJing 2017-11-11 02:10:00.0 2017-11-11 02:12:00.0 2 ShangHai 2017-11-11 02:00:00.0 2017-11-11 02:02:00.0 1 ShangHai 2017-11-11 04:10:00.0 2017-11-11 04:12:00.0 1 小结本篇概要的介绍了Apache Flink SQL 的所有核心算子，并以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job. 希望对大家有所帮助.]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - Table API 概览]]></title>
    <url>%2F2019%2F03%2F26%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20Table%20API%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[什么是Table API在《Apache Flink 漫谈系列(08) - SQL概览》中我们概要的向大家介绍了什么是好SQL，SQL和Table API是Apache Flink中的同一层次的API抽象. 如下图所示： Apache Flink 针对不同的用户场景提供了三层用户API,最下层ProcessFunction API可以对State，Timer等复杂机制进行有效的控制，但用户使用的便捷性很弱，也就是说即使很简单统计逻辑，也要较多的代码开发。第二层DataStream API对窗口，聚合等算子进行了封装，用户的便捷性有所增强。最上层是SQL/Table API，Table API是Apache Flink中的声明式，可被查询优化器优化的高级分析API。 Table API的特点Table API和SQL都是Apache Flink中最高层的分析API，SQL所具备的特点Table API也都具有，如下： 声明式 - 用户只关心做什么，不用关心怎么做； 高性能 - 支持查询优化，可以获取最好的执行性能； 流批统一 - 相同的统计逻辑，既可以流模式运行，也可以批模式运行； 标准稳定 - 语义遵循SQL标准，语法语义明确，不易变动。 当然除了SQL的特性，因为Table API是在Flink中专门设计的，所以Table API还具有自身的特点： 表达方式的扩展性 - 在Flink中可以为Table API开发很多便捷性功能，如：Row.flatten(), map/flatMap 等 功能的扩展性 - 在Flink中可以为Table API扩展更多的功能，如：Iteration，flatAggregate 等新功能 编译检查 - Table API支持java和scala语言开发，支持IDE中进行编译检查。 说明：上面说的map/flatMap/flatAggregate都是Apache Flink 社区 FLIP-29 中规划的新功能。 HelloWorld在介绍Table API所有算子之前我们先编写一个简单的HelloWorld来直观了解如何进行Table API的开发。 Maven 依赖在pom文件中增加如下配置，本篇以flink-1.7.0功能为准进行后续介绍。 123456789101112131415161718192021222324252627282930&lt;properties&gt; &lt;table.version&gt;1.7.0&lt;/table.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;table.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 程序结构在编写第一Flink Table API job之前我们先简单了解一下Flink Table API job的结构，如下图所示： 外部数据源，比如Kafka, Rabbitmq, CSV 等等； 查询计算逻辑，比如最简单的数据导入select，双流Join，Window Aggregate 等； 外部结果存储，比如Kafka，Cassandra，CSV等。 说明：1和3 在Apache Flink中统称为Connector。 主程序我们以一个统计单词数量的业务场景，编写第一个HelloWorld程序。根据上面Flink job基本结构介绍，要Table API完成WordCount的计算需求，我们需要完成三部分代码： TableSoruce Code - 用于创建数据源的代码 Table API Query - 用于进行word count统计的Table API 查询逻辑 TableSink Code - 用于保存word count计算结果的结果表代码 运行模式选择一个job我们要选择是Stream方式运行还是Batch模式运行，所以任何统计job的第一步是进行运行模式选择,如下我们选择Stream方式运行。 123// Stream运行环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) 构建测试Source我们用最简单的构建Source方式进行本次测试，代码如下： 1234 // 测试数据 val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;) // 最简单的获取Source方式val source = env.fromCollection(data).toTable(tEnv, &apos;word) WordCount 统计逻辑WordCount核心统计逻辑就是按照单词分组，然后计算每个单词的数量，统计逻辑如下： 1234// 单词统计核心逻辑val result = source .groupBy(&apos;word) // 单词分组 .select(&apos;word, &apos;word.count) // 单词统计 定义Sink将WordCount的统计结果写入Sink中，代码如下： 1234// 自定义Sink val sink = new RetractSink // 自定义Sink（下面有完整代码） // 计算结果写入sink result.toRetractStream[(String, Long)].addSink(sink) 完整的HelloWord代码为了方便大家运行WordCount查询统计，将完整的代码分享大家(基于flink-1.7.0)，如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import org.apache.flink.api.scala._import org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.&#123;RichSinkFunction, SinkFunction&#125;import org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.table.api.TableEnvironmentimport org.apache.flink.table.api.scala._import scala.collection.mutableobject HelloWord &#123; def main(args: Array[String]): Unit = &#123; // 测试数据 val data = Seq(&quot;Flink&quot;, &quot;Bob&quot;, &quot;Bob&quot;, &quot;something&quot;, &quot;Hello&quot;, &quot;Flink&quot;, &quot;Bob&quot;) // Stream运行环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) // 最简单的获取Source方式 val source = env.fromCollection(data).toTable(tEnv, &apos;word) // 单词统计核心逻辑 val result = source .groupBy(&apos;word) // 单词分组 .select(&apos;word, &apos;word.count) // 单词统计 // 自定义Sink val sink = new RetractSink // 计算结果写入sink result.toRetractStream[(String, Long)].addSink(sink) env.execute &#125;&#125;class RetractSink extends RichSinkFunction[(Boolean, (String, Long))] &#123; private var resultSet: mutable.Set[(String, Long)] = _ override def open(parameters: Configuration): Unit = &#123; // 初始化内存存储结构 resultSet = new mutable.HashSet[(String, Long)] &#125; override def invoke(v: (Boolean, (String, Long)), context: SinkFunction.Context[_]): Unit = &#123; if (v._1) &#123; // 计算数据 resultSet.add(v._2) &#125; else &#123; // 撤回数据 resultSet.remove(v._2) &#125; &#125; override def close(): Unit = &#123; // 打印写入sink的结果数据 resultSet.foreach(println) &#125;&#125; 运行结果如下： 虽然上面用了较长的纸墨介绍简单的WordCount统计逻辑，但source和sink部分都是可以在学习后面算子中被复用的。本例核心的统计逻辑只有一行代码:source.groupBy(&#39;word).select(&#39;word, &#39;word.count)所以Table API开发技术任务非常的简洁高效。 Table API 算子虽然Table API与SQL的算子语义一致，但在表达方式上面SQL以文本的方式展现，Table API是以java或者scala语言的方式进行开发。为了大家方便阅读，即便是在《Apache Flink 漫谈系列(08) - SQL概览》中介绍过的算子，在这里也会再次进行介绍，当然对于Table API和SQL不同的地方会进行详尽介绍。 示例数据及测试类测试数据 customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下： c_id c_name c_desc c_001 Kevin from JinLin c_002 Sunny from JinLin c_003 JinCheng from HeBei order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下： o_id c_id o_time o_desc o_oo1 c_002 2018-11-05 10:01:01 iphone o_002 c_001 2018-11-05 10:01:55 ipad o_003 c_001 2018-11-05 10:03:44 flink book Item_tab商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下： itemID itemType onSellTime price ITEM001 Electronic 2017-11-11 10:01:00 20 ITEM002 Electronic 2017-11-11 10:02:00 50 ITEM003 Electronic 2017-11-11 10:03:00 30 ITEM004 Electronic 2017-11-11 10:03:00 60 ITEM005 Electronic 2017-11-11 10:05:00 40 ITEM006 Electronic 2017-11-11 10:06:00 20 ITEM007 Electronic 2017-11-11 10:07:00 70 ITEM008 Clothes 2017-11-11 10:08:00 20 PageAccess_tab页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下： region userId accessTime ShangHai U0010 2017-11-11 10:01:00 BeiJing U1001 2017-11-11 10:01:00 BeiJing U2032 2017-11-11 10:10:00 BeiJing U1100 2017-11-11 10:11:00 ShangHai U0011 2017-11-11 12:10:00 PageAccessCount_tab页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下： region userCount accessTime ShangHai 100 2017.11.11 10:01:00 BeiJing 86 2017.11.11 10:01:00 BeiJing 210 2017.11.11 10:06:00 BeiJing 33 2017.11.11 10:10:00 ShangHai 129 2017.11.11 12:10:00 PageAccessSession_tab页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下： region userId accessTime ShangHai U0011 2017-11-11 10:01:00 ShangHai U0012 2017-11-11 10:02:00 ShangHai U0013 2017-11-11 10:03:00 ShangHai U0015 2017-11-11 10:05:00 ShangHai U0011 2017-11-11 10:10:00 BeiJing U0110 2017-11-11 10:10:00 ShangHai U2010 2017-11-11 10:11:00 ShangHai U0410 2017-11-11 12:16:00 测试类我们创建一个TableAPIOverviewITCase.scala 用于接下来介绍Flink Table API算子的功能体验。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217import org.apache.flink.api.scala._import org.apache.flink.runtime.state.StateBackendimport org.apache.flink.runtime.state.memory.MemoryStateBackendimport org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.sink.RichSinkFunctionimport org.apache.flink.streaming.api.functions.source.SourceFunctionimport org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContextimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.streaming.api.watermark.Watermarkimport org.apache.flink.table.api.&#123;Table, TableEnvironment&#125;import org.apache.flink.table.api.scala._import org.apache.flink.types.Rowimport org.junit.rules.TemporaryFolderimport org.junit.&#123;Rule, Test&#125;import scala.collection.mutableimport scala.collection.mutable.ArrayBufferclass Table APIOverviewITCase &#123; // 客户表数据 val customer_data = new mutable.MutableList[(String, String, String)] customer_data.+=((&quot;c_001&quot;, &quot;Kevin&quot;, &quot;from JinLin&quot;)) customer_data.+=((&quot;c_002&quot;, &quot;Sunny&quot;, &quot;from JinLin&quot;)) customer_data.+=((&quot;c_003&quot;, &quot;JinCheng&quot;, &quot;from HeBei&quot;)) // 订单表数据 val order_data = new mutable.MutableList[(String, String, String, String)] order_data.+=((&quot;o_001&quot;, &quot;c_002&quot;, &quot;2018-11-05 10:01:01&quot;, &quot;iphone&quot;)) order_data.+=((&quot;o_002&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:01:55&quot;, &quot;ipad&quot;)) order_data.+=((&quot;o_003&quot;, &quot;c_001&quot;, &quot;2018-11-05 10:03:44&quot;, &quot;flink book&quot;)) // 商品销售表数据 val item_data = Seq( Left((1510365660000L, (1510365660000L, 20, &quot;ITEM001&quot;, &quot;Electronic&quot;))), Right((1510365660000L)), Left((1510365720000L, (1510365720000L, 50, &quot;ITEM002&quot;, &quot;Electronic&quot;))), Right((1510365720000L)), Left((1510365780000L, (1510365780000L, 30, &quot;ITEM003&quot;, &quot;Electronic&quot;))), Left((1510365780000L, (1510365780000L, 60, &quot;ITEM004&quot;, &quot;Electronic&quot;))), Right((1510365780000L)), Left((1510365900000L, (1510365900000L, 40, &quot;ITEM005&quot;, &quot;Electronic&quot;))), Right((1510365900000L)), Left((1510365960000L, (1510365960000L, 20, &quot;ITEM006&quot;, &quot;Electronic&quot;))), Right((1510365960000L)), Left((1510366020000L, (1510366020000L, 70, &quot;ITEM007&quot;, &quot;Electronic&quot;))), Right((1510366020000L)), Left((1510366080000L, (1510366080000L, 20, &quot;ITEM008&quot;, &quot;Clothes&quot;))), Right((151036608000L))) // 页面访问表数据 val pageAccess_data = Seq( Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0010&quot;))), Right((1510365660000L)), Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, &quot;U1001&quot;))), Right((1510365660000L)), Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2032&quot;))), Right((1510366200000L)), Left((1510366260000L, (1510366260000L, &quot;BeiJing&quot;, &quot;U1100&quot;))), Right((1510366260000L)), Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, &quot;U0011&quot;))), Right((1510373400000L))) // 页面访问量表数据2 val pageAccessCount_data = Seq( Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, 100))), Right((1510365660000L)), Left((1510365660000L, (1510365660000L, &quot;BeiJing&quot;, 86))), Right((1510365660000L)), Left((1510365960000L, (1510365960000L, &quot;BeiJing&quot;, 210))), Right((1510366200000L)), Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, 33))), Right((1510366200000L)), Left((1510373400000L, (1510373400000L, &quot;ShangHai&quot;, 129))), Right((1510373400000L))) // 页面访问表数据3 val pageAccessSession_data = Seq( Left((1510365660000L, (1510365660000L, &quot;ShangHai&quot;, &quot;U0011&quot;))), Right((1510365660000L)), Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0012&quot;))), Right((1510365720000L)), Left((1510365720000L, (1510365720000L, &quot;ShangHai&quot;, &quot;U0013&quot;))), Right((1510365720000L)), Left((1510365900000L, (1510365900000L, &quot;ShangHai&quot;, &quot;U0015&quot;))), Right((1510365900000L)), Left((1510366200000L, (1510366200000L, &quot;ShangHai&quot;, &quot;U0011&quot;))), Right((1510366200000L)), Left((1510366200000L, (1510366200000L, &quot;BeiJing&quot;, &quot;U2010&quot;))), Right((1510366200000L)), Left((1510366260000L, (1510366260000L, &quot;ShangHai&quot;, &quot;U0011&quot;))), Right((1510366260000L)), Left((1510373760000L, (1510373760000L, &quot;ShangHai&quot;, &quot;U0410&quot;))), Right((1510373760000L))) val _tempFolder = new TemporaryFolder // Streaming 环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) env.setParallelism(1) env.setStateBackend(getStateBackend) def getProcTimeTables(): (Table, Table) = &#123; // 将order_tab, customer_tab 注册到catalog val customer = env.fromCollection(customer_data).toTable(tEnv).as(&apos;c_id, &apos;c_name, &apos;c_desc) val order = env.fromCollection(order_data).toTable(tEnv).as(&apos;o_id, &apos;c_id, &apos;o_time, &apos;o_desc) (customer, order) &#125; def getEventTimeTables(): (Table, Table, Table, Table) = &#123; env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) // 将item_tab, pageAccess_tab 注册到catalog val item = env.addSource(new EventTimeSourceFunction[(Long, Int, String, String)](item_data)) .toTable(tEnv, &apos;onSellTime, &apos;price, &apos;itemID, &apos;itemType, &apos;rowtime.rowtime) val pageAccess = env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccess_data)) .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime) val pageAccessCount = env.addSource(new EventTimeSourceFunction[(Long, String, Int)](pageAccessCount_data)) .toTable(tEnv, &apos;accessTime, &apos;region, &apos;accessCount, &apos;rowtime.rowtime) val pageAccessSession = env.addSource(new EventTimeSourceFunction[(Long, String, String)](pageAccessSession_data)) .toTable(tEnv, &apos;accessTime, &apos;region, &apos;userId, &apos;rowtime.rowtime) (item, pageAccess, pageAccessCount, pageAccessSession) &#125; @Rule def tempFolder: TemporaryFolder = _tempFolder def getStateBackend: StateBackend = &#123; new MemoryStateBackend() &#125; def procTimePrint(result: Table): Unit = &#123; val sink = new RetractingSink result.toRetractStream[Row].addSink(sink) env.execute() &#125; def rowTimePrint(result: Table): Unit = &#123; val sink = new RetractingSink result.toRetractStream[Row].addSink(sink) env.execute() &#125; @Test def testProc(): Unit = &#123; val (customer, order) = getProcTimeTables() val result = ...// 测试的查询逻辑 procTimePrint(result) &#125; @Test def testEvent(): Unit = &#123; val (item, pageAccess, pageAccessCount, pageAccessSession) = getEventTimeTables() val result = ...// 测试的查询逻辑 procTimePrint(result) &#125;&#125;// 自定义Sinkfinal class RetractingSink extends RichSinkFunction[(Boolean, Row)] &#123; var retractedResults: ArrayBuffer[String] = null override def open(parameters: Configuration): Unit = &#123; super.open(parameters) retractedResults = mutable.ArrayBuffer.empty[String] &#125; def invoke(v: (Boolean, Row)) &#123; retractedResults.synchronized &#123; val value = v._2.toString if (v._1) &#123; retractedResults += value &#125; else &#123; val idx = retractedResults.indexOf(value) if (idx &gt;= 0) &#123; retractedResults.remove(idx) &#125; else &#123; throw new RuntimeException(&quot;Tried to retract a value that wasn&apos;t added first. &quot; + &quot;This is probably an incorrectly implemented test. &quot; + &quot;Try to set the parallelism of the sink to 1.&quot;) &#125; &#125; &#125; &#125; override def close(): Unit = &#123; super.close() retractedResults.sorted.foreach(println(_)) &#125;&#125;// Water mark 生成器class EventTimeSourceFunction[T]( dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123; override def run(ctx: SourceContext[T]): Unit = &#123; dataWithTimestampList.foreach &#123; case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1) case Right(w) =&gt; ctx.emitWatermark(new Watermark(w)) &#125; &#125; override def cancel(): Unit = ???&#125; SELECTSELECT 用于从数据集/流中选择数据，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去或增加某些列, 如下图所示: Table API 示例从customer_tab选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下： 12val result = customer .select(&apos;c_name, concat_ws(&apos;c_name, &quot; come &quot;, &apos;c_desc)) Result c_name desc Kevin Kevin come from JinLin Sunny Sunny come from JinLin Jincheng Jincheng come from HeBei 特别说明大家看到在 SELECT 不仅可以使用普通的字段选择，还可以使用ScalarFunction,当然也包括User-Defined Function，同时还可以进行字段的alias设置。其实SELECT可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是去重的场景，示例如下： Table API示例在订单表查询所有的客户id，消除重复客户id, 如下： 123val result = order .groupBy(&apos;c_id) .select(&apos;c_id) Result c_id c_001 c_002 WHEREWHERE 用于从数据集/流中过滤数据，与SELECT一起使用，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示： Table API 示例在customer_tab查询客户id为c_001和c_003的客户信息，如下： 123val result = customer .where(&quot;c_id = &apos;c_001&apos; || c_id = &apos;c_003&apos;&quot;) .select( &apos;c_id, &apos;c_name, &apos;c_desc) Result c_id c_name c_desc c_001 Kevin from JinLin c_003 JinCheng from HeBei 特别说明我们发现WHERE是对满足一定条件的数据进行过滤，WHERE支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及&amp;&amp;， ||等表达式的组合，最终满足过滤条件的数据会被选择出来。 SQL中的IN和NOT IN在Table API里面用intersect 和 minus描述(flink-1.7.0版本)。 Intersect 示例Intersect只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在customer_tab查询已经下过订单的客户信息，如下： 12345678// 计算客户id，并去重val distinct_cids = order .groupBy(&apos;c_id) // 去重 .select(&apos;c_id as &apos;o_c_id)val result = customer .join(distinct_cids, &apos;c_id === &apos;o_c_id) .select(&apos;c_id, &apos;c_name, &apos;c_desc) Result c_id c_name c_desc c_001 Kevin from JinLin c_002 Sunny from JinLin Minus 示例Minus只在Batch模式下进行支持，Stream模式下我们可以利用双流JOIN来实现，如：在customer_tab查询没有下过订单的客户信息，如下： 12345678910 // 查询下过订单的客户id，并去重 val distinct_cids = order .groupBy(&apos;c_id) .select(&apos;c_id as &apos;o_c_id) // 查询没有下过订单的客户信息val result = customer .leftOuterJoin(distinct_cids, &apos;c_id === &apos;o_c_id) .where(&apos;o_c_id isNull) .select(&apos;c_id, &apos;c_name, &apos;c_desc) 说明上面实现逻辑比较复杂，我们后续考虑如何在流上支持更简洁的方式。 Result c_id c_name c_desc c_003 JinCheng from HeBei Intersect/Minus与关系代数如上介绍Intersect是关系代数中的Intersection， Minus是关系代数的Difference， 如下图示意： Intersect(Intersection) Minus(Difference) GROUP BYGROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下： Table API 示例将order_tab信息按c_id分组统计订单数量，简单示例如下： 123val result = order .groupBy(&apos;c_id) .select(&apos;c_id, &apos;o_id.count) Result c_id o_count c_001 2 c_002 1 特别说明 在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量： Table API 示例按时间进行分组，查询每分钟的订单数量，如下： 1234val result = order .select(&apos;o_id, &apos;c_id, &apos;o_time.substring(1, 16) as &apos;o_time_min) .groupBy(&apos;o_time_min) .select(&apos;o_time_min, &apos;o_id.count)Result o_time_min o_count 2018-11-05 10:01 2 2018-11-05 10:03 1 说明：如果我们时间字段是timestamp类型，建议使用内置的 DATE_FORMAT 函数。 UNION ALLUNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示： Table API 示例我们简单的将customer_tab查询2次，将查询结果合并起来，如下： 1val result = customer.unionAll(customer) Result c_id c_name c_desc c_001 Kevin from JinLin c_002 Sunny from JinLin c_003 JinCheng from HeBei c_001 Kevin from JinLin c_002 Sunny from JinLin c_003 JinCheng from HeBei 特别说明UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。 UNIONUNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下： Table API 示例我们简单的将customer_tab查询2次，将查询结果合并起来，如下： 1val result = customer.union(customer) 我们发现完全一样的表数据进行 UNION之后，数据是被去重的，UNION之后的数据并没有增加。 Result c_id c_name c_desc c_001 Kevin from JinLin c_002 Sunny from JinLin c_003 JinCheng from HeBei 特别说明UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。 JOINJOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型： JOIN - INNER JOIN LEFT JOIN - LEFT OUTER JOIN RIGHT JOIN - RIGHT OUTER JOIN FULL JOIN - FULL OUTER JOINJOIN与关系代数的Join语义相同，具体如下： Table API 示例 (JOIN)INNER JOIN只选择满足ON条件的记录，我们查询customer_tab 和 order_tab表，将有订单的客户和订单信息选择出来，如下： 12val result = customer .join(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id) Result c_id c_name c_desc o_id o_c_id o_time o_desc c_001 Kevin from JinLin o_002 c_001 2018-11-05 10:01:55 ipad c_001 Kevin from JinLin o_003 c_001 2018-11-05 10:03:44 flink book c_002 Sunny from JinLin o_oo1 c_002 2018-11-05 10:01:01 iphone Table API 示例 (LEFT JOIN)LEFT JOIN与INNER JOIN的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补NULL输出，语义如下：对应的SQL语句如下(LEFT JOIN)： 1SELECT ColA, ColB, T2.ColC, ColE FROM TI LEFT JOIN T2 ON T1.ColC = T2.ColC ; 细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。 我们查询customer_tab 和 order_tab表，将客户和订单信息选择出来如下： 12val result = customer .leftOuterJoin(order.select(&apos;o_id, &apos;c_id as &apos;o_c_id, &apos;o_time, &apos;o_desc), &apos;c_id === &apos;o_c_id) Result c_id c_name c_desc o_id c_id o_time o_desc c_001 Kevin from JinLin o_002 c_001 2018-11-05 10:01:55 ipad c_001 Kevin from JinLin o_003 c_001 2018-11-05 10:03:44 flink book c_002 Sunny from JinLin o_oo1 c_002 2018-11-05 10:01:01 iphone c_003 JinCheng from HeBei NULL NULL NULL NULL 特别说明RIGHT JOIN 相当于 LEFT JOIN 左右两个表交互一下位置。FULL JOIN相当于 RIGHT JOIN 和 LEFT JOIN 之后进行UNION ALL操作。 Time-Interval JOINTime-Interval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Time-Interval JOIN的语义和实现原理详见《Apache Flink 漫谈系列(12) - Time Interval(Time-windowed) JOIN》。其Table API核心的语法示例，如下： 123456...val result = left .join(right) // 定义Time Interval .where(&apos;a === &apos;d &amp;&amp; &apos;c &gt;= &apos;f - 5.seconds &amp;&amp; &apos;c &lt; &apos;f + 6.seconds) ... Lateral JOINApache Flink Lateral JOIN 是左边Table与一个UDTF进行JOIN，详细的语义和实现原理请参考《Apache Flink 漫谈系列(10) - JOIN LATERAL》。其Table API核心的语法示例，如下： 1234...val udtf = new UDTFval result = source.join(udtf(&apos;c) as (&apos;d, &apos;e))... Temporal Table JOINTemporal Table JOIN 是左边表与右边一个携带版本信息的表进行JOIN，详细的语法，语义和实现原理详见《Apache Flink 漫谈系列(11) - Temporal Table JOIN》，其Table API核心的语法示例，如下： 1234...val rates = tEnv.scan(&quot;versonedTable&quot;).createTemporalTableFunction(&apos;rowtime, &apos;r_currency)val result = left.join(rates(&apos;o_rowtime), &apos;r_currency === &apos;o_currency)... Window在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。 Over WindowApache Flink中对OVER Window的定义遵循标准SQL的定义语法。按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下: ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。 RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。 Bounded ROWS OVER WindowBounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。 语义我们以3个元素(2 PRECEDING)的窗口为例，如下图:上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。 Table API 示例利用item_tab测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。 123val result = item .window(Over partitionBy &apos;itemType orderBy &apos;rowtime preceding 2.rows following CURRENT_ROW as &apos;w) .select(&apos;itemID, &apos;itemType, &apos;onSellTime, &apos;price, &apos;price.max over &apos;w as &apos;maxPrice) Result itemID itemType onSellTime price maxPrice ITEM001 Electronic 2017-11-11 10:01:00 20 20 ITEM002 Electronic 2017-11-11 10:02:00 50 50 ITEM003 Electronic 2017-11-11 10:03:00 30 50 ITEM004 Electronic 2017-11-11 10:03:00 60 60 ITEM005 Electronic 2017-11-11 10:05:00 40 60 ITEM006 Electronic 2017-11-11 10:06:00 20 60 ITEM007 Electronic 2017-11-11 10:07:00 70 70 ITEM008 Clothes 2017-11-11 10:08:00 20 20 Bounded RANGE OVER WindowBounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。 语义我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图： 注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。 Tabel API 示例我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。 123val result = item .window(Over partitionBy 'itemType orderBy 'rowtime preceding 2.minute following CURRENT_RANGE as 'w) .select('itemID, 'itemType, 'onSellTime, 'price, 'price.max over 'w as 'maxPrice) Result（Bounded RANGE OVER Window） itemID itemType onSellTime price maxPrice ITEM001 Electronic 2017-11-11 10:01:00 20 20 ITEM002 Electronic 2017-11-11 10:02:00 50 50 ITEM003 Electronic 2017-11-11 10:03:00 30 60 ITEM004 Electronic 2017-11-11 10:03:00 60 60 ITEM005 Electronic 2017-11-11 10:05:00 40 60 ITEM006 Electronic 2017-11-11 10:06:00 20 40 ITEM007 Electronic 2017-11-11 10:07:00 70 70 ITEM008 Clothes 2017-11-11 10:08:00 20 20 特别说明OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在SELECT中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，SELECT可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下: GROUP BY - tab.groupBy(&#39;d).select(d, MAX(c)) OVER Window = tab.window(Over.. as &#39;w).select(&#39;a, &#39;b, &#39;c, &#39;d, c.max over &#39;w)如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。 Group Window根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw: Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加； Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加； Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。 说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。 Tumble语义Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下： Table API 示例利用pageAccess_tab测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。 1234val result = pageAccess .window(Tumble over 2.minute on &apos;rowtime as &apos;w) .groupBy(&apos;w, &apos;region) .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv) Result region winStart winEnd pv BeiJing 2017-11-11 02:00:00.0 2017-11-11 02:02:00.0 1 BeiJing 2017-11-11 02:10:00.0 2017-11-11 02:12:00.0 2 ShangHai 2017-11-11 02:00:00.0 2017-11-11 02:02:00.0 1 ShangHai 2017-11-11 04:10:00.0 2017-11-11 04:12:00.0 1 HopHop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。 语义Hop 滑动窗口语义如下所示： Table API 示例利用pageAccessCount_tab测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV). 1234val result = pageAccessCount .window(Slide over 10.minute every 5.minute on &apos;rowtime as &apos;w) .groupBy(&apos;w) .select(&apos;w.start, &apos;w.end, &apos;accessCount.sum as &apos;accessCount) Result winStart winEnd accessCount 2017-11-11 01:55:00.0 2017-11-11 02:05:00.0 186 2017-11-11 02:00:00.0 2017-11-11 02:10:00.0 396 2017-11-11 02:05:00.0 2017-11-11 02:15:00.0 243 2017-11-11 02:10:00.0 2017-11-11 02:20:00.0 33 2017-11-11 04:05:00.0 2017-11-11 04:15:00.0 129 2017-11-11 04:10:00.0 2017-11-11 04:20:00.0 129 SessionSeeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长. 语义Session 会话窗口语义如下所示： 1234val result = pageAccessSession .window(Session withGap 3.minute on &apos;rowtime as &apos;w) .groupBy(&apos;w, &apos;region) .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv) Result region winStart winEnd pv BeiJing 2017-11-11 02:10:00.0 2017-11-11 02:13:00.0 1 ShangHai 2017-11-11 02:01:00.0 2017-11-11 02:08:00.0 4 ShangHai 2017-11-11 02:10:00.0 2017-11-11 02:14:00.0 2 ShangHai 2017-11-11 04:16:00.0 2017-11-11 04:19:00.0 1 嵌套Window在Window之后再进行Window划分也是比较常见的统计需求，那么在一个Event-Time的Window之后，如何再写一个Event-Time的Window呢？一个Window之后再描述一个Event-Time的Window最重要的是Event-time属性的传递，在Table API中我们可以利用&#39;w.rowtime来传递时间属性，比如：Tumble Window之后再接一个Session Window 示例如下: 123456789 ... val result = pageAccess .window(Tumble over 2.minute on &apos;rowtime as &apos;w1) .groupBy(&apos;w1) .select(&apos;w1.rowtime as &apos;rowtime, &apos;col1.count as &apos;cnt) .window(Session withGap 3.minute on &apos;rowtime as &apos;w2) .groupBy(&apos;w2) .select(&apos;cnt.sum)... Source&amp;Sink上面我们介绍了Apache Flink Table API核心算子的语义和具体示例，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink Table API Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。具体数据如下： region userId accessTime ShangHai U0010 2017-11-11 10:01:00 BeiJing U1001 2017-11-11 10:01:00 BeiJing U2032 2017-11-11 10:10:00 BeiJing U1100 2017-11-11 10:11:00 ShangHai U0011 2017-11-11 12:10:00 Source 定义自定义Apache Flink Stream Source需要实现StreamTableSource, StreamTableSource中通过StreamExecutionEnvironment 的addSource方法获取DataStream, 所以我们需要自定义一个 SourceFunction, 并且要支持产生WaterMark，也就是要实现DefinedRowtimeAttributes接口。 Source Function定义支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据: 12345678910class MySourceFunction[T](dataWithTimestampList: Seq[Either[(Long, T), Long]]) extends SourceFunction[T] &#123; override def run(ctx: SourceContext[T]): Unit = &#123; dataWithTimestampList.foreach &#123; case Left(t) =&gt; ctx.collectWithTimestamp(t._2, t._1) case Right(w) =&gt; ctx.emitWatermark(new Watermark(w)) &#125; &#125; override def cancel(): Unit = ???&#125; 定义 StreamTableSource我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下: 1234567891011121314151617181920212223242526272829303132333435363738class MyTableSource extends StreamTableSource[Row] with DefinedRowtimeAttributes &#123; val fieldNames = Array(&quot;accessTime&quot;, &quot;region&quot;, &quot;userId&quot;) val schema = new TableSchema(fieldNames, Array(Types.SQL_TIMESTAMP, Types.STRING, Types.STRING)) val rowType = new RowTypeInfo( Array(Types.LONG, Types.STRING, Types.STRING).asInstanceOf[Array[TypeInformation[_]]], fieldNames) // 页面访问表数据 rows with timestamps and watermarks val data = Seq( Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;ShangHai&quot;, &quot;U0010&quot;)), Right(1510365660000L), Left(1510365660000L, Row.of(new JLong(1510365660000L), &quot;BeiJing&quot;, &quot;U1001&quot;)), Right(1510365660000L), Left(1510366200000L, Row.of(new JLong(1510366200000L), &quot;BeiJing&quot;, &quot;U2032&quot;)), Right(1510366200000L), Left(1510366260000L, Row.of(new JLong(1510366260000L), &quot;BeiJing&quot;, &quot;U1100&quot;)), Right(1510366260000L), Left(1510373400000L, Row.of(new JLong(1510373400000L), &quot;ShangHai&quot;, &quot;U0011&quot;)), Right(1510373400000L) ) override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = &#123; Collections.singletonList(new RowtimeAttributeDescriptor( &quot;accessTime&quot;, new ExistingField(&quot;accessTime&quot;), PreserveWatermarks.INSTANCE)) &#125; override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = &#123; execEnv.addSource(new MySourceFunction[Row](data)).returns(rowType).setParallelism(1) &#125; override def getReturnType: TypeInformation[Row] = rowType override def getTableSchema: TableSchema = schema&#125; Sink 定义我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下： 1234567891011def getCsvTableSink: TableSink[Row] = &#123; val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;) // 打印sink的文件路径，方便我们查看运行结果 println(&quot;Sink path : &quot; + tempFile) if (tempFile.exists()) &#123; tempFile.delete() &#125; new CsvTableSink(tempFile.getAbsolutePath).configure( Array[String](&quot;region&quot;, &quot;winStart&quot;, &quot;winEnd&quot;, &quot;pv&quot;), Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG)) &#125; 构建主程序主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下： 1234567891011121314151617181920212223242526272829303132def main(args: Array[String]): Unit = &#123; // Streaming 环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) // 设置EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) //方便我们查出输出数据 env.setParallelism(1) val sourceTableName = &quot;mySource&quot; // 创建自定义source数据结构 val tableSource = new MyTableSource val sinkTableName = &quot;csvSink&quot; // 创建CSV sink 数据结构 val tableSink = getCsvTableSink // 注册source tEnv.registerTableSource(sourceTableName, tableSource) // 注册sink tEnv.registerTableSink(sinkTableName, tableSink) val result = tEnv.scan(sourceTableName) .window(Tumble over 2.minute on &apos;accessTime as &apos;w) .groupBy(&apos;w, &apos;region) .select(&apos;region, &apos;w.start, &apos;w.end, &apos;region.count as &apos;pv) result.insertInto(sinkTableName) env.execute() &#125; 执行并查看运行结果执行主程序后我们会在控制台得到Sink的文件路径，如下： 1Sink path : /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem Cat 方式查看计算结果，如下： 12345jinchengsunjcdeMacBook-Pro:FlinkTable APIDemo jincheng.sunjc$ cat /var/folders/88/8n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911temShangHai,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1BeiJing,2017-11-11 02:00:00.0,2017-11-11 02:02:00.0,1BeiJing,2017-11-11 02:10:00.0,2017-11-11 02:12:00.0,2ShangHai,2017-11-11 04:10:00.0,2017-11-11 04:12:00.0,1 表格化如上结果：| region |winStart | winEnd | pv|| — | — | — | — || BeiJing |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1| BeiJing | 2017-11-11 02:10:00.0|2017-11-11 02:12:00.0|2| ShangHai |2017-11-11 02:00:00.0|2017-11-11 02:02:00.0|1| ShangHai |2017-11-11 04:10:00.0|2017-11-11 04:12:00.0|1 上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！ 小结本篇首先向大家介绍了什么是Table API, Table API的核心特点，然后以此介绍Table API的核心算子功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink Table API的Job收尾。希望对大家学习Apache Flink Table API 过程中有所帮助。 关于点赞和评论本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - Time Interval JOIN]]></title>
    <url>%2F2019%2F03%2F22%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20Time%20Interval%20JOIN%2F</url>
    <content type="text"><![CDATA[说什么JOIN 算子是数据处理的核心算子，前面我们在《Apache Flink 漫谈系列 - JOIN 算子》介绍了UnBounded的双流JOIN，在《Apache Flink 漫谈系列 - JOIN LATERAL》介绍了单流与UDTF的JOIN操作，在《Apache Flink 漫谈系列 - Temporal Table JOIN》又介绍了单流与版本表的JOIN，本篇将介绍在UnBounded数据流上按时间维度进行数据划分进行JOIN操作 - Time Interval(Time-windowed)JOIN, 后面我们叫做Interval JOIN。 实际问题前面章节我们介绍了Flink中对各种JOIN的支持，那么想想下面的查询需求之前介绍的JOIN能否满足？需求描述如下: 比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计下单一小时内付款的订单信息。 传统数据库解决方式 在传统刘数据库中完成上面的需求非常简单，查询sql如下:： 123456789SELECT o.orderId, o.productName, p.payType, o.orderTime， payTimeFROM Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime &lt; orderTime + 3600 // 秒 上面查询可以完美的完成查询需求，那么在Apache Flink里面应该如何完成上面的需求呢？ Apache Flink解决方式UnBounded 双流 JOIN上面查询需求我们很容易想到利用《Apache Flink 漫谈系列(09) - JOIN 算子》介绍了UnBounded的双流JOIN，SQL语句如下： 123456789SELECT o.orderId, o.productName, p.payType, o.orderTime， payTime FROM Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND p.payTime &gt;= orderTime AND p.payTime as timestamp &lt; TIMESTAMPADD(SECOND, 3600, orderTime) UnBounded双流JOIN可以解决上面问题，这个示例和本篇要介绍的Interval JOIN有什么关系呢？ 性能问题虽然我们利用UnBounded的JOIN能解决上面的问题，但是仔细分析用户需求，会发现这个需求场景订单信息和付款信息并不需要长期存储，比如2018-12-27 14:22:22的订单只需要保持1小时，因为超过1个小时的订单如果没有被付款就是无效订单了。同样付款信息也不需要长期保持，2018-12-27 14:22:22的订单付款信息如果是2018-12-27 15:22:22以后到达的那么我们也没有必要保存到State中。 而对于UnBounded的双流JOIN我们会一直将数据保存到State中，如下示意图： 这样的底层实现，对于当前需求有不必要的性能损失。所以我们有必要开发一种新的可以清除State的JOIN方式(Interval JOIN)来高性能的完成上面的查询需求。 功能扩展目前的UnBounded的双流JOIN是后面是没有办法再进行Event-Time的Window Aggregate的。也就是下面的语句在Apache Flink上面是无法支持的： 1234567SELECT COUNT(*) FROM ( SELECT ..., payTime FROM Orders AS o JOIN Payment AS p ON o.orderId = p.orderId ) GROUP BY TUMBLE(payTime, INTERVAL &apos;15&apos; MINUTE) 因为在UnBounded的双流JOIN中无法保证payTime的值一定大于WaterMark（WaterMark相关可以查阅&lt;&lt;Apache Flink 漫谈系列(03) - Watermark&gt;&gt;). Apache Flink的Interval JOIN之后可以进行Event-Time的Window Aggregate。 Interval JOIN为了完成上面需求，并且解决性能和功能扩展的问题，Apache Flink在1.4开始开发了Time-windowed Join，也就是本文所说的Interval JOIN。接下来我们详细介绍Interval JOIN的语法，语义和实现原理。 什么是Interval JOINInterval JOIN 相对于UnBounded的双流JOIN来说是Bounded JOIN。就是每条流的每一条数据会与另一条流上的不同时间区域的数据进行JOIN。对应Apache Flink官方文档的 Time-windowed JOIN(release-1.7之前都叫Time-Windowed JOIN)。 Interval JOIN 语法1SELECT ... FROM t1 JOIN t2 ON t1.key = t2.key AND TIMEBOUND_EXPRESSION TIMEBOUND_EXPRESSION 有两种写法，如下： L.time between LowerBound(R.time) and UpperBound(R.time) R.time between LowerBound(L.time) and UpperBound(L.time) 带有时间属性(L.time/R.time)的比较表达式。 Interval JOIN 语义Interval JOIN 的语义就是每条数据对应一个 Interval 的数据区间，比如有一个订单表Orders(orderId, productName, orderTime)和付款表Payment(orderId, payType, payTime)。 假设我们要统计在下单一小时内付款的订单信息。SQL查询如下: 1234567891011SELECT o.orderId, o.productName, p.payType, o.orderTime， cast(payTime as timestamp) as payTimeFROM Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND p.payTime BETWEEN orderTime AND orderTime + INTERVAL &apos;1&apos; HOUR Orders订单数据 orderId productName orderTime 001 iphone 2018-12-26 04:53:22.0 002 mac 2018-12-26 04:53:23.0 003 book 2018-12-26 04:53:24.0 004 cup 2018-12-26 04:53:38.0 Payment付款数据 orderId payType payTime 001 alipay 2018-12-26 05:51:41.0 002 card 2018-12-26 05:53:22.0 003 card 2018-12-26 05:53:30.0 004 alipay 2018-12-26 05:53:31.0 符合语义的预期结果是 订单id为003的信息不出现在结果表中，因为下单时间2018-12-26 04:53:24.0, 付款时间是 2018-12-26 05:53:30.0超过了1小时付款。那么预期的结果信息如下： orderId productName payType orderTime payTime 001 iphone alipay 2018-12-26 04:53:22.0 2018-12-26 05:51:41.0 002 mac card 2018-12-26 04:53:23.0 2018-12-26 05:53:22.0 004 cup alipay 2018-12-26 04:53:38.0 2018-12-26 05:53:31.0 这样Id为003的订单是无效订单，可以更新库存继续售卖。 接下来我们以图示的方式直观说明Interval JOIN的语义，我们对上面的示例需求稍微变化一下： 订单可以预付款(不管是否合理，我们只是为了说明语义）也就是订单 前后 1小时的付款都是有效的。SQL语句如下： 1234567SELECT ...FROM Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND p.payTime BETWEEN orderTime - INTERVAL &apos;1&apos; HOUR AND orderTime + INTERVAL &apos;1&apos; HOUR 这样的查询语义示意图如下： 上图有几个关键点，如下： 数据JOIN的区间 - 比如Order时间为3的订单会在付款时间为[2, 4]区间进行JOIN。 WaterMark - 比如图示Order最后一条数据时间是3，Payment最后一条数据时间是5，那么WaterMark是根据实际最小值减去UpperBound生成，即：Min(3,5)-1 = 2 过期数据 - 出于性能和存储的考虑，要将过期数据清除，如图当WaterMark是2的时候时间为2以前的数据过期了，可以被清除。 Interval JOIN 实现原理由于Interval JOIN和双流JOIN类似都要存储左右两边的数据，所以底层实现中仍然是利用State进行数据的存储。流计算的特点是数据不停的流入，我们可以不停的进行增量计算，也就是我们每条数据流入都可以进行JOIN计算。我们还是以具体示例和图示来说明内部计算逻辑，如下图： 简单解释一下每条记录的处理逻辑如下： 实际的内部逻辑会比描述的复杂的多，大家可以根据如上简述理解内部原理即可。 示例代码我们还是以订单和付款示例，将完整代码分享给大家，具体如下(代码基于flink-1.7.0)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.sql.Timestampimport org.apache.flink.api.scala._import org.apache.flink.streaming.api.TimeCharacteristicimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.streaming.api.windowing.time.Timeimport org.apache.flink.table.api.TableEnvironmentimport org.apache.flink.table.api.scala._import org.apache.flink.types.Rowimport scala.collection.mutableobject SimpleTimeIntervalJoin &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) // 构造订单数据 val ordersData = new mutable.MutableList[(String, String, Timestamp)] ordersData.+=((&quot;001&quot;, &quot;iphone&quot;, new Timestamp(1545800002000L))) ordersData.+=((&quot;002&quot;, &quot;mac&quot;, new Timestamp(1545800003000L))) ordersData.+=((&quot;003&quot;, &quot;book&quot;, new Timestamp(1545800004000L))) ordersData.+=((&quot;004&quot;, &quot;cup&quot;, new Timestamp(1545800018000L))) // 构造付款表 val paymentData = new mutable.MutableList[(String, String, Timestamp)] paymentData.+=((&quot;001&quot;, &quot;alipay&quot;, new Timestamp(1545803501000L))) paymentData.+=((&quot;002&quot;, &quot;card&quot;, new Timestamp(1545803602000L))) paymentData.+=((&quot;003&quot;, &quot;card&quot;, new Timestamp(1545803610000L))) paymentData.+=((&quot;004&quot;, &quot;alipay&quot;, new Timestamp(1545803611000L))) val orders = env .fromCollection(ordersData) .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]()) .toTable(tEnv, &apos;orderId, &apos;productName, &apos;orderTime.rowtime) val ratesHistory = env .fromCollection(paymentData) .assignTimestampsAndWatermarks(new TimestampExtractor[String, String]()) .toTable(tEnv, &apos;orderId, &apos;payType, &apos;payTime.rowtime) tEnv.registerTable(&quot;Orders&quot;, orders) tEnv.registerTable(&quot;Payment&quot;, ratesHistory) var sqlQuery = &quot;&quot;&quot; |SELECT | o.orderId, | o.productName, | p.payType, | o.orderTime, | cast(payTime as timestamp) as payTime |FROM | Orders AS o JOIN Payment AS p ON o.orderId = p.orderId AND | p.payTime BETWEEN orderTime AND orderTime + INTERVAL &apos;1&apos; HOUR |&quot;&quot;&quot;.stripMargin tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.sqlQuery(sqlQuery)) val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row] result.print() env.execute() &#125;&#125;class TimestampExtractor[T1, T2] extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123; override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123; element._3.getTime &#125;&#125; 运行结果如下： 小节本篇由实际业务需求场景切入，介绍了相同业务需求既可以利用Unbounded 双流JOIN实现，也可以利用Time Interval JOIN来实现，Time Interval JOIN 性能优于UnBounded的双流JOIN，并且Interval JOIN之后可以进行Window Aggregate算子计算。然后介绍了Interval JOIN的语法，语义和实现原理，最后将订单和付款的完整示例代码分享给大家。期望本篇能够让大家对Apache Flink Time Interval JOIN有一个具体的了解！ 关于点赞和评论本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - Temporal Table JOIN]]></title>
    <url>%2F2019%2F03%2F18%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20Temporal%20Table%20JOIN%2F</url>
    <content type="text"><![CDATA[什么是Temporal Table在《Apache Flink 漫谈系列 - JOIN LATERAL》中提到了Temporal Table JOIN，本篇就向大家详细介绍什么是Temporal Table JOIN。 在ANSI-SQL 2011 中提出了Temporal 的概念，Oracle，SQLServer，DB2等大的数据库厂商也先后实现了这个标准。Temporal Table记录了历史上任何时间点所有的数据改动，Temporal Table的工作流程如下：上图示意Temporal Table具有普通table的特性，有具体独特的DDL/DML/QUERY语法，时间是其核心属性。历史意味着时间，意味着快照Snapshot。 ANSI-SQL 2011 Temporal Table示例我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明： DDL 示例 123456789CREATE TABLE EmpENo INTEGER,Sys_Start TIMESTAMP(12) GENERATEDALWAYS AS ROW Start,Sys_end TIMESTAMP(12) GENERATEDALWAYS AS ROW END,EName VARCHAR(30),PERIOD FOR SYSTEM_TIME (Sys_Start,Sys_end)) WITH SYSTEM VERSIONING DML 示例 INSERT1INSERT INTO Emp (ENo, EName) VALUES (22217, &apos;Joe&apos;) 说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。 UPDATE 1UPDATE Emp SET EName = &apos;Tom&apos; WHERE ENo = 22217 说明: 假设是在 2012-02-03 10:00:00 执行的UPDATE，执行之后上一个值”Joe”的Sys_End值由9999-12-31 23:59:59 变成了 2012-02-03 10:00:00, 也就是下一个值”Tom”生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同。 DELETE (假设执行DELETE之前的表内容如下)1DELETE FROM Emp WHERE ENo = 22217 说明: 假设我们是在2012-06-01 00:00:00执行的DELETE，则Sys_End值由9999-12-31 23:59:59 变成了 2012-06-01 00:00:00, 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的操作时间。标识数据的有效期到DELETE执行那一刻为止。 SELECT12SELECT ENo,EName,Sys_Start,Sys_End FROM Emp FOR SYSTEM_TIME AS OF TIMESTAMP &apos;2011-01-02 00:00:00&apos; 说明: 这个查询会返回所有Sys_Start &lt;= 2011-01-02 00:00:00 并且 Sys_end &gt; 2011-01-02 00:00:00 的记录。 SQLServer Temporal Table 示例DDL1234567891011CREATE TABLE Department(DeptID int NOT NULL PRIMARY KEY CLUSTERED, DeptName varchar(50) NOT NULL, ManagerID INT NULL, ParentDeptID int NULL, SysStartTime datetime2 GENERATED ALWAYS AS ROW Start NOT NULL, SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL, PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime))WITH (SYSTEM_VERSIONING = ON); 执行上面的语句，在数据库会创建当前表和历史表，如下图：Department 显示是有版本控制的，历史表是默认的名字，我也可以指定名字如：SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)。 DML INSERT - 插入列不包含SysStartTime和SysEndTime列12INSERT INTO [dbo].[Department] ([DeptID] ,[DeptName] ,[ManagerID] ,[ParentDeptID])VALUES(10, &apos;Marketing&apos;, 101, 1); 执行之后我们分别查询当前表和历史表，如下图：我们第一条INSERT语句数据值的有效时间是操作那一刻2018-06-06 05:50:20.7913985 到永远 9999-12-31 23:59:59.9999999，但这时刻历史表还没有任何信息。我们接下来进行更新操作。 UPDATE1UPDATE [dbo].[Department] SET [ManagerID] = 501 WHERE [DeptID] = 10 执行之后当前表信息会更新并在历史表里面产生一条历史信息，如下：注意当前表的SysStartTime意见发生了变化,历史表产生了一条记录，SyStartTIme是原当前表记录的SysStartTime，SysEndTime是当前表记录的SystemStartTime。我们再更新一次： 1UPDATE [dbo].[Department] SET [ManagerID] = 201 WHERE [DeptID] = 10 到这里我们了解到SQLServer里面关于Temporal Table的逻辑是有当前表和历史表来存储数据，并且数据库内部以StartTime和EndTime的方式管理数据的版本。 SELECT 123SELECT [DeptID], [DeptName], [SysStartTime],[SysEndTime]FROM [dbo].[Department]FOR SYSTEM_TIME AS OF &apos;2018-06-06 05:50:21.0000000&apos; ; SELECT语句查询的是Department的表，实际返回的数据是从历史表里面查询出来的，查询的底层逻辑就是 SysStartTime &lt;= &#39;2018-06-06 05:50:21.0000000&#39; and SysEndTime &gt; &#39;2018-06-06 05:50:21.0000000&#39; 。 Apache Flink Temporal Table我们不止一次的提到Apache Flink遵循ANSI-SQL标准，Apache Flink中Temporal Table的概念也源于ANSI-2011的标准语义，但目前的实现在语法层面和ANSI-SQL略有差别，上面看到ANSI-2011中使用FOR SYSTEM_TIME AS OF的语法，目前Apache Flink中使用 LATERAL TABLE(TemporalTableFunction)的语法。这一点后续需要推动社区进行改进。 为啥需要 Temporal Table我们以具体的查询示例来说明为啥需要Temporal Table，假设我们有一张实时变化的汇率表(RatesHistory)，如下： rowtime(ts) currency(pk) rate 09:00:00 US Dollar 102 09:00:00 Euro 114 09:00:00 Yen 1 10:45:00 Euro 116 11:15:00 Euro 119 11:49:00 Pounds 108 RatesHistory代表了Yen汇率(Yen汇率为1),是不断变化的Append only的汇率表。例如，Euro兑Yen汇率从09:00至10:45的汇率为114。从10点45分到11点15分是116。 假设我们想在10:58输出所有当前汇率，我们需要以下SQL查询来计算结果表： 1234567SELECT *FROM RatesHistory AS rWHERE r.rowtime = ( SELECT MAX(rowtime) FROM RatesHistory AS r2 WHERE r2.currency = r.currency AND r2.rowtime &lt;= &apos;10:58&apos;); 相应Flink代码如下： 定义数据源-genRatesHistorySource 12345678910111213141516171819202122232425262728293031323334353637383940def genRatesHistorySource: CsvTableSource = &#123; val csvRecords = Seq( &quot;rowtime ,currency ,rate&quot;, &quot;09:00:00 ,US Dollar , 102&quot;, &quot;09:00:00 ,Euro , 114&quot;, &quot;09:00:00 ,Yen , 1&quot;, &quot;10:45:00 ,Euro , 116&quot;, &quot;11:15:00 ,Euro , 119&quot;, &quot;11:49:00 ,Pounds , 108&quot; ) // 测试数据写入临时文件 val tempFilePath = writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;), Array( Types.STRING,Types.STRING,Types.STRING ), fieldDelim = &quot;,&quot;, rowDelim = &quot;$&quot;, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; def writeToTempFile( contents: String, filePrefix: String, fileSuffix: String, charset: String = &quot;UTF-8&quot;): String = &#123; val tempFile = File.createTempFile(filePrefix, fileSuffix) val tmpWriter = new OutputStreamWriter(new FileOutputStream(tempFile), charset) tmpWriter.write(contents) tmpWriter.close() tempFile.getAbsolutePath &#125; 主程序代码 12345678910111213141516171819202122232425262728293031323334353637383940414243def main(args: Array[String]): Unit = &#123; // Streaming 环境 val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) //方便我们查出输出数据 env.setParallelism(1) val sourceTableName = &quot;RatesHistory&quot; // 创建CSV source数据结构 val tableSource = CsvTableSourceUtils.genRatesHistorySource // 注册source tEnv.registerTableSource(sourceTableName, tableSource) // 注册retract sink val sinkTableName = &quot;retractSink&quot; val fieldNames = Array(&quot;rowtime&quot;, &quot;currency&quot;, &quot;rate&quot;) val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.STRING, Types.STRING) tEnv.registerTableSink( sinkTableName, fieldNames, fieldTypes, new MemoryRetractSink) val SQL = &quot;&quot;&quot; |SELECT * |FROM RatesHistory AS r |WHERE r.rowtime = ( | SELECT MAX(rowtime) | FROM RatesHistory AS r2 | WHERE r2.currency = r.currency | AND r2.rowtime &lt;= &apos;10:58:00&apos; ) &quot;&quot;&quot;.stripMargin // 执行查询 val result = tEnv.SQLQuery(SQL) // 将结果插入sink result.insertInto(sinkTableName) env.execute() &#125; 执行结果如下图：结果表格化一下： rowtime(ts) currency(pk) rate 09:00:00 US Dollar 102 09:00:00 Yen 1 10:45:00 Euro 116 Temporal Table的概念旨在简化此类查询，加速它们的执行。Temporal Table是Append Only表上的参数化视图，它把Append Only的表变化解释为表的Changelog，并在特定时间点提供该表的版本（时间版本）。将Applend Only表解释为changelog需要指定主键属性和时间戳属性。主键确定覆盖哪些行，时间戳确定行有效的时间，也就是数据版本，与上面SQL Server示例的有效期的概念一致。 在上面的示例中，currency是RatesHistory表的主键，而rowtime是timestamp属性。 如何定义Temporal Table在Apache Flink中扩展了TableFunction的接口，在TableFunction接口的基础上添加了时间属性和pk属性。 内部TemporalTableFunction定义如下： 12345678910class TemporalTableFunction private( @transient private val underlyingHistoryTable: Table, // 时间属性，相当于版本信息 private val timeAttribute: Expression, // 主键定义 private val primaryKey: String, private val resultType: RowTypeInfo) extends TableFunction[Row] &#123; ...&#125; 用户创建TemporalTableFunction方式在Table中添加了createTemporalTableFunction方法，该方法需要传入时间属性和主键，接口定义如下： 1234567// Creates TemporalTableFunction backed up by this table as a history table.def createTemporalTableFunction( timeAttribute: Expression, primaryKey: Expression): TemporalTableFunction = &#123; ...&#125; 用户通过如下方式调用就可以得到一个TemporalTableFunction的实例，代码如下： 123val tab = ...val temporalTableFunction = tab.createTemporalTableFunction(&apos;time, &apos;pk)... 案例代码 需求描述假设我们有一张订单表Orders和一张汇率表Rates，那么订单来自于不同的地区，所以支付的币种各不一样，那么假设需要统计每个订单在下单时候Yen币种对应的金额。 Orders 数据 amount currency order_time 2 Euro 2 1 US Dollar 3 50 Yen 4 3 Euro 5 Rates 数据 currency rate rate_time US Dollar 102 1 Euro 114 1 Yen 1 1 Euro 116 5 Euro 117 7 统计需求对应的SQL 123456SELECT o.currency, o.amount, r.rate o.amount * r.rate AS yen_amountFROM Orders AS o, LATERAL TABLE (Rates(o.rowtime)) AS rWHERE r.currency = o.currency 预期结果 currency amount rate yen_amount US Dollar 1 102 102 Yen 50 1 50 Euro 2 114 228 Euro 3 116 348 Without connnector 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061object TemporalTableJoinTest &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) env.setParallelism(1)// 设置时间类型是 event-time env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) // 构造订单数据 val ordersData = new mutable.MutableList[(Long, String, Timestamp)] ordersData.+=((2L, &quot;Euro&quot;, new Timestamp(2L))) ordersData.+=((1L, &quot;US Dollar&quot;, new Timestamp(3L))) ordersData.+=((50L, &quot;Yen&quot;, new Timestamp(4L))) ordersData.+=((3L, &quot;Euro&quot;, new Timestamp(5L))) //构造汇率数据 val ratesHistoryData = new mutable.MutableList[(String, Long, Timestamp)] ratesHistoryData.+=((&quot;US Dollar&quot;, 102L, new Timestamp(1L))) ratesHistoryData.+=((&quot;Euro&quot;, 114L, new Timestamp(1L))) ratesHistoryData.+=((&quot;Yen&quot;, 1L, new Timestamp(1L))) ratesHistoryData.+=((&quot;Euro&quot;, 116L, new Timestamp(5L))) ratesHistoryData.+=((&quot;Euro&quot;, 119L, new Timestamp(7L)))// 进行订单表 event-time 的提取 val orders = env .fromCollection(ordersData) .assignTimestampsAndWatermarks(new OrderTimestampExtractor[Long, String]()) .toTable(tEnv, &apos;amount, &apos;currency, &apos;rowtime.rowtime)// 进行汇率表 event-time 的提取 val ratesHistory = env .fromCollection(ratesHistoryData) .assignTimestampsAndWatermarks(new OrderTimestampExtractor[String, Long]()) .toTable(tEnv, &apos;currency, &apos;rate, &apos;rowtime.rowtime)// 注册订单表和汇率表 tEnv.registerTable(&quot;Orders&quot;, orders) tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory) val tab = tEnv.scan(&quot;RatesHistory&quot;);// 创建TemporalTableFunction val temporalTableFunction = tab.createTemporalTableFunction(&apos;rowtime, &apos;currency)//注册TemporalTableFunctiontEnv.registerFunction(&quot;Rates&quot;,temporalTableFunction) val SQLQuery = &quot;&quot;&quot; |SELECT o.currency, o.amount, r.rate, | o.amount * r.rate AS yen_amount |FROM | Orders AS o, | LATERAL TABLE (Rates(o.rowtime)) AS r |WHERE r.currency = o.currency |&quot;&quot;&quot;.stripMargin tEnv.registerTable(&quot;TemporalJoinResult&quot;, tEnv.SQLQuery(SQLQuery)) val result = tEnv.scan(&quot;TemporalJoinResult&quot;).toAppendStream[Row] // 打印查询结果 result.print() env.execute() &#125;&#125; 在运行上面代码之前需要注意上面代码中对EventTime时间提取的过程,也就是说Apache Flink的TimeCharacteristic.EventTime 模式，需要调用assignTimestampsAndWatermarks方法设置EventTime的生成方式，这种方式也非常灵活，用户可以控制业务数据的EventTime的值和WaterMark的产生，WaterMark相关内容可以查阅《Apache Flink 漫谈系列(03) - Watermark》。 在本示例中提取EventTime的完整代码如下： 1234567891011import java.SQL.Timestampimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractorimport org.apache.flink.streaming.api.windowing.time.Timeclass OrderTimestampExtractor[T1, T2] extends BoundedOutOfOrdernessTimestampExtractor[(T1, T2, Timestamp)](Time.seconds(10)) &#123; override def extractTimestamp(element: (T1, T2, Timestamp)): Long = &#123; element._3.getTime &#125;&#125; 查看运行结果: With CSVConnector 实现代码在实际的生产开发中，都需要实际的Connector的定义，下面我们以CSV格式的Connector定义来开发Temporal Table JOIN Demo。 genEventRatesHistorySource 12345678910111213141516171819202122232425262728def genEventRatesHistorySource: CsvTableSource = &#123; val csvRecords = Seq( &quot;ts#currency#rate&quot;, &quot;1#US Dollar#102&quot;, &quot;1#Euro#114&quot;, &quot;1#Yen#1&quot;, &quot;3#Euro#116&quot;, &quot;5#Euro#119&quot;, &quot;7#Pounds#108&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;), Array( Types.LONG,Types.STRING,Types.LONG ), fieldDelim = &quot;#&quot;, rowDelim = CommonUtils.line, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; genRatesOrderSource 123456789101112131415161718192021222324def genRatesOrderSource: CsvTableSource = &#123; val csvRecords = Seq( &quot;ts#currency#amount&quot;, &quot;2#Euro#10&quot;, &quot;4#Euro#10&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;), Array( Types.LONG,Types.STRING,Types.LONG ), fieldDelim = &quot;#&quot;, rowDelim = CommonUtils.line, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; 主程序代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * &quot;License&quot;); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.flink.book.connectorsimport java.io.Fileimport org.apache.flink.api.common.typeinfo.&#123;TypeInformation, Types&#125;import org.apache.flink.book.utils.&#123;CommonUtils, FileUtils&#125;import org.apache.flink.table.sinks.&#123;CsvTableSink, TableSink&#125;import org.apache.flink.table.sources.CsvTableSourceimport org.apache.flink.types.Rowobject CsvTableSourceUtils &#123; def genWordCountSource: CsvTableSource = &#123; val csvRecords = Seq( &quot;words&quot;, &quot;Hello Flink&quot;, &quot;Hi, Apache Flink&quot;, &quot;Apache FlinkBook&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;words&quot;), Array( Types.STRING ), fieldDelim = &quot;#&quot;, rowDelim = &quot;$&quot;, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; def genRatesHistorySource: CsvTableSource = &#123; val csvRecords = Seq( &quot;rowtime ,currency ,rate&quot;, &quot;09:00:00 ,US Dollar , 102&quot;, &quot;09:00:00 ,Euro , 114&quot;, &quot;09:00:00 ,Yen , 1&quot;, &quot;10:45:00 ,Euro , 116&quot;, &quot;11:15:00 ,Euro , 119&quot;, &quot;11:49:00 ,Pounds , 108&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(&quot;$&quot;), &quot;csv_source_&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;rowtime&quot;,&quot;currency&quot;,&quot;rate&quot;), Array( Types.STRING,Types.STRING,Types.STRING ), fieldDelim = &quot;,&quot;, rowDelim = &quot;$&quot;, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; def genEventRatesHistorySource: CsvTableSource = &#123; val csvRecords = Seq( &quot;ts#currency#rate&quot;, &quot;1#US Dollar#102&quot;, &quot;1#Euro#114&quot;, &quot;1#Yen#1&quot;, &quot;3#Euro#116&quot;, &quot;5#Euro#119&quot;, &quot;7#Pounds#108&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_rate&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;ts&quot;,&quot;currency&quot;,&quot;rate&quot;), Array( Types.LONG,Types.STRING,Types.LONG ), fieldDelim = &quot;#&quot;, rowDelim = CommonUtils.line, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; def genRatesOrderSource: CsvTableSource = &#123; val csvRecords = Seq( &quot;ts#currency#amount&quot;, &quot;2#Euro#10&quot;, &quot;4#Euro#10&quot; ) // 测试数据写入临时文件 val tempFilePath = FileUtils.writeToTempFile(csvRecords.mkString(CommonUtils.line), &quot;csv_source_order&quot;, &quot;tmp&quot;) // 创建Source connector new CsvTableSource( tempFilePath, Array(&quot;ts&quot;,&quot;currency&quot;, &quot;amount&quot;), Array( Types.LONG,Types.STRING,Types.LONG ), fieldDelim = &quot;#&quot;, rowDelim = CommonUtils.line, ignoreFirstLine = true, ignoreComments = &quot;%&quot; ) &#125; /** * Example: * genCsvSink( * Array[String](&quot;word&quot;, &quot;count&quot;), * Array[TypeInformation[_] ](Types.STRING, Types.LONG)) */ def genCsvSink(fieldNames: Array[String], fieldTypes: Array[TypeInformation[_]]): TableSink[Row] = &#123; val tempFile = File.createTempFile(&quot;csv_sink_&quot;, &quot;tem&quot;) if (tempFile.exists()) &#123; tempFile.delete() &#125; new CsvTableSink(tempFile.getAbsolutePath).configure(fieldNames, fieldTypes) &#125;&#125; 运行结果如下 ： 内部实现原理我们还是以订单和汇率关系示例来说明Apache Flink内部实现Temporal Table JOIN的原理，如下图所示: Temporal Table JOIN vs 双流JOIN vs Lateral JOIN在《Apache Flink 漫谈系列(09) - JOIN算子》中我们介绍了双流JOIN，在《Apache Flink 漫谈系列(10) - JOIN LATERAL 》中我们介绍了 JOIN LATERAL(TableFunction)，那么本篇介绍的Temporal Table JOIN和双流JOIN/JOIN LATERAL(TableFunction)有什么本质区别呢？ 双流JOIN - 双流JOIN本质很明确是 Stream JOIN Stream,双流驱动。 LATERAL JOIN - Lateral JOIN的本质是Steam JOIN Table Function， 是单流驱动。 Temporal Table JOIN - Temporal Table JOIN 的本质就是 Stream JOIN Temporal Table 或者 Stream JOIN Table with snapshot。Temporal Table JOIN 特点单流驱动，Temporal Table 是被动查询。 Temporal Table JOIN vs LATERAL JOIN从功能上说Temporal Table JOIN和 LATERAL JOIN都是由左流一条数据获取多行数据，也就是单流驱动，并且都是被动查询，那么Temporal JOIN和LATERAL JOIN最本质的区别是什么呢？这里我们说最关键的一点是 State 的管理，LATERAL JOIN是一个TableFunction，不具备state的管理能力，数据不具备版本特性。而Temporal Table JOIN是一个具备版本信息的数据表。 Temporal Table JOIN vs 双流 JOINTemporal Table JOIN 和 双流 JOIN都可以管理State，那么他们的本质区别是什么? 那就是计算驱动的差别，Temporal Table JOIN是单边驱动，Temporal Table是被动的查询，而双流JOIN是双边驱动，两边都是主动的进行JOIN计算。 Temporal Table JOIN改进个人认为Apache Flink的Temporal Table JOIN功能不论在语法和语义上面都要遵循ANSI-SQL标准，后期会推动社区在Temporal Table上面支持ANSI-SQL的FOR SYSTEM_TIME AS OF标准语法。改进后的处理逻辑示意图:其中cache是一种性能考虑的优化，详细内容待社区完善后再细述。 小结本篇结合ANSI-SQL标准和SQL Server对Temporal Table的支持来开篇，然后介绍目前Apache Flink对Temporal Table的支持现状，以代码示例和内部处理逻辑示意图的方式让大家直观体验Temporal Table JOIN的语法和语义。 关于点赞和评论本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - JOIN LATERAL]]></title>
    <url>%2F2019%2F03%2F11%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20JOIN%20LATERAL%2F</url>
    <content type="text"><![CDATA[聊什么上一篇《Apache Flink 漫谈系列 - JOIN算子》我们对最常见的JOIN做了详尽的分析，本篇介绍一个特殊的JOIN，那就是JOIN LATERAL。JOIN LATERAL为什么特殊呢，直观说因为JOIN的右边不是一个实际的物理表，而是一个VIEW或者Table-valued Funciton。 如下图所示： 本篇会先介绍传统数据库对LATERAL JOIN的支持，然后介绍Apache Flink目前对LATERAL JOIN的支持情况。 实际问题假设我们有两张表，一张是Customers表(消费者id, 所在城市), 一张是Orders表(订单id,消费者id)，两张表的DDL(SQL Server)如下： Customers123456789CREATE TABLE Customers ( customerid char(5) NOT NULL, city varchar (10) NOT NULL)insert into Customers values(&apos;C001&apos;,&apos;Beijing&apos;);insert into Customers values(&apos;C002&apos;,&apos;Beijing&apos;);insert into Customers values(&apos;C003&apos;,&apos;Beijing&apos;);insert into Customers values(&apos;C004&apos;,&apos;HangZhou&apos;); 查看数据： Orders123456789CREATE TABLE Orders( orderid char(5) NOT NULL, customerid char(5) NULL)insert into Orders values(&apos;O001&apos;,&apos;C001&apos;);insert into Orders values(&apos;O002&apos;,&apos;C001&apos;);insert into Orders values(&apos;O003&apos;,&apos;C003&apos;);insert into Orders values(&apos;O004&apos;,&apos;C001&apos;); 查看数据： 问题示例假设我们想查询所有Customers的客户ID，地点和订单信息,我们想得到的信息是： 用INNER JOIN解决如果大家查阅了《Apache Flink 漫谈系列 - JOIN算子》，我想看到这样的查询需求会想到INNER JOIN来解决，SQL如下： 1234SELECT c.customerid, c.city, o.orderid FROM Customers c JOIN Orders o ON o.customerid = c.customerid 查询结果如下： 但如果我们真的用上面的方式来解决，就不会有本篇要介绍的内容了，所以我们换一种写法。 用 Correlated subquery解决Correlated subquery 是在subquery中使用关联表的字段，subquery可以在FROM Clause中也可以在WHERE Clause中。 WHERE Clause用WHERE Clause实现上面的查询需求，SQL如下：12345678SELECT c.customerid, c.cityFROM Customers c WHERE c.customerid IN ( SELECT o.customerid, o.orderid FROM Orders o WHERE o.customerid = c.customerid) 执行情况： 上面的问题是用在WHERE Clause里面subquery的查询列必须和需要比较的列对应，否则我们无法对o.orderid进行投影, 上面查询我为什么要加一个o.orderid呢，因为查询需求是需要o.orderid的，去掉o.orderid查询能成功，但是拿到的结果并不是我们想要的，如下： 12345678SELECT c.customerid, c.cityFROM Customers c WHERE c.customerid IN ( SELECT o.customerid FROM Orders o WHERE o.customerid = c.customerid) 查询结果： 可见上面查询结果缺少了o.orderid,不能满足我们的查询需求。 FROM Clause用FROM Clause实现上面的查询需求，SQL如下：12345678SELECT c.customerid, c.city, o.orderid FROM Customers c, ( SELECT o.orderid, o.customerid FROM Orders o WHERE o.customerid = c.customerid) as o 我们会得到如下错误：错误信息提示我们无法识别c.customerid。在ANSI-SQL里面FROM Clause里面的subquery是无法引用左边表信息的，所以简单的用FROM Clause里面的subquery，也无法解决上面的问题，那么上面的查询需求除了INNER JOIN 我们还可以如何解决呢？ JOIN LATERAL我们分析上面的需求，本质上是根据左表Customers的customerid，去查询右表的Orders信息，就像一个For循环一样，外层是遍历左表Customers所有数据，内层是根据左表Customers的每一个Customerid去右表Orders中进行遍历查询，然后再将符合条件的左右表数据进行JOIN，这种根据左表逐条数据动态生成右表进行JOIN的语义，SQL标准里面提出了LATERAL关键字，也叫做 lateral drive table。 CROSS APPLY和LATERAL上面的示例我们用的是SQL Server进行测试的，这里在多提一下在SQL Server里面是如何支持 LATERAL 的呢？SQL Server是用自己的方言 CROSS APPLY 来支持的。那么为啥不用ANSI-SQL的LATERAL而用CROSS APPLY呢？ 可能的原因是当时SQL Server为了解决TVF问题而引入的，同时LATERAL是SQL2003引入的，而CROSS APPLY是SQL Server 2005就支持了，SQL Server 2005的开发是在2000年就进行了，这个可能也有个时间差，等LATERAL出来的时候，CROSS APPLY在SQL Server里面已经开发完成了。所以种种原因SQL Server里面就采用了CROSS APPLY，但CROSS APPLY的语义与LATERAL却完全一致，同时后续支持LATERAL的Oracle12和PostgreSQL94同时支持了LATERAL和CROSS APPLY。 问题解决那么我们回到上面的问题，我们用SQL Server的CROSS APPLY来解决上面问题，SQL如下： 上面得到的结果完全满足查询需求。 JOIN LATERAL 与 INNER JOIN 关系上面的查询需求并没有体现JOIN LATERAL和INNER JOIN的区别，我们还是以SQL Server中两个查询执行Plan来观察一下： 上面我们发现经过SQL Server优化器优化之后的两个执行plan完全一致，那么为啥还要再造一个LATERAL 出来呢？ 性能方面我们将上面的查询需求稍微改变一下，我们查询所有Customer和Customers的第一份订单信息。 LATERAL 的写法123456789SELECT c.customerid, c.city, o.orderid FROM Customers c CROSS APPLY ( SELECT TOP(1) o.orderid, o.customerid FROM Orders o WHERE o.customerid = c.customerid ORDER BY o.customerid, o.orderid) as o 查询结果：我们发现虽然C001的Customer有三笔订单，但是我们查询的TOP1信息。 JOIN 写法123456789101112SELECT c.customerid, c.city, o.orderid FROM Customers c JOIN ( SELECT o2.*, ROW_NUMBER() OVER ( PARTITION BY customerid ORDER BY orderid ) AS rn FROM Orders o2 ) oON c.customerid = o.customerid AND o.rn = 1 查询结果： 如上我们都完成了查询需求，我们在来看一下执行Plan，如下：我们直观发现完成相同功能，使用CROSS APPLY进行查询，执行Plan简单许多。 功能方面在功能方面INNER JOIN本身在ANSI-SQL中是不允许 JOIN 一个Function的，这也是SQL Server当时引入CROSS APPLY的根本原因。我们以一个SQL Server中DMV（相当于TVF）查询为例： 1234SELECT name, log_backup_time FROM sys.databases AS s CROSS APPLY sys.dm_db_log_stats(s.database_id); 查询结果： Apache Flink对 LATERAL的支持前面我花费了大量的章节来向大家介绍ANSI-SQL和传统数据库以SQL Server为例如何支持LATERAL的，接下来我们看看Apache Flink对LATERAL的支持情况。 CalciteApache Flink 利用 Calcite进行SQL的解析和优化，目前Calcite完全支持LATERAL语法，示例如下： 12345678SELECT e.NAME, e.DEPTNO, d.NAME FROM EMPS e, LATERAL ( SELECT * FORM DEPTS d WHERE e.DEPTNO=d.DEPTNO ) as d; 查询结果：我使用的是Calcite官方自带测试数据。 Flink截止到Flink-1.6.2，Apache Flink 中有两种场景使用LATERAL，如下： UDTF(TVF) - User-defined Table Funciton Temporal Table - 涉及内容会在后续篇章单独介绍。 本篇我们以在TVF(UDTF)为例说明 Apache Fink中如何支持LATERAL。 UDTFUDTF- User-defined Table Function是Apache Flink中三大用户自定义函数（UDF，UDTF，UDAGG)之一。 自定义接口如下： 基类 123456789/** * Base class for all user-defined functions such as scalar functions, table functions, * or aggregation functions. */abstract class UserDefinedFunction extends Serializable &#123; // 关键是FunctionContext中提供了若干高级属性（在UDX篇会详细介绍） def open(context: FunctionContext): Unit = &#123;&#125; def close(): Unit = &#123;&#125;&#125; TableFunction 123456789101112131415161718192021/** * Base class for a user-defined table function (UDTF). A user-defined table functions works on * zero, one, or multiple scalar values as input and returns multiple rows as output. * * The behavior of a [[TableFunction]] can be defined by implementing a custom evaluation * method. An evaluation method must be declared publicly, not static and named &quot;eval&quot;. * Evaluation methods can also be overloaded by implementing multiple methods named &quot;eval&quot;. * * User-defined functions must have a default constructor and must be instantiable during runtime. * * By default the result type of an evaluation method is determined by Flink&apos;s type extraction * facilities. This is sufficient for basic types or simple POJOs but might be wrong for more * complex, custom, or composite types. In these cases [[TypeInformation]] of the result type * can be manually defined by overriding [[getResultType()]]. */abstract class TableFunction[T] extends UserDefinedFunction &#123; // 对于泛型T，如果是基础类型那么Flink框架可以自动识别， // 对于用户自定义的复杂对象，需要用户overwrite这个实现。 def getResultType: TypeInformation[T] = null&#125; 上面定义的核心是要求用户实现eval方法，我们写一个具体示例。 示例12345678910111213// 定义一个简单的UDTF返回类型，对应接口上的 T case class SimpleUser(name: String, age: Int)// 继承TableFunction，并实现evale方法// 核心功能是解析以#分割的字符串class SplitTVF extends TableFunction[SimpleUser] &#123; // make sure input element&apos;s format is &quot;&lt;string&gt;#&lt;int&gt;&quot; def eval(user: String): Unit = &#123; if (user.contains(&quot;#&quot;)) &#123; val splits = user.split(&quot;#&quot;) collect(SimpleUser(splits(0), splits(1).toInt)) &#125; &#125;&#125; 示例(完整的ITCase)： 测试数据我们构造一个只包含一个data字段的用户表，用户表数据如下： data Sunny#8 Kevin#36 Panpan#36 * 查询需求 查询的需求是将data字段flatten成为name和age两个字段的表，期望得到： name age Sunny 8 Kevin 36 Panpan 36 查询示例我们以ITCase方式完成如上查询需求，完整代码如下：12345678910111213141516171819202122232425@Test def testLateralTVF(): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) env.setStateBackend(getStateBackend) StreamITCase.clear val userData = new mutable.MutableList[(String)] userData.+=((&quot;Sunny#8&quot;)) userData.+=((&quot;Kevin#36&quot;)) userData.+=((&quot;Panpan#36&quot;)) val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot; val users = env.fromCollection(userData).toTable(tEnv, &apos;data) val tvf = new SplitTVF() tEnv.registerTable(&quot;userTab&quot;, users) tEnv.registerFunction(&quot;splitTVF&quot;, tvf) val result = tEnv.SQLQuery(SQLQuery).toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() StreamITCase.testResults.foreach(println(_)) &#125; 运行结果： 上面的核心语句是： 1val SQLQuery = &quot;SELECT data, name, age FROM userTab, LATERAL TABLE(splitTVF(data)) AS T(name, age)&quot; 如果大家想运行上面的示例，请查阅《Apache Flink 漫谈系列 - SQL概览》中 源码方式 搭建测试环境。 小结本篇重点向大家介绍了一种新的JOIN类型 - JOIN LATERAL。并向大家介绍了SQL Server中对LATERAL的支持方式，详细分析了JOIN LATERAL和INNER JOIN的区别与联系，最后切入到Apache Flink中，以UDTF示例说明了Apache Flink中对JOIN LATERAL的支持，后续篇章会介绍Apache Flink中另一种使用LATERAL的场景，就是Temporal JION，Temporal JION也是一种新的JOIN类型，我们下一篇再见! 关于点赞和评论本系列文章难免有很多缺陷和不足，真诚希望读者对有收获的篇章给予点赞鼓励，对有不足的篇章给予反馈和建议，先行感谢大家!]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - JOIN 算子]]></title>
    <url>%2F2019%2F03%2F06%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20JOIN%20%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[聊什么在《Apache Flink 漫谈系列 - SQL概览》中我们介绍了JOIN算子的语义和基本的使用方式，介绍过程中大家发现Apache Flink在语法语义上是遵循ANSI-SQL标准的，那么再深思一下传统数据库为啥需要有JOIN算子呢？在实现原理上面Apache Flink内部实现和传统数据库有什么区别呢？本篇将详尽的为大家介绍传统数据库为什么需要JOIN算子，以及JOIN算子在Apache Flink中的底层实现原理和在实际使用中的优化！ 什么是JOIN在《Apache Flink 漫谈系列 - SQL概览》中我对JOIN算子有过简单的介绍，这里我们以具体实例的方式让大家对JOIN算子加深印象。JOIN的本质是分别从N(N&gt;=1)张表中获取不同的字段，进而得到最完整的记录行。比如我们有一个查询需求：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。如下： 为啥需要JOINJOIN的本质是数据拼接，那么如果我们将所有数据列存储在一张大表中，是不是就不需要JOIN了呢？如果真的能将所需的数据都在一张表存储，我想就真的不需要JOIN的算子了，但现实业务中真的能做到将所需数据放到同一张大表里面吗？答案是否定的，核心原因有2个： 产生数据的源头可能不是一个系统； 产生数据的源头是同一个系统，但是数据冗余的沉重代价，迫使我们会遵循数据库范式，进行表的设计。简说NF如下： 1NF - 列不可再分； 2NF - 符合1NF，并且非主键属性全部依赖于主键属性； 3NF - 符合2NF，并且消除传递依赖，即：任何字段不能由其他字段派生出来； BCNF - 符合3NF，并且主键属性之间无依赖关系。 当然还有 4NF，5NF，不过在实际的数据库设计过程中做到BCNF已经足够了！（并非否定4NF,5NF存在的意义，只是个人还没有遇到一定要用4NF，5NF的场景，设计往往会按存储成本，查询性能等综合因素考量） JOIN种类JOIN 在传统数据库中有如下分类： CROSS JOIN - 交叉连接，计算笛卡儿积； INNER JOIN - 内连接，返回满足条件的记录； OUTER JOIN LEFT - 返回左表所有行，右表不存在补NULL； RIGHT - 返回右表所有行，左边不存在补NULL； FULL - 返回左表和右表的并集，不存在一边补NULL; SELF JOIN - 自连接，将表查询时候命名不同的别名。 JOIN语法JOIN 在SQL89和SQL92中有不同的语法，以INNER JOIN为例说明： SQL89 - 表之间用“，”逗号分割，链接条件和过滤条件都在Where子句指定: 123456SELECT a.colA, b.colAFROM tab1 AS a , tab2 AS bWHERE a.id = b.id and a.other &amp;gt; b.other SQL92 - SQL92将链接条件在ON子句指定，过滤条件在WHERE子句指定，逻辑更为清晰: 1234567SELECT a.colA, b.colAFROM tab1 AS a JOIN tab2 AS b ON a.id = b.idWHERE a.other &amp;gt; b.other 本篇中的后续示例将应用SQL92语法进行SQL的编写，语法如下： 1tableExpression [ LEFT|RIGHT|FULL|INNER|SELF ] JOIN tableExpression [ ON joinCondition ] [WHERE filterCondition] 语义示例说明在《Apache Flink 漫谈系列 - SQL概览》中对JOIN语义有过简单介绍，这里会进行展开介绍。 我们以开篇示例中的三张表：学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)来介绍各种JOIN的语义。 CROSS JOIN交叉连接会对两个表进行笛卡尔积，也就是LEFT表的每一行和RIGHT表的所有行进行联接，因此生成结果表的行数是两个表行数的乘积，如student和course表的CROSS JOIN结果如下： 123456789101112131415mysql&amp;gt; SELECT * FROM student JOIN course;+------+-------+------+-----+-------+--------+| no | name | sex | no | name | credit |+------+-------+------+-----+-------+--------+| S001 | Sunny | M | C01 | Java | 2 || S002 | Tom | F | C01 | Java | 2 || S003 | Kevin | M | C01 | Java | 2 || S001 | Sunny | M | C02 | Blink | 3 || S002 | Tom | F | C02 | Blink | 3 || S003 | Kevin | M | C02 | Blink | 3 || S001 | Sunny | M | C03 | Spark | 3 || S002 | Tom | F | C03 | Spark | 3 || S003 | Kevin | M | C03 | Spark | 3 |+------+-------+------+-----+-------+--------+9 rows in set (0.00 sec) 如上结果我们得到9行=student(3) x course(3)。交叉联接一般会消耗较大的资源，也被很多用户质疑交叉联接存在的意义？(任何时候我们都有质疑的权利，同时也建议我们养成自己质疑自己“质疑”的习惯，就像小时候不理解父母的“废话”一样)。我们以开篇的示例说明交叉联接的巧妙之一，开篇中我们的查询需求是：在学生表(学号，姓名，性别)，课程表(课程号，课程名，学分)和成绩表(学号，课程号，分数)中查询所有学生的姓名，课程名和考试分数。开篇中的SQL语句得到的结果如下： 123456789101112131415mysql&amp;gt; SELECT -&amp;gt; student.name, course.name, score -&amp;gt; FROM student JOIN score ON student.no = score.s_no -&amp;gt; JOIN course ON score.c_no = course.no;+-------+-------+-------+| name | name | score |+-------+-------+-------+| Sunny | Java | 80 || Sunny | Blink | 98 || Sunny | Spark | 76 || Kevin | Java | 78 || Kevin | Blink | 88 || Kevin | Spark | 68 |+-------+-------+-------+6 rows in set (0.00 sec) 如上INNER JOIN的结果我们发现少了Tom同学的成绩，原因是Tom同学没有参加考试，在score表中没有Tom的成绩，但是我们可能希望虽然Tom没有参加考试但仍然希望Tom的成绩能够在查询结果中显示(成绩 0 分)，面对这样的需求，我们怎么处理呢？交叉联接可以帮助我们: 第一步 student和course 进行交叉联接： 123456789101112131415161718mysql&amp;gt; SELECT -&amp;gt; stu.no, c.no, stu.name, c.name -&amp;gt; FROM student stu JOIN course c 笛卡尔积 -&amp;gt; ORDER BY stu.no; -- 排序只是方便大家查看:)+------+-----+-------+-------+| no | no | name | name |+------+-----+-------+-------+| S001 | C03 | Sunny | Spark || S001 | C01 | Sunny | Java || S001 | C02 | Sunny | Blink || S002 | C03 | Tom | Spark || S002 | C01 | Tom | Java || S002 | C02 | Tom | Blink || S003 | C02 | Kevin | Blink || S003 | C03 | Kevin | Spark || S003 | C01 | Kevin | Java |+------+-----+-------+-------+9 rows in set (0.00 sec) 第二步 将交叉联接的结果与score表进行左外联接，如下： 1234567891011121314151617181920212223mysql&amp;gt; SELECT -&amp;gt; stu.no, c.no, stu.name, c.name, -&amp;gt; CASE -&amp;gt; WHEN s.score IS NULL THEN 0 -&amp;gt; ELSE s.score -&amp;gt; END AS score -&amp;gt; FROM student stu JOIN course c -- 迪卡尔积 -&amp;gt; LEFT JOIN score s ON stu.no = s.s_no and c.no = s.c_no -- LEFT OUTER JOIN -&amp;gt; ORDER BY stu.no; -- 排序只是为了大家好看一点:)+------+-----+-------+-------+-------+| no | no | name | name | score |+------+-----+-------+-------+-------+| S001 | C03 | Sunny | Spark | 76 || S001 | C01 | Sunny | Java | 80 || S001 | C02 | Sunny | Blink | 98 || S002 | C02 | Tom | Blink | 0 | -- TOM 虽然没有参加考试，但是仍然看到他的信息| S002 | C03 | Tom | Spark | 0 || S002 | C01 | Tom | Java | 0 || S003 | C02 | Kevin | Blink | 88 || S003 | C03 | Kevin | Spark | 68 || S003 | C01 | Kevin | Java | 78 |+------+-----+-------+-------+-------+9 rows in set (0.00 sec) 经过CROSS JOIN帮我们将Tom的信息也查询出来了！（TOM 虽然没有参加考试，但是仍然看到他的信息） INNER JOIN内联接在SQL92中 ON 表示联接添加，可选的WHERE子句表示过滤条件，如开篇的示例就是一个多表的内联接，我们在看一个简单的示例: 查询成绩大于80分的学生学号，学生姓名和成绩: 1234567891011mysql&amp;gt; SELECT -&amp;gt; stu.no, stu.name , s.score -&amp;gt; FROM student stu JOIN score s ON stu.no = s.s_no -&amp;gt; WHERE s.score &amp;gt; 80;+------+-------+-------+| no | name | score |+------+-------+-------+| S001 | Sunny | 98 || S003 | Kevin | 88 |+------+-------+-------+2 rows in set (0.00 sec) 上面按语义的逻辑是: 第一步：先进行student和score的内连接，如下： 1234567891011121314mysql&amp;gt; SELECT -&amp;gt; stu.no, stu.name , s.score -&amp;gt; FROM student stu JOIN score s ON stu.no = s.s_no ;+------+-------+-------+| no | name | score |+------+-------+-------+| S001 | Sunny | 80 || S001 | Sunny | 98 || S001 | Sunny | 76 || S003 | Kevin | 78 || S003 | Kevin | 88 || S003 | Kevin | 68 |+------+-------+-------+6 rows in set (0.00 sec) 第二步：对内联结果进行过滤， score &gt; 80 得到，如下最终结果： 12345678-&amp;gt; WHERE s.score &amp;gt; 80;+------+-------+-------+| no | name | score |+------+-------+-------+| S001 | Sunny | 98 || S003 | Kevin | 88 |+------+-------+-------+2 rows in set (0.00 sec) 上面的查询过程符合语义，但是如果在filter条件能过滤很多数据的时候，先进行数据的过滤，在进行内联接会获取更好的性能，比如我们手工写一下： 12345678910mysql&amp;gt; SELECT -&amp;gt; no, name , score -&amp;gt; FROM student stu JOIN ( SELECT s_no, score FROM score s WHERE s.score &amp;gt;80) as sc ON no = s_no;+------+-------+-------+| no | name | score |+------+-------+-------+| S001 | Sunny | 98 || S003 | Kevin | 88 |+------+-------+-------+2 rows in set (0.00 sec) 上面写法语义和第一种写法语义一致，得到相同的查询结果，上面查询过程是： 第一步：执行过滤子查询 12345678mysql&amp;gt; SELECT s_no, score FROM score s WHERE s.score &amp;gt;80;+------+-------+| s_no | score |+------+-------+| S001 | 98 || S003 | 88 |+------+-------+2 rows in set (0.00 sec) 第二步：执行内连接 12345678-&amp;gt; ON no = s_no;+------+-------+-------+| no | name | score |+------+-------+-------+| S001 | Sunny | 98 || S003 | Kevin | 88 |+------+-------+-------+2 rows in set (0.00 sec) 如上两种写法在语义上一致，但查询性能在数量很大的情况下会有很大差距。上面为了和大家演示相同的查询语义，可以有不同的查询方式，不同的执行计划。实际上数据库本身的优化器会自动进行查询优化，在内联接中ON的联接条件和WHERE的过滤条件具有相同的优先级，具体的执行顺序可以由数据库的优化器根据性能消耗决定。也就是说物理执行计划可以先执行过滤条件进行查询优化，如果细心的读者可能发现，在第二个写法中，子查询我们不但有行的过滤，也进行了列的裁剪(去除了对查询结果没有用的c_no列)，这两个变化实际上对应了数据库中两个优化规则： filter push down project push down 如上优化规则以filter push down 为例，示意优化器对执行plan的优化变动： LEFT OUTER JOIN左外联接语义是返回左表所有行，右表不存在补NULL，为了演示作用，我们查询没有参加考试的所有学生的成绩单： 12345678910mysql&amp;gt; SELECT -&amp;gt; no, name , s.c_no, s.score -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no -&amp;gt; WHERE s.score is NULL;+------+------+------+-------+| no | name | c_no | score |+------+------+------+-------+| S002 | Tom | NULL | NULL |+------+------+------+-------+1 row in set (0.00 sec) 上面查询的执行逻辑上也是分成两步： 第一步：左外联接查询 123456789101112131415mysql&amp;gt; SELECT -&amp;gt; no, name , s.c_no, s.score -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no;+------+-------+------+-------+| no | name | c_no | score |+------+-------+------+-------+| S001 | Sunny | C01 | 80 || S001 | Sunny | C02 | 98 || S001 | Sunny | C03 | 76 || S002 | Tom | NULL | NULL | -- 右表不存在的补NULL| S003 | Kevin | C01 | 78 || S003 | Kevin | C02 | 88 || S003 | Kevin | C03 | 68 |+------+-------+------+-------+7 rows in set (0.00 sec) 第二步：过滤查询 12345678910mysql&amp;gt; SELECT -&amp;gt; no, name , s.c_no, s.score -&amp;gt; FROM student stu LEFT JOIN score s ON stu.no = s.s_no -&amp;gt; WHERE s.score is NULL;+------+------+------+-------+| no | name | c_no | score |+------+------+------+-------+| S002 | Tom | NULL | NULL |+------+------+------+-------+1 row in set (0.00 sec) 这两个过程和上面分析的INNER JOIN一样，但是这时候能否利用上面说的 filter push down的优化呢？根据LEFT OUTER JOIN的语义来讲，答案是否定的。我们手工操作看一下： 第一步：先进行过滤查询(获得一个空表） 12mysql&amp;gt; SELECT * FROM score s WHERE s.score is NULL;Empty set (0.00 sec) 第二步： 进行左外链接 1234567891011mysql&amp;gt; SELECT -&amp;gt; no, name , s.c_no, s.score -&amp;gt; FROM student stu LEFT JOIN (SELECT * FROM score s WHERE s.score is NULL) AS s ON stu.no = s.s_no;+------+-------+------+-------+| no | name | c_no | score |+------+-------+------+-------+| S001 | Sunny | NULL | NULL || S002 | Tom | NULL | NULL || S003 | Kevin | NULL | NULL |+------+-------+------+-------+3 rows in set (0.00 sec) 我们发现两种写法的结果不一致，第一种写法只返回Tom没有参加考试，是我们预期的。第二种写法返回了Sunny，Tom和Kevin三名同学都没有参加考试，这明显是非预期的查询结果。所有LEFT OUTER JOIN不能利用INNER JOIN的 filter push down优化。 RIGHT OUTER JOIN右外链接语义是返回右表所有行，左边不存在补NULL，如下： 123456789101112131415mysql&amp;gt; SELECT -&amp;gt; s.c_no, s.score, no, name -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no = s.s_no;+------+-------+------+-------+| c_no | score | no | name |+------+-------+------+-------+| C01 | 80 | S001 | Sunny || C02 | 98 | S001 | Sunny || C03 | 76 | S001 | Sunny || NULL | NULL | S002 | Tom | -- 左边没有的进行补 NULL| C01 | 78 | S003 | Kevin || C02 | 88 | S003 | Kevin || C03 | 68 | S003 | Kevin |+------+-------+------+-------+7 rows in set (0.00 sec) 上面右外链接我只是将上面左外链接查询的左右表交换了一下:)。 FULL OUTER JOIN全外链接语义返回左表和右表的并集，不存在一边补NULL,用于演示的MySQL数据库不支持FULL OUTER JOIN。这里不做演示了。 SELF JOIN上面介绍的INNER JOIN、OUTER JOIN都是不同表之间的联接查询，自联接是一张表以不同的别名做为左右两个表，可以进行如上的INNER JOIN和OUTER JOIN。如下看一个INNER 自联接： 123456789mysql&amp;gt; SELECT * FROM student l JOIN student r where l.no = r.no;+------+-------+------+------+-------+------+| no | name | sex | no | name | sex |+------+-------+------+------+-------+------+| S001 | Sunny | M | S001 | Sunny | M || S002 | Tom | F | S002 | Tom | F || S003 | Kevin | M | S003 | Kevin | M |+------+-------+------+------+-------+------+3 rows in set (0.00 sec) 不等值联接这里说的不等值联接是SQL92语法里面的ON子句里面只有不等值联接，比如： 1234567891011121314151617181920212223242526mysql&amp;gt; SELECT -&amp;gt; s.c_no, s.score, no, name -&amp;gt; FROM score s RIGHT JOIN student stu ON stu.no != s.c_no;+------+-------+------+-------+| c_no | score | no | name |+------+-------+------+-------+| C01 | 80 | S001 | Sunny || C01 | 80 | S002 | Tom || C01 | 80 | S003 | Kevin || C02 | 98 | S001 | Sunny || C02 | 98 | S002 | Tom || C02 | 98 | S003 | Kevin || C03 | 76 | S001 | Sunny || C03 | 76 | S002 | Tom || C03 | 76 | S003 | Kevin || C01 | 78 | S001 | Sunny || C01 | 78 | S002 | Tom || C01 | 78 | S003 | Kevin || C02 | 88 | S001 | Sunny || C02 | 88 | S002 | Tom || C02 | 88 | S003 | Kevin || C03 | 68 | S001 | Sunny || C03 | 68 | S002 | Tom || C03 | 68 | S003 | Kevin |+------+-------+------+-------+18 rows in set (0.00 sec) 上面这示例，其实没有什么实际业务价值，在实际的使用场景中，不等值联接往往是结合等值联接，将不等值条件在WHERE子句指定，即, 带有WHERE子句的等值联接。 Apache Flink双流JOIN CROSS INNER OUTER SELF ON WHERE Apache Flink N Y Y Y 必选 可选 Apache Flink目前支持INNER JOIN和LEFT OUTER JOIN（SELF 可以转换为普通的INNER和OUTER)。在语义上面Apache Flink严格遵守标准SQL的语义，与上面演示的语义一致。下面我重点介绍Apache Flink中JOIN的实现原理。 双流JOIN与传统数据库表JOIN的区别传统数据库表的JOIN是两张静态表的数据联接，在流上面是 动态表(关于流与动态表的关系请查阅 《Apache Flink 漫谈系列 - 流表对偶(duality)性)》，双流JOIN的数据不断流入与传统数据库表的JOIN有如下3个核心区别： 左右两边的数据集合无穷 - 传统数据库左右两个表的数据集合是有限的，双流JOIN的数据会源源不断的流入； JOIN的结果不断产生/更新 - 传统数据库表JOIN是一次执行产生最终结果后退出，双流JOIN会持续不断的产生新的结果。在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇也有相关介绍。 查询计算的双边驱动 - 双流JOIN由于左右两边的流的速度不一样，会导致左边数据到来的时候右边数据还没有到来，或者右边数据到来的时候左边数据没有到来，所以在实现中要将左右两边的流数据进行保存，以保证JOIN的语义。在Blink中会以State的方式进行数据的存储。State相关请查看《Apache Flink 漫谈系列 - State》篇。 数据Shuffle分布式流计算所有数据会进行Shuffle，怎么才能保障左右两边流的要JOIN的数据会在相同的节点进行处理呢？在双流JOIN的场景，我们会利用JOIN中ON的联接key进行partition，确保两个流相同的联接key会在同一个节点处理。 数据的保存不论是INNER JOIN还是OUTER JOIN 都需要对左右两边的流的数据进行保存，JOIN算子会开辟左右两个State进行数据存储，左右两边的数据到来时候，进行如下操作： LeftEvent到来存储到LState，RightEvent到来的时候存储到RState； LeftEvent会去RightState进行JOIN，并发出所有JOIN之后的Event到下游； RightEvent会去LeftState进行JOIN，并发出所有JOIN之后的Event到下游。 简单场景介绍实现原理INNER JOIN 实现JOIN有很多复杂的场景，我们先以最简单的场景进行实现原理的介绍，比如：最直接的两个进行INNER JOIN，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行INNER JOIN，JION条件是产品ID，具体如下：双流JOIN两边事件都会存储到State里面，如上，事件流按照标号先后流入到join节点，我们假设右边流比较快，先流入了3个事件，3个事件会存储到state中，但因为左边还没有数据，所有右边前3个事件流入时候，没有join结果流出，当左边第一个事件序号为4的流入时候，先存储左边state，再与右边已经流入的3个事件进行join，join的结果如图 三行结果会流入到下游节点sink。当第5号事件流入时候，也会和左边第4号事件进行join，流出一条jion结果到下游节点。这里关于INNER JOIN的语义和大家强调两点： INNER JOIN只有符合JOIN条件时候才会有JOIN结果流出到下游，比如右边最先来的1，2，3个事件，流入时候没有任何输出，因为左边还没有可以JOIN的事件； INNER JOIN两边的数据不论如何乱序，都能够保证和传统数据库语义一致，因为我们保存了左右两个流的所有事件到state中。 LEFT OUTER JOIN 实现LEFT OUTER JOIN 可以简写 LEFT JOIN，语义上和INNER JOIN的区别是不论右流是否有JOIN的事件，左流的事件都需要流入下游节点，但右流没有可以JION的事件时候，右边的事件补NULL。同样我们以最简单的场景说明LEFT JOIN的实现，比如查询产品库存和订单数量，库存变化事件流和订单事件流进行LEFT JOIN，JION条件是产品ID，具体如下：下图也是表达LEFT JOIN的语义，只是展现方式不同：上图主要关注点是当左边先流入1，2事件时候，右边没有可以join的事件时候会向下游发送左边事件并补NULL向下游发出，当右边第一个相同的Join key到来的时候会将左边先来的事件发出的带有NULL的事件撤回（对应上面command的-记录，+代表正向记录，-代表撤回记录）。这里强调三点： 左流的事件当右边没有JOIN的事件时候，将右边事件列补NULL后流向下游；* 当右边事件流入发现左边已经有可以JOIN的key的时候，并且是第一个可以JOIN上的右边事件（比如上面的3事件是第一个可以和左边JOIN key P001进行JOIN的事件）需要撤回左边下发的NULL记录，并下发JOIN完整（带有右边事件列）的事件到下游。后续来的4，5，6，8等待后续P001的事件是不会产生撤回记录的。 在Apache Flink系统内部事件类型分为正向事件标记为“+”和撤回事件标记为“-”。 RIGHT OUTER JOIN 和 FULL OUTER JOINRIGHT JOIN内部实现与LEFT JOIN类似， FULL JOIN和LEFT JOIN的区别是左右两边都会产生补NULL和撤回的操作。对于State的使用都是相似的，这里不再重复说明了。 复杂场景介绍State结构上面我们介绍了双流JOIN会使用State记录左右两边流的事件，同时我们示例数据的场景也是比较简单，比如流上没有更新事件（没有撤回事件），同时流上没有重复行事件。那么我们尝试思考下面的事件流在双流JOIN时候是怎么处理的？上图示例是连续产生了2笔销售数量一样的订单，同时在产生一笔销售数量为5的订单之后，又将该订单取消了（或者退货了），这样在事件流上面就会是上图的示意，这种情况Blink内部如何支撑呢？根据JOIN的语义以INNER JOIN为例，右边有两条相同的订单流入，我们就应该向下游输出两条JOIN结果，当有撤回的事件流入时候，我们也需要将已经下发下游的JOIN事件撤回，如下：上面的场景以及LEFT JOIN部分介绍的撤回情况，Apache Flink内部需要处理如下几个核心点： 记录重复记录（完整记录重复记录或者记录相同记录的个数） 记录正向记录和撤回记录（完整记录正向和撤回记录或者记录个数） 记录哪一条事件是第一个可以与左边事件进行JOIN的事件 双流JOIN的State数据结构在Apache Flink内部对不同的场景有特殊的数据结构优化，本篇我们只针对上面说的情况（通用设计）介绍一下双流JOIN的State的数据结构和用途： 数据结构 Map&lt;JoinKey, Map&lt;rowData, count&gt;&gt;; 第一级MAP的key是Join key，比如示例中的P001， value是流上面的所有完整事件; 第二级MAP的key是行数据，比如示例中的P001, 2，value是相同事件值的个数数据结构的利用 记录重复记录 - 利用第二级MAP的value记录重复记录的个数，这样大大减少存储和读取 正向记录和撤回记录 - 利用第二级MAP的value记录，当count=0时候删除该元素 判断右边是否产生撤回记录 - 根据第一级MAP的value的size来判断是否产生撤回，只有size由0变成1的时候（第一条和左可以JOIN的事件）才产生撤回 双流JOIN的应用优化构造更新流我们在 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》篇中以双流JOIN为例介绍了如何构造业务上的PK source，构造PK source本质上在保证业务语义的同时也是对双流JOIN的一种优化，比如多级LEFT JOIN会让流上的数据不断膨胀，造成JOIN节点性能较慢，JOIN之后的下游节点边堵(数据量大导致，非热点)。那么嫌少流入JOIN的数据，比如构造PK source就会大大减少JOIN数据的膨胀。这里不再重复举例，大家可以查阅 《Apache Flink 漫谈系列 - 持续查询(Continuous Queries)》 的双流JOIN示例部分。 NULL造成的热点比如我们有A LEFT JOIN B ON A.aCol = B.bCol LEFT JOIN C ON B.cCol = C.cCol 的业务，JOB的DAG如下：假设在实际业务中有这样的特点，大部分时候当A事件流入的时候，B还没有可以JOIN的数据，但是B来的时候，A已经有可以JOIN的数据了，这特点就会导致，A LEFT JOIN B 会产生大量的 (A, NULL),其中包括B里面的 cCol 列也是NULL，这时候当与C进行LEFT JOIN的时候，首先Blink内部会利用cCol对AB的JOIN产生的事件流进行Shuffle， cCol是NULL进而是下游节点大量的NULL事件流入，造成热点。那么这问题如何解决呢？我们可以改变JOIN的先后顺序，来保证A LEFT JOIN B 不会产生NULL的热点问题，如下： JOIN ReOrder对于JOIN算子的实现我们知道左右两边的事件都会存储到State中，在流入事件时候在从另一边读取所有事件进行JOIN计算，这样的实现逻辑在数据量很大的场景会有一定的state操作瓶颈，我们某些场景可以通过业务角度调整JOIN的顺序，来消除性能瓶颈，比如：A JOIN B ON A.acol = B.bcol JOIN C ON B.bcol = C.ccol. 这样的场景，如果 A与B进行JOIN产生数据量很大，但是B与C进行JOIN产生的数据量很小，那么我们可以强制调整JOIN的联接顺序，B JOIN C ON b.bcol = c.ccol JOIN A ON a.acol = b.bcol. 如下示意图： 小结本篇向大家介绍了数据库设计范式的要求和实际业务的查询需要是传统数据库JOIN算子存在的原因，并以具体示例的方式向大家介绍JOIN在数据库的查询过程，以及潜在的查询优化，再以实际的例子介绍Apache Flink上面的双流JOIN的实现原理和State数据结构设计，最后向大家介绍两个双流JOIN的使用优化。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - 持续查询(Continuous Queries)]]></title>
    <url>%2F2019%2F03%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20%E6%8C%81%E7%BB%AD%E6%9F%A5%E8%AF%A2(Continuous%20Queries)%2F</url>
    <content type="text"><![CDATA[实际问题我们知道在流计算场景中，数据是源源不断的流入的，数据流永远不会结束，那么计算就永远不会结束，如果计算永远不会结束的话，那么计算结果何时输出呢？本篇将介绍Apache Flink利用持续查询来对流计算结果进行持续输出的实现原理。 数据管理在介绍持续查询之前，我们先看看Apache Flink对数据的管理和传统数据库对数据管理的区别，以MySQL为例，如下图： 如上图所示传统数据库是数据存储和查询计算于一体的架构管理方式，这个很明显，oracle数据库不可能管理MySQL数据库数据，反之亦然，每种数据库厂商都有自己的数据库管理和存储的方式，各自有特有的实现。在这点上Apache Flink海纳百川(也有corner case)，将data store 进行抽象，分为source(读) 和 sink(写)两种类型接口，然后结合不同存储的特点提供常用数据存储的内置实现，当然也支持用户自定义的实现。 那么在宏观设计上Apache Flink与传统数据库一样都可以对数据表进行SQL查询，并将产出的结果写入到数据存储里面，那么Apache Flink上面的SQL查询和传统数据库查询的区别是什么呢？Apache Flink又是如何做到求同(语义相同)存异(实现机制不同)，完美支持ANSI-SQL的呢？ 静态查询传统数据库中对表(比如 flink_tab，有user和clicks两列，user主键)的一个查询SQL(select * from flink_tab)在数据量允许的情况下，会立刻返回表中的所有数据，在查询结果显示之后，对数据库表flink_tab的DML操作将与执行的SQL无关了。也就是说传统数据库下面对表的查询是静态查询，将计算的最终查询的结果立即输出，如下： 1234567select * from flink_tab;+----+------+--------+| id | user | clicks |+----+------+--------+| 1 | Mary | 1 |+----+------+--------+1 row in set (0.00 sec) 当我执行完上面的查询，查询结果立即返回，上面情况告诉我们表 flink_tab里面只有一条记录，id=1，user=Mary,clicks=1; 这样传统数据库表的一条查询语句就完全结束了。传统数据库表在查询那一刻我们这里叫Static table，是指在查询的那一刻数据库表的内容不再变化了，查询进行一次计算完成之后表的变化也与本次查询无关了，我们将在Static Table 上面的查询叫做静态查询。 持续查询什么是连续查询呢？连续查询发生在流计算上面，在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们提到过Dynamic Table，连续查询是作用在Dynamic table上面的，永远不会结束的，随着表内容的变化计算在不断的进行着… 静态/持续查询特点静态查询和持续查询的特点就是《Apache Flink 漫谈系列 - 流表对偶(duality)性》中所提到的批与流的计算特点，批一次查询返回一个计算结果就结束查询，流一次查询不断修正计算结果，查询永远不结束，表格示意如下：| 查询类型 | 计算次数 |计算结果 || — | — | — || 静态查询 | 1 | 最终结果 || 持续查询 | 无限 | 不断更新 | 静态/持续查询关系接下来我们以flink_tab表实际操作为例，体验一下静态查询与持续查询的关系。假如我们对flink_tab表再进行一条增加和一次更新操作，如下： 12345MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Bob&apos;, 1);Query OK, 1 row affected (0.08 sec)MySQL&gt; update flink_tab set clicks=2 where user=&apos;Mary&apos;;Query OK, 1 row affected (0.06 sec) 这时候我们再进行查询 select * from flink_tab ，结果如下： 12345678MySQL&gt; select * from flink_tab;+----+------+--------+| id | user | clicks |+----+------+--------+| 1 | Mary | 2 || 2 | Bob | 1 |+----+------+--------+2 rows in set (0.00 sec) 那么我们看见，相同的查询SQL(select * from flink_tab)，计算结果完全 不 一样了。这说明相同的sql语句，在不同的时刻执行计算，得到的结果可能不一样(有点像废话），就如下图一样： 假设不断的有人在对表flink_tab做操作，同时有一个人间歇性的发起对表数据的查询，上图我们只是在三个时间点进行了3次查询。并且在这段时间内数据表的内容也在变化。引起上面变化的DML如下： 12345678910MySQL&gt; insert into flink_tab(user, clicks) values (&apos;Llz&apos;, 1);Query OK, 1 row affected (0.08 sec)MySQL&gt; update flink_tab set clicks=2 where user=&apos;Bob&apos;;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0MySQL&gt; update flink_tab set clicks=3 where user=&apos;Mary&apos;;Query OK, 1 row affected (0.05 sec)Rows matched: 1 Changed: 1 Warnings: 0 到现在我们不难想象，上面图内容的核心要点如下： 时间 表数据变化 触发计算 计算结果更新 接下来我们利用传统数据库现有的机制模拟一下持续查询… 无PK的 Append only 场景接下来我们把上面隐式存在的时间属性timestamp作为表flink_tab_ts(timestamp，user，clicks三列，无主键)的一列，再写一个 触发器(Trigger) 示例观察一下： timestamp user clicks 1525099013 Mary 1 1525099026 Bob 1 1525099035 Mary 2 1525099047 Llz 1 1525099056 Bob 2 1525099065 Mary 3 12345678// INSERT 的时候查询一下数据flink_tab_ts，将结果写到trigger.sql中 DELIMITER ;;create trigger flink_tab_ts_trigger_insert after inserton flink_tab_ts for each row begin select ts, user, clicks from flink_tab_ts into OUTFILE &apos;/Users/jincheng.sunjc/testdir/atas/trigger.sql&apos;; end ;;DELIMITER ; 上面的trigger要将查询结果写入本地文件，默认MySQL是不允许写入的，我们查看一下： 12345678MySQL&gt; show variables like &apos;%secure%&apos;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| require_secure_transport | OFF || secure_file_priv | NULL |+--------------------------+-------+2 rows in set (0.00 sec) 上面secure_file_priv属性为NULL，说明MySQL不允许写入file，我需要修改my.cnf在添加secure_file_priv=’’打开写文件限制； 12345678MySQL&gt; show variables like &apos;%secure%&apos;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| require_secure_transport | OFF || secure_file_priv | |+--------------------------+-------+2 rows in set (0.00 sec) 下面我们对flink_tab_ts进行INSERT操作： 我们再来看看6次trigger 查询计算的结果： 大家到这里发现我写了Trigger的存储过程之后，每次在数据表flink_tab_ts进行DML操作的时候，Trigger就会触发一次查询计算，产出一份新的计算结果，观察上面的查询结果发现，结果表不停的增加（Append only)。 有PK的Update场景我们利用flink_tab_ts的6次DML操作和自定义的触发器TriggerL来介绍了什么是持续查询，做处理静态查询与持续查询的关系。那么上面的演示目的是为了说明持续查询，所有操作都是insert，没有基于主键的更新，也就是说Trigger产生的结果都是append only的，那么大家想一想，如果我们操作flink_tab这张表，按主键user进行插入和更新操作，同样利用Trigger机制来进行持续查询，结果是怎样的的呢？ 初始化表，trigger： 12345678910111213141516171819202122drop table flink_tab;create table flink_tab( user VARCHAR(100) NOT NULL, clicks INT NOT NULL, PRIMARY KEY (user) ); DELIMITER ;;create trigger flink_tab_trigger_insert after inserton flink_tab for each row begin select user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;; end ;;DELIMITER ;DELIMITER ;;create trigger flink_tab_trigger_ after updateon flink_tab for each row begin select ts, user, clicks from flink_tab into OUTFILE &apos;/tmp/trigger.sql&apos;; end ;;DELIMITER ; 同样我做如下6次DML操作，Trigger 6次查询计算： 在来看看这次的结果与append only 有什么不同？ 我想大家早就知道这结果了，数据库里面定义的PK所有变化会按PK更新，那么触发的6次计算中也会得到更新后的结果，这应该不难理解，查询结果也是不断更新的(Update)! 关系定义上面Append Only 和 Update两种场景在MySQL上面都可以利用Trigger机制模拟 持续查询的概念，也就是说数据表中每次数据变化，我们都触发一次相同的查询计算（只是计算时候数据的集合发生了变化），因为数据表不断的变化，这个表就可以看做是一个动态表Dynamic Table，而查询SQL(select * from flink_tab_ts) 被触发器Trigger在满足某种条件后不停的触发计算，进而也不断地产生新的结果。这种作用在Dynamic Table，并且有某种机制(Trigger)不断的触发计算的查询我们就称之为 持续查询。 那么到底静态查询和动态查询的关系是什么呢？在语义上 持续查询 中的每一次查询计算的触发都是一次静态查询(相对于当时查询的时间点), 在实现上 Apache Flink会利用上一次查询结果+当前记录 以增量的方式完成查询计算。 特别说明： 上面我们利用 数据变化+Trigger方式描述了持续查询的概念，这里有必要特别强调一下的是数据库中trigger机制触发的查询，每次都是一个全量查询，这与Apache Flink上面流计算的持续查询概念相同，但实现机制完全不同，Apache Flink上面的持续查询内部实现是增量处理的，随着时间的推移，每条数据的到来实时处理当前的那一条记录，不会处理曾经来过的历史记录! Apache Flink 如何做到持续查询动态表上面持续查询在 《Apache Flink 漫谈系列 - 流表对偶(duality)性》 中我们了解到流和表可以相互转换，在Apache Flink流计算中携带流事件的Schema，经过算子计算之后再产生具有新的Schema的事件，流入下游节点，在产生新的Schema的Event和不断流转的过程就是持续查询作用的结果，如下图： 增量计算我们进行查询大多数场景是进行数据聚合，比如查询SQL中利用count，sum等aggregate function进行聚合统计，那么流上的数据源源不断的流入，我们既不能等所有事件流入结束（永远不会结束）再计算，也不会每次来一条事件就像传统数据库一样将全部事件集合重新整体计算一次，在持续查询的计算过程中，Apache Flink采用增量计算的方式，也就是每次计算都会将计算结果存储到state中，下一条事件到来的时候利用上次计算的结果和当前的事件进行聚合计算，比如 有一个订单表，如下： 一个简单的计数和求和查询SQL： 12// 求订单总数和所有订单的总金额select count(id) as cnt，sum(amount)as sumAmount from order_tab; 这样一个简单的持续查询计算，Apache Flink内部是如何处理的呢？如下图： 如上图，Apache Flink中每来一条事件,就进行一次计算，并且每次计算后结果会存储到state中，供下一条事件到来时候进行计算,即: 1result(n) = calculation(result(n-1), n)。 无PK的Append Only 场景在实际的业务场景中，我们只需要进行简单的数据统计，然后就将统计结果写入到业务的数据存储系统里面，比如上面统计订单数量和总金额的场景，订单表本身是一个append only的数据源(假设没有更新，截止到2018.5.14日，Apache Flink内部支持的数据源都是append only的)，在持续查询过程中经过count(id),sum(amount)统计计算之后产生的动态表也是append only的，种场景Apache Flink内部只需要进行aggregate function的聚合统计计算就可以，如下： 有PK的Update 场景现在我们将上面的订单场景稍微变化一下，在数据表上面我们将金额字段amount，变为地区字段region，数据如下： 查询统计的变为，在计算具有相同订单数量的地区数量；查询SQL如下： 12345678910111213141516171819202122232425 CREATE TABLE order_tab( id BIGINT, region VARCHAR ) CREATE TABLE region_count_sink( order_cnt BIGINT, region_cnt BIGINT, PRIMARY KEY(order_cnt) -- 主键) -- 按地区分组计算每个地区的订单数量CREATE VIEW order_count_view AS SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region;-- 按订单数量分组统计具有相同订单数量的地区数量INSERT INTO region_count_sink SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt; 上面查询SQL的代码结构如下(这个图示在Alibaba 企业版Flink的集成IDE环境生成的，了解更多)： 上面SQL中我们发现有两层查询计算逻辑，第一个查询计算逻辑是与SOURCE相连的按地区统计订单数量的分组统计，第二个查询计算逻辑是在第一个查询产出的动态表上面进行按订单数量统计地区数量的分组统计，我们一层一层分析。 错误处理 第一层分析：SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region; 第二层分析：SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt; 按照第一层分析的结果，再分析第二层产出的结果，我们分析的过程是对的，但是最终写到sink表的计算结果是错误的，那我们错在哪里了呢？ 其实当 (SH,2)这条记录来的时候，以前来过的(SH, 1)已经是脏数据了，当(BJ, 2)来的时候，已经参与过计算的(BJ, 1)也变成脏数据了，同样当(BJ, 3)来的时候，(BJ, 2)也是脏数据了，上面的分析，没有处理脏数据进而导致最终结果的错误。那么Apache Flink内部是如何正确处理的呢？ 正确处理 第一层分析：SELECT region, count(id) AS order_cnt FROM order_tab GROUP BY region; 第二层分析：SELECT order_cnt, count(region) as region_cnt FROM order_count_view GROUP BY order_cnt; 上面我们将有更新的事件进行打标的方式来处理脏数据，这样在Apache Flink内部计算的时候 算子会根据事件的打标来处理事件，在aggregate function中有两个对应的方法(retract和accumulate)来处理不同标识的事件,如上面用到的count AGG，内部实现如下： 123456789def accumulate(acc: CountAccumulator): Unit = &#123; acc.f0 += 1L // acc.f0 存储记数&#125;def retract(acc: CountAccumulator, value: Any): Unit = &#123; if (value != null) &#123; acc.f0 -= 1L //acc.f0 存储记数 &#125;&#125; Apache Flink内部这种为事件进行打标的机制叫做 retraction。retraction机制保障了在流上已经流转到下游的脏数据需要被撤回问题，进而保障了持续查询的正确语义。 Apache Flink Connector 类型本篇一开始就对比了MySQL的数据存储和Apache Flink数据存储的区别，Apache Flink目前是一个计算平台，将数据的存储以高度抽象的插件机制与各种已有的数据存储无缝对接。目前Apache Flink中将数据插件称之为链接器Connector，Connnector又按数据的读和写分成Soruce（读）和Sink（写）两种类型。对于传统数据库表，PK是一个很重要的属性，在频繁的按某些字段(PK)进行更新的场景,在表上定义PK非常重要。那么作为完全支持ANSI-SQL的Apache Flink平台在Connector上面是否也支持PK的定义呢？ Apache Flink Source现在(2018.11.5)Apache Flink中用于数据流驱动的Source Connector上面无法定义PK，这样在某些业务场景下会造成数据量较大，造成计算资源不必要的浪费，甚至有聚合结果不是用户“期望”的情况。我们以双流JOIN为例来说明： 12345678910111213141516171819202122232425262728293031323334SQL：CREATE TABLE inventory_tab( product_id VARCHAR, product_count BIGINT); CREATE TABLE sales_tab( product_id VARCHAR, sales_count BIGINT ) ;CREATE TABLE join_sink( product_id VARCHAR, product_count BIGINT, sales_count BIGINT, PRIMARY KEY(product_id));CREATE VIEW join_view AS SELECT l.product_id, l.product_count, r.sales_count FROM inventory_tab l JOIN sales_tab r ON l.product_id = r.product_id;INSERT INTO join_sink SELECT product_id, product_count, sales_count FROM join_view ; 代码结构图： 实现示意图： 上图描述了一个双流JOIN的场景，双流JOIN的底层实现会将左(L)右(R)两面的数据都持久化到Apache Flink的State中，当L流入一条事件，首先会持久化到LState，然后在和RState中存储的R中所有事件进行条件匹配，这样的逻辑如果R流product_id为P001的产品销售记录已经流入4条，L流的(P001, 48) 流入的时候会匹配4条事件流入下游(join_sink)。 问题上面双流JOIN的场景，我们发现其实inventory和sales表是有业务的PK的，也就是两张表上面的product_id是唯一的，但是由于我们在Sorure上面无法定义PK字段，表上面所有的数据都会以append only的方式从source流入到下游计算节点JOIN，这样就导致了JOIN内部所有product_id相同的记录都会被匹配流入下游，上面的例子是 (P001, 48) 来到的时候，就向下游流入了4条记录，不难想象每个product_id相同的记录都会与历史上所有事件进行匹配，进而操作下游数据压力。 那么这样的压力是必要的吗？从业务的角度看，不是必要的，因为对于product_id相同的记录，我们只需要对左右两边最新的记录进行JOIN匹配就可以了。比如(P001, 48)到来了，业务上面只需要右流的(P001, 22)匹配就好，流入下游一条事件(P001, 48, 22)。 那么目前在Apache Flink上面如何做到这样的优化呢？ 解决方案上面的问题根本上我们要构建一张有PK的动态表，这样按照业务PK进行更新处理，我们可以在Source后面添加group by 操作生产一张有PK的动态表。如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849SQL:CREATE TABLE inventory_tab( product_id VARCHAR, product_count BIGINT ) CREATE TABLE sales_tab( product_id VARCHAR, sales_count BIGINT )CREATE VIEW inventory_view AS SELECT product_id, LAST_VALUE(product_count) AS product_count FROM inventory_tab GROUP BY product_id;CREATE VIEW sales_view AS SELECT product_id, LAST_VALUE(sales_count) AS sales_count FROM sales_tab GROUP BY product_id;CREATE TABLE join_sink( product_id VARCHAR, product_count BIGINT, sales_count BIGINT, PRIMARY KEY(product_id))WITH ( type = &apos;print&apos;) ;CREATE VIEW join_view AS SELECT l.product_id, l.product_count, r.sales_count FROM inventory_view l JOIN sales_view r ON l.product_id = r.product_id; INSERT INTO join_sink SELECT product_id, product_count, sales_count FROM join_view ; 代码结构： 示意图: 如上方式可以将无PK的source经过一次节点变成有PK的动态表，以Apache Flink的retract机制和业务要素解决数据瓶颈，减少计算资源的消耗。 说明1： 上面方案LAST_VALUE是Alibaba企业版Flink的功能，社区还没有支持。 Apache Flink Sink在Apache Flink上面可以根据实际外部存储的特点（是否支持PK），以及整体job的执行plan来动态推导Sink的执行模式，具体有如下三种类型: Append 模式 - 该模式用户在定义Sink的DDL时候不定义PK，在Apache Flink内部生成的所有只有INSERT语句； Upsert 模式 - 该模式用户在定义Sink的DDL时候可以定义PK，在Apache Flink内部会根据事件打标(retract机制)生成INSERT/UPDATE和DELETE 语句,其中如果定义了PK， UPDATE语句按PK进行更新，如果没有定义PK UPDATE会按整行更新； Retract 模式 - 该模式下会产生INSERT和DELETE两种信息，Sink Connector 根据这两种信息构造对应的数据操作指令； 小结本篇以MySQL为例介绍了传统数据库的静态查询和利用MySQL的Trigger+DML操作来模拟持续查询，并介绍了Apache Flink上面利用增量模式完成持续查询，并以双流JOIN为例说明了持续查询可能会遇到的问题，并且介绍Apache Flink以为事件打标产生delete事件的方式解决持续查询的问题，进而保证语义的正确性，完美的在流计算上支持续查询。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - 概述]]></title>
    <url>%2F2019%2F01%2F05%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Apache Flink 的命脉“命脉“ 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以”批是流的特例“的认知进行系统设计的。 唯快不破我们经常听说 “天下武功，唯快不破”，大概意思是说 “任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破”。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是”快“,也就是 “低延时“。 就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 “低延时“ 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 “快“呢？根本原因是Apache Flink 设计之初就认为 “批是流的特例“，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。 那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是”流是批的特例“ 和 “批是流的特例“ 两种不同认知产物。 Micro Batching 模式Micro-Batching 计算模式认为 “流是批的特例“， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图： 很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。 Native Streaming 模式Native Streaming 计算模式认为 “”批是流的特“, 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图： 很明显Native Streaming模式占据了流计算领域 “低延时“ 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现Native Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。 丰富的部署模式Apache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。 Local 模式该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。参考 Cluster模式该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster 参考 YARN Cluster 参考这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下： 其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N&gt;=1)个JM候选,进而解决SPOF问题，示意如下： 在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。 Cloud 模式该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE 参考，Amazon的EC2 参考，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。 完善的容错机制什么是容错容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。 容错的处理模式在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够”运行”外，还要求能”正确运行”,也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制： At Most Once：最多消费一次，这种处理机制会存在数据丢失的可能。 At Least Once：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。 Exactly Once：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。 Apache Flink的容错机制Apache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图: 目前Apache Flink 支持两种数据容错机制： At Least Once Exactly Once 其中 Exactly Once 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 End-to-End 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景： 场景一：Flink的Source Operator 在读取到Kafla中pos=2000的数据时候，由于某种原因宕机了，这个时候Flink框架会分配一个新的节点继续读取Kafla数据，那么新的处理节点怎样处理才能保证数据处理且只被处理一次呢？ 场景二：Flink Data Flow内部某个节点，如果上图的agg()节点发生问题，在恢复之后怎样处理才能保持map()流出的数据处理且只被处理一次？ 场景三：Flink的Sink Operator 在写入Kafka过程中自身节点出现问题，在恢复之后如何处理，计算结果才能保证写入且只被写入一次？ 系统内部容错Apache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 Lightweight Asynchronous Snapshots for Distributed Dataflows 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 Determining-Global-States-of-a-Distributed-System Paper。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了At Least Once 和 Exactly Once 两种容错处理模式。 Apache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图： 这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。 外部Source容错Apache Flink 要做到 End-to-End 的 Exactly Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。 外部Sink容错Apache Flink 要做到 End-to-End 的 Exactly Once 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 exactly once的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly Once 语义(重复写入就变成了At Least Once了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。 流批统一的计算引擎批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？ 统一的数据传输层开篇我们就介绍Apache Flink 的 “命脉“是以”批是流的特例“为导向来进行引擎的设计的，系统设计成为 “Native Streaming“的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。 Apache Flink 在网络传输层面有两种数据传输模式： PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。 BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。 对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。 统一任务调度层Apache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。 统一的用户API层Apache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。 求同存异Apache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。 Apache Flink 架构组件栈我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下：TableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？ TableAPI&amp;SQL到DataStrem&amp;DataSet的架构TableAPI&amp;SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下： 对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。 ANSI-SQL的支持Apache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下： Declarative - 用户只需要表达我想要什么，不用关心如何计算。 Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。 Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。 Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。 Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。 无限扩展的优化机制Apache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner： HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。 VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。 Flink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。 丰富的类库和算子Apache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。 类库 CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。 ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。 GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。 算子Apache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。 多流操作 UNION - 将多个字段类型一致数据流合并为一个数据流，如下示意： JOIN - 将多个数据流(数据类型可以不一致)联接为一个数据流，如下示意： 如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。 单流操作将多流变成单流之后，我们按数据输入输出的不同归类如下：| 类型 | 输入 | 输出|Table/SQL算子|DataStream/DataSet算子|| — | — | — | — |— || Scalar Function | 1 | 1 | Built-in &amp; UDF, |Map|| Table Function| 1 | N(N&gt;=0) | Built-in &amp; UDTF |FlatMap|| Aggregate Function| N(N&gt;=0) |1 | Built-in &amp; UDAF|Reduce| 如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。 存在的问题Apache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下： 这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。 小结本篇概要的介绍了”批是流的特例“这一设计观点是Apache Flink的”命脉“，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的”低延迟”需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Alibaba企业版的Flink的架构，以及对开源Apache Flink所作出的优化。 本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！ 最后感谢大家将宝贵的时间分配在对本篇分享的查阅上！]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - Watermark]]></title>
    <url>%2F2019%2F01%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20%20Watermark%2F</url>
    <content type="text"><![CDATA[实际问题（乱序）在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图：那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？ Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： ProcessingTime是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。 IngestionTimeIngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。 EventTimeEventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。 开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。 什么是WatermarkWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: Watermark的产生方式目前Apache Flink 有两种生产Watermark的方式，如下： Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。 Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。 所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。 Watermark的接口定义对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下： Periodic Watermarks - AssignerWithPeriodicWatermarks 1234567891011121314151617181920212223/** * Returns the current watermark. This method is periodically called by the * system to retrieve the current watermark. The method may return &#123;@code null&#125; to * indicate that no new Watermark is available. * * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp * is larger than that of the previously emitted watermark (to preserve the contract of * ascending watermarks). If the current watermark is still * identical to the previous one, no progress in EventTime has happened since * the previous call to this method. If a null value is returned, or theTimestamp * of the returned watermark is smaller than that of the last emitted one, then no * new watermark will be generated. * * &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated * depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;. * * @see org.Apache.flink.streaming.api.watermark.Watermark * @see ExecutionConfig#getAutoWatermarkInterval() * * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit. */ @Nullable Watermark getCurrentWatermark(); Punctuated Watermarks - AssignerWithPunctuatedWatermarks 1234567891011121314151617181920public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;/** * Asks this implementation if it wants to emit a watermark. This method is called right after * the &#123;@link #extractTimestamp(Object, long)&#125; method. * * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp * is larger than that of the previously emitted watermark (to preserve the contract of * ascending watermarks). If a null value is returned, or theTimestamp of the returned * watermark is smaller than that of the last emitted one, then no new watermark will * be generated. * * &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of * &#123;@link AssignerWithPunctuatedWatermarks this class&#125;. * * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit. */ @NullableWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);&#125; AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner 1234567891011121314151617public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;/** * Assigns aTimestamp to an element, in milliseconds since the Epoch. * * &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element. * That previousTimestamp may have been assigned from a previous assigner, * by ingestionTime. If the element did not carry aTimestamp before, this value is * &#123;@code Long.MIN_VALUE&#125;. * * @param element The element that theTimestamp is wil be assigned to. * @param previousElementTimestamp The previous internalTimestamp of the element, * or a negative value, if noTimestamp has been assigned, yet. * @return The newTimestamp. */long extractTimestamp(T element, long previousElementTimestamp);&#125; 从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。 Watermark解决如上问题从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。 回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下：上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： CREATE TABLE source( …, Event_timeTimeStamp, WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0)) with ( …); 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下： 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下：CREATE TABLE source( …, Event_timeTimeStamp, WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000)) with ( …);上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s. 多流的Watermark处理在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示： Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图: 小结本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - Fault Tolerance]]></title>
    <url>%2F2019%2F01%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20Fault%20Tolerance%2F</url>
    <content type="text"><![CDATA[实际问题在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。那么在计算过程中如果网络、机器等原因导致Task运行失败了，Apache Flink会如何处理呢？在 《Apache Flink 漫谈系列 - State》一篇中我们介绍了 Apache Flink 会利用State记录计算的状态，在Failover时候Task会根据State进行恢复。但State的内容是如何记录的？Apache Flink 是如何保证 Exactly-Once 语义的呢？这就涉及到了Apache Flink的 容错(Fault Tolerance) 机制，本篇将会为大家进行相关内容的介绍。 什么是Fault Tolerance容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来，并使系统能够自动恢复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不包含系统故障所引起的差错。 传统数据库Fault Tolerance我们知道MySql的binlog是一个Append Only的日志文件，Mysql的主备复制是高可用的主要方式，binlog是主备复制的核心手段（当然mysql高可用细节很复杂也有多种不同的优化点，如 纯异步复制优化为半同步和同步复制以保证异步复制binlog导致的master和slave的同步时候网络坏掉，导致主备不一致问题等)。Mysql主备复制，是Mysql容错机制的一部分，在容错机制之中也包括事物控制，在传统数据库中事物可以设置不同的事物级别，以保证不同的数据质量，级别由低到高 如下： Read uncommitted - 读未提交，就是一个事务可以读取另一个未提交事务的数据。那么这种事物控制成本最低，但是会导致另一个事物读到脏数据，那么如何解决读到脏数据的问题呢？利用Read committed 级别… Read committed - 读提交，就是一个事务要等另一个事务提交后才能读取数据。这种级别可以解决读脏数据的问题，那么这种级别有什么问题呢？这个级别还有一个 不能重复读的问题，即：开启一个读事物T1，先读取字段F1值是V1，这时候另一个事物T2可以UPDATA这个字段值V2，导致T1再次读取字段值时候获得V2了，同一个事物中的两次读取不一致了。那么如何解决不可重复读的问题呢？利用 Repeatable read 级别… Repeatable read - 重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。重复读模式要有事物顺序的等待，需要一定的成本达到高质量的数据信息，那么重复读还会有什么问题吗？是的，重复读级别还有一个问题就是 幻读，幻读产生的原因是INSERT，那么幻读怎么解决呢？利用Serializable级别… Serializable - 序列化 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 主备复制，事物控制都是传统数据库容错的机制。 流计算Fault Tolerance的挑战流计算Fault Tolerance的一个很大的挑战是低延迟，很多Apache Flink任务都是7 x 24小时不间断，端到端的秒级延迟，要想在遇上网络闪断，机器坏掉等非预期的问题时候快速恢复正常，并且不影响计算结果正确性是一件极其困难的事情。同时除了流计算的低延时要求，还有计算模式上面的挑战，在Apache Flink中支持Exactly-Once和At-Least-Once两种计算模式，如何做到在Failover时候不重复计算,进而精准的做到Exactly-Once也是流计算Fault Tolerance要重点解决的问题。 Apache Flink的Fault Tolerance 机制Apache Flink的Fault Tolerance机制核心是持续创建分布式流数据及其状态的快照。这些快照在系统遇到故障时，作为一个回退点。Apache Flink中创建快照的机制叫做Checkpointing，Checkpointing的理论基础 Stephan 在 Lightweight Asynchronous Snapshots for Distributed Dataflows 进行了细节描述，该机制源于由K. MANI CHANDY和LESLIE LAMPORT 发表的 Determining-Global-States-of-a-Distributed-System Paper，该Paper描述了在分布式系统如何解决全局状态一致性问题。 在Apache Flink中以Checkpointing的机制进行容错，Checkpointing会产生类似binlog一样的、可以用来恢复任务状态的数据文件。Apache Flink中也有类似于数据库事物控制一样的数据计算语义控制，比如：At-Least-Once和Exactly-Once。 Checkpointing 的算法逻辑上面我们说Checkpointing是Apache Flink中Fault Tolerance的核心机制，我们以Checkpointing的方式创建包含timer，connector，window，user-defined state 等stateful Operator的快照。在Determining-Global-States-of-a-Distributed-System的全局状态一致性算法中重点描述了全局状态的对齐问题，在Lightweight Asynchronous Snapshots for Distributed Dataflows中核心描述了对齐的方式，在Apache Flink中采用以在流信息中插入barrier的方式完成DAG中异步快照。 如下图(from Lightweight Asynchronous Snapshots for Distributed Dataflows)描述了Asynchronous barrier snapshots for acyclic graphs，也是Apache Flink中采用的方式。 上图描述的是一个增量计算word count的Job逻辑，核心逻辑是如下几点： barrier 由source节点发出； barrier会将流上event切分到不同的checkpoint中； 汇聚到当前节点的多流的barrier要对齐； barrier对齐之后会进行Checkpointing，生成snapshot； 完成snapshot之后向下游发出barrier，继续直到Sink节点； 这样在整个流计算中以barrier方式进行Checkpointing，随着时间的推移，整个流的计算过程中按时间顺序不断的进行Checkpointing，如下图： 生成的snapshot会存储到StateBackend中，相关State的介绍可以查阅 《Apache Flink 漫谈系列 - State》。这样在进行Failover时候，从最后一次成功的checkpoint进行恢复。 Checkpointing 的控制上面我们了解到整个流上面我们会随这时间推移不断的做Checkpointing，不断的产生snapshot存储到Statebackend中，那么多久进行一次Checkpointing？对产生的snapshot如何持久化的呢？带着这些疑问，我们看看Apache Flink对于Checkpointing如何控制的？有哪些可配置的参数:(这些参数都在 CheckpointCoordinator 中进行定义） checkpointMode - 检查点模式，分为 AT_LEAST_ONCE 和 EXACTLY_ONCE 两种模式； checkpointInterval - 检查点时间间隔，单位是毫秒。 checkpointTimeout - 检查点超时时间，单位毫秒。 在Apache Flink中还有一些其他配置，比如：是否将存储到外部存储的checkpoints数据删除，如果不删除，即使job被cancel掉，checkpoint信息也不会删除，当恢复job时候可以利用checkpoint进行状态恢复。我们有两种配置方式，如下： ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints不会删除。 ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION - 当job被cancel时候，外部存储的checkpoints会被删除。 Apache Flink 如何做到Exactly-once通过上面内容我们了解了Apache Flink中Exactly-Once和At-Least-Once只是在进行checkpointing时候的配置模式，两种模式下进行checkpointing的原理是一致的，那么在实现上有什么本质区别呢？ 语义 At-Least-Once - 语义是流上所有数据至少被处理过一次（不要丢数据） Exactly-Once - 语义是流上所有数据必须被处理且只能处理一次（不丢数据，且不能重复） 从语义上面Exactly-Once 比 At-Least-Once对数据处理的要求更严格，更精准，那么更高的要求就意味着更高的代价，这里的代价就是 延迟。 实现那在实现上面Apache Flink中At-Least-Once 和 Exactly-Once有什么区别呢？区别体现在多路输入的时候（比如 Join），当所有输入的barrier没有完全到来的时候，早到来的event在Exactly-Once模式下会进行缓存（不进行处理），而在At-Least-Once模式下即使所有输入的barrier没有完全到来，早到来的event也会进行处理。也就是说对于At-Least-Once模式下，对于下游节点而言，本来数据属于checkpoint N 的数据在checkpoint N-1 里面也可能处理过了。 我以Exactly-Once为例说明Exactly-Once模式相对于At-Least-Once模式为啥会有更高的延时？如下图： 上图示意了某个节点进行Checkpointing的过程： 当Operator接收到某个上游发下来的第barrier时候开始进行barrier的对齐阶段； 在进行对齐期间早到的input的数据会被缓存到buffer中； 当Operator接收到上游所有barrier的时候，当前Operator会进行Checkpointing，生成snapshot并持久化； 当完Checkpointing时候将barrier广播给下游Operator； 多路输入的barrier没有对齐的时候，barrier先到的输入数据会缓存在buffer中，不进行处理，这样对于下游而言buffer的数据越多就有更大的延迟。这个延时带来的好处就是相邻Checkpointing所记录的数据（计算结果或event)没有重复。相对At-Least-Once模式数据不会被buffer，减少延时的利好是以容忍数据重复计算为代价的。 在Apache Flink的代码实现上用CheckpointBarrierHandler类处理barrier，其核心接口是： 123456public interface CheckpointBarrierHandler &#123; ... //返回operator消费的下一个BufferOrEvent。这个调用会导致阻塞直到获取到下一个BufferOrEvent BufferOrEvent getNextNonBlocked() throws Exception; ...&#125; 其中BufferOrEvent，可能是正常的data event，也可能是特殊的event，比如barrier event。对应At-Least-Once和Exactly-Once有两种不同的实现，具体如下: Exactly-Once模式 - BarrierBuffer BarrierBuffer用于提供Exactly-Once一致性保证，其行为是：它将以barrier阻塞输入直到所有的输入都接收到基于某个检查点的barrier，也就是上面所说的对齐。为了避免背压输入流，BarrierBuffer将从被阻塞的channel中持续地接收buffer并在内部存储它们，直到阻塞被解除。 BarrierBuffer 实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。该方法是阻塞调用，直到获取到下一个记录。其中这里的记录包括两种，一种是来自于上游未被标记为blocked的输入，比如上图中的 event(a),；另一种是，从已blocked输入中缓冲区队列中被释放的记录，比如上图中的event(1,2,3,4)。 At-Least-Once模式 - BarrierTrackerBarrierTracker会对各个输入接收到的检查点的barrier进行跟踪。一旦它观察到某个检查点的所有barrier都已经到达，它将会通知监听器检查点已完成，以触发相应地回调处理。不像BarrierBuffer的处理逻辑，BarrierTracker不阻塞已经发送了barrier的输入，也就说明不采用对齐机制，因此本检查点的数据会及时被处理，并且因此下一个检查点的数据可能会在该检查点还没有完成时就已经到来。这样在恢复时只能提供At-Least-Once的语义保证。 BarrierTracker也实现了CheckpointBarrierHandler的getNextNonBlocked, 该方法用于获取待处理的下一条记录。与BarrierBuffer相比它实现很简单，只是阻塞的获取要处理的event。 如上两个CheckpointBarrierHandler实现的核心区别是BarrierBuffer会维护多路输入是否要blocked，缓存被blocked的输入的record。所谓有得必有失，有失必有得，舍得舍得在这里也略有体现哈 :)。 完整Job的Checkpointing过程在 《Apache Flink 漫谈系列 - State》中我们有过对Apache Flink存储到State中的内容做过介绍，比如在connector会利用OperatorState记录读取位置的offset，那么一个完整的Apache Flink任务的执行图是一个DAG，上面我们描述了DAG中一个节点的过程，那么整体来看Checkpointing的过程是怎样的呢？在产生checkpoint并分布式持久到HDFS的过程是怎样的呢？ 整体Checkpointing流程 上图我们看到一个完整的Apache Flink Job进行Checkpointing的过程，JM触发Soruce发射barriers,当某个Operator接收到上游发下来的barrier，开始进行barrier的处理，整体根据DAG自上而下的逐个节点进行Checkpointing，并持久化到Statebackend，一直到DAG的sink节点。 Incremental Checkpointing对于一个流计算的任务，数据会源源不断的流入，比如要进行双流join(Apache Flink 漫谈系列 - Join 篇会详细介绍)，由于两边的流event的到来有先后顺序问题，我们必须将left和right的数据都会在state中进行存储，Left event流入会在Right的State中进行join数据，Right event流入会在Left的State中进行join数据，如下图左右两边的数据都会持久化到State中： 由于流上数据源源不断，随着时间的增加，每次checkpoint产生的snapshot的文件（RocksDB的sst文件）会变的非常庞大，增加网络IO，拉长checkpoint时间，最终导致无法完成checkpoint，进而导致Apache Flink失去Failover的能力。为了解决checkpoint不断变大的问题，Apache Flink内部实现了Incremental Checkpointing，这种增量进行checkpoint的机制，会大大减少checkpoint时间，并且如果业务数据稳定的情况下每次checkpoint的时间是相对稳定的，根据不同的业务需求设定checkpoint的interval，稳定快速的进行Checkpointing，保障Apache Flink任务在遇到故障时候可以顺利的进行Failover。Incremental Checkpointing的优化对于Apache Flink成百上千的任务节点带来的利好不言而喻。 端到端exactly-once根据上面的介绍我们知道Apache Flink内部支持Exactly-Once语义，要想达到端到端（Soruce到Sink）的Exactly-Once，需要Apache Flink外部Soruce和Sink的支持，具体如下： 外部Source的容错要求Apache Flink 要做到 End-to-End 的 Exactly-Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部Source提供读取数据的Position和支持根据Position进行数据读取。 外部Sink的容错要求Apache Flink 要做到 End-to-End 的 Exactly-Once相对比较困难，以Kafka作为Sink为例，当Sink Operator节点宕机时候，根据Apache Flink 内部Exactly-Once模式的容错保证, 系统会回滚到上次成功的Checkpoint继续写入，但是上次成功checkpoint之后当前checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly-Once 语义(重复写入就变成了At-Least-Once了)，如果要解决这一问题，Apache Flink 利用Two Phase Commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。 小结本篇和大家介绍了Apache Flink的容错（Fault Tolerance）机制，本篇内容结合《Apache Flink 漫谈系列 - State》 一起查阅相信大家会对Apache Flink的State和Fault Tolerance会有更好的理解，也会对后面介绍window，retraction都会有很好的帮助。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - State]]></title>
    <url>%2F2019%2F01%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20State%2F</url>
    <content type="text"><![CDATA[实际问题在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: “上一次的计算结果保存在哪里，保存在内存可以吗？”， 答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。 什么是State这个问题似乎有些”弱智”？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。 为什么需要State与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。 State 实现Apache Flink内部有四种state的存储实现，具体如下： 基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用； 基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳； 基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化； 还有一个是基于Niagara(Alibaba企业版Flink)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；State 持久化逻辑Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。State 分类Apache Flink 内部按照算子和数据分组角度将State划分为如下两类： KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的； OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。 State 扩容重新分配Apache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。 Apache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。 如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示: 在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。 OperatorState对扩容的处理我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？state 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed&lt;T extends Serializable&gt;，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下： 12345public interface ListCheckpointed&amp;lt;T extends Serializable&amp;gt; &#123; List&amp;lt;T&amp;gt; snapshotState(long var1, long var3) throws Exception; void restoreState(List&amp;lt;T&amp;gt; var1) throws Exception;&#125; 我们发现 snapshotState方法的返回值是一个List&lt;T&gt;,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下： 123public interface InputSplit extends Serializable &#123; int getSplitNumber();&#125; 也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下： 如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下： 123456List&amp;lt;Integer&amp;gt; assignedPartitions = new LinkedList&amp;lt;&amp;gt;();for (int i = 0; i &amp;lt; partitions; i++) &#123; if (i % consumerCount == consumerIndex) &#123; assignedPartitions.add(i); &#125;&#125; 这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下： 那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List&lt;T&gt;的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。 KeyedState对扩容的处理对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。 什么是Key-GroupsKey-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下： 12345678public class KeyGroupRange implements KeyGroupsList, Serializable &#123; ... ... private final int startKeyGroup; private final int endKeyGroup; ... ...&#125; KeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。 什么决定Key-Groups的个数key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。 GroupRange.of(0, maxParallelism)如何决定key属于哪个Key-Group确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下： 123456789101112public static int assignToKeyGroup(Object key, int maxParallelism) &#123; return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);&#125;public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) &#123; return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);&#125;@Overridepublic int partition(T key, int numPartitions) &#123; return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;&#125; 如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示： 如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。每个Operator实例如何获取Key-Groups 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法： 123456789101112131415161718192021222324252627282930public static KeyGroupRange computeKeyGroupRangeForOperatorIndex( int maxParallelism, int parallelism, int operatorIndex) &#123; GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex); int startGroup = splitRange.getStartGroup(); int endGroup = splitRange.getEndGroup(); return new KeyGroupRange(startGroup, endGroup - 1);&#125;public GroupRange getSplitRange(int numSplits, int splitIndex) &#123; ... final int numGroupsPerSplit = getNumGroups() / numSplits; final int numFatSplits = getNumGroups() % numSplits; int startGroupForThisSplit; int endGroupForThisSplit; if (splitIndex &amp;lt; numFatSplits) &#123; startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1); endGroupForThisSplit = startGroupForThisSplit + numGroupsPerSplit + 1; &#125; else &#123; startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits; endGroupForThisSplit = startGroupForThisSplit + numGroupsPerSplit; &#125; if (startGroupForThisSplit &amp;gt;= endGroupForThisSplit) &#123; return GroupRange.emptyGroupRange(); &#125; else &#123; return new GroupRange(startGroupForThisSplit, endGroupForThisSplit); &#125;&#125; 上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图：如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。 小结本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。同时本篇没有介绍Alibaba 企业版 Flink的Niagara版本的State。Niagara是Alibaba精心打造的新一代适用于流计算场景的StateBackend存储实现，相关内容后续在合适时间再向大家介绍。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - 流表对偶(duality)性]]></title>
    <url>%2F2019%2F01%2F01%2FApache%20Flink%20%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97%20-%20%E6%B5%81%E8%A1%A8%E5%AF%B9%E5%81%B6(duality)%E6%80%A7%2F</url>
    <content type="text"><![CDATA[实际问题很多大数据计算产品，都对用户提供了SQL API，比如Hive, Spark, Flink等，那么SQL作为传统关系数据库的查询语言，是应用在批查询场景的。Hive和Spark本质上都是Batch的计算模式(在《Apache Flink 漫谈系列 - 概述》我们介绍过Spark是Micro Batching模式)，提供SQL API很容易被人理解，但是Flink是纯流（Native Streaming）的计算模式, 流与批在数据集和计算过程上有很大的区别. 如图所示： 批查询场景的特点 - 有限数据集，一次查询返回一个计算结果就结束查询 流查询场景的特点 - 无限数据集，一次查询不断修正计算结果，查询永远不结束 我们发现批与流的查询场景在数据集合和计算过程上都有很大的不同，那么基于Native Streaming模式的Apache Flink为啥也能为用户提供SQL API呢？ 流与批的语义关系我们知道SQL都是作用于关系表的，在传统数据库中进行查询时候，SQL所要查询的表在触发查询时候数据是不会变化的，也就是说在查询那一刻，表是一张静态表，相当于是一个有限的批数据，这样也说明SQL是源于对批计算的查询的，那么要回答Apache Flink为啥也能为用户提供SQL API，我们首先要理解流与批在语义层面的关系。我们以一个具体示例说明，如下图：上图展现的是一个携带时间戳和用户名的点击事件流，我们先对这些事件流进行流式统计，同时在最后的流事件上触发批计算。流计算中每接收一个数据都会触发一次计算，我们以2018/4/30 22:37:45 Mary到来那一时间切片看，无论是在流还是批上计算结果都是6。也就是说在相同的数据源，相同的查询逻辑下，流和批的计算结果是相同的。相同的SQL在流和批这两种模式下，最终结果是一致的，那么流与批在语义上是完全相同的。 流与表的关系流与批在语义上是一致的，SQL是作用于表的，那么要回答Apache Flink为啥也能为用户提供SQL API的问题，就变成了流与表是否具有等价性，也就是本篇要重点介绍的为什么流表具有对偶(duality)性？如下图所示，一张表可以看做为流吗？同样流可以看做是一张表吗？如果可以需要怎样的条件和变通？ MySQL主备复制在介绍流与表的关系之前我们先聊聊MySQL的主备复制，binlog是MySQL实现主备复制的核心手段，简单来说MySQL主备复制实现分成三个步骤： Master将改变(change logs)以二进制日志事件(binary log events)形式记录到二进制日志(binlog)中； Slave将Master的binary log events拷贝到它的中继日志(relay log)； Slave重做中继日志中的事件，将改变反映到数据； 具体如下图所示: binlog接下来我们从binlog模式，binlog格式以及通过查看binlog的具体内容来详尽介绍binlog与表的关系。 binlog模式上面介绍的MySQL主备复制的核心手段是利用binlog实现的，那边binlog会记录那些内容呢？binlog记录了数据库所有的增、删、更新等操作。MySQL支持三种方式记录binlog: statement-based logging - Events contain SQL statements that produce data changes (inserts, updates, deletes); row-based logging - Events describe changes to individual rows; mixed-base logging - 该模式默认是statement-based，当遇到如下情况会自动切换到row-based: NDB存储引擎，DML操作以row格式记录; 使用UUID()、USER()、CURRENT_USER()、FOUND_ROWS()等不确定函数; 使用Insert Delay语句; 使用用户自定义函数(UDF); 使用临时表; binlog格式我们以row-based 模式为例介绍一下binlog的存储格式 ，所有的 binary log events都是字节序列，由两部分组成： event header event data 关于event header和event data 的格式在数据库的不同版本略有不同，但共同的地方如下： 123456789101112131415+=====================================+| event | timestamp 0 : 4 || header +----------------------------+| | type_code 4 : 1 || +----------------------------+| | server_id 5 : 4 || +----------------------------+| | event_length 9 : 4 || +----------------------------+| |不同版本不一样(省略) |+=====================================+| event | fixed part || data +----------------------------+| | variable part |+=====================================+ 这里有个值得我们注意的地方就是在binlog的header中有一个属性是timestamp，这个属性是标识了change发生的先后顺序，在备库进行复制时候会严格按照时间顺序进行log的重放。 binlog的生成我们以对MySQL进行实际操作的方式，直观的介绍一下binlog的生成，binlog是二进制存储的，下面我们会利用工具查看binlog的文本内容。 查看一下binlog是否打开： 12345678show variables like &apos;log_bin&apos; -&amp;gt; ;+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+1 row in set (0.00 sec) 查看一下binlog的模式(我需要row-base模式)： 1234567show variables like &apos;binlog_format&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+1 row in set (0.00 sec) 清除现有的binlog 12345678910111213141516MySQL&amp;gt; reset master;Query OK, 0 rows affected (0.00 sec)创建一张我们做实验的表MySQL&amp;gt; create table tab( -&amp;gt; id INT NOT NULL AUTO_INCREMENT, -&amp;gt; user VARCHAR(100) NOT NULL, -&amp;gt; clicks INT NOT NULL, -&amp;gt; PRIMARY KEY (id) -&amp;gt; );Query OK, 0 rows affected (0.10 sec)MySQL&amp;gt; show tables;+-------------------+| Tables_in_Apache Flinkdb |+-------------------+| tab |+-------------------+1 row in set (0.00 sec) 进行DML操作 12345678910111213141516171819202122232425262728293031MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Mary&apos;, 1);Query OK, 1 row affected (0.03 sec)MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Bob&apos;, 1);Query OK, 1 row affected (0.08 sec)MySQL&amp;gt; update tab set clicks=2 where user=&apos;Mary&apos; -&amp;gt; ;Query OK, 1 row affected (0.06 sec)Rows matched: 1 Changed: 1 Warnings: 0MySQL&amp;gt; insert into tab(user, clicks) values (&apos;Llz&apos;, 1);Query OK, 1 row affected (0.08 sec)MySQL&amp;gt; update tab set clicks=2 where user=&apos;Bob&apos;;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0MySQL&amp;gt; update tab set clicks=3 where user=&apos;Mary&apos;;Query OK, 1 row affected (0.05 sec)Rows matched: 1 Changed: 1 Warnings: 0MySQL&amp;gt; select * from tab;+----+------+--------+| id | user | clicks |+----+------+--------+| 1 | Mary | 3 || 2 | Bob | 2 || 3 | Llz | 1 |+----+------+--------+3 rows in set (0.00 sec) 查看正在操作的binlog 12345678MySQL&amp;gt; show master status\G*************************** 1. row *************************** File: binlog.000001 Position: 2547 Binlog_Do_DB: Binlog_Ignore_DB: Executed_Gtid_Set: 1 row in set (0.00 sec) 上面 binlog.000001 文件是我们正在操作的binlog。 查看binlog.000001文件的操作记录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647MySQL&amp;gt; show binlog events in &apos;binlog.000001&apos;;+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+| binlog.000001 | 4 | Format_desc | 1 | 124 | Server ver: 8.0.11, Binlog ver: 4 || binlog.000001 | 124 | Previous_gtids | 1 | 155 | || binlog.000001 | 155 | Anonymous_Gtid | 1 | 228 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 228 | Query | 1 | 368 | use `Apache Flinkdb`; DROP TABLE `tab` /* generated by server */ /* xid=22 */ || binlog.000001 | 368 | Anonymous_Gtid | 1 | 443 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 443 | Query | 1 | 670 | use `Apache Flinkdb`; create table tab( id INT NOT NULL AUTO_INCREMENT, user VARCHAR(100) NOT NULL, clicks INT NOT NULL, PRIMARY KEY (id)) /* xid=23 */ || binlog.000001 | 670 | Anonymous_Gtid | 1 | 745 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 745 | Query | 1 | 823 | BEGIN || binlog.000001 | 823 | Table_map | 1 | 890 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 890 | Write_rows | 1 | 940 | table_id: 96 flags: STMT_END_F || binlog.000001 | 940 | Xid | 1 | 971 | COMMIT /* xid=25 */ || binlog.000001 | 971 | Anonymous_Gtid | 1 | 1046 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 1046 | Query | 1 | 1124 | BEGIN || binlog.000001 | 1124 | Table_map | 1 | 1191 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 1191 | Write_rows | 1 | 1240 | table_id: 96 flags: STMT_END_F || binlog.000001 | 1240 | Xid | 1 | 1271 | COMMIT /* xid=26 */ || binlog.000001 | 1271 | Anonymous_Gtid | 1 | 1346 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 1346 | Query | 1 | 1433 | BEGIN || binlog.000001 | 1433 | Table_map | 1 | 1500 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 1500 | Update_rows | 1 | 1566 | table_id: 96 flags: STMT_END_F || binlog.000001 | 1566 | Xid | 1 | 1597 | COMMIT /* xid=27 */ || binlog.000001 | 1597 | Anonymous_Gtid | 1 | 1672 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 1672 | Query | 1 | 1750 | BEGIN || binlog.000001 | 1750 | Table_map | 1 | 1817 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 1817 | Write_rows | 1 | 1866 | table_id: 96 flags: STMT_END_F || binlog.000001 | 1866 | Xid | 1 | 1897 | COMMIT /* xid=28 */ || binlog.000001 | 1897 | Anonymous_Gtid | 1 | 1972 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 1972 | Query | 1 | 2059 | BEGIN || binlog.000001 | 2059 | Table_map | 1 | 2126 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 2126 | Update_rows | 1 | 2190 | table_id: 96 flags: STMT_END_F || binlog.000001 | 2190 | Xid | 1 | 2221 | COMMIT /* xid=29 */ || binlog.000001 | 2221 | Anonymous_Gtid | 1 | 2296 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || binlog.000001 | 2296 | Query | 1 | 2383 | BEGIN || binlog.000001 | 2383 | Table_map | 1 | 2450 | table_id: 96 (Apache Flinkdb.tab) || binlog.000001 | 2450 | Update_rows | 1 | 2516 | table_id: 96 flags: STMT_END_F || binlog.000001 | 2516 | Xid | 1 | 2547 | COMMIT /* xid=30 */ |+---------------+------+----------------+-----------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+36 rows in set (0.00 sec) 上面我们进行了3次insert和3次update，那么在binlog中我们看到了三条Write_rows和三条Update_rows，并且在记录顺序和操作顺序保持一致，接下来我们看看Write_rows和Update_rows的具体timestamp和data明文。 导出明文1sudo MySQLbinlog --start-datetime=&apos;2018-04-29 00:00:03&apos; --stop-datetime=&apos;2018-05-02 00:30:00&apos; --base64-output=decode-rows -v /usr/local/MySQL/data/binlog.000001 &amp;gt; ~/binlog.txt 打开binlog.txt 内容如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#180430 22:29:33 server id 1 end_log_pos 124 CRC32 0xff61797c Start: binlog v 4, server v 8.0.11 created 180430 22:29:33 at startup# Warning: this binlog is either in use or was not closed properly.ROLLBACK/*!*/;# at 124#180430 22:29:33 server id 1 end_log_pos 155 CRC32 0x629ae755 Previous-GTIDs# [empty]# at 155#180430 22:32:11 server id 1 end_log_pos 228 CRC32 0xbde49fca Anonymous_GTID last_committed=0 sequence_number=1 rbr_only=no original_committed_timestamp=1525098731207902 immediate_commit_timestamp=1525098731207902 transaction_length=213# original_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)# immediate_commit_timestamp=1525098731207902 (2018-04-30 22:32:11.207902 CST)/*!80001 SET @@session.original_commit_timestamp=1525098731207902*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 228#180430 22:32:11 server id 1 end_log_pos 368 CRC32 0xe5f330e7 Query thread_id=9 exec_time=0 error_code=0 Xid = 22use `Apache Flinkdb`/*!*/;SET TIMESTAMP=1525098731/*!*/;SET @@session.pseudo_thread_id=9/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1168113696/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\C utf8mb4 *//*!*/;SET @@session.character_set_client=255,@@session.collation_connection=255,@@session.collation_server=255/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;/*!80005 SET @@session.default_collation_for_utf8mb4=255*//*!*/;DROP TABLE `tab` /* generated by server *//*!*/;# at 368#180430 22:32:21 server id 1 end_log_pos 443 CRC32 0x50e5acb7 Anonymous_GTID last_committed=1 sequence_number=2 rbr_only=no original_committed_timestamp=1525098741628960 immediate_commit_timestamp=1525098741628960 transaction_length=302# original_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)# immediate_commit_timestamp=1525098741628960 (2018-04-30 22:32:21.628960 CST)/*!80001 SET @@session.original_commit_timestamp=1525098741628960*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 443#180430 22:32:21 server id 1 end_log_pos 670 CRC32 0xe1353dd6 Query thread_id=9 exec_time=0 error_code=0 Xid = 23SET TIMESTAMP=1525098741/*!*/;create table tab( id INT NOT NULL AUTO_INCREMENT, user VARCHAR(100) NOT NULL, clicks INT NOT NULL, PRIMARY KEY (id))/*!*/;# at 670#180430 22:36:53 server id 1 end_log_pos 745 CRC32 0xcf436fbb Anonymous_GTID last_committed=2 sequence_number=3 rbr_only=yes original_committed_timestamp=1525099013988373 immediate_commit_timestamp=1525099013988373 transaction_length=301/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)# immediate_commit_timestamp=1525099013988373 (2018-04-30 22:36:53.988373 CST)/*!80001 SET @@session.original_commit_timestamp=1525099013988373*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 745#180430 22:36:53 server id 1 end_log_pos 823 CRC32 0x71c64dd2 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099013/*!*/;BEGIN/*!*/;# at 823#180430 22:36:53 server id 1 end_log_pos 890 CRC32 0x63792f6b Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 890#180430 22:36:53 server id 1 end_log_pos 940 CRC32 0xf2dade22 Write_rows: table id 96 flags: STMT_END_F### INSERT INTO `Apache Flinkdb`.`tab`### SET### @1=1### @2=&apos;Mary&apos;### @3=1# at 940#180430 22:36:53 server id 1 end_log_pos 971 CRC32 0x7db3e61e Xid = 25COMMIT/*!*/;# at 971#180430 22:37:06 server id 1 end_log_pos 1046 CRC32 0xd05dd12c Anonymous_GTID last_committed=3 sequence_number=4 rbr_only=yes original_committed_timestamp=1525099026328547 immediate_commit_timestamp=1525099026328547 transaction_length=300/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)# immediate_commit_timestamp=1525099026328547 (2018-04-30 22:37:06.328547 CST)/*!80001 SET @@session.original_commit_timestamp=1525099026328547*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 1046#180430 22:37:06 server id 1 end_log_pos 1124 CRC32 0x80f259e0 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099026/*!*/;BEGIN/*!*/;# at 1124#180430 22:37:06 server id 1 end_log_pos 1191 CRC32 0x255903ba Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 1191#180430 22:37:06 server id 1 end_log_pos 1240 CRC32 0xe76bfc79 Write_rows: table id 96 flags: STMT_END_F### INSERT INTO `Apache Flinkdb`.`tab`### SET### @1=2### @2=&apos;Bob&apos;### @3=1# at 1240#180430 22:37:06 server id 1 end_log_pos 1271 CRC32 0x83cddfef Xid = 26COMMIT/*!*/;# at 1271#180430 22:37:15 server id 1 end_log_pos 1346 CRC32 0x7095baee Anonymous_GTID last_committed=4 sequence_number=5 rbr_only=yes original_committed_timestamp=1525099035811597 immediate_commit_timestamp=1525099035811597 transaction_length=326/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)# immediate_commit_timestamp=1525099035811597 (2018-04-30 22:37:15.811597 CST)/*!80001 SET @@session.original_commit_timestamp=1525099035811597*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 1346#180430 22:37:15 server id 1 end_log_pos 1433 CRC32 0x70ef97e2 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099035/*!*/;BEGIN/*!*/;# at 1433#180430 22:37:15 server id 1 end_log_pos 1500 CRC32 0x75f1f399 Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 1500#180430 22:37:15 server id 1 end_log_pos 1566 CRC32 0x256bd4b8 Update_rows: table id 96 flags: STMT_END_F### UPDATE `Apache Flinkdb`.`tab`### WHERE### @1=1### @2=&apos;Mary&apos;### @3=1### SET### @1=1### @2=&apos;Mary&apos;### @3=2# at 1566#180430 22:37:15 server id 1 end_log_pos 1597 CRC32 0x93c86579 Xid = 27COMMIT/*!*/;# at 1597#180430 22:37:27 server id 1 end_log_pos 1672 CRC32 0xe8bd63e7 Anonymous_GTID last_committed=5 sequence_number=6 rbr_only=yes original_committed_timestamp=1525099047219517 immediate_commit_timestamp=1525099047219517 transaction_length=300/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)# immediate_commit_timestamp=1525099047219517 (2018-04-30 22:37:27.219517 CST)/*!80001 SET @@session.original_commit_timestamp=1525099047219517*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 1672#180430 22:37:27 server id 1 end_log_pos 1750 CRC32 0x5356c3c7 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099047/*!*/;BEGIN/*!*/;# at 1750#180430 22:37:27 server id 1 end_log_pos 1817 CRC32 0x37e6b1ce Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 1817#180430 22:37:27 server id 1 end_log_pos 1866 CRC32 0x6ab1bbe6 Write_rows: table id 96 flags: STMT_END_F### INSERT INTO `Apache Flinkdb`.`tab`### SET### @1=3### @2=&apos;Llz&apos;### @3=1# at 1866#180430 22:37:27 server id 1 end_log_pos 1897 CRC32 0x3b62b153 Xid = 28COMMIT/*!*/;# at 1897#180430 22:37:36 server id 1 end_log_pos 1972 CRC32 0x603134c1 Anonymous_GTID last_committed=6 sequence_number=7 rbr_only=yes original_committed_timestamp=1525099056866022 immediate_commit_timestamp=1525099056866022 transaction_length=324/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)# immediate_commit_timestamp=1525099056866022 (2018-04-30 22:37:36.866022 CST)/*!80001 SET @@session.original_commit_timestamp=1525099056866022*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 1972#180430 22:37:36 server id 1 end_log_pos 2059 CRC32 0xe17df4e4 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099056/*!*/;BEGIN/*!*/;# at 2059#180430 22:37:36 server id 1 end_log_pos 2126 CRC32 0x53888b05 Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 2126#180430 22:37:36 server id 1 end_log_pos 2190 CRC32 0x85f34996 Update_rows: table id 96 flags: STMT_END_F### UPDATE `Apache Flinkdb`.`tab`### WHERE### @1=2### @2=&apos;Bob&apos;### @3=1### SET### @1=2### @2=&apos;Bob&apos;### @3=2# at 2190#180430 22:37:36 server id 1 end_log_pos 2221 CRC32 0x877f1e23 Xid = 29COMMIT/*!*/;# at 2221#180430 22:37:45 server id 1 end_log_pos 2296 CRC32 0xfbc7e868 Anonymous_GTID last_committed=7 sequence_number=8 rbr_only=yes original_committed_timestamp=1525099065089940 immediate_commit_timestamp=1525099065089940 transaction_length=326/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;# original_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)# immediate_commit_timestamp=1525099065089940 (2018-04-30 22:37:45.089940 CST)/*!80001 SET @@session.original_commit_timestamp=1525099065089940*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 2296#180430 22:37:45 server id 1 end_log_pos 2383 CRC32 0x8a514364 Query thread_id=9 exec_time=0 error_code=0SET TIMESTAMP=1525099065/*!*/;BEGIN/*!*/;# at 2383#180430 22:37:45 server id 1 end_log_pos 2450 CRC32 0xdf18ca60 Table_map: `Apache Flinkdb`.`tab` mapped to number 96# at 2450#180430 22:37:45 server id 1 end_log_pos 2516 CRC32 0xd50de69f Update_rows: table id 96 flags: STMT_END_F### UPDATE `Apache Flinkdb`.`tab`### WHERE### @1=1### @2=&apos;Mary&apos;### @3=2### SET### @1=1### @2=&apos;Mary&apos;### @3=3# at 2516#180430 22:37:45 server id 1 end_log_pos 2547 CRC32 0x94f89393 Xid = 30COMMIT/*!*/;SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by MySQLbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 梳理操作和binlog的记录关系 DMLbinlog-header(timestamp)datainsert into blink_tab(user, clicks) values ('Mary', 1);1525099013(2018/4/30 22:36:53)## INSERT INTO `blinkdb`.`blink_tab`### SET### @1=1### @2='Mary'### @3=1insert into blink_tab(user, clicks) values ('Bob', 1);1525099026(2018/4/30 22:37:06)### INSERT INTO `blinkdb`.`blink_tab`### SET### @1=2### @2='Bob'### @3=1&nbsp;&nbsp;update blink_tab set clicks=2 where user='Mary';&nbsp;1525099035(2018/4/30 22:37:15)&nbsp;### UPDATE `blinkdb`.`blink_tab`### WHERE### @1=1### @2='Mary'### @3=1### SET### @1=1### @2='Mary'### @3=2&nbsp;insert into blink_tab(user, clicks) values ('Llz', 1);1525099047(2018/4/30 22:37:27)&nbsp;### INSERT INTO `blinkdb`.`blink_tab`### SET### @1=3### @2='Llz'### @3=1&nbsp;&nbsp;update blink_tab set clicks=2 where user='Bob';&nbsp;1525099056(2018/4/30 22:37:36)### UPDATE `blinkdb`.`blink_tab`### WHERE### @1=2### @2='Bob'### @3=1### SET### @1=2### @2='Bob'### @3=2&nbsp;&nbsp;update blink_tab set clicks=3 where user='Mary';1525099065(2018/4/30 22:37:45)&nbsp;### UPDATE `blinkdb`.`blink_tab`### WHERE### @1=1### @2='Mary'### @3=2### SET### @1=1### @2='Mary'### @3=3&nbsp; 简化一下binlog replay binlog会得到如下表数据（按timestamp顺序) 表与binlog的关系简单示意如下 流表对偶(duality)性前面我花费了一些时间介绍了MySQL主备复制机制和binlog的数据格式，binlog中携带时间戳，我们将所有表的操作都按时间进行记录下来形成binlog，而对binlog的event进行重放的过程就是流数据处理的过程，重放的结果恰恰又形成了一张表。也就是表的操作会形成携带时间的事件流，对流的处理又会形成一张不断变化的表，表和流具有等价性，可以互转。随着时间推移，DML操作不断进行，那么表的内容也不断变化，具体如下： 如上图所示内容，流和表具备相同的特征： 表 - Schema，Data，DML操作时间 流 - Schema，Data, Data处理时间 我们发现，虽然大多数表上面没有明确的显示出DML操作时间，但本质上数据库系统里面是有数据操作时间信息的，这个和流上数据的处理时间（processing time)/产生时间（event-time)相对应。流与表具备相同的特征，可以信息无损的相互转换，我称之为流表对偶(duality)性。 上面我们描述的表，在流上称之为动态表(Dynamic Table)，原因是在流上面任何一个事件的到来都是对表上数据的一次更新(包括插入和删除),表的内容是不断的变化的，任何一个时刻流的状态和表的快照一一对应。流与动态表(Dynamic Table)在时间维度上面具有等价性，这种等价性我们称之为流和动态表(Dynamic Table)的对偶(duality)性。 小结本篇主要介绍Apache Flink作为一个流计算平台为什么可以为用户提供SQL API。其根本原因是如果将流上的数据看做是结构化的数据，流任务的核心是将一个具有时间属性的结构化数据变成同样具有时间属性的另一个结构化数据，而表的数据变化过程binlog恰恰就是一份具有时间属性的流数据，流与表具有信息无损的相互转换的特性，这种流表对偶性也决定了Apache Flink可以采用SQL作为流任务的开发语言。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 漫谈系列 - 序]]></title>
    <url>%2F2019%2F01%2F01%2FApache-Flink-%E6%BC%AB%E8%B0%88%E7%B3%BB%E5%88%97-%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Who本人 孙金城，淘宝花名”金竹”，阿里巴巴高级技术专家，Apache Flink PMC。自2015年以来一直专注于大数据计算领域，并持续贡献于Apache Flink 社区。 WhatApache Flink 漫谈系列会分享什么呢？本系列分享的核心内容会围绕 Apache Flink的核心特征以及阿里巴巴对Apache Flink功能的丰富和性能、架构的优化进行深入剖析，从系统架构到具体每个算子的语义都会向读者进行细致分享，并且以图文和具体示例的方式讲解具体算子的实现原理。 Why闪速成为Apache顶级项目Apache Flink是时代的产物，是当前纯流式计算引擎的领头羊。最初Apache Flink的名字叫Stratosphere，是位于德国柏林的一所大学的几个博士和研究生发明的，很短的时间便于2014年3月份成为Apache Incubator project。 互联网巨头的认可Apache Flink 于2015年在德国Berlin举行了第一次Flink forward。也是在2015年阿里巴巴的 蒋晓伟 也在带领团队研发基于Apache Flink的新一代计算引擎Blink。并于2016年的Flink forward上面对Blink在Alibaba生态的应用进行了分享。此后Apache Flink在流计算领域风靡至今，Blink也在2015，2016，2017的阿里巴巴双十一狂欢节中创造了很多奇迹，其中 2017年双11创下了每秒处理4.72亿实时日志，每秒32.5万笔支付交易的佳绩。 迅速增长的用户群在阿里巴巴集团内部基于Apache Flink的新一代计算平台Blink被广泛应用，同时在阿里云官网对外开放了Blink，目前业界大量的用户对Apache Flink 抱有浓厚的学习兴趣。所以本系列专题应需而生，目的在于向广大用户和Apache Flink爱好者深入全面的分析Apache Flink的实现原理，让更多的人更好的了解Apache Flink，并能在Apache Flink中得到更大的利好。 WhenApache Flink 漫谈系列什么时候会完成呢？ 我想本系列会随着Apache Flink的不断发展而一直持续下去，我会尽力定期为大家更新分享。]]></content>
      <categories>
        <category>Apache Flink 漫谈</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
</search>
